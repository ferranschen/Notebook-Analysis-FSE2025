{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7f7a39",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b271f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.033073Z",
     "iopub.status.busy": "2023-05-10T00:51:34.032821Z",
     "iopub.status.idle": "2023-05-10T00:51:34.118274Z",
     "shell.execute_reply": "2023-05-10T00:51:34.118006Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ecb8c",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2d6118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.119894Z",
     "iopub.status.busy": "2023-05-10T00:51:34.119788Z",
     "iopub.status.idle": "2023-05-10T00:51:34.126279Z",
     "shell.execute_reply": "2023-05-10T00:51:34.126015Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace end of line character with space\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_raw\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c016c4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.127664Z",
     "iopub.status.busy": "2023-05-10T00:51:34.127583Z",
     "iopub.status.idle": "2023-05-10T00:51:34.130206Z",
     "shell.execute_reply": "2023-05-10T00:51:34.129978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e39555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.131582Z",
     "iopub.status.busy": "2023-05-10T00:51:34.131484Z",
     "iopub.status.idle": "2023-05-10T00:51:34.133147Z",
     "shell.execute_reply": "2023-05-10T00:51:34.132919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1815cdf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.134458Z",
     "iopub.status.busy": "2023-05-10T00:51:34.134384Z",
     "iopub.status.idle": "2023-05-10T00:51:34.136030Z",
     "shell.execute_reply": "2023-05-10T00:51:34.135808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace1051",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d13daa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.137362Z",
     "iopub.status.busy": "2023-05-10T00:51:34.137282Z",
     "iopub.status.idle": "2023-05-10T00:51:34.143693Z",
     "shell.execute_reply": "2023-05-10T00:51:34.143478Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c78b612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.145064Z",
     "iopub.status.busy": "2023-05-10T00:51:34.144991Z",
     "iopub.status.idle": "2023-05-10T00:51:34.151336Z",
     "shell.execute_reply": "2023-05-10T00:51:34.151111Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671c3a1",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5441f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.152718Z",
     "iopub.status.busy": "2023-05-10T00:51:34.152646Z",
     "iopub.status.idle": "2023-05-10T00:51:34.172582Z",
     "shell.execute_reply": "2023-05-10T00:51:34.172326Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80e865",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17bfda1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.173993Z",
     "iopub.status.busy": "2023-05-10T00:51:34.173910Z",
     "iopub.status.idle": "2023-05-10T00:51:34.180364Z",
     "shell.execute_reply": "2023-05-10T00:51:34.180125Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d86ec",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9c7698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.181673Z",
     "iopub.status.busy": "2023-05-10T00:51:34.181593Z",
     "iopub.status.idle": "2023-05-10T00:51:34.187685Z",
     "shell.execute_reply": "2023-05-10T00:51:34.187434Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace end of line character with space\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_raw\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d07072b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.189026Z",
     "iopub.status.busy": "2023-05-10T00:51:34.188947Z",
     "iopub.status.idle": "2023-05-10T00:51:34.191640Z",
     "shell.execute_reply": "2023-05-10T00:51:34.191429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c893e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.193034Z",
     "iopub.status.busy": "2023-05-10T00:51:34.192953Z",
     "iopub.status.idle": "2023-05-10T00:51:34.194645Z",
     "shell.execute_reply": "2023-05-10T00:51:34.194444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94cabd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.195949Z",
     "iopub.status.busy": "2023-05-10T00:51:34.195878Z",
     "iopub.status.idle": "2023-05-10T00:51:34.197388Z",
     "shell.execute_reply": "2023-05-10T00:51:34.197159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079ea50",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d349c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.198699Z",
     "iopub.status.busy": "2023-05-10T00:51:34.198629Z",
     "iopub.status.idle": "2023-05-10T00:51:34.204828Z",
     "shell.execute_reply": "2023-05-10T00:51:34.204580Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04989d23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.206143Z",
     "iopub.status.busy": "2023-05-10T00:51:34.206069Z",
     "iopub.status.idle": "2023-05-10T00:51:34.212227Z",
     "shell.execute_reply": "2023-05-10T00:51:34.211991Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b549637",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55c9d9f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.213540Z",
     "iopub.status.busy": "2023-05-10T00:51:34.213466Z",
     "iopub.status.idle": "2023-05-10T00:51:34.219629Z",
     "shell.execute_reply": "2023-05-10T00:51:34.219386Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba425b0",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a9aa834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.220914Z",
     "iopub.status.busy": "2023-05-10T00:51:34.220838Z",
     "iopub.status.idle": "2023-05-10T00:51:34.227400Z",
     "shell.execute_reply": "2023-05-10T00:51:34.227181Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26329c",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab043b4",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f26784b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.228809Z",
     "iopub.status.busy": "2023-05-10T00:51:34.228727Z",
     "iopub.status.idle": "2023-05-10T00:51:34.231271Z",
     "shell.execute_reply": "2023-05-10T00:51:34.231035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c22d26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.232563Z",
     "iopub.status.busy": "2023-05-10T00:51:34.232488Z",
     "iopub.status.idle": "2023-05-10T00:51:34.234280Z",
     "shell.execute_reply": "2023-05-10T00:51:34.234064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10778e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.235680Z",
     "iopub.status.busy": "2023-05-10T00:51:34.235599Z",
     "iopub.status.idle": "2023-05-10T00:51:34.237237Z",
     "shell.execute_reply": "2023-05-10T00:51:34.237011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8386ba3",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09b6dfb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.238479Z",
     "iopub.status.busy": "2023-05-10T00:51:34.238402Z",
     "iopub.status.idle": "2023-05-10T00:51:34.244704Z",
     "shell.execute_reply": "2023-05-10T00:51:34.244470Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d94ca353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.246039Z",
     "iopub.status.busy": "2023-05-10T00:51:34.245967Z",
     "iopub.status.idle": "2023-05-10T00:51:34.251995Z",
     "shell.execute_reply": "2023-05-10T00:51:34.251776Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361727d",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd78db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.253298Z",
     "iopub.status.busy": "2023-05-10T00:51:34.253223Z",
     "iopub.status.idle": "2023-05-10T00:51:34.259604Z",
     "shell.execute_reply": "2023-05-10T00:51:34.259347Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951dba",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8750f3c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.261137Z",
     "iopub.status.busy": "2023-05-10T00:51:34.260996Z",
     "iopub.status.idle": "2023-05-10T00:51:34.267870Z",
     "shell.execute_reply": "2023-05-10T00:51:34.267620Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71dd54",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ea8b1",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "737766b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.269394Z",
     "iopub.status.busy": "2023-05-10T00:51:34.269281Z",
     "iopub.status.idle": "2023-05-10T00:51:34.272072Z",
     "shell.execute_reply": "2023-05-10T00:51:34.271857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e28ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.273486Z",
     "iopub.status.busy": "2023-05-10T00:51:34.273390Z",
     "iopub.status.idle": "2023-05-10T00:51:34.275121Z",
     "shell.execute_reply": "2023-05-10T00:51:34.274873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c872aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.276522Z",
     "iopub.status.busy": "2023-05-10T00:51:34.276412Z",
     "iopub.status.idle": "2023-05-10T00:51:34.278044Z",
     "shell.execute_reply": "2023-05-10T00:51:34.277811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f154b6",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f44d7",
   "metadata": {},
   "source": [
    "Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40ae76c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.279503Z",
     "iopub.status.busy": "2023-05-10T00:51:34.279413Z",
     "iopub.status.idle": "2023-05-10T00:51:34.286000Z",
     "shell.execute_reply": "2023-05-10T00:51:34.285725Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6333b",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cefe5ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.287627Z",
     "iopub.status.busy": "2023-05-10T00:51:34.287515Z",
     "iopub.status.idle": "2023-05-10T00:51:34.293857Z",
     "shell.execute_reply": "2023-05-10T00:51:34.293631Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c71dbb",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c68f4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.295236Z",
     "iopub.status.busy": "2023-05-10T00:51:34.295159Z",
     "iopub.status.idle": "2023-05-10T00:51:34.301901Z",
     "shell.execute_reply": "2023-05-10T00:51:34.301672Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35e30e",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a360912",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb6d3da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.303318Z",
     "iopub.status.busy": "2023-05-10T00:51:34.303238Z",
     "iopub.status.idle": "2023-05-10T00:51:34.305944Z",
     "shell.execute_reply": "2023-05-10T00:51:34.305722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d44db433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.307304Z",
     "iopub.status.busy": "2023-05-10T00:51:34.307224Z",
     "iopub.status.idle": "2023-05-10T00:51:34.308817Z",
     "shell.execute_reply": "2023-05-10T00:51:34.308600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27e331b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.310019Z",
     "iopub.status.busy": "2023-05-10T00:51:34.309941Z",
     "iopub.status.idle": "2023-05-10T00:51:34.311426Z",
     "shell.execute_reply": "2023-05-10T00:51:34.311222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539b504",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ca6fc90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.312766Z",
     "iopub.status.busy": "2023-05-10T00:51:34.312687Z",
     "iopub.status.idle": "2023-05-10T00:51:34.318999Z",
     "shell.execute_reply": "2023-05-10T00:51:34.318763Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da334891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.320308Z",
     "iopub.status.busy": "2023-05-10T00:51:34.320232Z",
     "iopub.status.idle": "2023-05-10T00:51:34.326199Z",
     "shell.execute_reply": "2023-05-10T00:51:34.325980Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cb424",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "404a74bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.327574Z",
     "iopub.status.busy": "2023-05-10T00:51:34.327492Z",
     "iopub.status.idle": "2023-05-10T00:51:34.333622Z",
     "shell.execute_reply": "2023-05-10T00:51:34.333344Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a7890",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf704415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.334952Z",
     "iopub.status.busy": "2023-05-10T00:51:34.334869Z",
     "iopub.status.idle": "2023-05-10T00:51:34.341068Z",
     "shell.execute_reply": "2023-05-10T00:51:34.340847Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fccb9",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995c29e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98b26e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.342412Z",
     "iopub.status.busy": "2023-05-10T00:51:34.342340Z",
     "iopub.status.idle": "2023-05-10T00:51:34.345120Z",
     "shell.execute_reply": "2023-05-10T00:51:34.344904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "982a8bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.346392Z",
     "iopub.status.busy": "2023-05-10T00:51:34.346312Z",
     "iopub.status.idle": "2023-05-10T00:51:34.348008Z",
     "shell.execute_reply": "2023-05-10T00:51:34.347778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b2fe3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.349325Z",
     "iopub.status.busy": "2023-05-10T00:51:34.349245Z",
     "iopub.status.idle": "2023-05-10T00:51:34.350865Z",
     "shell.execute_reply": "2023-05-10T00:51:34.350631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10f0f6",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c16dea5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.352152Z",
     "iopub.status.busy": "2023-05-10T00:51:34.352077Z",
     "iopub.status.idle": "2023-05-10T00:51:34.358255Z",
     "shell.execute_reply": "2023-05-10T00:51:34.358003Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3ca6623",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.359637Z",
     "iopub.status.busy": "2023-05-10T00:51:34.359557Z",
     "iopub.status.idle": "2023-05-10T00:51:34.365596Z",
     "shell.execute_reply": "2023-05-10T00:51:34.365337Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b901f5",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a6b2feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.366888Z",
     "iopub.status.busy": "2023-05-10T00:51:34.366815Z",
     "iopub.status.idle": "2023-05-10T00:51:34.372912Z",
     "shell.execute_reply": "2023-05-10T00:51:34.372661Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd05095",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "451119b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.374241Z",
     "iopub.status.busy": "2023-05-10T00:51:34.374162Z",
     "iopub.status.idle": "2023-05-10T00:51:34.380813Z",
     "shell.execute_reply": "2023-05-10T00:51:34.380578Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638a1c9",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ddfcd",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3815b284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.382219Z",
     "iopub.status.busy": "2023-05-10T00:51:34.382142Z",
     "iopub.status.idle": "2023-05-10T00:51:34.384818Z",
     "shell.execute_reply": "2023-05-10T00:51:34.384606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b282fb2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.386121Z",
     "iopub.status.busy": "2023-05-10T00:51:34.386045Z",
     "iopub.status.idle": "2023-05-10T00:51:34.387738Z",
     "shell.execute_reply": "2023-05-10T00:51:34.387538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88e5ab32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.389016Z",
     "iopub.status.busy": "2023-05-10T00:51:34.388943Z",
     "iopub.status.idle": "2023-05-10T00:51:34.390611Z",
     "shell.execute_reply": "2023-05-10T00:51:34.390393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b5cad",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f274667f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.391926Z",
     "iopub.status.busy": "2023-05-10T00:51:34.391855Z",
     "iopub.status.idle": "2023-05-10T00:51:34.398023Z",
     "shell.execute_reply": "2023-05-10T00:51:34.397796Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a866481f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.399312Z",
     "iopub.status.busy": "2023-05-10T00:51:34.399237Z",
     "iopub.status.idle": "2023-05-10T00:51:34.405385Z",
     "shell.execute_reply": "2023-05-10T00:51:34.405117Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2f291",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6affb26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.407090Z",
     "iopub.status.busy": "2023-05-10T00:51:34.406952Z",
     "iopub.status.idle": "2023-05-10T00:51:34.413385Z",
     "shell.execute_reply": "2023-05-10T00:51:34.413094Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.imshow(wc)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "# plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c877e",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5615dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.414980Z",
     "iopub.status.busy": "2023-05-10T00:51:34.414860Z",
     "iopub.status.idle": "2023-05-10T00:51:34.421255Z",
     "shell.execute_reply": "2023-05-10T00:51:34.421005Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b2e49",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb30feb",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc0271b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.422584Z",
     "iopub.status.busy": "2023-05-10T00:51:34.422512Z",
     "iopub.status.idle": "2023-05-10T00:51:34.425177Z",
     "shell.execute_reply": "2023-05-10T00:51:34.424967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc766ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.426562Z",
     "iopub.status.busy": "2023-05-10T00:51:34.426489Z",
     "iopub.status.idle": "2023-05-10T00:51:34.428168Z",
     "shell.execute_reply": "2023-05-10T00:51:34.427937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4bfbe2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.429386Z",
     "iopub.status.busy": "2023-05-10T00:51:34.429310Z",
     "iopub.status.idle": "2023-05-10T00:51:34.431036Z",
     "shell.execute_reply": "2023-05-10T00:51:34.430744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e48a5",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d04119a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.432366Z",
     "iopub.status.busy": "2023-05-10T00:51:34.432291Z",
     "iopub.status.idle": "2023-05-10T00:51:34.438551Z",
     "shell.execute_reply": "2023-05-10T00:51:34.438321Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3859c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.439872Z",
     "iopub.status.busy": "2023-05-10T00:51:34.439791Z",
     "iopub.status.idle": "2023-05-10T00:51:34.446192Z",
     "shell.execute_reply": "2023-05-10T00:51:34.445963Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a379a80",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ac2a6",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9f086",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e25d9ce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.447882Z",
     "iopub.status.busy": "2023-05-10T00:51:34.447751Z",
     "iopub.status.idle": "2023-05-10T00:51:34.454569Z",
     "shell.execute_reply": "2023-05-10T00:51:34.454329Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e83e3",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8370a",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b9f6c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.455939Z",
     "iopub.status.busy": "2023-05-10T00:51:34.455864Z",
     "iopub.status.idle": "2023-05-10T00:51:34.458610Z",
     "shell.execute_reply": "2023-05-10T00:51:34.458396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5755de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.459906Z",
     "iopub.status.busy": "2023-05-10T00:51:34.459829Z",
     "iopub.status.idle": "2023-05-10T00:51:34.461535Z",
     "shell.execute_reply": "2023-05-10T00:51:34.461306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0213571a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.462944Z",
     "iopub.status.busy": "2023-05-10T00:51:34.462862Z",
     "iopub.status.idle": "2023-05-10T00:51:34.465179Z",
     "shell.execute_reply": "2023-05-10T00:51:34.464669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dd256",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcdaed0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.466684Z",
     "iopub.status.busy": "2023-05-10T00:51:34.466583Z",
     "iopub.status.idle": "2023-05-10T00:51:34.473642Z",
     "shell.execute_reply": "2023-05-10T00:51:34.473403Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a480984b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.474972Z",
     "iopub.status.busy": "2023-05-10T00:51:34.474887Z",
     "iopub.status.idle": "2023-05-10T00:51:34.481129Z",
     "shell.execute_reply": "2023-05-10T00:51:34.480833Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7c25f",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7e921",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56e8ff60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.482609Z",
     "iopub.status.busy": "2023-05-10T00:51:34.482511Z",
     "iopub.status.idle": "2023-05-10T00:51:34.489065Z",
     "shell.execute_reply": "2023-05-10T00:51:34.488835Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwords_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "words_list.plot.bar(x=\"word\",y=\"freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25387a",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3017195c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.490412Z",
     "iopub.status.busy": "2023-05-10T00:51:34.490333Z",
     "iopub.status.idle": "2023-05-10T00:51:34.496662Z",
     "shell.execute_reply": "2023-05-10T00:51:34.496424Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11facf",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae7364",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a92ba02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.498207Z",
     "iopub.status.busy": "2023-05-10T00:51:34.498113Z",
     "iopub.status.idle": "2023-05-10T00:51:34.500841Z",
     "shell.execute_reply": "2023-05-10T00:51:34.500623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a78ef23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.502184Z",
     "iopub.status.busy": "2023-05-10T00:51:34.502093Z",
     "iopub.status.idle": "2023-05-10T00:51:34.503700Z",
     "shell.execute_reply": "2023-05-10T00:51:34.503474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79b2ba25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.504998Z",
     "iopub.status.busy": "2023-05-10T00:51:34.504918Z",
     "iopub.status.idle": "2023-05-10T00:51:34.506537Z",
     "shell.execute_reply": "2023-05-10T00:51:34.506316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ff42f",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74ff79ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.507872Z",
     "iopub.status.busy": "2023-05-10T00:51:34.507796Z",
     "iopub.status.idle": "2023-05-10T00:51:34.514203Z",
     "shell.execute_reply": "2023-05-10T00:51:34.513896Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0303b795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.515817Z",
     "iopub.status.busy": "2023-05-10T00:51:34.515707Z",
     "iopub.status.idle": "2023-05-10T00:51:34.521889Z",
     "shell.execute_reply": "2023-05-10T00:51:34.521662Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410a606",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d0509",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a21a684b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.523267Z",
     "iopub.status.busy": "2023-05-10T00:51:34.523191Z",
     "iopub.status.idle": "2023-05-10T00:51:34.529725Z",
     "shell.execute_reply": "2023-05-10T00:51:34.529477Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list)\n",
    "df.plot.bar(x=\"word\",y=\"freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4b7e9",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99584bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.531303Z",
     "iopub.status.busy": "2023-05-10T00:51:34.531175Z",
     "iopub.status.idle": "2023-05-10T00:51:34.537588Z",
     "shell.execute_reply": "2023-05-10T00:51:34.537374Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865f21a",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331752e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4297cb21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.538928Z",
     "iopub.status.busy": "2023-05-10T00:51:34.538845Z",
     "iopub.status.idle": "2023-05-10T00:51:34.541542Z",
     "shell.execute_reply": "2023-05-10T00:51:34.541324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "844994d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.542781Z",
     "iopub.status.busy": "2023-05-10T00:51:34.542703Z",
     "iopub.status.idle": "2023-05-10T00:51:34.544402Z",
     "shell.execute_reply": "2023-05-10T00:51:34.544191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd2492f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.545670Z",
     "iopub.status.busy": "2023-05-10T00:51:34.545590Z",
     "iopub.status.idle": "2023-05-10T00:51:34.547307Z",
     "shell.execute_reply": "2023-05-10T00:51:34.547009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e25613",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d4f79b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.548730Z",
     "iopub.status.busy": "2023-05-10T00:51:34.548648Z",
     "iopub.status.idle": "2023-05-10T00:51:34.555611Z",
     "shell.execute_reply": "2023-05-10T00:51:34.555368Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fcaf049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.556979Z",
     "iopub.status.busy": "2023-05-10T00:51:34.556896Z",
     "iopub.status.idle": "2023-05-10T00:51:34.563069Z",
     "shell.execute_reply": "2023-05-10T00:51:34.562821Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbb38f",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f2766",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "300ad7b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.564639Z",
     "iopub.status.busy": "2023-05-10T00:51:34.564532Z",
     "iopub.status.idle": "2023-05-10T00:51:34.570912Z",
     "shell.execute_reply": "2023-05-10T00:51:34.570670Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list)\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d95df",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a037804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.572236Z",
     "iopub.status.busy": "2023-05-10T00:51:34.572159Z",
     "iopub.status.idle": "2023-05-10T00:51:34.578320Z",
     "shell.execute_reply": "2023-05-10T00:51:34.578096Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d98043",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022a6c1",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae0ef154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.579647Z",
     "iopub.status.busy": "2023-05-10T00:51:34.579570Z",
     "iopub.status.idle": "2023-05-10T00:51:34.582403Z",
     "shell.execute_reply": "2023-05-10T00:51:34.582122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a7ff5ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.583788Z",
     "iopub.status.busy": "2023-05-10T00:51:34.583708Z",
     "iopub.status.idle": "2023-05-10T00:51:34.585419Z",
     "shell.execute_reply": "2023-05-10T00:51:34.585204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80d7402f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.586762Z",
     "iopub.status.busy": "2023-05-10T00:51:34.586687Z",
     "iopub.status.idle": "2023-05-10T00:51:34.588311Z",
     "shell.execute_reply": "2023-05-10T00:51:34.588080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b47d6",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35249aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.589596Z",
     "iopub.status.busy": "2023-05-10T00:51:34.589521Z",
     "iopub.status.idle": "2023-05-10T00:51:34.595770Z",
     "shell.execute_reply": "2023-05-10T00:51:34.595544Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "acb27a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.597194Z",
     "iopub.status.busy": "2023-05-10T00:51:34.597112Z",
     "iopub.status.idle": "2023-05-10T00:51:34.603423Z",
     "shell.execute_reply": "2023-05-10T00:51:34.603196Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8658e3",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf371335",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e28320d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.604777Z",
     "iopub.status.busy": "2023-05-10T00:51:34.604696Z",
     "iopub.status.idle": "2023-05-10T00:51:34.611026Z",
     "shell.execute_reply": "2023-05-10T00:51:34.610801Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39mwords)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=words)\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbed5e6",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac107006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.612378Z",
     "iopub.status.busy": "2023-05-10T00:51:34.612299Z",
     "iopub.status.idle": "2023-05-10T00:51:34.618756Z",
     "shell.execute_reply": "2023-05-10T00:51:34.618510Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce1306",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7be61",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab31d10d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.620195Z",
     "iopub.status.busy": "2023-05-10T00:51:34.620118Z",
     "iopub.status.idle": "2023-05-10T00:51:34.622705Z",
     "shell.execute_reply": "2023-05-10T00:51:34.622480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d39219e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.623996Z",
     "iopub.status.busy": "2023-05-10T00:51:34.623926Z",
     "iopub.status.idle": "2023-05-10T00:51:34.625672Z",
     "shell.execute_reply": "2023-05-10T00:51:34.625443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9a3c311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.626903Z",
     "iopub.status.busy": "2023-05-10T00:51:34.626829Z",
     "iopub.status.idle": "2023-05-10T00:51:34.628309Z",
     "shell.execute_reply": "2023-05-10T00:51:34.628068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39b426",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5df73b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.629754Z",
     "iopub.status.busy": "2023-05-10T00:51:34.629674Z",
     "iopub.status.idle": "2023-05-10T00:51:34.636722Z",
     "shell.execute_reply": "2023-05-10T00:51:34.636488Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "542648de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.638093Z",
     "iopub.status.busy": "2023-05-10T00:51:34.638011Z",
     "iopub.status.idle": "2023-05-10T00:51:34.644085Z",
     "shell.execute_reply": "2023-05-10T00:51:34.643876Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecb075",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad3b9e",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "375a824c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.645401Z",
     "iopub.status.busy": "2023-05-10T00:51:34.645323Z",
     "iopub.status.idle": "2023-05-10T00:51:34.651885Z",
     "shell.execute_reply": "2023-05-10T00:51:34.651650Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d7f62",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12b999fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.653377Z",
     "iopub.status.busy": "2023-05-10T00:51:34.653283Z",
     "iopub.status.idle": "2023-05-10T00:51:34.659598Z",
     "shell.execute_reply": "2023-05-10T00:51:34.659339Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ae9a2",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10ae8c",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a069bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.660963Z",
     "iopub.status.busy": "2023-05-10T00:51:34.660888Z",
     "iopub.status.idle": "2023-05-10T00:51:34.663559Z",
     "shell.execute_reply": "2023-05-10T00:51:34.663326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87af042c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.665023Z",
     "iopub.status.busy": "2023-05-10T00:51:34.664932Z",
     "iopub.status.idle": "2023-05-10T00:51:34.666666Z",
     "shell.execute_reply": "2023-05-10T00:51:34.666453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f855ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.667982Z",
     "iopub.status.busy": "2023-05-10T00:51:34.667905Z",
     "iopub.status.idle": "2023-05-10T00:51:34.669549Z",
     "shell.execute_reply": "2023-05-10T00:51:34.669334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff46ed7",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "46d92442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.670775Z",
     "iopub.status.busy": "2023-05-10T00:51:34.670703Z",
     "iopub.status.idle": "2023-05-10T00:51:34.676984Z",
     "shell.execute_reply": "2023-05-10T00:51:34.676728Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0759932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.678320Z",
     "iopub.status.busy": "2023-05-10T00:51:34.678243Z",
     "iopub.status.idle": "2023-05-10T00:51:34.684437Z",
     "shell.execute_reply": "2023-05-10T00:51:34.684209Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73fc2f",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359fde0",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "210b168e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.685783Z",
     "iopub.status.busy": "2023-05-10T00:51:34.685703Z",
     "iopub.status.idle": "2023-05-10T00:51:34.692345Z",
     "shell.execute_reply": "2023-05-10T00:51:34.692111Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82a692",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cda95d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.693639Z",
     "iopub.status.busy": "2023-05-10T00:51:34.693564Z",
     "iopub.status.idle": "2023-05-10T00:51:34.699734Z",
     "shell.execute_reply": "2023-05-10T00:51:34.699492Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f9ab5",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80aea7",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ea58ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.701055Z",
     "iopub.status.busy": "2023-05-10T00:51:34.700982Z",
     "iopub.status.idle": "2023-05-10T00:51:34.703719Z",
     "shell.execute_reply": "2023-05-10T00:51:34.703503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee2fbcc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.705044Z",
     "iopub.status.busy": "2023-05-10T00:51:34.704964Z",
     "iopub.status.idle": "2023-05-10T00:51:34.706649Z",
     "shell.execute_reply": "2023-05-10T00:51:34.706422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19e1bc64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.707878Z",
     "iopub.status.busy": "2023-05-10T00:51:34.707804Z",
     "iopub.status.idle": "2023-05-10T00:51:34.709449Z",
     "shell.execute_reply": "2023-05-10T00:51:34.709229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e14bf1",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "57de8957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.710753Z",
     "iopub.status.busy": "2023-05-10T00:51:34.710675Z",
     "iopub.status.idle": "2023-05-10T00:51:34.717753Z",
     "shell.execute_reply": "2023-05-10T00:51:34.717529Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0e7057f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.719099Z",
     "iopub.status.busy": "2023-05-10T00:51:34.719019Z",
     "iopub.status.idle": "2023-05-10T00:51:34.725078Z",
     "shell.execute_reply": "2023-05-10T00:51:34.724858Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4e6a8",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f472fd3",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ece3facd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.726371Z",
     "iopub.status.busy": "2023-05-10T00:51:34.726298Z",
     "iopub.status.idle": "2023-05-10T00:51:34.732996Z",
     "shell.execute_reply": "2023-05-10T00:51:34.732740Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380a550",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "868f757c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.734319Z",
     "iopub.status.busy": "2023-05-10T00:51:34.734241Z",
     "iopub.status.idle": "2023-05-10T00:51:34.740690Z",
     "shell.execute_reply": "2023-05-10T00:51:34.740460Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4b27d",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d308dc3",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7afccb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.742003Z",
     "iopub.status.busy": "2023-05-10T00:51:34.741926Z",
     "iopub.status.idle": "2023-05-10T00:51:34.744656Z",
     "shell.execute_reply": "2023-05-10T00:51:34.744429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "74437198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.745982Z",
     "iopub.status.busy": "2023-05-10T00:51:34.745909Z",
     "iopub.status.idle": "2023-05-10T00:51:34.747625Z",
     "shell.execute_reply": "2023-05-10T00:51:34.747366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3d5fa670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.749043Z",
     "iopub.status.busy": "2023-05-10T00:51:34.748970Z",
     "iopub.status.idle": "2023-05-10T00:51:34.750504Z",
     "shell.execute_reply": "2023-05-10T00:51:34.750257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40180b0f",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ced7536d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.751830Z",
     "iopub.status.busy": "2023-05-10T00:51:34.751746Z",
     "iopub.status.idle": "2023-05-10T00:51:34.758623Z",
     "shell.execute_reply": "2023-05-10T00:51:34.758413Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(words_list)\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9a7cdc0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.759949Z",
     "iopub.status.busy": "2023-05-10T00:51:34.759875Z",
     "iopub.status.idle": "2023-05-10T00:51:34.766265Z",
     "shell.execute_reply": "2023-05-10T00:51:34.766024Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f7f4cf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.767642Z",
     "iopub.status.busy": "2023-05-10T00:51:34.767540Z",
     "iopub.status.idle": "2023-05-10T00:51:34.773667Z",
     "shell.execute_reply": "2023-05-10T00:51:34.773399Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba359b",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ea78e",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "abd74bba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.775060Z",
     "iopub.status.busy": "2023-05-10T00:51:34.774982Z",
     "iopub.status.idle": "2023-05-10T00:51:34.776955Z",
     "shell.execute_reply": "2023-05-10T00:51:34.776736Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1507832862.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [113], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.DataFrame.from(words_list,columns=['words'])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21384b",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5ce584ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.778209Z",
     "iopub.status.busy": "2023-05-10T00:51:34.778140Z",
     "iopub.status.idle": "2023-05-10T00:51:34.784589Z",
     "shell.execute_reply": "2023-05-10T00:51:34.784355Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6049eeb",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f66022",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e31d07c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.785990Z",
     "iopub.status.busy": "2023-05-10T00:51:34.785906Z",
     "iopub.status.idle": "2023-05-10T00:51:34.788635Z",
     "shell.execute_reply": "2023-05-10T00:51:34.788425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3b67e97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.795403Z",
     "iopub.status.busy": "2023-05-10T00:51:34.795246Z",
     "iopub.status.idle": "2023-05-10T00:51:34.797835Z",
     "shell.execute_reply": "2023-05-10T00:51:34.797210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a058f6ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.799768Z",
     "iopub.status.busy": "2023-05-10T00:51:34.799635Z",
     "iopub.status.idle": "2023-05-10T00:51:34.801460Z",
     "shell.execute_reply": "2023-05-10T00:51:34.801181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f662ac",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b3b59118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.803172Z",
     "iopub.status.busy": "2023-05-10T00:51:34.803033Z",
     "iopub.status.idle": "2023-05-10T00:51:34.809949Z",
     "shell.execute_reply": "2023-05-10T00:51:34.809725Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(words_list)\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db84fd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.811280Z",
     "iopub.status.busy": "2023-05-10T00:51:34.811201Z",
     "iopub.status.idle": "2023-05-10T00:51:34.818371Z",
     "shell.execute_reply": "2023-05-10T00:51:34.818110Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d85f0832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.819770Z",
     "iopub.status.busy": "2023-05-10T00:51:34.819670Z",
     "iopub.status.idle": "2023-05-10T00:51:34.825797Z",
     "shell.execute_reply": "2023-05-10T00:51:34.825567Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [120], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d44b77",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32790d",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6f30b822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.827099Z",
     "iopub.status.busy": "2023-05-10T00:51:34.827022Z",
     "iopub.status.idle": "2023-05-10T00:51:34.833520Z",
     "shell.execute_reply": "2023-05-10T00:51:34.833293Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bc2f5",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "75a36584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:34.834925Z",
     "iopub.status.busy": "2023-05-10T00:51:34.834841Z",
     "iopub.status.idle": "2023-05-10T00:51:35.418780Z",
     "shell.execute_reply": "2023-05-10T00:51:35.418434Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [122], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533e0c6",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82c40f",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "845d5adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.420436Z",
     "iopub.status.busy": "2023-05-10T00:51:35.420292Z",
     "iopub.status.idle": "2023-05-10T00:51:35.423869Z",
     "shell.execute_reply": "2023-05-10T00:51:35.423485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "85686cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.425623Z",
     "iopub.status.busy": "2023-05-10T00:51:35.425500Z",
     "iopub.status.idle": "2023-05-10T00:51:35.427370Z",
     "shell.execute_reply": "2023-05-10T00:51:35.427078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "22278047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.428741Z",
     "iopub.status.busy": "2023-05-10T00:51:35.428628Z",
     "iopub.status.idle": "2023-05-10T00:51:35.430419Z",
     "shell.execute_reply": "2023-05-10T00:51:35.430055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef8b1d",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "093cb42a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.432800Z",
     "iopub.status.busy": "2023-05-10T00:51:35.432646Z",
     "iopub.status.idle": "2023-05-10T00:51:35.501727Z",
     "shell.execute_reply": "2023-05-10T00:51:35.501412Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d8814824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.503301Z",
     "iopub.status.busy": "2023-05-10T00:51:35.503188Z",
     "iopub.status.idle": "2023-05-10T00:51:35.509824Z",
     "shell.execute_reply": "2023-05-10T00:51:35.509567Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d557c156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.511366Z",
     "iopub.status.busy": "2023-05-10T00:51:35.511259Z",
     "iopub.status.idle": "2023-05-10T00:51:35.518330Z",
     "shell.execute_reply": "2023-05-10T00:51:35.518074Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab3846",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d974b5a",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c7768e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.519988Z",
     "iopub.status.busy": "2023-05-10T00:51:35.519871Z",
     "iopub.status.idle": "2023-05-10T00:51:35.526187Z",
     "shell.execute_reply": "2023-05-10T00:51:35.525939Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [129], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4c7cc",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c7497037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.527499Z",
     "iopub.status.busy": "2023-05-10T00:51:35.527419Z",
     "iopub.status.idle": "2023-05-10T00:51:35.534358Z",
     "shell.execute_reply": "2023-05-10T00:51:35.534078Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [130], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c08ae",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06927d64",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "722e5341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.535927Z",
     "iopub.status.busy": "2023-05-10T00:51:35.535816Z",
     "iopub.status.idle": "2023-05-10T00:51:35.538504Z",
     "shell.execute_reply": "2023-05-10T00:51:35.538285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6faa5597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.539867Z",
     "iopub.status.busy": "2023-05-10T00:51:35.539768Z",
     "iopub.status.idle": "2023-05-10T00:51:35.541408Z",
     "shell.execute_reply": "2023-05-10T00:51:35.541169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "70c55649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.542819Z",
     "iopub.status.busy": "2023-05-10T00:51:35.542730Z",
     "iopub.status.idle": "2023-05-10T00:51:35.544435Z",
     "shell.execute_reply": "2023-05-10T00:51:35.544210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bcc6f7",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b7a6ceae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.545798Z",
     "iopub.status.busy": "2023-05-10T00:51:35.545724Z",
     "iopub.status.idle": "2023-05-10T00:51:35.572737Z",
     "shell.execute_reply": "2023-05-10T00:51:35.572424Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "12dce2dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.574431Z",
     "iopub.status.busy": "2023-05-10T00:51:35.574313Z",
     "iopub.status.idle": "2023-05-10T00:51:35.580669Z",
     "shell.execute_reply": "2023-05-10T00:51:35.580396Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [135], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "78a84802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.583040Z",
     "iopub.status.busy": "2023-05-10T00:51:35.582897Z",
     "iopub.status.idle": "2023-05-10T00:51:35.589351Z",
     "shell.execute_reply": "2023-05-10T00:51:35.589115Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d47e52",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2da713",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "03fc6c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.590786Z",
     "iopub.status.busy": "2023-05-10T00:51:35.590701Z",
     "iopub.status.idle": "2023-05-10T00:51:35.596989Z",
     "shell.execute_reply": "2023-05-10T00:51:35.596745Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [137], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8efbd",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e6d44078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.598388Z",
     "iopub.status.busy": "2023-05-10T00:51:35.598305Z",
     "iopub.status.idle": "2023-05-10T00:51:35.741092Z",
     "shell.execute_reply": "2023-05-10T00:51:35.740814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [138], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43mDownloaderShell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md) Download\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml) List\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq) Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloader> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"Forward raw_input to frontends\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03mRaises\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m------\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mStdinNotImplementedError if active frontend doesn't support stdin.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb6ace",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120c26e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6c0a3b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.742631Z",
     "iopub.status.busy": "2023-05-10T00:51:35.742532Z",
     "iopub.status.idle": "2023-05-10T00:51:35.745193Z",
     "shell.execute_reply": "2023-05-10T00:51:35.744962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1a29b734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.746638Z",
     "iopub.status.busy": "2023-05-10T00:51:35.746532Z",
     "iopub.status.idle": "2023-05-10T00:51:35.748213Z",
     "shell.execute_reply": "2023-05-10T00:51:35.747965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2a52061e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.749662Z",
     "iopub.status.busy": "2023-05-10T00:51:35.749563Z",
     "iopub.status.idle": "2023-05-10T00:51:35.751166Z",
     "shell.execute_reply": "2023-05-10T00:51:35.750935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b037e7",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4b298f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.752645Z",
     "iopub.status.busy": "2023-05-10T00:51:35.752566Z",
     "iopub.status.idle": "2023-05-10T00:51:35.779539Z",
     "shell.execute_reply": "2023-05-10T00:51:35.779258Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "493db781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.781111Z",
     "iopub.status.busy": "2023-05-10T00:51:35.781004Z",
     "iopub.status.idle": "2023-05-10T00:51:35.795143Z",
     "shell.execute_reply": "2023-05-10T00:51:35.793279Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [143], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2f51b11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.803196Z",
     "iopub.status.busy": "2023-05-10T00:51:35.803024Z",
     "iopub.status.idle": "2023-05-10T00:51:35.811782Z",
     "shell.execute_reply": "2023-05-10T00:51:35.811267Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [144], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2c34c",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341664f6",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0c28ed8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.813488Z",
     "iopub.status.busy": "2023-05-10T00:51:35.813398Z",
     "iopub.status.idle": "2023-05-10T00:51:35.821326Z",
     "shell.execute_reply": "2023-05-10T00:51:35.821029Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [145], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275ecff",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "da71b4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.822813Z",
     "iopub.status.busy": "2023-05-10T00:51:35.822695Z",
     "iopub.status.idle": "2023-05-10T00:51:35.844486Z",
     "shell.execute_reply": "2023-05-10T00:51:35.844199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [146], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43mDownloaderShell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md) Download\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml) List\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq) Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloader> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"Forward raw_input to frontends\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03mRaises\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m------\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mStdinNotImplementedError if active frontend doesn't support stdin.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097ca84",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a426757",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b6236b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.846089Z",
     "iopub.status.busy": "2023-05-10T00:51:35.845973Z",
     "iopub.status.idle": "2023-05-10T00:51:35.849236Z",
     "shell.execute_reply": "2023-05-10T00:51:35.848802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "774419dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.851087Z",
     "iopub.status.busy": "2023-05-10T00:51:35.850976Z",
     "iopub.status.idle": "2023-05-10T00:51:35.852802Z",
     "shell.execute_reply": "2023-05-10T00:51:35.852566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "85707828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.854247Z",
     "iopub.status.busy": "2023-05-10T00:51:35.854168Z",
     "iopub.status.idle": "2023-05-10T00:51:35.855809Z",
     "shell.execute_reply": "2023-05-10T00:51:35.855511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c82e4",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d6775ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.857128Z",
     "iopub.status.busy": "2023-05-10T00:51:35.857044Z",
     "iopub.status.idle": "2023-05-10T00:51:35.919967Z",
     "shell.execute_reply": "2023-05-10T00:51:35.919663Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "49cedbd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.921570Z",
     "iopub.status.busy": "2023-05-10T00:51:35.921466Z",
     "iopub.status.idle": "2023-05-10T00:51:35.927983Z",
     "shell.execute_reply": "2023-05-10T00:51:35.927704Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [151], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "119dbd84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.929618Z",
     "iopub.status.busy": "2023-05-10T00:51:35.929502Z",
     "iopub.status.idle": "2023-05-10T00:51:35.936881Z",
     "shell.execute_reply": "2023-05-10T00:51:35.936593Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [152], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd530e",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f37f62",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "27326bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.938318Z",
     "iopub.status.busy": "2023-05-10T00:51:35.938207Z",
     "iopub.status.idle": "2023-05-10T00:51:35.944925Z",
     "shell.execute_reply": "2023-05-10T00:51:35.944614Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [153], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee210dd",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "32c4fd4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.946412Z",
     "iopub.status.busy": "2023-05-10T00:51:35.946307Z",
     "iopub.status.idle": "2023-05-10T00:51:35.954196Z",
     "shell.execute_reply": "2023-05-10T00:51:35.953896Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af96ce0",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f40a0",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ea9c3074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.955665Z",
     "iopub.status.busy": "2023-05-10T00:51:35.955579Z",
     "iopub.status.idle": "2023-05-10T00:51:35.958370Z",
     "shell.execute_reply": "2023-05-10T00:51:35.958091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6d66ffbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.959759Z",
     "iopub.status.busy": "2023-05-10T00:51:35.959650Z",
     "iopub.status.idle": "2023-05-10T00:51:35.961390Z",
     "shell.execute_reply": "2023-05-10T00:51:35.961131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "510c47ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.962742Z",
     "iopub.status.busy": "2023-05-10T00:51:35.962644Z",
     "iopub.status.idle": "2023-05-10T00:51:35.964276Z",
     "shell.execute_reply": "2023-05-10T00:51:35.964056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c2561",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cbda3c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.966649Z",
     "iopub.status.busy": "2023-05-10T00:51:35.966370Z",
     "iopub.status.idle": "2023-05-10T00:51:35.993604Z",
     "shell.execute_reply": "2023-05-10T00:51:35.993308Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "be955524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:35.995215Z",
     "iopub.status.busy": "2023-05-10T00:51:35.995094Z",
     "iopub.status.idle": "2023-05-10T00:51:36.002410Z",
     "shell.execute_reply": "2023-05-10T00:51:36.002119Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [159], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9c936c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.003902Z",
     "iopub.status.busy": "2023-05-10T00:51:36.003798Z",
     "iopub.status.idle": "2023-05-10T00:51:36.010139Z",
     "shell.execute_reply": "2023-05-10T00:51:36.009894Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [160], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288afb0",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50971879",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "03100f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.011551Z",
     "iopub.status.busy": "2023-05-10T00:51:36.011469Z",
     "iopub.status.idle": "2023-05-10T00:51:36.019112Z",
     "shell.execute_reply": "2023-05-10T00:51:36.018832Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [161], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad79b63",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "832275c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.020605Z",
     "iopub.status.busy": "2023-05-10T00:51:36.020491Z",
     "iopub.status.idle": "2023-05-10T00:51:36.027336Z",
     "shell.execute_reply": "2023-05-10T00:51:36.027081Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [162], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5df1d",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4e840",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6d49dde5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.028726Z",
     "iopub.status.busy": "2023-05-10T00:51:36.028633Z",
     "iopub.status.idle": "2023-05-10T00:51:36.031532Z",
     "shell.execute_reply": "2023-05-10T00:51:36.031261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6c147008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.032944Z",
     "iopub.status.busy": "2023-05-10T00:51:36.032867Z",
     "iopub.status.idle": "2023-05-10T00:51:36.034720Z",
     "shell.execute_reply": "2023-05-10T00:51:36.034392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "78dc39c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.036199Z",
     "iopub.status.busy": "2023-05-10T00:51:36.036107Z",
     "iopub.status.idle": "2023-05-10T00:51:36.037831Z",
     "shell.execute_reply": "2023-05-10T00:51:36.037587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7027d2a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.039182Z",
     "iopub.status.busy": "2023-05-10T00:51:36.039096Z",
     "iopub.status.idle": "2023-05-10T00:51:36.066226Z",
     "shell.execute_reply": "2023-05-10T00:51:36.065929Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [166], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eb80fb89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.068105Z",
     "iopub.status.busy": "2023-05-10T00:51:36.067914Z",
     "iopub.status.idle": "2023-05-10T00:51:36.074654Z",
     "shell.execute_reply": "2023-05-10T00:51:36.074382Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [167], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "867c8bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.076050Z",
     "iopub.status.busy": "2023-05-10T00:51:36.075965Z",
     "iopub.status.idle": "2023-05-10T00:51:36.083126Z",
     "shell.execute_reply": "2023-05-10T00:51:36.082783Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [168], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492f6bd",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e864bc5",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cefda71d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.084617Z",
     "iopub.status.busy": "2023-05-10T00:51:36.084513Z",
     "iopub.status.idle": "2023-05-10T00:51:36.091567Z",
     "shell.execute_reply": "2023-05-10T00:51:36.091247Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [169], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab23ce",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "97b99380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.093225Z",
     "iopub.status.busy": "2023-05-10T00:51:36.093116Z",
     "iopub.status.idle": "2023-05-10T00:51:36.100546Z",
     "shell.execute_reply": "2023-05-10T00:51:36.100185Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [170], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0d801",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb72e1",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "04b6c226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.102385Z",
     "iopub.status.busy": "2023-05-10T00:51:36.102262Z",
     "iopub.status.idle": "2023-05-10T00:51:36.105155Z",
     "shell.execute_reply": "2023-05-10T00:51:36.104884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bd1752d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.106591Z",
     "iopub.status.busy": "2023-05-10T00:51:36.106486Z",
     "iopub.status.idle": "2023-05-10T00:51:36.108138Z",
     "shell.execute_reply": "2023-05-10T00:51:36.107917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4aa1e361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.109446Z",
     "iopub.status.busy": "2023-05-10T00:51:36.109366Z",
     "iopub.status.idle": "2023-05-10T00:51:36.110964Z",
     "shell.execute_reply": "2023-05-10T00:51:36.110726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1d25ff93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.112282Z",
     "iopub.status.busy": "2023-05-10T00:51:36.112199Z",
     "iopub.status.idle": "2023-05-10T00:51:36.141115Z",
     "shell.execute_reply": "2023-05-10T00:51:36.140798Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [174], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=8\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "04d524c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.142677Z",
     "iopub.status.busy": "2023-05-10T00:51:36.142568Z",
     "iopub.status.idle": "2023-05-10T00:51:36.149816Z",
     "shell.execute_reply": "2023-05-10T00:51:36.149369Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [175], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "287b22e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.151535Z",
     "iopub.status.busy": "2023-05-10T00:51:36.151415Z",
     "iopub.status.idle": "2023-05-10T00:51:36.158477Z",
     "shell.execute_reply": "2023-05-10T00:51:36.158224Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [176], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5e64d",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d26ee",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "39f18c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.159977Z",
     "iopub.status.busy": "2023-05-10T00:51:36.159876Z",
     "iopub.status.idle": "2023-05-10T00:51:36.167368Z",
     "shell.execute_reply": "2023-05-10T00:51:36.167093Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [177], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4967e",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "42d7189d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.168876Z",
     "iopub.status.busy": "2023-05-10T00:51:36.168773Z",
     "iopub.status.idle": "2023-05-10T00:51:36.175585Z",
     "shell.execute_reply": "2023-05-10T00:51:36.175324Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [178], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526ac1a",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb6a9b",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8baf781f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.177005Z",
     "iopub.status.busy": "2023-05-10T00:51:36.176920Z",
     "iopub.status.idle": "2023-05-10T00:51:36.179690Z",
     "shell.execute_reply": "2023-05-10T00:51:36.179427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a3c7647c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.181587Z",
     "iopub.status.busy": "2023-05-10T00:51:36.181408Z",
     "iopub.status.idle": "2023-05-10T00:51:36.183462Z",
     "shell.execute_reply": "2023-05-10T00:51:36.183177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4e487679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.185021Z",
     "iopub.status.busy": "2023-05-10T00:51:36.184912Z",
     "iopub.status.idle": "2023-05-10T00:51:36.186540Z",
     "shell.execute_reply": "2023-05-10T00:51:36.186287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ff8aad34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.187973Z",
     "iopub.status.busy": "2023-05-10T00:51:36.187893Z",
     "iopub.status.idle": "2023-05-10T00:51:36.215324Z",
     "shell.execute_reply": "2023-05-10T00:51:36.214936Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [182], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d0872b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.216969Z",
     "iopub.status.busy": "2023-05-10T00:51:36.216856Z",
     "iopub.status.idle": "2023-05-10T00:51:36.223332Z",
     "shell.execute_reply": "2023-05-10T00:51:36.223077Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [183], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f7f7e9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.224712Z",
     "iopub.status.busy": "2023-05-10T00:51:36.224627Z",
     "iopub.status.idle": "2023-05-10T00:51:36.232518Z",
     "shell.execute_reply": "2023-05-10T00:51:36.232246Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [184], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908ebcd",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49110c65",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2c0da8be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.234033Z",
     "iopub.status.busy": "2023-05-10T00:51:36.233930Z",
     "iopub.status.idle": "2023-05-10T00:51:36.240878Z",
     "shell.execute_reply": "2023-05-10T00:51:36.240602Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [185], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837542e1",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e6c9085b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.242337Z",
     "iopub.status.busy": "2023-05-10T00:51:36.242230Z",
     "iopub.status.idle": "2023-05-10T00:51:36.249597Z",
     "shell.execute_reply": "2023-05-10T00:51:36.249248Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [186], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cf712",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a7d7c",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d9e70e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.251158Z",
     "iopub.status.busy": "2023-05-10T00:51:36.251041Z",
     "iopub.status.idle": "2023-05-10T00:51:36.253780Z",
     "shell.execute_reply": "2023-05-10T00:51:36.253505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "021cfc83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.255232Z",
     "iopub.status.busy": "2023-05-10T00:51:36.255134Z",
     "iopub.status.idle": "2023-05-10T00:51:36.256784Z",
     "shell.execute_reply": "2023-05-10T00:51:36.256552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1d08b534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.258137Z",
     "iopub.status.busy": "2023-05-10T00:51:36.258058Z",
     "iopub.status.idle": "2023-05-10T00:51:36.259560Z",
     "shell.execute_reply": "2023-05-10T00:51:36.259321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4919f6a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.260868Z",
     "iopub.status.busy": "2023-05-10T00:51:36.260789Z",
     "iopub.status.idle": "2023-05-10T00:51:36.286964Z",
     "shell.execute_reply": "2023-05-10T00:51:36.286676Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [190], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c839f32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.288461Z",
     "iopub.status.busy": "2023-05-10T00:51:36.288375Z",
     "iopub.status.idle": "2023-05-10T00:51:36.294817Z",
     "shell.execute_reply": "2023-05-10T00:51:36.294564Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [191], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "32746785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.296185Z",
     "iopub.status.busy": "2023-05-10T00:51:36.296108Z",
     "iopub.status.idle": "2023-05-10T00:51:36.302231Z",
     "shell.execute_reply": "2023-05-10T00:51:36.301963Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [192], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3dcf28",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07804f7",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ed3d02f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.303534Z",
     "iopub.status.busy": "2023-05-10T00:51:36.303450Z",
     "iopub.status.idle": "2023-05-10T00:51:36.310537Z",
     "shell.execute_reply": "2023-05-10T00:51:36.310304Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [193], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe8935",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6e91f427",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.311886Z",
     "iopub.status.busy": "2023-05-10T00:51:36.311807Z",
     "iopub.status.idle": "2023-05-10T00:51:36.318196Z",
     "shell.execute_reply": "2023-05-10T00:51:36.317952Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [194], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4fb256",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186be5ba",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a6207a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.319511Z",
     "iopub.status.busy": "2023-05-10T00:51:36.319435Z",
     "iopub.status.idle": "2023-05-10T00:51:36.322160Z",
     "shell.execute_reply": "2023-05-10T00:51:36.321936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "905e4605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.323473Z",
     "iopub.status.busy": "2023-05-10T00:51:36.323394Z",
     "iopub.status.idle": "2023-05-10T00:51:36.325076Z",
     "shell.execute_reply": "2023-05-10T00:51:36.324846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "992bda33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.326374Z",
     "iopub.status.busy": "2023-05-10T00:51:36.326291Z",
     "iopub.status.idle": "2023-05-10T00:51:36.327853Z",
     "shell.execute_reply": "2023-05-10T00:51:36.327606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "91fd4fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.329168Z",
     "iopub.status.busy": "2023-05-10T00:51:36.329090Z",
     "iopub.status.idle": "2023-05-10T00:51:36.355021Z",
     "shell.execute_reply": "2023-05-10T00:51:36.354735Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [198], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "eb7747bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.356490Z",
     "iopub.status.busy": "2023-05-10T00:51:36.356411Z",
     "iopub.status.idle": "2023-05-10T00:51:36.362647Z",
     "shell.execute_reply": "2023-05-10T00:51:36.362409Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [199], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c8ac92ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.364011Z",
     "iopub.status.busy": "2023-05-10T00:51:36.363931Z",
     "iopub.status.idle": "2023-05-10T00:51:36.370064Z",
     "shell.execute_reply": "2023-05-10T00:51:36.369837Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [200], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d8a39",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885ddf2",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3eca9b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.371560Z",
     "iopub.status.busy": "2023-05-10T00:51:36.371480Z",
     "iopub.status.idle": "2023-05-10T00:51:36.378730Z",
     "shell.execute_reply": "2023-05-10T00:51:36.378489Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [201], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m rslt\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "plt.to_file('Trump.png')\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a5615",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6365859e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.380123Z",
     "iopub.status.busy": "2023-05-10T00:51:36.380043Z",
     "iopub.status.idle": "2023-05-10T00:51:36.386364Z",
     "shell.execute_reply": "2023-05-10T00:51:36.386124Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [202], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93409daf",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00be95",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "43c75fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.387811Z",
     "iopub.status.busy": "2023-05-10T00:51:36.387730Z",
     "iopub.status.idle": "2023-05-10T00:51:36.390423Z",
     "shell.execute_reply": "2023-05-10T00:51:36.390162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "02c5afdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.391677Z",
     "iopub.status.busy": "2023-05-10T00:51:36.391602Z",
     "iopub.status.idle": "2023-05-10T00:51:36.393280Z",
     "shell.execute_reply": "2023-05-10T00:51:36.393066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1435409a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.394590Z",
     "iopub.status.busy": "2023-05-10T00:51:36.394511Z",
     "iopub.status.idle": "2023-05-10T00:51:36.395998Z",
     "shell.execute_reply": "2023-05-10T00:51:36.395758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2f912175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.397238Z",
     "iopub.status.busy": "2023-05-10T00:51:36.397166Z",
     "iopub.status.idle": "2023-05-10T00:51:36.423210Z",
     "shell.execute_reply": "2023-05-10T00:51:36.422932Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [206], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7ea3eb01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.424709Z",
     "iopub.status.busy": "2023-05-10T00:51:36.424627Z",
     "iopub.status.idle": "2023-05-10T00:51:36.430747Z",
     "shell.execute_reply": "2023-05-10T00:51:36.430506Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [207], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "efe4c1f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.432164Z",
     "iopub.status.busy": "2023-05-10T00:51:36.432084Z",
     "iopub.status.idle": "2023-05-10T00:51:36.438391Z",
     "shell.execute_reply": "2023-05-10T00:51:36.438131Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [208], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b40b7",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82f188",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7628b1f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.439890Z",
     "iopub.status.busy": "2023-05-10T00:51:36.439771Z",
     "iopub.status.idle": "2023-05-10T00:51:36.446447Z",
     "shell.execute_reply": "2023-05-10T00:51:36.446221Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [209], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m rslt\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "plt.savefig('Trump.png')\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc02cc5",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "46a00fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.447889Z",
     "iopub.status.busy": "2023-05-10T00:51:36.447809Z",
     "iopub.status.idle": "2023-05-10T00:51:36.455853Z",
     "shell.execute_reply": "2023-05-10T00:51:36.455594Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [210], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768a114",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661be3fb",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3b102438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.457284Z",
     "iopub.status.busy": "2023-05-10T00:51:36.457194Z",
     "iopub.status.idle": "2023-05-10T00:51:36.460103Z",
     "shell.execute_reply": "2023-05-10T00:51:36.459856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8d68d46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.461438Z",
     "iopub.status.busy": "2023-05-10T00:51:36.461364Z",
     "iopub.status.idle": "2023-05-10T00:51:36.463094Z",
     "shell.execute_reply": "2023-05-10T00:51:36.462826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1f4886da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.464678Z",
     "iopub.status.busy": "2023-05-10T00:51:36.464561Z",
     "iopub.status.idle": "2023-05-10T00:51:36.466562Z",
     "shell.execute_reply": "2023-05-10T00:51:36.466266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9c6a127b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.468715Z",
     "iopub.status.busy": "2023-05-10T00:51:36.468563Z",
     "iopub.status.idle": "2023-05-10T00:51:36.496260Z",
     "shell.execute_reply": "2023-05-10T00:51:36.495953Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [214], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "15b01c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.497883Z",
     "iopub.status.busy": "2023-05-10T00:51:36.497767Z",
     "iopub.status.idle": "2023-05-10T00:51:36.504850Z",
     "shell.execute_reply": "2023-05-10T00:51:36.504599Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [215], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e20b672c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.506317Z",
     "iopub.status.busy": "2023-05-10T00:51:36.506211Z",
     "iopub.status.idle": "2023-05-10T00:51:36.512604Z",
     "shell.execute_reply": "2023-05-10T00:51:36.512337Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [216], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ec099",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc8880",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa30d1",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "535da44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.514084Z",
     "iopub.status.busy": "2023-05-10T00:51:36.513988Z",
     "iopub.status.idle": "2023-05-10T00:51:36.521506Z",
     "shell.execute_reply": "2023-05-10T00:51:36.521235Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()\n",
    "plt.savefig('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1a897",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c4a32e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.522985Z",
     "iopub.status.busy": "2023-05-10T00:51:36.522880Z",
     "iopub.status.idle": "2023-05-10T00:51:36.529466Z",
     "shell.execute_reply": "2023-05-10T00:51:36.529220Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [218], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2db0e",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26685589",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "75b9137b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.530792Z",
     "iopub.status.busy": "2023-05-10T00:51:36.530711Z",
     "iopub.status.idle": "2023-05-10T00:51:36.533925Z",
     "shell.execute_reply": "2023-05-10T00:51:36.533626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9e2d287f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.535413Z",
     "iopub.status.busy": "2023-05-10T00:51:36.535331Z",
     "iopub.status.idle": "2023-05-10T00:51:36.537090Z",
     "shell.execute_reply": "2023-05-10T00:51:36.536836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5ffbf41f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.538432Z",
     "iopub.status.busy": "2023-05-10T00:51:36.538351Z",
     "iopub.status.idle": "2023-05-10T00:51:36.539934Z",
     "shell.execute_reply": "2023-05-10T00:51:36.539698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dc3bf525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.541378Z",
     "iopub.status.busy": "2023-05-10T00:51:36.541288Z",
     "iopub.status.idle": "2023-05-10T00:51:36.569333Z",
     "shell.execute_reply": "2023-05-10T00:51:36.569034Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [222], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8e671a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.570918Z",
     "iopub.status.busy": "2023-05-10T00:51:36.570808Z",
     "iopub.status.idle": "2023-05-10T00:51:36.577332Z",
     "shell.execute_reply": "2023-05-10T00:51:36.577085Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [223], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "14fc48d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.578770Z",
     "iopub.status.busy": "2023-05-10T00:51:36.578685Z",
     "iopub.status.idle": "2023-05-10T00:51:36.585631Z",
     "shell.execute_reply": "2023-05-10T00:51:36.585266Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [224], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66b128",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ddd7a",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56567cb",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "babf027f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.587167Z",
     "iopub.status.busy": "2023-05-10T00:51:36.587062Z",
     "iopub.status.idle": "2023-05-10T00:51:36.593809Z",
     "shell.execute_reply": "2023-05-10T00:51:36.593558Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [225], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d833508",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4267f8fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.595186Z",
     "iopub.status.busy": "2023-05-10T00:51:36.595109Z",
     "iopub.status.idle": "2023-05-10T00:51:36.601598Z",
     "shell.execute_reply": "2023-05-10T00:51:36.601349Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [226], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b5387",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3efaa65",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c5d8e1a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.602976Z",
     "iopub.status.busy": "2023-05-10T00:51:36.602893Z",
     "iopub.status.idle": "2023-05-10T00:51:36.605636Z",
     "shell.execute_reply": "2023-05-10T00:51:36.605396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "176a12e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.606906Z",
     "iopub.status.busy": "2023-05-10T00:51:36.606829Z",
     "iopub.status.idle": "2023-05-10T00:51:36.608584Z",
     "shell.execute_reply": "2023-05-10T00:51:36.608336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "058d6766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.609901Z",
     "iopub.status.busy": "2023-05-10T00:51:36.609825Z",
     "iopub.status.idle": "2023-05-10T00:51:36.611422Z",
     "shell.execute_reply": "2023-05-10T00:51:36.611186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5593b5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.612761Z",
     "iopub.status.busy": "2023-05-10T00:51:36.612690Z",
     "iopub.status.idle": "2023-05-10T00:51:36.639215Z",
     "shell.execute_reply": "2023-05-10T00:51:36.638943Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [230], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b28540c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.640628Z",
     "iopub.status.busy": "2023-05-10T00:51:36.640547Z",
     "iopub.status.idle": "2023-05-10T00:51:36.646862Z",
     "shell.execute_reply": "2023-05-10T00:51:36.646626Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [231], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3fcfee1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.648297Z",
     "iopub.status.busy": "2023-05-10T00:51:36.648214Z",
     "iopub.status.idle": "2023-05-10T00:51:36.654313Z",
     "shell.execute_reply": "2023-05-10T00:51:36.654087Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [232], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972c421",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3fd2d",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d98149",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "bf9bf7bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.655587Z",
     "iopub.status.busy": "2023-05-10T00:51:36.655506Z",
     "iopub.status.idle": "2023-05-10T00:51:36.661995Z",
     "shell.execute_reply": "2023-05-10T00:51:36.661786Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [233], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m rslt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "rslt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f06d0",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "342e326d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.663298Z",
     "iopub.status.busy": "2023-05-10T00:51:36.663225Z",
     "iopub.status.idle": "2023-05-10T00:51:36.669656Z",
     "shell.execute_reply": "2023-05-10T00:51:36.669419Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [234], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08cacb",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7e103",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "21907c60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.670999Z",
     "iopub.status.busy": "2023-05-10T00:51:36.670917Z",
     "iopub.status.idle": "2023-05-10T00:51:36.673558Z",
     "shell.execute_reply": "2023-05-10T00:51:36.673346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "76e05816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.674858Z",
     "iopub.status.busy": "2023-05-10T00:51:36.674775Z",
     "iopub.status.idle": "2023-05-10T00:51:36.676513Z",
     "shell.execute_reply": "2023-05-10T00:51:36.676284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "89016e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.677885Z",
     "iopub.status.busy": "2023-05-10T00:51:36.677807Z",
     "iopub.status.idle": "2023-05-10T00:51:36.679288Z",
     "shell.execute_reply": "2023-05-10T00:51:36.679052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9f465e13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.680571Z",
     "iopub.status.busy": "2023-05-10T00:51:36.680493Z",
     "iopub.status.idle": "2023-05-10T00:51:36.707287Z",
     "shell.execute_reply": "2023-05-10T00:51:36.706986Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [238], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9f74eced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.708863Z",
     "iopub.status.busy": "2023-05-10T00:51:36.708762Z",
     "iopub.status.idle": "2023-05-10T00:51:36.715147Z",
     "shell.execute_reply": "2023-05-10T00:51:36.714905Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [239], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "00fc6430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.717049Z",
     "iopub.status.busy": "2023-05-10T00:51:36.716913Z",
     "iopub.status.idle": "2023-05-10T00:51:36.723315Z",
     "shell.execute_reply": "2023-05-10T00:51:36.723050Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [240], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e6dbe",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c33272",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f748dda",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "393935f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:36.724655Z",
     "iopub.status.busy": "2023-05-10T00:51:36.724575Z",
     "iopub.status.idle": "2023-05-10T00:51:36.731264Z",
     "shell.execute_reply": "2023-05-10T00:51:36.730857Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [241], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
