{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3fe320",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ecbd99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.582603Z",
     "iopub.status.busy": "2023-05-09T21:26:27.582465Z",
     "iopub.status.idle": "2023-05-09T21:26:27.664940Z",
     "shell.execute_reply": "2023-05-09T21:26:27.664687Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e65ef",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8021b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.666523Z",
     "iopub.status.busy": "2023-05-09T21:26:27.666409Z",
     "iopub.status.idle": "2023-05-09T21:26:27.672773Z",
     "shell.execute_reply": "2023-05-09T21:26:27.672523Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace end of line character with space\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_raw\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2ae25f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.674174Z",
     "iopub.status.busy": "2023-05-09T21:26:27.674089Z",
     "iopub.status.idle": "2023-05-09T21:26:27.676822Z",
     "shell.execute_reply": "2023-05-09T21:26:27.676595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cd60e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.678178Z",
     "iopub.status.busy": "2023-05-09T21:26:27.678096Z",
     "iopub.status.idle": "2023-05-09T21:26:27.679846Z",
     "shell.execute_reply": "2023-05-09T21:26:27.679633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36506873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.681202Z",
     "iopub.status.busy": "2023-05-09T21:26:27.681125Z",
     "iopub.status.idle": "2023-05-09T21:26:27.682731Z",
     "shell.execute_reply": "2023-05-09T21:26:27.682525Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d81ba",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2dedde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.684019Z",
     "iopub.status.busy": "2023-05-09T21:26:27.683942Z",
     "iopub.status.idle": "2023-05-09T21:26:27.690290Z",
     "shell.execute_reply": "2023-05-09T21:26:27.690062Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de2d489",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.691598Z",
     "iopub.status.busy": "2023-05-09T21:26:27.691523Z",
     "iopub.status.idle": "2023-05-09T21:26:27.697684Z",
     "shell.execute_reply": "2023-05-09T21:26:27.697441Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639196d1",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd520dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.699076Z",
     "iopub.status.busy": "2023-05-09T21:26:27.698993Z",
     "iopub.status.idle": "2023-05-09T21:26:27.705378Z",
     "shell.execute_reply": "2023-05-09T21:26:27.705129Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b7bcc",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6dcfa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.706759Z",
     "iopub.status.busy": "2023-05-09T21:26:27.706675Z",
     "iopub.status.idle": "2023-05-09T21:26:27.713258Z",
     "shell.execute_reply": "2023-05-09T21:26:27.713022Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f074e",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bebc319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.714612Z",
     "iopub.status.busy": "2023-05-09T21:26:27.714530Z",
     "iopub.status.idle": "2023-05-09T21:26:27.733820Z",
     "shell.execute_reply": "2023-05-09T21:26:27.733585Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace end of line character with space\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_raw\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5056989a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.735269Z",
     "iopub.status.busy": "2023-05-09T21:26:27.735184Z",
     "iopub.status.idle": "2023-05-09T21:26:27.738045Z",
     "shell.execute_reply": "2023-05-09T21:26:27.737811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "355f33b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.739389Z",
     "iopub.status.busy": "2023-05-09T21:26:27.739314Z",
     "iopub.status.idle": "2023-05-09T21:26:27.741027Z",
     "shell.execute_reply": "2023-05-09T21:26:27.740812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b400b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.742344Z",
     "iopub.status.busy": "2023-05-09T21:26:27.742264Z",
     "iopub.status.idle": "2023-05-09T21:26:27.743719Z",
     "shell.execute_reply": "2023-05-09T21:26:27.743499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a2424",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620d07a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.745001Z",
     "iopub.status.busy": "2023-05-09T21:26:27.744919Z",
     "iopub.status.idle": "2023-05-09T21:26:27.751157Z",
     "shell.execute_reply": "2023-05-09T21:26:27.750914Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ae4bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.752543Z",
     "iopub.status.busy": "2023-05-09T21:26:27.752461Z",
     "iopub.status.idle": "2023-05-09T21:26:27.758361Z",
     "shell.execute_reply": "2023-05-09T21:26:27.758143Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ac47b",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf2b4c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.759703Z",
     "iopub.status.busy": "2023-05-09T21:26:27.759631Z",
     "iopub.status.idle": "2023-05-09T21:26:27.765714Z",
     "shell.execute_reply": "2023-05-09T21:26:27.765467Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29e742",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be045174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.767052Z",
     "iopub.status.busy": "2023-05-09T21:26:27.766978Z",
     "iopub.status.idle": "2023-05-09T21:26:27.773190Z",
     "shell.execute_reply": "2023-05-09T21:26:27.772963Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5480396",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4b014",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d9593b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.774568Z",
     "iopub.status.busy": "2023-05-09T21:26:27.774485Z",
     "iopub.status.idle": "2023-05-09T21:26:27.777169Z",
     "shell.execute_reply": "2023-05-09T21:26:27.776947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e74eb0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.778484Z",
     "iopub.status.busy": "2023-05-09T21:26:27.778410Z",
     "iopub.status.idle": "2023-05-09T21:26:27.780141Z",
     "shell.execute_reply": "2023-05-09T21:26:27.779914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da87558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.781468Z",
     "iopub.status.busy": "2023-05-09T21:26:27.781387Z",
     "iopub.status.idle": "2023-05-09T21:26:27.782942Z",
     "shell.execute_reply": "2023-05-09T21:26:27.782713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ec821",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20f2139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.784280Z",
     "iopub.status.busy": "2023-05-09T21:26:27.784200Z",
     "iopub.status.idle": "2023-05-09T21:26:27.790428Z",
     "shell.execute_reply": "2023-05-09T21:26:27.790197Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a78662b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.791783Z",
     "iopub.status.busy": "2023-05-09T21:26:27.791709Z",
     "iopub.status.idle": "2023-05-09T21:26:27.798167Z",
     "shell.execute_reply": "2023-05-09T21:26:27.797921Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ff3ee",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e6743dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.799541Z",
     "iopub.status.busy": "2023-05-09T21:26:27.799459Z",
     "iopub.status.idle": "2023-05-09T21:26:27.805685Z",
     "shell.execute_reply": "2023-05-09T21:26:27.805435Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833488e",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "893ebf65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.806988Z",
     "iopub.status.busy": "2023-05-09T21:26:27.806910Z",
     "iopub.status.idle": "2023-05-09T21:26:27.813036Z",
     "shell.execute_reply": "2023-05-09T21:26:27.812789Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26373608",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd76bef",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6029b492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.814444Z",
     "iopub.status.busy": "2023-05-09T21:26:27.814362Z",
     "iopub.status.idle": "2023-05-09T21:26:27.817032Z",
     "shell.execute_reply": "2023-05-09T21:26:27.816789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78a1cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.818364Z",
     "iopub.status.busy": "2023-05-09T21:26:27.818284Z",
     "iopub.status.idle": "2023-05-09T21:26:27.820015Z",
     "shell.execute_reply": "2023-05-09T21:26:27.819805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e155662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.821411Z",
     "iopub.status.busy": "2023-05-09T21:26:27.821340Z",
     "iopub.status.idle": "2023-05-09T21:26:27.822975Z",
     "shell.execute_reply": "2023-05-09T21:26:27.822766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b666c",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cd144",
   "metadata": {},
   "source": [
    "Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4037607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.824339Z",
     "iopub.status.busy": "2023-05-09T21:26:27.824261Z",
     "iopub.status.idle": "2023-05-09T21:26:27.830302Z",
     "shell.execute_reply": "2023-05-09T21:26:27.830063Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3055fa",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08346bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.831666Z",
     "iopub.status.busy": "2023-05-09T21:26:27.831587Z",
     "iopub.status.idle": "2023-05-09T21:26:27.837786Z",
     "shell.execute_reply": "2023-05-09T21:26:27.837505Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb1364",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0e881b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.839110Z",
     "iopub.status.busy": "2023-05-09T21:26:27.839035Z",
     "iopub.status.idle": "2023-05-09T21:26:27.845266Z",
     "shell.execute_reply": "2023-05-09T21:26:27.845028Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd293b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caa53a",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb8f5bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.846588Z",
     "iopub.status.busy": "2023-05-09T21:26:27.846511Z",
     "iopub.status.idle": "2023-05-09T21:26:27.849125Z",
     "shell.execute_reply": "2023-05-09T21:26:27.848904Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a801a08e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.850433Z",
     "iopub.status.busy": "2023-05-09T21:26:27.850358Z",
     "iopub.status.idle": "2023-05-09T21:26:27.852000Z",
     "shell.execute_reply": "2023-05-09T21:26:27.851801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17fbe461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.853328Z",
     "iopub.status.busy": "2023-05-09T21:26:27.853259Z",
     "iopub.status.idle": "2023-05-09T21:26:27.854838Z",
     "shell.execute_reply": "2023-05-09T21:26:27.854613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337c304",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce8a8015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.856109Z",
     "iopub.status.busy": "2023-05-09T21:26:27.856033Z",
     "iopub.status.idle": "2023-05-09T21:26:27.862414Z",
     "shell.execute_reply": "2023-05-09T21:26:27.862165Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "740675bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.863753Z",
     "iopub.status.busy": "2023-05-09T21:26:27.863684Z",
     "iopub.status.idle": "2023-05-09T21:26:27.870117Z",
     "shell.execute_reply": "2023-05-09T21:26:27.869899Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aac10d",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab7c843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.871513Z",
     "iopub.status.busy": "2023-05-09T21:26:27.871438Z",
     "iopub.status.idle": "2023-05-09T21:26:27.877479Z",
     "shell.execute_reply": "2023-05-09T21:26:27.877253Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42ed9b",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc3dcc95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.878794Z",
     "iopub.status.busy": "2023-05-09T21:26:27.878720Z",
     "iopub.status.idle": "2023-05-09T21:26:27.884939Z",
     "shell.execute_reply": "2023-05-09T21:26:27.884719Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ab051",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e130a9",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "644fb5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.886276Z",
     "iopub.status.busy": "2023-05-09T21:26:27.886191Z",
     "iopub.status.idle": "2023-05-09T21:26:27.888878Z",
     "shell.execute_reply": "2023-05-09T21:26:27.888671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aad5490b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.890207Z",
     "iopub.status.busy": "2023-05-09T21:26:27.890125Z",
     "iopub.status.idle": "2023-05-09T21:26:27.891815Z",
     "shell.execute_reply": "2023-05-09T21:26:27.891609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "282c552e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.893179Z",
     "iopub.status.busy": "2023-05-09T21:26:27.893108Z",
     "iopub.status.idle": "2023-05-09T21:26:27.894706Z",
     "shell.execute_reply": "2023-05-09T21:26:27.894498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59596b03",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6209d866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.896014Z",
     "iopub.status.busy": "2023-05-09T21:26:27.895943Z",
     "iopub.status.idle": "2023-05-09T21:26:27.902078Z",
     "shell.execute_reply": "2023-05-09T21:26:27.901836Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e90512f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.903440Z",
     "iopub.status.busy": "2023-05-09T21:26:27.903364Z",
     "iopub.status.idle": "2023-05-09T21:26:27.909386Z",
     "shell.execute_reply": "2023-05-09T21:26:27.909138Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ca1d8",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbb0ea32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.910711Z",
     "iopub.status.busy": "2023-05-09T21:26:27.910640Z",
     "iopub.status.idle": "2023-05-09T21:26:27.916838Z",
     "shell.execute_reply": "2023-05-09T21:26:27.916587Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615539d",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "428ad46c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.918189Z",
     "iopub.status.busy": "2023-05-09T21:26:27.918106Z",
     "iopub.status.idle": "2023-05-09T21:26:27.924379Z",
     "shell.execute_reply": "2023-05-09T21:26:27.924143Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208da080",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6154d6",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38a73aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.925726Z",
     "iopub.status.busy": "2023-05-09T21:26:27.925649Z",
     "iopub.status.idle": "2023-05-09T21:26:27.928388Z",
     "shell.execute_reply": "2023-05-09T21:26:27.928152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce03eb40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.929753Z",
     "iopub.status.busy": "2023-05-09T21:26:27.929674Z",
     "iopub.status.idle": "2023-05-09T21:26:27.931205Z",
     "shell.execute_reply": "2023-05-09T21:26:27.930981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0bdae92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.932547Z",
     "iopub.status.busy": "2023-05-09T21:26:27.932468Z",
     "iopub.status.idle": "2023-05-09T21:26:27.934004Z",
     "shell.execute_reply": "2023-05-09T21:26:27.933777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dd68d",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50b93463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.935308Z",
     "iopub.status.busy": "2023-05-09T21:26:27.935231Z",
     "iopub.status.idle": "2023-05-09T21:26:27.941363Z",
     "shell.execute_reply": "2023-05-09T21:26:27.941142Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de377245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.942671Z",
     "iopub.status.busy": "2023-05-09T21:26:27.942596Z",
     "iopub.status.idle": "2023-05-09T21:26:27.949140Z",
     "shell.execute_reply": "2023-05-09T21:26:27.948922Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba529e62",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2816dcac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.950477Z",
     "iopub.status.busy": "2023-05-09T21:26:27.950393Z",
     "iopub.status.idle": "2023-05-09T21:26:27.956658Z",
     "shell.execute_reply": "2023-05-09T21:26:27.956432Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.imshow(wc)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "# plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4aae99",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cc45274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.958000Z",
     "iopub.status.busy": "2023-05-09T21:26:27.957927Z",
     "iopub.status.idle": "2023-05-09T21:26:27.964220Z",
     "shell.execute_reply": "2023-05-09T21:26:27.963989Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee7138",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500da91e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7be5a1ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.965543Z",
     "iopub.status.busy": "2023-05-09T21:26:27.965470Z",
     "iopub.status.idle": "2023-05-09T21:26:27.968115Z",
     "shell.execute_reply": "2023-05-09T21:26:27.967889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f7a531b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.969453Z",
     "iopub.status.busy": "2023-05-09T21:26:27.969374Z",
     "iopub.status.idle": "2023-05-09T21:26:27.971025Z",
     "shell.execute_reply": "2023-05-09T21:26:27.970809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a58cbd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.972289Z",
     "iopub.status.busy": "2023-05-09T21:26:27.972207Z",
     "iopub.status.idle": "2023-05-09T21:26:27.973756Z",
     "shell.execute_reply": "2023-05-09T21:26:27.973530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840e852",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fa11c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.975069Z",
     "iopub.status.busy": "2023-05-09T21:26:27.974990Z",
     "iopub.status.idle": "2023-05-09T21:26:27.981123Z",
     "shell.execute_reply": "2023-05-09T21:26:27.980881Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "243ce4f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.982495Z",
     "iopub.status.busy": "2023-05-09T21:26:27.982420Z",
     "iopub.status.idle": "2023-05-09T21:26:27.988644Z",
     "shell.execute_reply": "2023-05-09T21:26:27.988396Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfeef74",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2e360",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41085d",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38a72653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.990011Z",
     "iopub.status.busy": "2023-05-09T21:26:27.989932Z",
     "iopub.status.idle": "2023-05-09T21:26:27.996122Z",
     "shell.execute_reply": "2023-05-09T21:26:27.995890Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d2ed8",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bb1e6",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51db4d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:27.997656Z",
     "iopub.status.busy": "2023-05-09T21:26:27.997572Z",
     "iopub.status.idle": "2023-05-09T21:26:28.000562Z",
     "shell.execute_reply": "2023-05-09T21:26:28.000288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cbb68be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.002000Z",
     "iopub.status.busy": "2023-05-09T21:26:28.001919Z",
     "iopub.status.idle": "2023-05-09T21:26:28.003657Z",
     "shell.execute_reply": "2023-05-09T21:26:28.003431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2eb78012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.005025Z",
     "iopub.status.busy": "2023-05-09T21:26:28.004932Z",
     "iopub.status.idle": "2023-05-09T21:26:28.006764Z",
     "shell.execute_reply": "2023-05-09T21:26:28.006430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29796c4a",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3cf3a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.008199Z",
     "iopub.status.busy": "2023-05-09T21:26:28.008124Z",
     "iopub.status.idle": "2023-05-09T21:26:28.014509Z",
     "shell.execute_reply": "2023-05-09T21:26:28.014273Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5e93095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.015917Z",
     "iopub.status.busy": "2023-05-09T21:26:28.015838Z",
     "iopub.status.idle": "2023-05-09T21:26:28.021833Z",
     "shell.execute_reply": "2023-05-09T21:26:28.021599Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2c319",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542ee00",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "115deed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.024565Z",
     "iopub.status.busy": "2023-05-09T21:26:28.024421Z",
     "iopub.status.idle": "2023-05-09T21:26:28.031909Z",
     "shell.execute_reply": "2023-05-09T21:26:28.031679Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwords_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "words_list.plot.bar(x=\"word\",y=\"freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e6790",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59f57f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.033254Z",
     "iopub.status.busy": "2023-05-09T21:26:28.033176Z",
     "iopub.status.idle": "2023-05-09T21:26:28.039801Z",
     "shell.execute_reply": "2023-05-09T21:26:28.039480Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1618f",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a23db",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61498176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.041394Z",
     "iopub.status.busy": "2023-05-09T21:26:28.041285Z",
     "iopub.status.idle": "2023-05-09T21:26:28.043972Z",
     "shell.execute_reply": "2023-05-09T21:26:28.043727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ae36944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.045254Z",
     "iopub.status.busy": "2023-05-09T21:26:28.045166Z",
     "iopub.status.idle": "2023-05-09T21:26:28.046785Z",
     "shell.execute_reply": "2023-05-09T21:26:28.046533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a433c2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.048036Z",
     "iopub.status.busy": "2023-05-09T21:26:28.047959Z",
     "iopub.status.idle": "2023-05-09T21:26:28.049551Z",
     "shell.execute_reply": "2023-05-09T21:26:28.049337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a411289",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60a5b0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.050877Z",
     "iopub.status.busy": "2023-05-09T21:26:28.050802Z",
     "iopub.status.idle": "2023-05-09T21:26:28.057124Z",
     "shell.execute_reply": "2023-05-09T21:26:28.056876Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "049b98f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.058536Z",
     "iopub.status.busy": "2023-05-09T21:26:28.058456Z",
     "iopub.status.idle": "2023-05-09T21:26:28.064633Z",
     "shell.execute_reply": "2023-05-09T21:26:28.064406Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6e3ad",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6897a",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3505300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.065959Z",
     "iopub.status.busy": "2023-05-09T21:26:28.065879Z",
     "iopub.status.idle": "2023-05-09T21:26:28.072489Z",
     "shell.execute_reply": "2023-05-09T21:26:28.072235Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list)\n",
    "df.plot.bar(x=\"word\",y=\"freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4849b5a",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "423a0841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.074069Z",
     "iopub.status.busy": "2023-05-09T21:26:28.073953Z",
     "iopub.status.idle": "2023-05-09T21:26:28.080741Z",
     "shell.execute_reply": "2023-05-09T21:26:28.080480Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a19030",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63839104",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11321af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.082188Z",
     "iopub.status.busy": "2023-05-09T21:26:28.082109Z",
     "iopub.status.idle": "2023-05-09T21:26:28.084822Z",
     "shell.execute_reply": "2023-05-09T21:26:28.084585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0222a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.086145Z",
     "iopub.status.busy": "2023-05-09T21:26:28.086063Z",
     "iopub.status.idle": "2023-05-09T21:26:28.087750Z",
     "shell.execute_reply": "2023-05-09T21:26:28.087534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22a43ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.089304Z",
     "iopub.status.busy": "2023-05-09T21:26:28.089190Z",
     "iopub.status.idle": "2023-05-09T21:26:28.091023Z",
     "shell.execute_reply": "2023-05-09T21:26:28.090783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1f1c6",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7b10896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.092392Z",
     "iopub.status.busy": "2023-05-09T21:26:28.092318Z",
     "iopub.status.idle": "2023-05-09T21:26:28.098607Z",
     "shell.execute_reply": "2023-05-09T21:26:28.098364Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82198257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.099957Z",
     "iopub.status.busy": "2023-05-09T21:26:28.099877Z",
     "iopub.status.idle": "2023-05-09T21:26:28.106088Z",
     "shell.execute_reply": "2023-05-09T21:26:28.105743Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eff880",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b81c14",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5979d0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.107485Z",
     "iopub.status.busy": "2023-05-09T21:26:28.107408Z",
     "iopub.status.idle": "2023-05-09T21:26:28.114531Z",
     "shell.execute_reply": "2023-05-09T21:26:28.114285Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list)\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5386f1d",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84fc102f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.115859Z",
     "iopub.status.busy": "2023-05-09T21:26:28.115779Z",
     "iopub.status.idle": "2023-05-09T21:26:28.121905Z",
     "shell.execute_reply": "2023-05-09T21:26:28.121639Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b18f82",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81517732",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c46a517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.123437Z",
     "iopub.status.busy": "2023-05-09T21:26:28.123346Z",
     "iopub.status.idle": "2023-05-09T21:26:28.126043Z",
     "shell.execute_reply": "2023-05-09T21:26:28.125811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fbf6ad13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.127421Z",
     "iopub.status.busy": "2023-05-09T21:26:28.127346Z",
     "iopub.status.idle": "2023-05-09T21:26:28.129109Z",
     "shell.execute_reply": "2023-05-09T21:26:28.128874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fd55af8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.130505Z",
     "iopub.status.busy": "2023-05-09T21:26:28.130429Z",
     "iopub.status.idle": "2023-05-09T21:26:28.132085Z",
     "shell.execute_reply": "2023-05-09T21:26:28.131862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836f458",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6c6490e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.133421Z",
     "iopub.status.busy": "2023-05-09T21:26:28.133350Z",
     "iopub.status.idle": "2023-05-09T21:26:28.139742Z",
     "shell.execute_reply": "2023-05-09T21:26:28.139501Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca52f96f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.141090Z",
     "iopub.status.busy": "2023-05-09T21:26:28.141017Z",
     "iopub.status.idle": "2023-05-09T21:26:28.147116Z",
     "shell.execute_reply": "2023-05-09T21:26:28.146886Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8998e",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba139f",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "60ee472b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.148500Z",
     "iopub.status.busy": "2023-05-09T21:26:28.148421Z",
     "iopub.status.idle": "2023-05-09T21:26:28.154818Z",
     "shell.execute_reply": "2023-05-09T21:26:28.154583Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39mwords)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=words)\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834f2d4",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6de5ecd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.156523Z",
     "iopub.status.busy": "2023-05-09T21:26:28.156409Z",
     "iopub.status.idle": "2023-05-09T21:26:28.162863Z",
     "shell.execute_reply": "2023-05-09T21:26:28.162606Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ada075",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df5bbd",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0efa8cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.164273Z",
     "iopub.status.busy": "2023-05-09T21:26:28.164191Z",
     "iopub.status.idle": "2023-05-09T21:26:28.166953Z",
     "shell.execute_reply": "2023-05-09T21:26:28.166721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55559028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.168243Z",
     "iopub.status.busy": "2023-05-09T21:26:28.168167Z",
     "iopub.status.idle": "2023-05-09T21:26:28.169953Z",
     "shell.execute_reply": "2023-05-09T21:26:28.169720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "044ee96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.171285Z",
     "iopub.status.busy": "2023-05-09T21:26:28.171212Z",
     "iopub.status.idle": "2023-05-09T21:26:28.173003Z",
     "shell.execute_reply": "2023-05-09T21:26:28.172726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b893c",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e4b07a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.174347Z",
     "iopub.status.busy": "2023-05-09T21:26:28.174269Z",
     "iopub.status.idle": "2023-05-09T21:26:28.180602Z",
     "shell.execute_reply": "2023-05-09T21:26:28.180380Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "44d00b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.181940Z",
     "iopub.status.busy": "2023-05-09T21:26:28.181859Z",
     "iopub.status.idle": "2023-05-09T21:26:28.187952Z",
     "shell.execute_reply": "2023-05-09T21:26:28.187675Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09789c",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1ca65",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5430f14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.189526Z",
     "iopub.status.busy": "2023-05-09T21:26:28.189412Z",
     "iopub.status.idle": "2023-05-09T21:26:28.196592Z",
     "shell.execute_reply": "2023-05-09T21:26:28.196379Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93b45d",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b4d0d69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.197998Z",
     "iopub.status.busy": "2023-05-09T21:26:28.197916Z",
     "iopub.status.idle": "2023-05-09T21:26:28.204218Z",
     "shell.execute_reply": "2023-05-09T21:26:28.203985Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe2ffb",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ecda42",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "71db8f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.205580Z",
     "iopub.status.busy": "2023-05-09T21:26:28.205501Z",
     "iopub.status.idle": "2023-05-09T21:26:28.208389Z",
     "shell.execute_reply": "2023-05-09T21:26:28.208156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b116090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.209709Z",
     "iopub.status.busy": "2023-05-09T21:26:28.209624Z",
     "iopub.status.idle": "2023-05-09T21:26:28.211303Z",
     "shell.execute_reply": "2023-05-09T21:26:28.211078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e83cc4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.212620Z",
     "iopub.status.busy": "2023-05-09T21:26:28.212542Z",
     "iopub.status.idle": "2023-05-09T21:26:28.214168Z",
     "shell.execute_reply": "2023-05-09T21:26:28.213950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24914548",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9b5a0787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.215464Z",
     "iopub.status.busy": "2023-05-09T21:26:28.215387Z",
     "iopub.status.idle": "2023-05-09T21:26:28.221698Z",
     "shell.execute_reply": "2023-05-09T21:26:28.221455Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f2d289df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.223121Z",
     "iopub.status.busy": "2023-05-09T21:26:28.223032Z",
     "iopub.status.idle": "2023-05-09T21:26:28.229259Z",
     "shell.execute_reply": "2023-05-09T21:26:28.229001Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316358c6",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e49eef",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0ed847b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.230644Z",
     "iopub.status.busy": "2023-05-09T21:26:28.230564Z",
     "iopub.status.idle": "2023-05-09T21:26:28.237157Z",
     "shell.execute_reply": "2023-05-09T21:26:28.236925Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59d6c7",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dced9827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.238470Z",
     "iopub.status.busy": "2023-05-09T21:26:28.238388Z",
     "iopub.status.idle": "2023-05-09T21:26:28.245006Z",
     "shell.execute_reply": "2023-05-09T21:26:28.244775Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2208b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c494b",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "965562a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.246451Z",
     "iopub.status.busy": "2023-05-09T21:26:28.246370Z",
     "iopub.status.idle": "2023-05-09T21:26:28.249061Z",
     "shell.execute_reply": "2023-05-09T21:26:28.248849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d87546da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.250420Z",
     "iopub.status.busy": "2023-05-09T21:26:28.250343Z",
     "iopub.status.idle": "2023-05-09T21:26:28.251961Z",
     "shell.execute_reply": "2023-05-09T21:26:28.251734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c944e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.253254Z",
     "iopub.status.busy": "2023-05-09T21:26:28.253178Z",
     "iopub.status.idle": "2023-05-09T21:26:28.254856Z",
     "shell.execute_reply": "2023-05-09T21:26:28.254612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e0cfd",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "88438b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.256325Z",
     "iopub.status.busy": "2023-05-09T21:26:28.256242Z",
     "iopub.status.idle": "2023-05-09T21:26:28.262548Z",
     "shell.execute_reply": "2023-05-09T21:26:28.262311Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "460042a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.263898Z",
     "iopub.status.busy": "2023-05-09T21:26:28.263819Z",
     "iopub.status.idle": "2023-05-09T21:26:28.269857Z",
     "shell.execute_reply": "2023-05-09T21:26:28.269585Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867e892",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f755479",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0228af1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.271208Z",
     "iopub.status.busy": "2023-05-09T21:26:28.271135Z",
     "iopub.status.idle": "2023-05-09T21:26:28.278723Z",
     "shell.execute_reply": "2023-05-09T21:26:28.278481Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f0250",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aa57ffd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.280189Z",
     "iopub.status.busy": "2023-05-09T21:26:28.280097Z",
     "iopub.status.idle": "2023-05-09T21:26:28.286415Z",
     "shell.execute_reply": "2023-05-09T21:26:28.286151Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ac492",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ff38e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ca09f23b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.287837Z",
     "iopub.status.busy": "2023-05-09T21:26:28.287763Z",
     "iopub.status.idle": "2023-05-09T21:26:28.290546Z",
     "shell.execute_reply": "2023-05-09T21:26:28.290294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ad39cb0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.291871Z",
     "iopub.status.busy": "2023-05-09T21:26:28.291796Z",
     "iopub.status.idle": "2023-05-09T21:26:28.293546Z",
     "shell.execute_reply": "2023-05-09T21:26:28.293326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ccfbbf2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.294927Z",
     "iopub.status.busy": "2023-05-09T21:26:28.294820Z",
     "iopub.status.idle": "2023-05-09T21:26:28.296472Z",
     "shell.execute_reply": "2023-05-09T21:26:28.296254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fb84e",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a5b0865a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.297708Z",
     "iopub.status.busy": "2023-05-09T21:26:28.297630Z",
     "iopub.status.idle": "2023-05-09T21:26:28.304627Z",
     "shell.execute_reply": "2023-05-09T21:26:28.304380Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(words_list)\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20585ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.306230Z",
     "iopub.status.busy": "2023-05-09T21:26:28.306116Z",
     "iopub.status.idle": "2023-05-09T21:26:28.312527Z",
     "shell.execute_reply": "2023-05-09T21:26:28.312274Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8aa19972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.313892Z",
     "iopub.status.busy": "2023-05-09T21:26:28.313812Z",
     "iopub.status.idle": "2023-05-09T21:26:28.319866Z",
     "shell.execute_reply": "2023-05-09T21:26:28.319590Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46522ecb",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d8ee3",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d4ea7fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.321198Z",
     "iopub.status.busy": "2023-05-09T21:26:28.321121Z",
     "iopub.status.idle": "2023-05-09T21:26:28.323260Z",
     "shell.execute_reply": "2023-05-09T21:26:28.323003Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1507832862.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [113], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.DataFrame.from(words_list,columns=['words'])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d2eb3",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6228f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.324627Z",
     "iopub.status.busy": "2023-05-09T21:26:28.324554Z",
     "iopub.status.idle": "2023-05-09T21:26:28.330990Z",
     "shell.execute_reply": "2023-05-09T21:26:28.330741Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a4b3d",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ecf71",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fa18b431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.332376Z",
     "iopub.status.busy": "2023-05-09T21:26:28.332294Z",
     "iopub.status.idle": "2023-05-09T21:26:28.334944Z",
     "shell.execute_reply": "2023-05-09T21:26:28.334725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0760fbbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.336230Z",
     "iopub.status.busy": "2023-05-09T21:26:28.336155Z",
     "iopub.status.idle": "2023-05-09T21:26:28.337919Z",
     "shell.execute_reply": "2023-05-09T21:26:28.337688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "101446f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.339384Z",
     "iopub.status.busy": "2023-05-09T21:26:28.339300Z",
     "iopub.status.idle": "2023-05-09T21:26:28.341011Z",
     "shell.execute_reply": "2023-05-09T21:26:28.340781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3929f24",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac85bb2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.342265Z",
     "iopub.status.busy": "2023-05-09T21:26:28.342182Z",
     "iopub.status.idle": "2023-05-09T21:26:28.348857Z",
     "shell.execute_reply": "2023-05-09T21:26:28.348624Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(words_list)\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd002b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.350203Z",
     "iopub.status.busy": "2023-05-09T21:26:28.350120Z",
     "iopub.status.idle": "2023-05-09T21:26:28.356481Z",
     "shell.execute_reply": "2023-05-09T21:26:28.356225Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b0505228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.357966Z",
     "iopub.status.busy": "2023-05-09T21:26:28.357861Z",
     "iopub.status.idle": "2023-05-09T21:26:28.363971Z",
     "shell.execute_reply": "2023-05-09T21:26:28.363757Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [120], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f617f",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d49c9f",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3cba6091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.365256Z",
     "iopub.status.busy": "2023-05-09T21:26:28.365183Z",
     "iopub.status.idle": "2023-05-09T21:26:28.372352Z",
     "shell.execute_reply": "2023-05-09T21:26:28.372053Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c144e1f",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "22cfe48f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:28.373846Z",
     "iopub.status.busy": "2023-05-09T21:26:28.373743Z",
     "iopub.status.idle": "2023-05-09T21:26:29.435252Z",
     "shell.execute_reply": "2023-05-09T21:26:29.434970Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [122], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe4fc7",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696e8af",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "73e906b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.436711Z",
     "iopub.status.busy": "2023-05-09T21:26:29.436607Z",
     "iopub.status.idle": "2023-05-09T21:26:29.439424Z",
     "shell.execute_reply": "2023-05-09T21:26:29.439193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f30d01bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.440771Z",
     "iopub.status.busy": "2023-05-09T21:26:29.440670Z",
     "iopub.status.idle": "2023-05-09T21:26:29.442356Z",
     "shell.execute_reply": "2023-05-09T21:26:29.442103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8727a646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.443690Z",
     "iopub.status.busy": "2023-05-09T21:26:29.443611Z",
     "iopub.status.idle": "2023-05-09T21:26:29.445227Z",
     "shell.execute_reply": "2023-05-09T21:26:29.444989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e105d",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0a2b0dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.446568Z",
     "iopub.status.busy": "2023-05-09T21:26:29.446489Z",
     "iopub.status.idle": "2023-05-09T21:26:29.511356Z",
     "shell.execute_reply": "2023-05-09T21:26:29.511085Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e442095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.512873Z",
     "iopub.status.busy": "2023-05-09T21:26:29.512774Z",
     "iopub.status.idle": "2023-05-09T21:26:29.519286Z",
     "shell.execute_reply": "2023-05-09T21:26:29.519008Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d47b0664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.520715Z",
     "iopub.status.busy": "2023-05-09T21:26:29.520635Z",
     "iopub.status.idle": "2023-05-09T21:26:29.526715Z",
     "shell.execute_reply": "2023-05-09T21:26:29.526490Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d9a93",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5488d",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7588af03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.528044Z",
     "iopub.status.busy": "2023-05-09T21:26:29.527968Z",
     "iopub.status.idle": "2023-05-09T21:26:29.534255Z",
     "shell.execute_reply": "2023-05-09T21:26:29.534021Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [129], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bfbad",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "39ee7fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.535616Z",
     "iopub.status.busy": "2023-05-09T21:26:29.535535Z",
     "iopub.status.idle": "2023-05-09T21:26:29.541952Z",
     "shell.execute_reply": "2023-05-09T21:26:29.541711Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [130], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c17dc8b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0881918",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46499c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.543327Z",
     "iopub.status.busy": "2023-05-09T21:26:29.543244Z",
     "iopub.status.idle": "2023-05-09T21:26:29.545841Z",
     "shell.execute_reply": "2023-05-09T21:26:29.545637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9daed90c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.547188Z",
     "iopub.status.busy": "2023-05-09T21:26:29.547109Z",
     "iopub.status.idle": "2023-05-09T21:26:29.548735Z",
     "shell.execute_reply": "2023-05-09T21:26:29.548516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "92deb485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.550045Z",
     "iopub.status.busy": "2023-05-09T21:26:29.549970Z",
     "iopub.status.idle": "2023-05-09T21:26:29.551572Z",
     "shell.execute_reply": "2023-05-09T21:26:29.551320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593945cd",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f47d60ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.552877Z",
     "iopub.status.busy": "2023-05-09T21:26:29.552795Z",
     "iopub.status.idle": "2023-05-09T21:26:29.579071Z",
     "shell.execute_reply": "2023-05-09T21:26:29.578791Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "58930cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.580610Z",
     "iopub.status.busy": "2023-05-09T21:26:29.580500Z",
     "iopub.status.idle": "2023-05-09T21:26:29.586742Z",
     "shell.execute_reply": "2023-05-09T21:26:29.586509Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [135], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d907ffe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.588070Z",
     "iopub.status.busy": "2023-05-09T21:26:29.587993Z",
     "iopub.status.idle": "2023-05-09T21:26:29.594487Z",
     "shell.execute_reply": "2023-05-09T21:26:29.594233Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbdfbf5",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef245c",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2010f342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.595920Z",
     "iopub.status.busy": "2023-05-09T21:26:29.595818Z",
     "iopub.status.idle": "2023-05-09T21:26:29.602142Z",
     "shell.execute_reply": "2023-05-09T21:26:29.601917Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [137], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95de88a",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ed62aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.603577Z",
     "iopub.status.busy": "2023-05-09T21:26:29.603497Z",
     "iopub.status.idle": "2023-05-09T21:26:29.747054Z",
     "shell.execute_reply": "2023-05-09T21:26:29.746779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [138], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43mDownloaderShell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md) Download\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml) List\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq) Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloader> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"Forward raw_input to frontends\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03mRaises\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m------\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mStdinNotImplementedError if active frontend doesn't support stdin.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a5dd5",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecad32d",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6d0e0256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.748590Z",
     "iopub.status.busy": "2023-05-09T21:26:29.748486Z",
     "iopub.status.idle": "2023-05-09T21:26:29.751177Z",
     "shell.execute_reply": "2023-05-09T21:26:29.750932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d03fa18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.752564Z",
     "iopub.status.busy": "2023-05-09T21:26:29.752464Z",
     "iopub.status.idle": "2023-05-09T21:26:29.754096Z",
     "shell.execute_reply": "2023-05-09T21:26:29.753876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "30fa831a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.755405Z",
     "iopub.status.busy": "2023-05-09T21:26:29.755324Z",
     "iopub.status.idle": "2023-05-09T21:26:29.756917Z",
     "shell.execute_reply": "2023-05-09T21:26:29.756702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fbe67",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3e07e31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.758219Z",
     "iopub.status.busy": "2023-05-09T21:26:29.758143Z",
     "iopub.status.idle": "2023-05-09T21:26:29.783934Z",
     "shell.execute_reply": "2023-05-09T21:26:29.783637Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8454754b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.785343Z",
     "iopub.status.busy": "2023-05-09T21:26:29.785262Z",
     "iopub.status.idle": "2023-05-09T21:26:29.791452Z",
     "shell.execute_reply": "2023-05-09T21:26:29.791219Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [143], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3e110abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.792930Z",
     "iopub.status.busy": "2023-05-09T21:26:29.792848Z",
     "iopub.status.idle": "2023-05-09T21:26:29.799124Z",
     "shell.execute_reply": "2023-05-09T21:26:29.798840Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [144], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a842d",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e6e4a",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b1136334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.800665Z",
     "iopub.status.busy": "2023-05-09T21:26:29.800551Z",
     "iopub.status.idle": "2023-05-09T21:26:29.807408Z",
     "shell.execute_reply": "2023-05-09T21:26:29.807169Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [145], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03daf2",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f8fbd7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.808882Z",
     "iopub.status.busy": "2023-05-09T21:26:29.808776Z",
     "iopub.status.idle": "2023-05-09T21:26:29.829919Z",
     "shell.execute_reply": "2023-05-09T21:26:29.829620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [146], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43mDownloaderShell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md) Download\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml) List\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq) Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloader> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"Forward raw_input to frontends\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03mRaises\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m------\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mStdinNotImplementedError if active frontend doesn't support stdin.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e566774b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcaa7cb",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cfa248ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.831328Z",
     "iopub.status.busy": "2023-05-09T21:26:29.831251Z",
     "iopub.status.idle": "2023-05-09T21:26:29.833983Z",
     "shell.execute_reply": "2023-05-09T21:26:29.833697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50cc9eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.835393Z",
     "iopub.status.busy": "2023-05-09T21:26:29.835313Z",
     "iopub.status.idle": "2023-05-09T21:26:29.837057Z",
     "shell.execute_reply": "2023-05-09T21:26:29.836819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2a78dfab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.838328Z",
     "iopub.status.busy": "2023-05-09T21:26:29.838245Z",
     "iopub.status.idle": "2023-05-09T21:26:29.839861Z",
     "shell.execute_reply": "2023-05-09T21:26:29.839646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a11ffd",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7be3f940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.841228Z",
     "iopub.status.busy": "2023-05-09T21:26:29.841144Z",
     "iopub.status.idle": "2023-05-09T21:26:29.867252Z",
     "shell.execute_reply": "2023-05-09T21:26:29.866961Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1fbaab5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.868662Z",
     "iopub.status.busy": "2023-05-09T21:26:29.868583Z",
     "iopub.status.idle": "2023-05-09T21:26:29.875278Z",
     "shell.execute_reply": "2023-05-09T21:26:29.875059Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [151], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bb3431c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.876616Z",
     "iopub.status.busy": "2023-05-09T21:26:29.876538Z",
     "iopub.status.idle": "2023-05-09T21:26:29.914679Z",
     "shell.execute_reply": "2023-05-09T21:26:29.914402Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [152], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0811f",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a1965",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "66dc6d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.916091Z",
     "iopub.status.busy": "2023-05-09T21:26:29.916008Z",
     "iopub.status.idle": "2023-05-09T21:26:29.922304Z",
     "shell.execute_reply": "2023-05-09T21:26:29.922047Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [153], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acdf11",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "82e977c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.923671Z",
     "iopub.status.busy": "2023-05-09T21:26:29.923590Z",
     "iopub.status.idle": "2023-05-09T21:26:29.930119Z",
     "shell.execute_reply": "2023-05-09T21:26:29.929879Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c758c44",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad790d93",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a28a5945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.931533Z",
     "iopub.status.busy": "2023-05-09T21:26:29.931455Z",
     "iopub.status.idle": "2023-05-09T21:26:29.933983Z",
     "shell.execute_reply": "2023-05-09T21:26:29.933764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "29eb6bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.935343Z",
     "iopub.status.busy": "2023-05-09T21:26:29.935267Z",
     "iopub.status.idle": "2023-05-09T21:26:29.936992Z",
     "shell.execute_reply": "2023-05-09T21:26:29.936774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "45808cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.938332Z",
     "iopub.status.busy": "2023-05-09T21:26:29.938250Z",
     "iopub.status.idle": "2023-05-09T21:26:29.939941Z",
     "shell.execute_reply": "2023-05-09T21:26:29.939733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc364b1e",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a41f3863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.941234Z",
     "iopub.status.busy": "2023-05-09T21:26:29.941153Z",
     "iopub.status.idle": "2023-05-09T21:26:29.967141Z",
     "shell.execute_reply": "2023-05-09T21:26:29.966864Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "81d3cd0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.968540Z",
     "iopub.status.busy": "2023-05-09T21:26:29.968461Z",
     "iopub.status.idle": "2023-05-09T21:26:29.974618Z",
     "shell.execute_reply": "2023-05-09T21:26:29.974371Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [159], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c9575fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.976007Z",
     "iopub.status.busy": "2023-05-09T21:26:29.975915Z",
     "iopub.status.idle": "2023-05-09T21:26:29.981998Z",
     "shell.execute_reply": "2023-05-09T21:26:29.981769Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [160], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f78de4",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97cfd6",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8e9bb377",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.983403Z",
     "iopub.status.busy": "2023-05-09T21:26:29.983321Z",
     "iopub.status.idle": "2023-05-09T21:26:29.989579Z",
     "shell.execute_reply": "2023-05-09T21:26:29.989341Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [161], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5be9b",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "811fb8bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.990957Z",
     "iopub.status.busy": "2023-05-09T21:26:29.990879Z",
     "iopub.status.idle": "2023-05-09T21:26:29.997399Z",
     "shell.execute_reply": "2023-05-09T21:26:29.997164Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [162], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7701af",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7c700",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a628a923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:29.998724Z",
     "iopub.status.busy": "2023-05-09T21:26:29.998649Z",
     "iopub.status.idle": "2023-05-09T21:26:30.001289Z",
     "shell.execute_reply": "2023-05-09T21:26:30.001064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "406e86bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.002616Z",
     "iopub.status.busy": "2023-05-09T21:26:30.002532Z",
     "iopub.status.idle": "2023-05-09T21:26:30.004229Z",
     "shell.execute_reply": "2023-05-09T21:26:30.004004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e5707bab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.005573Z",
     "iopub.status.busy": "2023-05-09T21:26:30.005494Z",
     "iopub.status.idle": "2023-05-09T21:26:30.007100Z",
     "shell.execute_reply": "2023-05-09T21:26:30.006873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "332f12c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.008431Z",
     "iopub.status.busy": "2023-05-09T21:26:30.008353Z",
     "iopub.status.idle": "2023-05-09T21:26:30.034917Z",
     "shell.execute_reply": "2023-05-09T21:26:30.034648Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [166], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "175f01f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.036338Z",
     "iopub.status.busy": "2023-05-09T21:26:30.036248Z",
     "iopub.status.idle": "2023-05-09T21:26:30.042488Z",
     "shell.execute_reply": "2023-05-09T21:26:30.042249Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [167], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ac6baf45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.043796Z",
     "iopub.status.busy": "2023-05-09T21:26:30.043719Z",
     "iopub.status.idle": "2023-05-09T21:26:30.049734Z",
     "shell.execute_reply": "2023-05-09T21:26:30.049499Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [168], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c6579",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c760d2",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5b81d54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.051112Z",
     "iopub.status.busy": "2023-05-09T21:26:30.051031Z",
     "iopub.status.idle": "2023-05-09T21:26:30.057263Z",
     "shell.execute_reply": "2023-05-09T21:26:30.057025Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [169], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a895a",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "57c1ce6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.058614Z",
     "iopub.status.busy": "2023-05-09T21:26:30.058532Z",
     "iopub.status.idle": "2023-05-09T21:26:30.064782Z",
     "shell.execute_reply": "2023-05-09T21:26:30.064539Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [170], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c1c65",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbc8f9",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2709e468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.066131Z",
     "iopub.status.busy": "2023-05-09T21:26:30.066051Z",
     "iopub.status.idle": "2023-05-09T21:26:30.068698Z",
     "shell.execute_reply": "2023-05-09T21:26:30.068480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "79e483c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.069999Z",
     "iopub.status.busy": "2023-05-09T21:26:30.069915Z",
     "iopub.status.idle": "2023-05-09T21:26:30.071526Z",
     "shell.execute_reply": "2023-05-09T21:26:30.071286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9bc21207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.072954Z",
     "iopub.status.busy": "2023-05-09T21:26:30.072861Z",
     "iopub.status.idle": "2023-05-09T21:26:30.074632Z",
     "shell.execute_reply": "2023-05-09T21:26:30.074369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9fc9b1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.076009Z",
     "iopub.status.busy": "2023-05-09T21:26:30.075926Z",
     "iopub.status.idle": "2023-05-09T21:26:30.102671Z",
     "shell.execute_reply": "2023-05-09T21:26:30.102396Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [174], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=8\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "62a42246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.104107Z",
     "iopub.status.busy": "2023-05-09T21:26:30.104024Z",
     "iopub.status.idle": "2023-05-09T21:26:30.110397Z",
     "shell.execute_reply": "2023-05-09T21:26:30.110180Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [175], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "acdaffe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.111705Z",
     "iopub.status.busy": "2023-05-09T21:26:30.111629Z",
     "iopub.status.idle": "2023-05-09T21:26:30.117763Z",
     "shell.execute_reply": "2023-05-09T21:26:30.117509Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [176], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded84bd2",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f4a9b",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "46a86ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.119115Z",
     "iopub.status.busy": "2023-05-09T21:26:30.119036Z",
     "iopub.status.idle": "2023-05-09T21:26:30.125186Z",
     "shell.execute_reply": "2023-05-09T21:26:30.124936Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [177], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e22a38",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f9a4e08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.126572Z",
     "iopub.status.busy": "2023-05-09T21:26:30.126488Z",
     "iopub.status.idle": "2023-05-09T21:26:30.132787Z",
     "shell.execute_reply": "2023-05-09T21:26:30.132518Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [178], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a062d5c",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34b65f",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "050ae871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.134141Z",
     "iopub.status.busy": "2023-05-09T21:26:30.134063Z",
     "iopub.status.idle": "2023-05-09T21:26:30.136684Z",
     "shell.execute_reply": "2023-05-09T21:26:30.136475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0edcccbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.138024Z",
     "iopub.status.busy": "2023-05-09T21:26:30.137948Z",
     "iopub.status.idle": "2023-05-09T21:26:30.139582Z",
     "shell.execute_reply": "2023-05-09T21:26:30.139374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f5643701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.140882Z",
     "iopub.status.busy": "2023-05-09T21:26:30.140799Z",
     "iopub.status.idle": "2023-05-09T21:26:30.142419Z",
     "shell.execute_reply": "2023-05-09T21:26:30.142181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e765427d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.143686Z",
     "iopub.status.busy": "2023-05-09T21:26:30.143614Z",
     "iopub.status.idle": "2023-05-09T21:26:30.169814Z",
     "shell.execute_reply": "2023-05-09T21:26:30.169538Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [182], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "82ad1877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.171270Z",
     "iopub.status.busy": "2023-05-09T21:26:30.171193Z",
     "iopub.status.idle": "2023-05-09T21:26:30.177512Z",
     "shell.execute_reply": "2023-05-09T21:26:30.177275Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [183], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "15e33139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.178853Z",
     "iopub.status.busy": "2023-05-09T21:26:30.178775Z",
     "iopub.status.idle": "2023-05-09T21:26:30.184785Z",
     "shell.execute_reply": "2023-05-09T21:26:30.184554Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [184], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eff988",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc92e09",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c15d1e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.186131Z",
     "iopub.status.busy": "2023-05-09T21:26:30.186050Z",
     "iopub.status.idle": "2023-05-09T21:26:30.192226Z",
     "shell.execute_reply": "2023-05-09T21:26:30.192004Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [185], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603315e6",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "640b4dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.193595Z",
     "iopub.status.busy": "2023-05-09T21:26:30.193517Z",
     "iopub.status.idle": "2023-05-09T21:26:30.199860Z",
     "shell.execute_reply": "2023-05-09T21:26:30.199623Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [186], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe8c72",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b471e6",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "94c3cebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.201263Z",
     "iopub.status.busy": "2023-05-09T21:26:30.201172Z",
     "iopub.status.idle": "2023-05-09T21:26:30.203899Z",
     "shell.execute_reply": "2023-05-09T21:26:30.203687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ab1585d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.205270Z",
     "iopub.status.busy": "2023-05-09T21:26:30.205200Z",
     "iopub.status.idle": "2023-05-09T21:26:30.206901Z",
     "shell.execute_reply": "2023-05-09T21:26:30.206685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b2ef6d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.208212Z",
     "iopub.status.busy": "2023-05-09T21:26:30.208130Z",
     "iopub.status.idle": "2023-05-09T21:26:30.209768Z",
     "shell.execute_reply": "2023-05-09T21:26:30.209539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "16c26fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.211003Z",
     "iopub.status.busy": "2023-05-09T21:26:30.210928Z",
     "iopub.status.idle": "2023-05-09T21:26:30.237441Z",
     "shell.execute_reply": "2023-05-09T21:26:30.237158Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [190], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9fb868b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.238933Z",
     "iopub.status.busy": "2023-05-09T21:26:30.238851Z",
     "iopub.status.idle": "2023-05-09T21:26:30.245083Z",
     "shell.execute_reply": "2023-05-09T21:26:30.244864Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [191], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "01b9c440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.246454Z",
     "iopub.status.busy": "2023-05-09T21:26:30.246372Z",
     "iopub.status.idle": "2023-05-09T21:26:30.252388Z",
     "shell.execute_reply": "2023-05-09T21:26:30.252157Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [192], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21523578",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd8b50",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fab929df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.253705Z",
     "iopub.status.busy": "2023-05-09T21:26:30.253625Z",
     "iopub.status.idle": "2023-05-09T21:26:30.260046Z",
     "shell.execute_reply": "2023-05-09T21:26:30.259801Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [193], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc013c",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e11825fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.261365Z",
     "iopub.status.busy": "2023-05-09T21:26:30.261281Z",
     "iopub.status.idle": "2023-05-09T21:26:30.267618Z",
     "shell.execute_reply": "2023-05-09T21:26:30.267392Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [194], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247dc8ce",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d38317",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9a7b57d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.268996Z",
     "iopub.status.busy": "2023-05-09T21:26:30.268914Z",
     "iopub.status.idle": "2023-05-09T21:26:30.271556Z",
     "shell.execute_reply": "2023-05-09T21:26:30.271336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "92b2cc01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.272923Z",
     "iopub.status.busy": "2023-05-09T21:26:30.272843Z",
     "iopub.status.idle": "2023-05-09T21:26:30.274403Z",
     "shell.execute_reply": "2023-05-09T21:26:30.274176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7fda2728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.275679Z",
     "iopub.status.busy": "2023-05-09T21:26:30.275602Z",
     "iopub.status.idle": "2023-05-09T21:26:30.277081Z",
     "shell.execute_reply": "2023-05-09T21:26:30.276853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "180b29db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.278426Z",
     "iopub.status.busy": "2023-05-09T21:26:30.278349Z",
     "iopub.status.idle": "2023-05-09T21:26:30.304856Z",
     "shell.execute_reply": "2023-05-09T21:26:30.304583Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [198], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "09c205e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.306284Z",
     "iopub.status.busy": "2023-05-09T21:26:30.306205Z",
     "iopub.status.idle": "2023-05-09T21:26:30.312372Z",
     "shell.execute_reply": "2023-05-09T21:26:30.312126Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [199], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e4b2c137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.313691Z",
     "iopub.status.busy": "2023-05-09T21:26:30.313614Z",
     "iopub.status.idle": "2023-05-09T21:26:30.319625Z",
     "shell.execute_reply": "2023-05-09T21:26:30.319390Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [200], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995ba43",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707685e7",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c6f008b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.321014Z",
     "iopub.status.busy": "2023-05-09T21:26:30.320938Z",
     "iopub.status.idle": "2023-05-09T21:26:30.327400Z",
     "shell.execute_reply": "2023-05-09T21:26:30.327169Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [201], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m rslt\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "plt.to_file('Trump.png')\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab4d21",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d5acebd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.328699Z",
     "iopub.status.busy": "2023-05-09T21:26:30.328622Z",
     "iopub.status.idle": "2023-05-09T21:26:30.334966Z",
     "shell.execute_reply": "2023-05-09T21:26:30.334721Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [202], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49114b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924a20d",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "93321190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.336326Z",
     "iopub.status.busy": "2023-05-09T21:26:30.336248Z",
     "iopub.status.idle": "2023-05-09T21:26:30.338912Z",
     "shell.execute_reply": "2023-05-09T21:26:30.338678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "980412de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.340259Z",
     "iopub.status.busy": "2023-05-09T21:26:30.340181Z",
     "iopub.status.idle": "2023-05-09T21:26:30.341964Z",
     "shell.execute_reply": "2023-05-09T21:26:30.341741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "42c8e47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.343239Z",
     "iopub.status.busy": "2023-05-09T21:26:30.343160Z",
     "iopub.status.idle": "2023-05-09T21:26:30.344760Z",
     "shell.execute_reply": "2023-05-09T21:26:30.344531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0d36e1da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.346013Z",
     "iopub.status.busy": "2023-05-09T21:26:30.345943Z",
     "iopub.status.idle": "2023-05-09T21:26:30.372460Z",
     "shell.execute_reply": "2023-05-09T21:26:30.372186Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [206], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4f343bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.373850Z",
     "iopub.status.busy": "2023-05-09T21:26:30.373771Z",
     "iopub.status.idle": "2023-05-09T21:26:30.379925Z",
     "shell.execute_reply": "2023-05-09T21:26:30.379690Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [207], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2408acc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.381351Z",
     "iopub.status.busy": "2023-05-09T21:26:30.381263Z",
     "iopub.status.idle": "2023-05-09T21:26:30.387294Z",
     "shell.execute_reply": "2023-05-09T21:26:30.387060Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [208], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3de0c",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af3885",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f5322935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.388633Z",
     "iopub.status.busy": "2023-05-09T21:26:30.388558Z",
     "iopub.status.idle": "2023-05-09T21:26:30.395044Z",
     "shell.execute_reply": "2023-05-09T21:26:30.394800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [209], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m rslt\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "plt.savefig('Trump.png')\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd44040",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cfb65e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.396457Z",
     "iopub.status.busy": "2023-05-09T21:26:30.396378Z",
     "iopub.status.idle": "2023-05-09T21:26:30.402640Z",
     "shell.execute_reply": "2023-05-09T21:26:30.402402Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [210], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2165f75",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701bae9",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a710eefd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.404008Z",
     "iopub.status.busy": "2023-05-09T21:26:30.403935Z",
     "iopub.status.idle": "2023-05-09T21:26:30.406576Z",
     "shell.execute_reply": "2023-05-09T21:26:30.406354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8c25f1b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.407877Z",
     "iopub.status.busy": "2023-05-09T21:26:30.407805Z",
     "iopub.status.idle": "2023-05-09T21:26:30.409518Z",
     "shell.execute_reply": "2023-05-09T21:26:30.409295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "19bb8c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.410805Z",
     "iopub.status.busy": "2023-05-09T21:26:30.410725Z",
     "iopub.status.idle": "2023-05-09T21:26:30.412310Z",
     "shell.execute_reply": "2023-05-09T21:26:30.412051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f6e838eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.413617Z",
     "iopub.status.busy": "2023-05-09T21:26:30.413541Z",
     "iopub.status.idle": "2023-05-09T21:26:30.440044Z",
     "shell.execute_reply": "2023-05-09T21:26:30.439760Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [214], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2e251159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.441458Z",
     "iopub.status.busy": "2023-05-09T21:26:30.441376Z",
     "iopub.status.idle": "2023-05-09T21:26:30.447566Z",
     "shell.execute_reply": "2023-05-09T21:26:30.447330Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [215], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f3af28b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.448887Z",
     "iopub.status.busy": "2023-05-09T21:26:30.448811Z",
     "iopub.status.idle": "2023-05-09T21:26:30.454816Z",
     "shell.execute_reply": "2023-05-09T21:26:30.454577Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [216], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a99dc",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d6b37",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886c346",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "21b54895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.456179Z",
     "iopub.status.busy": "2023-05-09T21:26:30.456098Z",
     "iopub.status.idle": "2023-05-09T21:26:30.462681Z",
     "shell.execute_reply": "2023-05-09T21:26:30.462414Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()\n",
    "plt.savefig('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e5b15",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5d9df8c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.464013Z",
     "iopub.status.busy": "2023-05-09T21:26:30.463940Z",
     "iopub.status.idle": "2023-05-09T21:26:30.470169Z",
     "shell.execute_reply": "2023-05-09T21:26:30.469947Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [218], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d3ad0",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221d990",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "88ebf49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.471548Z",
     "iopub.status.busy": "2023-05-09T21:26:30.471471Z",
     "iopub.status.idle": "2023-05-09T21:26:30.474034Z",
     "shell.execute_reply": "2023-05-09T21:26:30.473797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c61c5077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.475360Z",
     "iopub.status.busy": "2023-05-09T21:26:30.475285Z",
     "iopub.status.idle": "2023-05-09T21:26:30.476901Z",
     "shell.execute_reply": "2023-05-09T21:26:30.476643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "de245ea7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.478242Z",
     "iopub.status.busy": "2023-05-09T21:26:30.478161Z",
     "iopub.status.idle": "2023-05-09T21:26:30.479667Z",
     "shell.execute_reply": "2023-05-09T21:26:30.479443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f69e9075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.480951Z",
     "iopub.status.busy": "2023-05-09T21:26:30.480858Z",
     "iopub.status.idle": "2023-05-09T21:26:30.507237Z",
     "shell.execute_reply": "2023-05-09T21:26:30.506946Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [222], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "914a3d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.508626Z",
     "iopub.status.busy": "2023-05-09T21:26:30.508540Z",
     "iopub.status.idle": "2023-05-09T21:26:30.514676Z",
     "shell.execute_reply": "2023-05-09T21:26:30.514450Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [223], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a2e76b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.515966Z",
     "iopub.status.busy": "2023-05-09T21:26:30.515886Z",
     "iopub.status.idle": "2023-05-09T21:26:30.522018Z",
     "shell.execute_reply": "2023-05-09T21:26:30.521783Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [224], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a832de7",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2309b2",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012e952",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "387cd1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.523325Z",
     "iopub.status.busy": "2023-05-09T21:26:30.523245Z",
     "iopub.status.idle": "2023-05-09T21:26:30.529788Z",
     "shell.execute_reply": "2023-05-09T21:26:30.529563Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [225], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c04a63",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0da25dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.531146Z",
     "iopub.status.busy": "2023-05-09T21:26:30.531066Z",
     "iopub.status.idle": "2023-05-09T21:26:30.537385Z",
     "shell.execute_reply": "2023-05-09T21:26:30.537146Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [226], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026d56f",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d478b",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d8a0a85c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.538764Z",
     "iopub.status.busy": "2023-05-09T21:26:30.538686Z",
     "iopub.status.idle": "2023-05-09T21:26:30.541334Z",
     "shell.execute_reply": "2023-05-09T21:26:30.541106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c243ee58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.542710Z",
     "iopub.status.busy": "2023-05-09T21:26:30.542631Z",
     "iopub.status.idle": "2023-05-09T21:26:30.544374Z",
     "shell.execute_reply": "2023-05-09T21:26:30.544115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e55e7bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.545743Z",
     "iopub.status.busy": "2023-05-09T21:26:30.545666Z",
     "iopub.status.idle": "2023-05-09T21:26:30.547300Z",
     "shell.execute_reply": "2023-05-09T21:26:30.547083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ebc25ccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.548607Z",
     "iopub.status.busy": "2023-05-09T21:26:30.548534Z",
     "iopub.status.idle": "2023-05-09T21:26:30.575361Z",
     "shell.execute_reply": "2023-05-09T21:26:30.575090Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [230], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6584e2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.576915Z",
     "iopub.status.busy": "2023-05-09T21:26:30.576801Z",
     "iopub.status.idle": "2023-05-09T21:26:30.583184Z",
     "shell.execute_reply": "2023-05-09T21:26:30.582972Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [231], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6bc1ce99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.584612Z",
     "iopub.status.busy": "2023-05-09T21:26:30.584528Z",
     "iopub.status.idle": "2023-05-09T21:26:30.590723Z",
     "shell.execute_reply": "2023-05-09T21:26:30.590470Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [232], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499cbf6",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd252a",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb939c2",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "fe74e6d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.592083Z",
     "iopub.status.busy": "2023-05-09T21:26:30.591999Z",
     "iopub.status.idle": "2023-05-09T21:26:30.598576Z",
     "shell.execute_reply": "2023-05-09T21:26:30.598369Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [233], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m rslt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "rslt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2bbca",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a042e9b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.599927Z",
     "iopub.status.busy": "2023-05-09T21:26:30.599849Z",
     "iopub.status.idle": "2023-05-09T21:26:30.606247Z",
     "shell.execute_reply": "2023-05-09T21:26:30.606011Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [234], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c868ee3",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1919a94",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "180bb8d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.607585Z",
     "iopub.status.busy": "2023-05-09T21:26:30.607500Z",
     "iopub.status.idle": "2023-05-09T21:26:30.610188Z",
     "shell.execute_reply": "2023-05-09T21:26:30.609943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b5ce89a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.611467Z",
     "iopub.status.busy": "2023-05-09T21:26:30.611383Z",
     "iopub.status.idle": "2023-05-09T21:26:30.613095Z",
     "shell.execute_reply": "2023-05-09T21:26:30.612879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0e417191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.614458Z",
     "iopub.status.busy": "2023-05-09T21:26:30.614382Z",
     "iopub.status.idle": "2023-05-09T21:26:30.615957Z",
     "shell.execute_reply": "2023-05-09T21:26:30.615732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "65d89af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.617245Z",
     "iopub.status.busy": "2023-05-09T21:26:30.617169Z",
     "iopub.status.idle": "2023-05-09T21:26:30.643291Z",
     "shell.execute_reply": "2023-05-09T21:26:30.643027Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [238], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8742a3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.644707Z",
     "iopub.status.busy": "2023-05-09T21:26:30.644624Z",
     "iopub.status.idle": "2023-05-09T21:26:30.650788Z",
     "shell.execute_reply": "2023-05-09T21:26:30.650538Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [239], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "867103d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.652170Z",
     "iopub.status.busy": "2023-05-09T21:26:30.652086Z",
     "iopub.status.idle": "2023-05-09T21:26:30.658150Z",
     "shell.execute_reply": "2023-05-09T21:26:30.657919Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [240], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99d3b1",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c4b35e",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1141229",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7ca123a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T21:26:30.659496Z",
     "iopub.status.busy": "2023-05-09T21:26:30.659409Z",
     "iopub.status.idle": "2023-05-09T21:26:30.665671Z",
     "shell.execute_reply": "2023-05-09T21:26:30.665420Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [241], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
