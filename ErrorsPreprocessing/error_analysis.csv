File,Original_Index,Error_Cell,Fixed_Index,Fixed_Cell,Error_Type
./executed/file1.ipynb,21,"print(f'intercept: {reg.intercept_}')
coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
print(coef)",23,"print(f'intercept: {reg.intercept_}')
coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
print(coef)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file1.ipynb,30,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data()",32,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file1.ipynb,46,"prediction_data = pd.merge(unemploy_wage_data, prediction_data, on=['uu_id'], how='inner')
prediction_data.head()",47,"prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
prediction_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file2.ipynb,6,unemployment_data.columns,14,unemployment_data,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file2.ipynb,9,len(unemployment_data),14,unemployment_data,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file2.ipynb,25,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),28,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,28,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),31,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,31,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),35,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,35,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),38,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,38,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),41,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,41,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),44,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,44,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),49,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,49,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),52,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,52,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),55,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,55,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),58,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,65,predict_claims('00f962ce727b8dbbf20925abd5a253dd'),68,predict_claims('00f962ce727b8dbbf20925abd5a253dd'),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file2.ipynb,72,uuids[:10],75,uuids[:10],[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file2.ipynb,77,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),81,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,81,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),84,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,84,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),87,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,87,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),90,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,90,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),93,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,93,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),96,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,96,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),102,"predict_claims('066fa7cc5a96dcdeca8485c68e2993b8',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,99,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),102,"predict_claims('066fa7cc5a96dcdeca8485c68e2993b8',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file2.ipynb,129,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",136,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file2.ipynb,142,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",145,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file2.ipynb,148,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",154,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mUnboundLocalError[0m
./executed/file2.ipynb,151,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",154,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mUnboundLocalError[0m
./executed/file2.ipynb,154,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",157,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file3.ipynb,28,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",32,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,52,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",64,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,56,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",64,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,60,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",64,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,68,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",76,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file3.ipynb,72,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",76,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,84,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",87,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,90,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",96,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,93,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",96,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,110,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
data.plot(subplots=True, figsize=(20,24))",114,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.plot(subplots=True, figsize=(20,24))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file3.ipynb,118,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.cor()",122,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.corr()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file3.ipynb,140,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",142,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file3.ipynb,168,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",170,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file4.ipynb,110,"# d = load_raw_full()
# run_prophet(get_subset(d, 6))
plot_prophet(n=36)",120,"# d = load_raw_full()
# run_prophet(get_subset(d, 6))
plot_prophet(n=36)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file4.ipynb,115,"# d = load_raw_full()
# run_prophet(get_subset(d, 6))
plot_prophet(n=36)",120,"# d = load_raw_full()
# run_prophet(get_subset(d, 6))
plot_prophet(n=36)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file5.ipynb,18,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file5.ipynb,19,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file5.ipynb,21,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file5.ipynb,38,"plt.figure(figsize=(12,10))
cor = X.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file5.ipynb,39,"plt.figure(figsize=(12,10))
cor = pd.dataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file5.ipynb,40,"plt.figure(figsize=(12,10))
cor = pd.dataframe(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file6.ipynb,18,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file6.ipynb,19,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file6.ipynb,21,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file6.ipynb,38,"plt.figure(figsize=(12,10))
cor = X.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file6.ipynb,39,"plt.figure(figsize=(12,10))
cor = pd.dataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file6.ipynb,40,"plt.figure(figsize=(12,10))
cor = pd.dataframe(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file7.ipynb,5,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,13,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,15,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,23,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",25,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,27,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,29,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file7.ipynb,31,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file7.ipynb,33,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file7.ipynb,35,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,5,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,13,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,15,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,23,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",25,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,27,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,29,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file8.ipynb,31,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file8.ipynb,33,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file8.ipynb,35,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file9.ipynb,230,"# Importing the required libraries
import nltk
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",244,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file9.ipynb,244,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",258,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",[0;31m---------------------------------------------------------------------------[0m[0;31mStdinNotImplementedError[0m
./executed/file10.ipynb,5,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unempolyment_data = query_job.to_dataframe()
unemployment_data.head()",12,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file10.ipynb,6,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",12,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file10.ipynb,8,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unempolyment_data = query_job.to_dataframe()
unemployment_data.head()",12,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file10.ipynb,9,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",12,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file10.ipynb,11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unempolyment_data = query_job.to_dataframe()
unemployment_data.head()",12,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file10.ipynb,14,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unempolyment_data = query_job.to_dataframe()
unemployment_data.head()",15,"query_job = bigquery_client.query(query2)
wage_data = query_job.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file10.ipynb,17,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unempolyment_data = query_job.to_dataframe()
unemployment_data.head()",18,"query_job2 = bigquery_client.query(query2)
wage_data = query_job2.to_dataframe()
wage_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file11.ipynb,8,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",12,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file11.ipynb,10,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",12,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file11.ipynb,18,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",20,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file11.ipynb,20,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",22,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file12.ipynb,38,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
final_data",39,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')
final_data",[0;36m
./executed/file12.ipynb,47,fin.shape(),48,fin.shape,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file12.ipynb,100,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file12.ipynb,101,"tt = pred_data.merge(fin,how = 'left', final_data= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file13.ipynb,7,"query_job = bigquery_client.query(query)
print(query_job)
unemployment_data = query_job.to_dataframe()
print(unemployment_cases_data)",9,"query_job = bigquery_client.query(query)
print(query_job)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file13.ipynb,43,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",49,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file13.ipynb,45,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",49,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file13.ipynb,47,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",49,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file13.ipynb,53,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",57,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file13.ipynb,55,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",57,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file13.ipynb,65,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",67,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file13.ipynb,82,"# Drop duplicate columns
relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis=1,thresh=1).info()",83,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis=1,thresh=1).info()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file13.ipynb,84,"# Drop duplicate columns
relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns').info()",85,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns').info()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file13.ipynb,86,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True).info()",87,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True)
relevant_unemployment_df",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file13.ipynb,89,relevant_unemployment_df[1],93,relevant_unemployment_df['top_category_employer1'],[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file13.ipynb,90,relevant_unemployment_df['top_category_employer1'][0,91,relevant_unemployment_df['top_category_employer1'][0],[0;36m
./executed/file13.ipynb,92,relevant_unemployment_df['top_category_employer1'][0].dtype,93,relevant_unemployment_df['top_category_employer1'],[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file13.ipynb,96,relevant_unemployment_df['uu_id'][0,97,relevant_unemployment_df['uu_id'][0],[0;36m
./executed/file13.ipynb,105,relevant_unemployment_df.convert_dtypes(inplace=True),106,relevant_unemployment_df.convert_dtypes(),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file13.ipynb,107,"# Drop duplicate columns
relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True)
relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()",113,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True)
relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].apply(str)
relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].apply(str)
relevant_unemployment_df.info()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file14.ipynb,1,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.drop_duplicates(inplace = True)
print(unemployment_data)",11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file14.ipynb,3,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.drop_duplicates(inplace = True)
print(unemployment_data)",11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file14.ipynb,9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file15.ipynb,58,tract[0].head(),63,tract_dict[0].head(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file15.ipynb,82,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file15.ipynb,83,"#compare the previous predictor to the new predictor
tract_dict[1][""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file15.ipynb,84,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file15.ipynb,86,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",87,"#compare the previous predictor to the new predictor
tract_dict[1][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file16.ipynb,12,"df3 = df.copy()
df3.columns",17,"df3 = df.copy()
df3.columns",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file17.ipynb,53,"df1 = df.head(25)
df1.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()",60,"df1 = df.head(25)
df1.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file18.ipynb,1,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",5,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file18.ipynb,3,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",5,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file19.ipynb,8,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",13,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file19.ipynb,10,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",13,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file19.ipynb,15,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",20,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file19.ipynb,18,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",20,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file20.ipynb,81,data_balance.head（）,82,data_balance.head(),[0;36m
./executed/file20.ipynb,90,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    print(data_balance[data_balance[""uu_id""] == uu_id_list[i] and data_balance[""week_numer""] == 4])
    break",91,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    print(data_balance[data_balance[""uu_id""] == uu_id_list[i] & data_balance[""week_numer""] == 4])
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file20.ipynb,91,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    print(data_balance[data_balance[""uu_id""] == uu_id_list[i] & data_balance[""week_numer""] == 4])
    break",92,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    print(data_balance[data_balance[""uu_id""] == uu_id_list[i] & data_balance[""week_number""] == 4])
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file20.ipynb,92,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    print(data_balance[data_balance[""uu_id""] == uu_id_list[i] & data_balance[""week_number""] == 4])
    break",94,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    df = data_balance[(data_balance[""uu_id""] == uu_id_list[i]) & (data_balance[""week_number""] == 4)]
    print(df)
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file20.ipynb,93,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    print(data_balance[data_balance[""uu_id""] == uu_id_list[i] && data_balance[""week_number""] == 4])
    break",94,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    df = data_balance[(data_balance[""uu_id""] == uu_id_list[i]) & (data_balance[""week_number""] == 4)]
    print(df)
    break",[0;36m
./executed/file20.ipynb,116,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
    week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]
    print(week_3[0])
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']]= (week_3 + week_5)/2
    
    week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
    week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
    
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = (week_22 + week_24)/2
    break",117,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
    week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]
    print(week_3.item())
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']]= (week_3 + week_5)/2
    
    week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
    week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
    
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = (week_22 + week_24)/2
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file20.ipynb,117,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
    week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]
    print(week_3.item())
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']]= (week_3 + week_5)/2
    
    week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
    week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
    
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = (week_22 + week_24)/2
    break",118,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
    week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]
    print(week_3.values[0])
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']]= (week_3 + week_5)/2
    
    week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
    week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
    
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = (week_22 + week_24)/2
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file20.ipynb,126,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
    week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]

    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']]= (week_3.values[0][0] + week_5.values[0][0])/2
    
    week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
    week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
    
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = (week_22.values[0][0] + week_24.values[0][0])/2
    break",127,"uu_id_list = pd.unique(data_balance[""uu_id""])
# In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
for i in range(len(uu_id_list)):
    print(i)
    week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
    week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]

    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']]= int((week_3.values[0][0] + week_5.values[0][0])/2)
    
    week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
    week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
    
    data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = int((week_22.values[0][0] + week_24.values[0][0])/2)
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file21.ipynb,6,unemployment_data.head(,7,unemployment_data.head(),[0;36m
./executed/file21.ipynb,21,unmeployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file21.ipynb,22,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,23,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,25,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,26,unemployment_data['total_claims'].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,28,"unemployment_data.groupby('week_number','race_asian')['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,29,"unemployment_data.groupby(('week_number','race_asian'))['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,30,"unemployment_data.groupby[('week_number','race_asian')]['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file21.ipynb,33,unemployment_data.groupby('uuid')['total_claims'].mean(),34,unemployment_data.groupby('uu_id')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file21.ipynb,42,prediction_list.head(,43,prediction_list.head(),[0;36m
./executed/file21.ipynb,76,unemplyment_data.value_counts(),77,unemployment_data.value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file22.ipynb,4,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)
9/59:
#SPLIT training set",13,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",[0;36m
./executed/file22.ipynb,9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",13,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file22.ipynb,25,"#For retrieving the slope:
print(regressor.coef_)
 y_pred = regressor.predict(X_test)",39,"#For retrieving the slope:
print(regressor.coef_)
y_pred = regressor.predict(X_test)",[0;36m
./executed/file22.ipynb,32,"#For retrieving the slope:
print(regressor.coef_)
 y_pred = regressor.predict(X_test)",39,"#For retrieving the slope:
print(regressor.coef_)
y_pred = regressor.predict(X_test)",[0;36m
./executed/file22.ipynb,40,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",41,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file23.ipynb,7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file23.ipynb,9,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file23.ipynb,15,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",21,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file23.ipynb,17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",21,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file23.ipynb,19,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",21,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file24.ipynb,15,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
data.iloc[X_test.index(),'uu_id']",19,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
temp",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file24.ipynb,16,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
data.iloc[X_test.index,'uu_id']",19,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
temp",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file24.ipynb,20,"a=pd.Dataframe(data=temp)
a",21,"a=pd.DataFrame(data=temp)
a",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file24.ipynb,22,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a",24,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file24.ipynb,25,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a[count].astype('int')
a",26,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file24.ipynb,27,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;36m
./executed/file24.ipynb,28,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a.loc[:,columns]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file24.ipynb,29,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a.iloc[:,columns]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file24.ipynb,30,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[[columns]]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file24.ipynb,31,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week','count']]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file24.ipynb,34,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a.rename('week_number':'week')
a",35,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a.rename(columns={'week_number':'week'})
a",[0;36m
./executed/file24.ipynb,39,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a['week_number']=a['week_number+1']
a=a.rename(columns={'week_number':'week'})
a.to_string(index=False)",40,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a['week_number']=a['week_number']+1
a=a.rename(columns={'week_number':'week'})
a.to_string(index=False)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file25.ipynb,85,"results_df = pd.DataFrame.from_dict(results, orient='index')
results_df.columns = ['actual', 'prediction']
results_df.explode(['actual','prediction])",86,"results_df = pd.DataFrame.from_dict(results, orient='index')
results_df.columns = ['actual', 'prediction']
results_df.explode(['actual','prediction'])",[0;36m
./executed/file26.ipynb,124,pred,130,pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file26.ipynb,128,pred,130,pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file27.ipynb,124,pred,130,pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file27.ipynb,128,pred,130,pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file28.ipynb,6,unemployment_data.head(,7,unemployment_data.head(),[0;36m
./executed/file28.ipynb,21,unmeployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file28.ipynb,22,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,23,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,25,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,26,unemployment_data['total_claims'].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,28,"unemployment_data.groupby('week_number','race_asian')['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,29,"unemployment_data.groupby(('week_number','race_asian'))['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,30,"unemployment_data.groupby[('week_number','race_asian')]['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file28.ipynb,33,unemployment_data.groupby('uuid')['total_claims'].mean(),34,unemployment_data.groupby('uu_id')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file28.ipynb,42,prediction_list.head(,43,prediction_list.head(),[0;36m
./executed/file28.ipynb,76,unemplyment_data.value_counts(),77,unemployment_data.value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file29.ipynb,4,"plt.figure()
for key1,values1 in vals.items():
    #print(values1)
    if key1!=""0.4"": continue
    C_arr = []
    R_arr = []
    for key2,values2 in values1.items():
        R = values2[0]
        C = values2[1]
        C_arr.append(C)
        R_arr.append(R)
    plt.plot(C_arr,label=""Clean samples"",linewidth=3)
    plt.plot(R_arr,label=""Refurbished samples"",linewidth=3)
plt.legend()
plt.xticks(ticks=range(0,5),labels=[""6"",""7"",""8"",""9"",""10""])
plt.grid()
plt.xlabel(""epochs"")
plt.ylabel(""Sample count"")
plt.savefig(""clean-samples.png"")",13,"plt.figure()
for key1,values1 in vals.items():
    #print(values1)
    loss_arr = []
    err_arr = []
    for key2,values2 in values1.items():
        loss = values2[0]
        acc = values2[1]
        err = 100-acc
        loss_arr.append(loss)
        err_arr.append(err)
    plt.plot(loss_arr,label=""Noise rate=""+key1)
    #plt.plot(err_arr,label=""Noise rate=""+key1)
plt.legend()
plt.grid()
plt.xlabel(""epochs"")
plt.ylabel(""Test-loss"")
plt.savefig(""test-loss.png"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file30.ipynb,15,"query_job = bigquery_client.query(query)
unemployment_data_table = query_job.to_dataframe()
print unemployment_data_table",17,"query_job = bigquery_client.query(query)
unemployment_data_table = query_job.to_dataframe()
print(unemployment_data_table)",[0;36m
./executed/file30.ipynb,23,"query_job = bigquery_client.query(query)
unemployment_total_claims_by_week = query_job.to_dataframe()
print(""Columns:"")
print('\n'.join(unemployment_total_claims_by_week.columns))
print(""\nResults:"")
print(unemployment_total_claims_by_week.head())",27,"query_job = bigquery_client.query(query)
unemployment_data_table = query_job.to_dataframe()
print(""Columns:"")
print('\n'.join(unemployment_data_table.columns))
print(""\nResults:"")
print(unemployment_data_table.head())",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file31.ipynb,8,"pip install google-cloud-bigquery
pip install google-cloud-bigquery[pandas]",9,"get_ipython().system('pip install google-cloud-bigquery')
get_ipython().system('pip install google-cloud-bigquery[pandas]')",[0;36m
./executed/file32.ipynb,53,"df1 = df.head(25)
df1.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()",60,"df1 = df.head(25)
df1.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file33.ipynb,49,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",52,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file33.ipynb,77,"transformed_data, best_lambda = boxcox(unemploy1)
sns.distpolt(transformed_data,)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file33.ipynb,78,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distpolt(transformed_data,)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file33.ipynb,79,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distpolt(transformed_data)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file33.ipynb,81,"claim = np.array(transformed_data)
sqrt_claim = np.sqrt(transformed_data)
log_claim = np.logtransformed_data)
f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
#f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
f1.hist(claim, 30)
f2.hist(sqrt_claim, 30)
f3.hist(log_claim, 30)",83,"claim = np.array(transformed_data)
sqrt_claim = np.sqrt(transformed_data)
log_claim = np.log(transformed_data)
f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
#f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
f1.hist(claim, 30)
f2.hist(sqrt_claim, 30)
f3.hist(log_claim, 30)",[0;36m
./executed/file33.ipynb,95,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy[""edu_grades_9_11""])",96,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy1[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file33.ipynb,96,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy1[""edu_grades_9_11""])",97,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(unemploy1[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file33.ipynb,119,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1),columns=unemploy1)",122,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
sns.distplot(unemploy_ish)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file33.ipynb,120,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_quiv""]))",122,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
sns.distplot(unemploy_ish)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file33.ipynb,128,"stats.probplot(unemploy_ish,dist=""norm"",plot=pylab)
pylab.show()",132,"stats.probplot(unemploy_ish1,dist=""norm"",plot=pylab)
pylab.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file33.ipynb,154,"sns.distplot(unemploy1[""race_whaite""])",155,"sns.distplot(unemploy1[""race_white""])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file33.ipynb,159,"sns.distplot(unemploy1[""race_noanswere""])",160,"sns.distplot(unemploy1[""race_noanswer""])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file33.ipynb,164,"trial[""uuid""].unique()",165,"trial[""uu_id""].unique()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file34.ipynb,49,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",52,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file34.ipynb,77,"transformed_data, best_lambda = boxcox(unemploy1)
sns.distpolt(transformed_data,)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file34.ipynb,78,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distpolt(transformed_data,)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file34.ipynb,79,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distpolt(transformed_data)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file34.ipynb,81,"claim = np.array(transformed_data)
sqrt_claim = np.sqrt(transformed_data)
log_claim = np.logtransformed_data)
f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
#f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
f1.hist(claim, 30)
f2.hist(sqrt_claim, 30)
f3.hist(log_claim, 30)",83,"claim = np.array(transformed_data)
sqrt_claim = np.sqrt(transformed_data)
log_claim = np.log(transformed_data)
f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
#f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
f1.hist(claim, 30)
f2.hist(sqrt_claim, 30)
f3.hist(log_claim, 30)",[0;36m
./executed/file34.ipynb,95,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy[""edu_grades_9_11""])",96,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy1[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file34.ipynb,96,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy1[""edu_grades_9_11""])",97,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(unemploy1[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file34.ipynb,119,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1),columns=unemploy1)",122,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
sns.distplot(unemploy_ish)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file34.ipynb,120,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_quiv""]))",122,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
sns.distplot(unemploy_ish)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file34.ipynb,128,"stats.probplot(unemploy_ish,dist=""norm"",plot=pylab)
pylab.show()",132,"stats.probplot(unemploy_ish1,dist=""norm"",plot=pylab)
pylab.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file34.ipynb,154,"sns.distplot(unemploy1[""race_whaite""])",155,"sns.distplot(unemploy1[""race_white""])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file34.ipynb,159,"sns.distplot(unemploy1[""race_noanswere""])",160,"sns.distplot(unemploy1[""race_noanswer""])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file34.ipynb,164,"trial[""uuid""].unique()",165,"trial[""uu_id""].unique()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file35.ipynb,4,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
pdf = query1_job.to_dataframe()
pdf.head()",11,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
pdf = query1_job.to_dataframe()
pdf.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file36.ipynb,8,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",11,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file37.ipynb,29,"for i in data.columns:
    if data[i].isnull().sum()>= 0.5*len(data)
    data=data.drop(i,axis=1)",40,"for i in data.columns:
    if data[i].isnull().sum()>0:
        print(data[i].value_counts())",[0;36m
./executed/file37.ipynb,41,"for i in data.columns:
    if data[i].isnull().sum()>0:
        print('The value counts of feature',i
        print(data[i].value_counts())",42,"for i in data.columns:
    if data[i].isnull().sum()>0:
        print('The value counts of feature',i)
        print(data[i].value_counts())",[0;36m
./executed/file38.ipynb,42,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
pylab.show()",44,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
pylab.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file38.ipynb,52,"scaled.plot.hist(subplots=True, legend=True, layout=(8, 2))",53,"scaled.plot.hist(subplots=True, legend=True, layout=(9, 2))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file38.ipynb,55,scale_log,56,scale_logs,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file38.ipynb,67,sns.distplot(unemp_sqrt[edu_grades_9_11]),68,"sns.distplot(unemp_sqrt[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file38.ipynb,83,"print('statistics=%.3f, p=%.3f\n' %(statisticss, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",85,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file39.ipynb,4,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)
9/59:
#SPLIT training set",13,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",[0;36m
./executed/file39.ipynb,9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",13,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file39.ipynb,25,"#For retrieving the slope:
print(regressor.coef_)
 y_pred = regressor.predict(X_test)",39,"#For retrieving the slope:
print(regressor.coef_)
y_pred = regressor.predict(X_test)",[0;36m
./executed/file39.ipynb,32,"#For retrieving the slope:
print(regressor.coef_)
 y_pred = regressor.predict(X_test)",39,"#For retrieving the slope:
print(regressor.coef_)
y_pred = regressor.predict(X_test)",[0;36m
./executed/file39.ipynb,40,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",41,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file40.ipynb,14,"for x in c:
    xCount.append(xValues.count(x))",28,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file40.ipynb,22,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(xValues,xCount)",25,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file40.ipynb,31,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xxValues)
plt.bar(c,xCount)",34,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xValues))
plt.bar(c,xCount)",[0;36m
./executed/file40.ipynb,42,"xValues = []
xCount = []
for i in uuid[:2]:
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",44,"for i in uuid[:2]:
    xValues = []
    xCount = []
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file41.ipynb,1,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.drop_duplicates(inplace = True)
print(unemployment_data)",11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file41.ipynb,3,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.drop_duplicates(inplace = True)
print(unemployment_data)",11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file41.ipynb,9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",11,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file42.ipynb,7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file42.ipynb,9,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file42.ipynb,15,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",21,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file42.ipynb,17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",21,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file42.ipynb,19,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",21,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file43.ipynb,124,pred,130,pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file43.ipynb,128,pred,130,pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file44.ipynb,3,"query = """"""
SELECT *
FROM `ironhacks-data.ironhacks_competition.unemployment_data`
""""""
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
covid19_cases_data.head()",13,"query = """"""
SELECT *
FROM `ironhacks-data.ironhacks_competition.unemployment_data`
""""""
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
covid19_cases_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file45.ipynb,46,"prediction_data[uu_id = ""bbcb018f0e5e49e13636f6e78ce9f60f""]",57,"df_pred.where(df_pred[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f"")",[0;36m
./executed/file45.ipynb,47,"prediction_data[uu_id == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",57,"df_pred.where(df_pred[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file45.ipynb,48,prediction_data[],49,prediction_data,[0;36m
./executed/file45.ipynb,65,"df_un[""uu_id"" == ""bbcb018f0e5e49e13636f6e78ce9f60f""]).dropna()",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;36m
./executed/file45.ipynb,66,"df_un[""uu_id"" == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file45.ipynb,67,"df[df_un[""uu_id""] = ""bbcb018f0e5e49e13636f6e78ce9f60f""]]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;36m
./executed/file45.ipynb,68,"df[df_un[""uu_id""] = ""bbcb018f0e5e49e13636f6e78ce9f60f""]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;36m
./executed/file45.ipynb,69,"df[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file45.ipynb,71,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sortby(""week_number"")",73,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""])",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file45.ipynb,72,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort(by = ""week_number"")",73,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""])",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file46.ipynb,24,"query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(3)",31,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file47.ipynb,15,"query_job = bigquery_client.query(query)
unemployment_data_table = query_job.to_dataframe()
print unemployment_data_table",17,"query_job = bigquery_client.query(query)
unemployment_data_table = query_job.to_dataframe()
print(unemployment_data_table)",[0;36m
./executed/file47.ipynb,23,"query_job = bigquery_client.query(query)
unemployment_total_claims_by_week = query_job.to_dataframe()
print(""Columns:"")
print('\n'.join(unemployment_total_claims_by_week.columns))
print(""\nResults:"")
print(unemployment_total_claims_by_week.head())",27,"query_job = bigquery_client.query(query)
unemployment_data_table = query_job.to_dataframe()
print(""Columns:"")
print('\n'.join(unemployment_data_table.columns))
print(""\nResults:"")
print(unemployment_data_table.head())",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file48.ipynb,6,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_dataframe(data['date'])
data.head()",10,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file48.ipynb,8,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_dataframe(data['date'])
data.head()",10,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,28,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
print(data)
# data['date']=pd.to_datetime(data['date'])
data.dtype()",34,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
print(data)
# data['date']=pd.to_datetime(data['date'])
# pd.dtype()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,30,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
# print(data)
# data['date']=pd.to_datetime(data['date'])
pd.dtype()",34,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
print(data)
# data['date']=pd.to_datetime(data['date'])
# pd.dtype()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,32,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
print(data)
# data['date']=pd.to_datetime(data['date'])
# pd.dtype()",34,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
print(data)
# data['date']=pd.to_datetime(data['date'])
# pd.dtype()",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file48.ipynb,36,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = pd.Dataframe()
print(data)
# data['date']=pd.to_datetime(data['date'])
# pd.dtype()",42,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,38,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = pd.Dataframe()
data['date']=pd.to_datetime(data['date'])
pd.dtype()",42,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,40,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
pd.dtype()",42,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,50,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data[:,:]",52,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file48.ipynb,54,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.data_type()",56,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
type(data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,70,"query_job = bigquery_client.query(query)
# data = query_job.to_dataframe()
df = pd.DataFrame(data)
data['date']=pd.to_datetime(data['date'])
df.dtype()",72,"query_job = bigquery_client.query(query)
# data = query_job.to_dataframe()
df = pd.DataFrame(data)
data['date']=pd.to_datetime(data['date'])
df.dtypes",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file48.ipynb,102,"X=data.drop(""date"",""potential_water_deficit"",1)
y=data[""mean_temperature""]
print(X)
print(y)
# data.head()",103,"X=data.drop([""date"",""potential_water_deficit""],1)
y=data[""mean_temperature""]
print(X)
print(y)
# data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file49.ipynb,39,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    rows.append(list(uuid, 39, predict_claims(uuid, 39))",41,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    rows.append(list(uuid, 39, predict_claims(uuid, 39)))",[0;36m
./executed/file49.ipynb,41,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    rows.append(list(uuid, 39, predict_claims(uuid, 39)))",43,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    rows.append([uuid, 39, predict_claims(uuid, 39)])",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file50.ipynb,8,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",11,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file51.ipynb,29,"get_ipython().system('pip install localpip')
localpip install neuralprophet",30,"get_ipython().system('pip install localpip')
get_ipython().system('localpip install neuralprophet')",[0;36m
./executed/file52.ipynb,42,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
pylab.show()",44,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
pylab.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file52.ipynb,52,"scaled.plot.hist(subplots=True, legend=True, layout=(8, 2))",53,"scaled.plot.hist(subplots=True, legend=True, layout=(9, 2))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file52.ipynb,55,scale_log,56,scale_logs,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file52.ipynb,67,sns.distplot(unemp_sqrt[edu_grades_9_11]),68,"sns.distplot(unemp_sqrt[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file52.ipynb,83,"print('statistics=%.3f, p=%.3f\n' %(statisticss, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",85,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file53.ipynb,28,y_true=test.loc[['week_number'==35]],33,"y_true=test.loc[test['week_number']==35]
y_true",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file53.ipynb,29,y_true=test.loc[test['week_number'==35]],33,"y_true=test.loc[test['week_number']==35]
y_true",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file53.ipynb,30,test['week_number],31,test['week_number'],[0;36m
./executed/file53.ipynb,34,"y_true=test.loc[test['week_number']==35]
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",36,"y_true=test.loc[test['week_number']==35]
print(y_true.shape)
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file53.ipynb,35,"y_true=test.loc[test['week_number']==35]
y_true.shape
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",36,"y_true=test.loc[test['week_number']==35]
print(y_true.shape)
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file54.ipynb,22,wage_data.order_by('uu_id'),23,wage_data.sort_values('uu_id'),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file54.ipynb,54,"relevant_unemployment_df['week_number'].value_counts().plot.bar()
plt.plot(relevant_unemployment_df)",59,relevant_unemployment_df['week_number'].value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,56,relevant_unemployment_df['uu_id'].value_counts().values(),57,relevant_unemployment_df['uu_id'].value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,58,"plt.plot(relevant_unemployment_df['week_number'],relevant_unemployment_df['week_number'].value_counts())",60,plt.plot(relevant_unemployment_df['week_number'].value_counts()),[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file54.ipynb,61,plt.bar.plot(relevant_unemployment_df['week_number'].value_counts()),68,pd.DataFrame(relevant_unemployment_df.groupby(['week_number'])['week_number'].count()),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file54.ipynb,62,plt.bar(relevant_unemployment_df['week_number'].value_counts()),63,relevant_unemployment_df.groupby(['week_number'])['week_number'].count(),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,65,pd.DataFrame(relevant_unemployment_df.groupby(['week_number'])['week_number'].count()),68,pd.DataFrame(relevant_unemployment_df.groupby(['week_number'])['week_number'].count()),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file54.ipynb,94,"relevant_unemployment_df.loc[relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"" & relevant_unemployment_df.week_number == 1,]",99,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") & (relevant_unemployment_df.week_number == 1)),]",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,95,"relevant_unemployment_df.loc[relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"" and relevant_unemployment_df.week_number == 1,]",99,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") & (relevant_unemployment_df.week_number == 1)),]",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file54.ipynb,96,"relevant_unemployment_df.loc[(relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") and (relevant_unemployment_df.week_number == 1),]",99,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") & (relevant_unemployment_df.week_number == 1)),]",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file54.ipynb,97,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") and (relevant_unemployment_df.week_number == 1)),]",99,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") & (relevant_unemployment_df.week_number == 1)),]",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file54.ipynb,98,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") a&nd (relevant_unemployment_df.week_number == 1)),]",99,"relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") & (relevant_unemployment_df.week_number == 1)),]",[0;36m
./executed/file54.ipynb,107,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==uu_id) 
                                      & (relevant_unemployment_df.week_number == week)),]
row",108,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file54.ipynb,114,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.concat([None, None, None, None, None, None, None, None, None])",115,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
pd.concat(row, [None, None, None, None, None, None, None, None, None])",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file54.ipynb,115,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
pd.concat(row, [None, None, None, None, None, None, None, None, None])",116,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
pd.concat(row, pd.Series([None, None, None, None, None, None, None, None, None]))",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,116,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
pd.concat(row, pd.Series([None, None, None, None, None, None, None, None, None]))",119,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append(pd.Series([None, None, None, None, None, None, None, None, None]))",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,117,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
pd.concat(pd.Series(row), pd.Series([None, None, None, None, None, None, None, None, None]))",119,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append(pd.Series([None, None, None, None, None, None, None, None, None]))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file54.ipynb,118,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
pd.append(row, pd.Series([None, None, None, None, None, None, None, None, None]))",119,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append(pd.Series([None, None, None, None, None, None, None, None, None]))",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file54.ipynb,119,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append(pd.Series([None, None, None, None, None, None, None, None, None]))",121,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append([None, None, None, None, None, None, None, None, None])",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file54.ipynb,120,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append([None, None, None, None, None, None, None, None, None], columns=row.columns)",121,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
                                      & (relevant_unemployment_df.week_number == 1)),]
row.append([None, None, None, None, None, None, None, None, None])",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file55.ipynb,21,"print(f'intercept: {reg.intercept_}')
coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
print(coef)",23,"print(f'intercept: {reg.intercept_}')
coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
print(coef)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file55.ipynb,30,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data()",32,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file55.ipynb,46,"prediction_data = pd.merge(unemploy_wage_data, prediction_data, on=['uu_id'], how='inner')
prediction_data.head()",47,"prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
prediction_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file56.ipynb,0,"data[data[""top_category_employer1""]]",8,"data[""top_category_employer1""]",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file56.ipynb,7,"data[data[""top_category_employer1""]]",8,"data[""top_category_employer1""]",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file57.ipynb,5,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,11,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,13,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,15,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",17,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,23,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",25,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,27,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,29,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file57.ipynb,31,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file57.ipynb,33,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file57.ipynb,35,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file58.ipynb,0,data.info(),8,data.info(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file58.ipynb,1,data.describe(),9,data.describe(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file59.ipynb,0,data.info(),8,data.info(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file59.ipynb,1,data.describe(),9,data.describe(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file60.ipynb,17,unemploy.groupby('uu_id').get_group(0),18,unemploy.groupby('uu_id').mean('total_claims'),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file60.ipynb,29,pred.to_numpy(,30,pred.to_numpy(),[0;36m
./executed/file60.ipynb,52,unemploy.groupby('uu_id').apply(lambda x: print(type(x)),53,unemploy.groupby('uu_id').apply(lambda x: print(type(x))),[0;36m
./executed/file61.ipynb,230,"# Importing the required libraries
import nltk
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",244,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file61.ipynb,244,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",258,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",[0;31m---------------------------------------------------------------------------[0m[0;31mStdinNotImplementedError[0m
./executed/file62.ipynb,230,"# Importing the required libraries
import nltk
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",244,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file62.ipynb,244,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",258,"# Importing the required libraries
import nltk
nltk.download()
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd",[0;31m---------------------------------------------------------------------------[0m[0;31mStdinNotImplementedError[0m
./executed/file63.ipynb,28,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",32,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,52,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",64,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,56,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",64,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,60,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",64,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,68,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",76,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file63.ipynb,72,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",76,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,84,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",87,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,90,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",96,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,93,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",96,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,110,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
data.plot(subplots=True, figsize=(20,24))",114,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.plot(subplots=True, figsize=(20,24))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file63.ipynb,118,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.cor()",122,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.corr()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file63.ipynb,140,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",142,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file63.ipynb,168,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",170,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
print(wage_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file64.ipynb,3,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",9,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file64.ipynb,4,"covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",9,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file64.ipynb,18,"query_job = bigquery_client.query(query)
import db_dtypes
date_dtype_name = db.DateDtype.name
data = query_job.to_dataframe()
data.head()",22,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file64.ipynb,20,"query_job = bigquery_client.query(query)
date_dtype_name = db.DateDtype.name
data = query_job.to_dataframe()
data.head()",22,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file65.ipynb,3,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file65.ipynb,5,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",7,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file66.ipynb,1,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
pdf = query1_job.to_dataframe()
pdf.head()",7,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
pdf = query1_job.to_dataframe()
pdf.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file66.ipynb,168,"# understanding the datatypes for each columns of the dataframe
mdf.drop(""Unnamed: 0"", axis =0)
mdf.dtypes",169,"# understanding the datatypes for each columns of the dataframe
mdf = mdf.drop(""Unnamed: 0"", axis =1)
mdf.dtypes",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file66.ipynb,198,"mdf_6 = pd.DataFrame(dct)
mdf_6 = mdf_6.append(mdf_1, ignore_index = True)
len(mdf_6)",212,"mdf_6 = pd.DataFrame(dct)
mdf_6 = mdf_6.append(mdf_1, ignore_index = True)
len(mdf_6)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file66.ipynb,202,"print(mdf_1.columns())
dct.keys()",206,"print(mdf_1.columns)
dct.keys()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file66.ipynb,240,"lxt1 = [i for i in range(1,501)]
wk = [44]*500
d = {'uu_id': mdf_p.index, ""total_claims"": predict, 'week_number': wk}
rmdf = pd.DataFrame(d)
rmdf.index = lxt1
print(len(rmdf))
rmdf.head(5)",242,"lxt1 = [i for i in range(1,len(predict)+1)]
wk = [44]*len(predict)
d = {'uu_id': mdf_p.index, ""total_claims"": predict, 'week_number': wk}
rmdf = pd.DataFrame(d)
rmdf.index = lxt1
print(len(rmdf))
rmdf.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file66.ipynb,241,"lxt1 = [i for i in range(1,501)]
wk = [44]*len(predict)
d = {'uu_id': mdf_p.index, ""total_claims"": predict, 'week_number': wk}
rmdf = pd.DataFrame(d)
rmdf.index = lxt1
print(len(rmdf))
rmdf.head(5)",242,"lxt1 = [i for i in range(1,len(predict)+1)]
wk = [44]*len(predict)
d = {'uu_id': mdf_p.index, ""total_claims"": predict, 'week_number': wk}
rmdf = pd.DataFrame(d)
rmdf.index = lxt1
print(len(rmdf))
rmdf.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file67.ipynb,4,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment = query_job.to_dataframe()
unemployment.head()",8,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
covid19_cases_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file67.ipynb,37,wage.column,38,wage.columns,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file68.ipynb,58,tract[0].head(),63,tract_dict[0].head(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file68.ipynb,82,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file68.ipynb,83,"#compare the previous predictor to the new predictor
tract_dict[1][""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file68.ipynb,84,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file68.ipynb,86,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",87,"#compare the previous predictor to the new predictor
tract_dict[1][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file69.ipynb,7,covid19_cases_data.head(,8,covid19_cases_data.head(),[0;36m
./executed/file69.ipynb,20,unemployment_data.head(,21,unemployment_data.head(),[0;36m
./executed/file70.ipynb,33,"#print(merged_data[""week_number""])
merged_data[""datetime""] = [get_datetime(val) for val in merged_data[""week_number""]]
merged_data = merged_data.set_index(['datetime'])
merged_data = merged_data.sort_index()
merged_data.head(5)",44,"#print(merged_data[""week_number""])
merged_data[""datetime""] = [get_datetime(val) for val in merged_data[""week_number""]]
merged_data = merged_data.set_index(['datetime'])
merged_data = merged_data.sort_index()
merged_data.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file70.ipynb,48,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    #print(data_train)
    #SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(method=""ffill""),
    #                       start_p=0, start_q=0,test=""adf"",
    #                       max_p=5, max_q=5,m=4,d=None,
    #                       trace=False,
    #                       suppress_warnings=True, 
    #                       maxiter=200,
    #                       stepwise=True,seasonal=True,D=None)
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),alpha=0.05,d=None,max_order=0)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",50,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),alpha=0.05,d=None,max_order=0)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file70.ipynb,64,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""])
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid",69,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
plt.plot(trend)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,65,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0))
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid",69,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
plt.plot(trend)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,75,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0))
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()",77,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=10)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,76,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=20)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()",77,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=10)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,81,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=15)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(data_train[""total_claims""].fillna(0))",83,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=12)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(data_train[""total_claims""].fillna(0))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,86,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=15)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(data_train[""total_claims""].fillna(0))",88,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(data_train[""total_claims""].fillna(0))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,96,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),alpha=0.05,m=14,maxiter=200)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",97,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),alpha=0.05,m=14)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,97,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),alpha=0.05,m=14)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",98,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5, m=12, max_order=None,
                           trace=True)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,98,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5, m=12, max_order=None,
                           trace=True)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",99,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5, m=12, max_order=None)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,99,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5, m=12, max_order=None)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",101,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5,  max_order=None)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,100,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5, m=None, max_order=None)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",101,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
       'top_category_employer1', 'top_category_employer2',
       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
ids = merged_data[""uu_id""].unique()[:]
output = []
for cnt,idd in enumerate(ids):
    if cnt > 100: break
    data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
    data = data.asfreq('W-MON')
    is_na = []
    for idx,row in data.iterrows():
        if pd.isna(row[""total_claims""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data[""is_na""] = is_na
    #data[""total_claims""] = data[""total_claims""].fillna(0)
    start_week = 0
    #print(data[""week_number""].max())
    boundary_week = data[""week_number""].max()-2
    data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
    is_na = []
    for idx,row in data_train.iterrows():
        if pd.isna(row[""week_number""]):
            #print(f'NA found in week {idx}')
            is_na.append(1)
        else:
            is_na.append(0)
    data_train[""is_na""] = is_na
    data_train = data_train.asfreq('W-MON')
    SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),start_p=0, start_q=0, max_p=5,
                           max_q=5, start_P=0, start_Q=0, max_P=5,
                           max_Q=5,  max_order=None)
    max_week_no = data[""week_number""].max()
    pred_periods =  max_week_no - data_train[""week_number""].max()
    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
    #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
    out = prediction.loc[prediction.index[-1]]
    #print(prediction)
    #print(prediction.loc[prediction.index[-1]])
    output.append([idd,max_week_no,out])
    #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
    if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
        print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
    else:
        print(idd,max_week_no,out,""--"",len(data_train.index))
    #break",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file70.ipynb,110,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0).diff(),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,111,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].diff(),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,112,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].diff().fillna(0),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,113,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].diff().diff().fillna(0),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,114,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].diff().diff().diff().fillna(0),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,115,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,117,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""].fillna(0),period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,118,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=14)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,119,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""])
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,121,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""])
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",122,"from statsmodels.tsa.seasonal import seasonal_decompose
# Taking the decomposition
decomposition = seasonal_decompose(data_train[""total_claims""],period=7)
# Gathering and plotting the trend, seasonality, and residuals 
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
decomposition.plot()
print(len(data_train[""total_claims""].fillna(0)))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,126,"model = LinearRegression()
X = [i for i in range(len(data_train[""total_claims""].fillna(0)))]
y = data_train[""total_claims""].fillna(0)
model.fit(X,y)",138,"model = LinearRegression()
X = [i for i in range(0,len(data_train[""total_claims""].fillna(0)))]
X = np.reshape(X,(len(X),1))
y = data_train[""total_claims""].fillna(0)
model.fit(X,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,130,"model = LinearRegression()
X = [i for i in range(0,len(data_train[""total_claims""].fillna(0)))]
y = data_train[""total_claims""].fillna(0)
model.fit(X,y)",138,"model = LinearRegression()
X = [i for i in range(0,len(data_train[""total_claims""].fillna(0)))]
X = np.reshape(X,(len(X),1))
y = data_train[""total_claims""].fillna(0)
model.fit(X,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file70.ipynb,131,"trend = model.predict(X)
plt.plot(y)
plt.plot(trend)
plt.plot(show)",143,"trend = model.predict(X)
plt.plot(y)
plt.plot(trend)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFittedError[0m
./executed/file70.ipynb,134,"model = LinearRegression()
X = [i for i in range(0,len(data_train[""total_claims""].fillna(0)))].reshape(-1,1)
y = data_train[""total_claims""].fillna(0)
model.fit(X,y)",138,"model = LinearRegression()
X = [i for i in range(0,len(data_train[""total_claims""].fillna(0)))]
X = np.reshape(X,(len(X),1))
y = data_train[""total_claims""].fillna(0)
model.fit(X,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file70.ipynb,135,"trend = model.predict(X)
plt.plot(y)
plt.plot(trend)
plt.plot(show)",143,"trend = model.predict(X)
plt.plot(y)
plt.plot(trend)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFittedError[0m
./executed/file70.ipynb,139,"trend = model.predict(X)
plt.plot(y)
plt.plot(trend)
plt.plot(show)",143,"trend = model.predict(X)
plt.plot(y)
plt.plot(trend)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file71.ipynb,33,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",35,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file71.ipynb,37,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",41,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file71.ipynb,39,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",41,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file72.ipynb,49,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",52,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file72.ipynb,77,"transformed_data, best_lambda = boxcox(unemploy1)
sns.distpolt(transformed_data,)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file72.ipynb,78,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distpolt(transformed_data,)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file72.ipynb,79,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distpolt(transformed_data)",80,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file72.ipynb,81,"claim = np.array(transformed_data)
sqrt_claim = np.sqrt(transformed_data)
log_claim = np.logtransformed_data)
f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
#f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
f1.hist(claim, 30)
f2.hist(sqrt_claim, 30)
f3.hist(log_claim, 30)",83,"claim = np.array(transformed_data)
sqrt_claim = np.sqrt(transformed_data)
log_claim = np.log(transformed_data)
f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
#f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
f1.hist(claim, 30)
f2.hist(sqrt_claim, 30)
f3.hist(log_claim, 30)",[0;36m
./executed/file72.ipynb,95,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy[""edu_grades_9_11""])",96,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy1[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file72.ipynb,96,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(transformed_data)
ed_9_11=boxcox(unemploy1[""edu_grades_9_11""])",97,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
sns.distplot(unemploy1[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file72.ipynb,119,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1),columns=unemploy1)",122,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
sns.distplot(unemploy_ish)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file72.ipynb,120,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_quiv""]))",122,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
sns.distplot(unemploy_ish)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file72.ipynb,128,"stats.probplot(unemploy_ish,dist=""norm"",plot=pylab)
pylab.show()",132,"stats.probplot(unemploy_ish1,dist=""norm"",plot=pylab)
pylab.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file72.ipynb,154,"sns.distplot(unemploy1[""race_whaite""])",155,"sns.distplot(unemploy1[""race_white""])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file72.ipynb,159,"sns.distplot(unemploy1[""race_noanswere""])",160,"sns.distplot(unemploy1[""race_noanswer""])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file72.ipynb,164,"trial[""uuid""].unique()",165,"trial[""uu_id""].unique()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file73.ipynb,71,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
MSPE(data_estimate_m1, data_valid_y)
MAPE(data_estimate_m1, data_valid_y)",85,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
MSPE(data_estimate_m1, data_valid_y)
MAPE(data_estimate_m1, data_valid_y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,113,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims""], template = 'plotly_dark')
fig.show()",121,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""], template = 'plotly_dark')
fig.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,115,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims""], template = 'plotly_dark')
fig.show()",121,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""], template = 'plotly_dark')
fig.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,117,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims""], template = 'plotly_dark')
fig.show()",121,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""], template = 'plotly_dark')
fig.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,118,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims""], template = 'plotly_dark')
fig.show()",121,"fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""], template = 'plotly_dark')
fig.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,126,"data_balance_ave_train = data_balance_ave[[""week_num"",""total_claims""]]",128,"data_balance_ave_train = data_balance_ave[[""week_number"",""total_claims""]]
data_balance_ave_train.set_index(""week_number"")",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file73.ipynb,129,"model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)
model.fit(data_balance_ave_train)",135,"model = auto_arima(data_balance_ave_train, trace=True, error_action='ignore', suppress_warnings=True)
model.fit(data_balance_ave_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file73.ipynb,132,"model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)
model.fit(data_balance_ave_train)",135,"model = auto_arima(data_balance_ave_train, trace=True, error_action='ignore', suppress_warnings=True)
model.fit(data_balance_ave_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file73.ipynb,133,"model = auto_arima(data_balance_ave_train, trace=True, error_action='ignore', suppress_warnings=True)
model.fit(data_balance_ave_train)",135,"model = auto_arima(data_balance_ave_train, trace=True, error_action='ignore', suppress_warnings=True)
model.fit(data_balance_ave_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,146,model.forecast(),150,model.predict(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file73.ipynb,151,"model.predict(,steps = 1)",152,model.predict(steps = 1),[0;36m
./executed/file73.ipynb,158,"data_balance_ave_model = data_balance_ave[[""week_number"",""total_claims""]]
data_balance_ave_model = data_balance_ave_train.set_index(""week_number"")
cutoff = int(0.9 * len(data_balance_ave_model))
print(cutoff)",160,"data_balance_ave_model = data_balance_ave[[""week_number"",""total_claims""]]
data_balance_ave_model = data_balance_ave_model.set_index(""week_number"")
cutoff = int(0.9 * len(data_balance_ave_model))
print(cutoff)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file73.ipynb,203,"# validation
df_forecast = pd.DataFrame(forecast)
df_forecast.index.name = ""week_number""
df_forecast.rename(""total_claim_pred"")
df_forecast",206,"# validation
df_forecast = pd.DataFrame(forecast)
df_forecast.index.name = ""week_number""
df_forecast.columns = [""total_claim_pred""]
df_forecast",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file73.ipynb,204,"# validation
df_forecast = pd.DataFrame(forecast)
df_forecast.index.name = ""week_number""
df_forecast.rename(columns = [""total_claim_pred""])
df_forecast",206,"# validation
df_forecast = pd.DataFrame(forecast)
df_forecast.index.name = ""week_number""
df_forecast.columns = [""total_claim_pred""]
df_forecast",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file73.ipynb,205,"# validation
df_forecast = pd.DataFrame(forecast)
df_forecast.index.name = ""week_number""
df_forecast.rename(columns = {""total_claim_pred""})
df_forecast",206,"# validation
df_forecast = pd.DataFrame(forecast)
df_forecast.index.name = ""week_number""
df_forecast.columns = [""total_claim_pred""]
df_forecast",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file73.ipynb,231,MSPE_series,233,MSPE_series,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file73.ipynb,244,"# model train and validation
data_balance_ave_model = data_balance_ave[[""week_number"",""total_claims""]]
data_balance_ave_model = data_balance_ave_model.set_index(""week_number"")
forecast = ARIMA_predict(data_balance_ave_model, cutoff = 0.8, n_period = 15)",245,"# model train and validation
data_balance_ave_model = data_balance_ave[[""week_number"",""total_claims""]]
data_balance_ave_model = data_balance_ave_model.set_index(""week_number"")
forecast = ARIMA_predict(data_balance_ave_model, cutoff_rate = 0.8, n_period = 15)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file73.ipynb,294,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.iloc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.iloc[i][""total_claims""] = total_claim_pre
    
    if i > 10:
        break",296,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.iloc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.iloc[i][""total_claims""] = total_claim_pre
    
    print(i)
    print(print(data_pred.iloc[i][""week_number""]))
    print(total_claim_pre)
    
    if i > 10:
        break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,296,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.iloc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.iloc[i][""total_claims""] = total_claim_pre
    
    print(i)
    print(print(data_pred.iloc[i][""week_number""]))
    print(total_claim_pre)
    
    if i > 10:
        break",297,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.iloc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.iloc[i][""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 10:
        break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,297,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.iloc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.iloc[i][""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 10:
        break",305,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.loc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.loc[i][""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 10:
        break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,305,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.loc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.loc[i][""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 10:
        break",307,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.loc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.loc[i,""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 10:
        break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file73.ipynb,307,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.loc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.loc[i,""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 10:
        break",318,"for i in range(len(data_pred)):
    uu_id_pre = data_pred.loc[i][""uu_id""]
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 1, n_period = 10)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    
    total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
    
    data_pred.loc[i,""total_claims""] = total_claim_pre
    
    print(i)
    print(data_pred.iloc[i][""week_number""])
    print(total_claim_pre)
    
    if i > 2:
        break",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,21,"print(df_unemployment[""week_number""].unique().sort())  ",22,"print(sorted(df_unemployment[""week_number""].unique())) # 35",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file74.ipynb,33,"#sns.lmplot(x=""week_number"", y = ""total_claims"", hue = ""uu_id"", data = test_data)
sns.scatterplot(data = test_data, x = 'week_number', y = 'total_claims', data =  'uu_id')",34,"#sns.lmplot(x=""week_number"", y = ""total_claims"", hue = ""uu_id"", data = test_data)
sns.scatterplot(data = test_data, x = 'week_number', y = 'total_claims', hue =  'uu_id')",[0;36m
./executed/file74.ipynb,48,"x_train = np.reshape(test_data[""week_number""], (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""], (test_data.shape[0],1))
logreg.fit(x_train, y_train)",54,"x_train = np.reshape(test_data[""week_number""].to_numpy(), (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""].to_numpy(), (test_data.shape[0],1))
logisticRegr.fit(x_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,51,"x_train = np.reshape(test_data[""week_number""].to_numpy(), (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""].to_numpy(), (test_data.shape[0],1))
logreg.fit(x_train, y_train)",54,"x_train = np.reshape(test_data[""week_number""].to_numpy(), (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""].to_numpy(), (test_data.shape[0],1))
logisticRegr.fit(x_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file74.ipynb,54,"x_train = np.reshape(test_data[""week_number""].to_numpy(), (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""].to_numpy(), (test_data.shape[0],1))
logisticRegr.fit(x_train, y_train)",57,"x_train = np.reshape(test_data[""week_number""].to_numpy(), (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""].to_numpy(), (test_data.shape[0],1))
logisticRegr.fit(x_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,57,"x_train = np.reshape(test_data[""week_number""].to_numpy(), (test_data.shape[0],1))
y_train = np.reshape(test_data[""total_claims""].to_numpy(), (test_data.shape[0],1))
logisticRegr.fit(x_train, y_train)",64,"x_train = np.array(test_data[""week_number""]).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""]).reshape(-1, 1)
logisticRegr.fit(x_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,64,"x_train = np.array(test_data[""week_number""]).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""]).reshape(-1, 1)
logisticRegr.fit(x_train, y_train)",67,"x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""].values).reshape(-1, 1)
logisticRegr.fit(x_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,67,"x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""].values).reshape(-1, 1)
logisticRegr.fit(x_train, y_train)",69,"x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""].values)
print(y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,68,"x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""].values)
logisticRegr.fit(x_train, y_train)",69,"x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
y_train = np.array(test_data[""total_claims""].values)
print(y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,86,"x_pred = np.array([39]) 
y_pred = int(sgdr.predict(x_pred))
print(y_pred)",89,"x_pred = np.array([39]).reshape(-1, 1)
y_pred = int(sgdr.predict(x_pred))
print(y_pred)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file74.ipynb,123,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pd.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = df.append({'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",136,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pandas.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = res.append({'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file74.ipynb,131,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pd.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = df.append({'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",136,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pandas.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = res.append({'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file74.ipynb,141,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pandas.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = res.concat({'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",146,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pandas.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = pandas.concat(res, {'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file74.ipynb,146,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    res = pandas.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
    res = pandas.concat(res, {'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",159,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    cur_row =  pd.DataFrame([[cur_uu_id, 39, y_pred]], columns=['uu_id', 'week_number', 'total_claims'] )
    res = pd.concat(res,cur_row,ignore_index = True)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file74.ipynb,154,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    cur_row =  pd.DataFrame( {'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred})
    res = pd.concat(res,cur_row,ignore_index = True)",159,"for cur_uu_id in df_pred_list['uu_id']:
    #print(uu_id)
    test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
    x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
    y_train = np.array(test_data[""total_claims""].values) 
    sgdr.fit(x_train, y_train)
    x_pred = np.array([39]).reshape(-1, 1)
    y_pred = int(sgdr.predict(x_pred))
    cur_row =  pd.DataFrame([[cur_uu_id, 39, y_pred]], columns=['uu_id', 'week_number', 'total_claims'] )
    res = pd.concat(res,cur_row,ignore_index = True)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file75.ipynb,7,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",11,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file75.ipynb,9,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",11,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file75.ipynb,27,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.dtypes()
# data = data.drop([""tract_name"",""edu_8th_or_less"",",33,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'],axis=1)
data",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file75.ipynb,29,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop([""tract_name"",""edu_8th_or_less""])
data",33,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'],axis=1)
data",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file75.ipynb,31,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'])
data",33,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'],axis=1)
data",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file76.ipynb,17,"y_pred = model.predict(X_test)
y_pred.shape()",18,"y_pred = model.predict(X_test)
y_pred.shape",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file77.ipynb,2,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()",5,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file77.ipynb,7,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()
head(unemp, n=3)",9,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()
unemp.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file77.ipynb,12,unemp['tract_name'].value_counts()['tract_name'],13,print(unemp['tract_name'].value_counts()),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file77.ipynb,14,print(sort(unemp['tract_name'].value_counts())),15,print(unemp['tract_name'].value_counts()),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file78.ipynb,17,"y_pred = model.predict(X_test)
y_pred.shape()",18,"y_pred = model.predict(X_test)
y_pred.shape",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,28,"dopred(33,7)",29,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,32,"head(pd.merge(uupred, empdata, on='uu_id'))",33,"pd.merge(uupred, empdata, on='uu_id').head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file79.ipynb,35,"pd.join(uupred, empdata, on='uu_id')",48,"pd.merge(uupred, empdata, on='uu_id')",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file79.ipynb,36,"uupred.join(empdata, on='uu_id')",48,"pd.merge(uupred, empdata, on='uu_id')",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,37,"uupred.join(empdata.loc[empdata['week_number'] == 37][['total_claims']], on='uu_id')",50,"pd.merge(uupred, empdata.loc[empdata['week_number'] == 37], how='inner', on='uu_id')",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,39,"uupred.loc[uupred['uu_id'] == uu, ['total_claims']]",40,"uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d', ['total_claims']]",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file79.ipynb,44,uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']].flatten,47,uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']].values,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file79.ipynb,45,uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']].flatten(),47,uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']].values,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file79.ipynb,46,uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']]values,47,uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']].values,[0;36m
./executed/file79.ipynb,62,empdata.loc[empdata['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d']],63,empdata.loc[empdata['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'],[0;36m
./executed/file79.ipynb,85,"dopred(33,37)",89,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,87,"dopred(33,37)",89,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,89,"dopred(33,37)",91,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,91,"dopred(33,37)",95,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,93,"dopred(33,37)",95,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,95,"dopred(33,37)",99,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,97,"dopred(33,37)",99,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,99,"dopred(33,37)",101,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,101,"dopred(33,37)",103,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,103,"dopred(33,37)",105,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,105,"dopred(33,37)",111,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,107,"dopred(33,37)",111,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file79.ipynb,109,"dopred(33,37)",111,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file79.ipynb,111,"dopred(33,37)",113,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,113,"dopred(33,37)",115,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,115,"dopred(33,37)",117,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,133,"dopred(33,37)",148,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,139,"dopred(33,37)",148,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,140,allweeks.dtype,141,allweeks.dtypes,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file79.ipynb,144,"dopred(33,37)",148,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mUFuncTypeError[0m
./executed/file79.ipynb,150,"dopred(33,37)",156,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,154,"dopred(33,37)",156,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,158,allweeks.to_numpy(dtype='float64').shape(),159,allweeks.to_numpy(dtype='float64').shape,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,163,"dopred(33,37)",169,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,165,"dopred(33,37)",169,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,167,"dopred(33,37)",169,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mUFuncTypeError[0m
./executed/file79.ipynb,174,"dopred(33,37)",184,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file79.ipynb,178,"dopred(33,37)",184,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,180,"dopred(33,37)",184,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file79.ipynb,182,"dopred(33,37)",184,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file79.ipynb,187,"dopred(33,37)",189,"dopred(33,37)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file79.ipynb,221,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
testuu_id = testuu_id.set_index('week_number')
allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
m = ExponentialSmoothing(allweeks['total_claims'].to_numpy())
pred = m.fit().predict(lastw, predw)
print(uu, pred)",223,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
testuu_id = testuu_id.set_index('week_number')
allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
m = ExponentialSmoothing(allweeks['total_claims'].to_numpy())
pred = m.fit().predict(37, 37)
print(pred)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file79.ipynb,222,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
testuu_id = testuu_id.set_index('week_number')
allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
m = ExponentialSmoothing(allweeks['total_claims'].to_numpy())
pred = m.fit().predict(37, 37)
print(uu, pred)",223,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
testuu_id = testuu_id.set_index('week_number')
allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
m = ExponentialSmoothing(allweeks['total_claims'].to_numpy())
pred = m.fit().predict(37, 37)
print(pred)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file80.ipynb,14,"for x in c:
    xCount.append(xValues.count(x))",28,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file80.ipynb,22,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(xValues,xCount)",25,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file80.ipynb,31,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xxValues)
plt.bar(c,xCount)",34,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xValues))
plt.bar(c,xCount)",[0;36m
./executed/file80.ipynb,42,"xValues = []
xCount = []
for i in uuid[:2]:
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",44,"for i in uuid[:2]:
    xValues = []
    xCount = []
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file81.ipynb,72,"pip install db_dtypes
import os
import pandas as pd
import db_dtypes
from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud.bigquery import magics",73,"get_ipython().system('pip install db_dtypes')
import os
import pandas as pd
import db_dtypes
from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud.bigquery import magics",[0;36m
./executed/file82.ipynb,1,print(X_train),8,print(X_train),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file83.ipynb,4,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)
9/59:
#SPLIT training set",13,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",[0;36m
./executed/file83.ipynb,9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",13,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(5)
print(unemployment_data.columns)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file83.ipynb,25,"#For retrieving the slope:
print(regressor.coef_)
 y_pred = regressor.predict(X_test)",39,"#For retrieving the slope:
print(regressor.coef_)
y_pred = regressor.predict(X_test)",[0;36m
./executed/file83.ipynb,32,"#For retrieving the slope:
print(regressor.coef_)
 y_pred = regressor.predict(X_test)",39,"#For retrieving the slope:
print(regressor.coef_)
y_pred = regressor.predict(X_test)",[0;36m
./executed/file83.ipynb,40,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",41,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file84.ipynb,132,"crime_model = ols(""claim ~ uu_id + tract + age + single"", data=dta).fit()
print(crime_model.summary())",133,"crime_model = ols(""murder ~ urban + poverty + hs_grad + single"", data=dta).fit()
print(crime_model.summary())",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file85.ipynb,46,"prediction_data[uu_id = ""bbcb018f0e5e49e13636f6e78ce9f60f""]",57,"df_pred.where(df_pred[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f"")",[0;36m
./executed/file85.ipynb,47,"prediction_data[uu_id == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",57,"df_pred.where(df_pred[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file85.ipynb,48,prediction_data[],49,prediction_data,[0;36m
./executed/file85.ipynb,65,"df_un[""uu_id"" == ""bbcb018f0e5e49e13636f6e78ce9f60f""]).dropna()",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;36m
./executed/file85.ipynb,66,"df_un[""uu_id"" == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file85.ipynb,67,"df[df_un[""uu_id""] = ""bbcb018f0e5e49e13636f6e78ce9f60f""]]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;36m
./executed/file85.ipynb,68,"df[df_un[""uu_id""] = ""bbcb018f0e5e49e13636f6e78ce9f60f""]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;36m
./executed/file85.ipynb,69,"df[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",70,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""]",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file85.ipynb,71,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sortby(""week_number"")",73,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""])",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file85.ipynb,72,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort(by = ""week_number"")",73,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""])",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file86.ipynb,60,"for i in range(len(uu_id_list)):
    print(i)
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[i]]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    data_balance_ave_valid_check = data_balance_ave_valid.merge(df_forecast, on = ""week_number"")
    MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
    MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
    
    MAPE_list.append(MAPE_series)
    MSPE_list.append(MSPE_series)",63,"for i in range(len(uu_id_list)):
    print(i)
    data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[i]]
    data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
    data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
    forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
    
    df_forecast = pd.DataFrame(forecast)
    df_forecast.index.name = ""week_number""
    df_forecast.columns = [""total_claim_pred""]
    
    data_balance_ave_valid_check = data_balance_tract_model.merge(df_forecast, on = ""week_number"")
    MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
    MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
    
    MAPE_list.append(MAPE_series)
    MSPE_list.append(MSPE_series)
    break",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file86.ipynb,106,"df_bottom_level = data_balance.pivot(index=""week_number"", columns=""county_tract"", values=""total_claims"")
df_middle_level = data_balance.groupby([""week_number"", ""state""]) \
                    .sum() \
                    .reset_index(drop=False) \
                    .pivot(index=""week_number"", columns=""countyfips"", values=""total_claims"")
df_total = data_balance.groupby(""week_number"")[""total_claims""] \
             .sum() \
             .to_frame() \
             .rename(columns={""total_claims"": ""total""})",107,"df_bottom_level = data_balance.pivot(index=""week_number"", columns=""county_tract"", values=""total_claims"")
df_middle_level = data_balance.groupby([""week_number"", ""countyfips""]) \
                    .sum() \
                    .reset_index(drop=False) \
                    .pivot(index=""week_number"", columns=""countyfips"", values=""total_claims"")
df_total = data_balance.groupby(""week_number"")[""total_claims""] \
             .sum() \
             .to_frame() \
             .rename(columns={""total_claims"": ""total""})",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file86.ipynb,118,hierarchy_df = Null,123,hierarchy_df,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file86.ipynb,119,hierarchy_df = null,123,hierarchy_df,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file86.ipynb,129,"print(f""Number of time series at the bottom level: {df_bottom_level.shape[1]}"")
print(f""Number of time series at the middle level: {df_middle_level.shape[1]}"")
hierarchy_df.fillNA(0)",131,"print(f""Number of time series at the bottom level: {df_bottom_level.shape[1]}"")
print(f""Number of time series at the middle level: {df_middle_level.shape[1]}"")
hierarchy_df.fillna(0)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file86.ipynb,226,data_pred_join,237,data_pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file86.ipynb,235,data_pred_join,237,data_pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file86.ipynb,252,data_pred_join,253,data_pred,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file87.ipynb,2,"query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(3)",7,"query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file87.ipynb,27,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
No_claims_after_week_35 = Final_df.groupby(['uu_id'])[Final_df[""total_claims""] > 35]",28,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
No_claims_after_week_35 = Final_df[Final_df[""week_number""] > 35]
No_claims_after_week_35",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file87.ipynb,31,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
No_claims_after_week_35 = Final_df[Final_df[""week_number""] > 35]
No_claims_after_week_35.unique.uu_id",33,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
No_claims_after_week_35 = Final_df[Final_df[""week_number""] > 35]
No_claims_after_week_35.uu_id.unique()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file87.ipynb,32,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
No_claims_after_week_35 = Final_df[Final_df[""week_number""] > 35]
No_claims_after_week_35.nunique.uu_id",33,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
No_claims_after_week_35 = Final_df[Final_df[""week_number""] > 35]
No_claims_after_week_35.uu_id.unique()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file87.ipynb,74,total_claims_prediction.round(total_claims),75,total_claims_prediction.round(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file87.ipynb,97,"Let's experiment with some more UUID's
#'050a624d618a68e43fe31189909c644f' '074f501122885ab9aef5e9d07004209d'
#'0dc217a2798a141c59b99f5bcff29fa9' '0e6523fb3fc17f6a2ac7050972bd4bfd'
#'2b6b2f2e6d3340e7d9ae46cd41eaef1b'
uuid6 = Final_df[Final_df['uu_id']=='050a624d618a68e43fe31189909c644f'].sort_values('week_number')
uuid6.tail(3)",98,"#Let's experiment with some more UUID's
#'050a624d618a68e43fe31189909c644f' '074f501122885ab9aef5e9d07004209d'
#'0dc217a2798a141c59b99f5bcff29fa9' '0e6523fb3fc17f6a2ac7050972bd4bfd'
#'2b6b2f2e6d3340e7d9ae46cd41eaef1b'
uuid6 = Final_df[Final_df['uu_id']=='050a624d618a68e43fe31189909c644f'].sort_values('week_number')
uuid6.tail(3)",[0;36m
./executed/file88.ipynb,38,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
final_data",39,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')
final_data",[0;36m
./executed/file88.ipynb,47,fin.shape(),48,fin.shape,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file88.ipynb,100,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file88.ipynb,101,"tt = pred_data.merge(fin,how = 'left', final_data= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file89.ipynb,38,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
final_data",39,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')
final_data",[0;36m
./executed/file89.ipynb,47,fin.shape(),48,fin.shape,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file89.ipynb,100,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file89.ipynb,101,"tt = pred_data.merge(fin,how = 'left', final_data= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file90.ipynb,8,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",12,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file90.ipynb,10,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",12,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file90.ipynb,18,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",20,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file90.ipynb,20,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",22,"query_job = bigquery_client.query(query)
print(query_job)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file91.ipynb,33,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)
profile = ProfileReport(df3, title=""report"")
profile",34,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file91.ipynb,44,"features =df4[abs(df4.total_claims)>0.6].index
features",55,"features =df4[abs(df4.total_claims)>0.6].index
features",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file91.ipynb,62,"for feature in p:
    sns.violinplot(data=houseprices_num[feature].values, 
     inner='quartile', color='white')
    plt.show()",63,"for feature in p:
    sns.violinplot(data=df3[feature].values, 
     inner='quartile', color='white')
    plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file91.ipynb,100,"import itertools
colors = itertools.cycle(sns.color_palette(""tab10""))
for feature in features:
    fig, ax = plt.subplots(figsize=(12,8)) 
    c = next(colors)
    print(feature, c)
    #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
    sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
    sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
    plt.show()",101,"import itertools
from scipy import stats
from scipy.stats import norm
colors = itertools.cycle(sns.color_palette(""tab10""))
for feature in features:
    #fig, ax = plt.subplots(figsize=(12,8)) 
    c = next(colors)
    print(feature, c)
    #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
    #sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
    sns.distplot(df3[feature], color = c, fit = norm)
    stats.probplot(df3[feature])
    plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file91.ipynb,101,"import itertools
from scipy import stats
from scipy.stats import norm
colors = itertools.cycle(sns.color_palette(""tab10""))
for feature in features:
    #fig, ax = plt.subplots(figsize=(12,8)) 
    c = next(colors)
    print(feature, c)
    #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
    #sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
    sns.distplot(df3[feature], color = c, fit = norm)
    stats.probplot(df3[feature])
    plt.show()",102,"import itertools
get_ipython().run_line_magic('matplotlib', 'inline')
colors = itertools.cycle(sns.color_palette(""tab10""))
for feature in features:
    fig, ax = plt.subplots(figsize=(12,8)) 
    c = next(colors)
    print(feature, c)
    #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
    sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
    sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
    plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file91.ipynb,102,"import itertools
get_ipython().run_line_magic('matplotlib', 'inline')
colors = itertools.cycle(sns.color_palette(""tab10""))
for feature in features:
    fig, ax = plt.subplots(figsize=(12,8)) 
    c = next(colors)
    print(feature, c)
    #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
    sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
    sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
    plt.show()",103,"import itertools
colors = itertools.cycle(sns.color_palette(""tab10""))
for feature in features:
    fig, ax = plt.subplots(figsize=(12,8)) 
    c = next(colors)
    print(feature, c)
    #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
    sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
    sns.boxplot(y= feature, x = ""week_number"", data =df3, color = c)
    plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file91.ipynb,128,"for row in query_job1.result():
    print(row.term)",129,"for row in query_job1.result():
    print(row)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file92.ipynb,28,y_true=test.loc[['week_number'==35]],33,"y_true=test.loc[test['week_number']==35]
y_true",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file92.ipynb,29,y_true=test.loc[test['week_number'==35]],33,"y_true=test.loc[test['week_number']==35]
y_true",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file92.ipynb,30,test['week_number],31,test['week_number'],[0;36m
./executed/file92.ipynb,34,"y_true=test.loc[test['week_number']==35]
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",36,"y_true=test.loc[test['week_number']==35]
print(y_true.shape)
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file92.ipynb,35,"y_true=test.loc[test['week_number']==35]
y_true.shape
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",36,"y_true=test.loc[test['week_number']==35]
print(y_true.shape)
MAE=metrics.mean_absolute_error(y_true,y_pred)
MSE=metrics.mean_squared_error(y_true,y_pred)
print(MAE)
MSE",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file93.ipynb,29,"for i in data.columns:
    if data[i].isnull().sum()>= 0.5*len(data)
    data=data.drop(i,axis=1)",40,"for i in data.columns:
    if data[i].isnull().sum()>0:
        print(data[i].value_counts())",[0;36m
./executed/file93.ipynb,41,"for i in data.columns:
    if data[i].isnull().sum()>0:
        print('The value counts of feature',i
        print(data[i].value_counts())",42,"for i in data.columns:
    if data[i].isnull().sum()>0:
        print('The value counts of feature',i)
        print(data[i].value_counts())",[0;36m
./executed/file94.ipynb,8,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
#rn that error is expected, they r still tryna fix it",21,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file94.ipynb,17,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
#rn that error is expected, they r still tryna fix it",21,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file94.ipynb,25,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
unemployment_data.columns()",27,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
unemployment_data.columns",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file94.ipynb,40,"pip install google-cloud-bigquery
get_ipython().system('pip install google-cloud-bigquery[pandas]')",41,"get_ipython().system('pip install google-cloud-bigquery')
get_ipython().system('pip install google-cloud-bigquery[pandas]')",[0;36m
./executed/file94.ipynb,46,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
print(unemployment_data.columns)
dtype(unemployment_data)",48,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
print(unemployment_data.columns)
unemployment_data.dtypes",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file94.ipynb,123,"gender = edu_level.groupby(['total_claims'])['gender_female'].count().reset_index(
  name='Count').sort_values(['Count'], ascending=True)",135,"genders = gender.groupby(['total_claims', 'uu_id'])['gender_female', 'gender_male'].count().reset_index(
  name='Count').sort_values(['Count'], ascending=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file94.ipynb,162,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",166,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,163,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",166,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,165,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",166,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,176,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",184,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file94.ipynb,180,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",184,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file94.ipynb,188,"df2 = pd.DataFrame({'Actual': y_test.flatten() , 'Predicted': y_pred.flatten()})
df2",190,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,189,"df2 = pd.DataFrame({'Actual': y_test.flatten , 'Predicted': y_pred.flatten()})
df2",190,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,192,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",198,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file94.ipynb,194,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",198,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file94.ipynb,196,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",198,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file94.ipynb,284,"#fit the model to the data
model = LinearRegression()  
model.fit(X_train, y_train)",291,"#fit the model to the data
model = LinearRegression()  
model.fit(X_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file94.ipynb,285,"#To retrieve the intercept:
print(model.intercept_)",292,"#To retrieve the intercept:
print(model.intercept_)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,286,"#For retrieving the slope:
print(model.coef_)",292,"#To retrieve the intercept:
print(model.intercept_)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file94.ipynb,298,"df2 = pd.DataFrame({'Actual': y_test.flatten() , 'Predicted': y_pred.flatten()})
df2",307,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file95.ipynb,4,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
pdf = query1_job.to_dataframe()
pdf.head()",11,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
pdf = query1_job.to_dataframe()
pdf.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file96.ipynb,15,"# Replace end of line character with space
text_raw.replace('\n', ' ')",27,"# Replace end of line character with space
text_raw.replace('\n', ' ')",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file97.ipynb,21,"print(f'intercept: {reg.intercept_}')
coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
print(coef)",23,"print(f'intercept: {reg.intercept_}')
coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
print(coef)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file97.ipynb,30,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data()",32,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file97.ipynb,46,"prediction_data = pd.merge(unemploy_wage_data, prediction_data, on=['uu_id'], how='inner')
prediction_data.head()",47,"prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
prediction_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file98.ipynb,11,Merged_unemployment_wage.head(3),20,Merged_unemployment_wage.head(3),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file98.ipynb,48,"ord_enc = OrdinalEncoder()
df[""uu_id_code""] = ord_enc.fit_transform(df[[""uu_id""]])
df[[""uu_id"", ""uu_id_code""]].head()",50,"ord_enc = OrdinalEncoder()
Final_df[""uu_id_code""] = ord_enc.fit_transform(Final_df[[""uu_id""]])
Final_df[[""uu_id"", ""uu_id_code""]].head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file99.ipynb,34,week_available.shape,36,len(week_available),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file99.ipynb,35,week_available.shape(),36,len(week_available),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file100.ipynb,9,"query = """"""
SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
""""""
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",18,"query = """"""
SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
""""""
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file101.ipynb,6,unemployment_data.head(,7,unemployment_data.head(),[0;36m
./executed/file101.ipynb,21,unmeployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file101.ipynb,22,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,23,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,25,unemployment_data[['total_claims']].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,26,unemployment_data['total_claims'].groupby('week_number').mean(),27,unemployment_data.groupby('week_number')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,28,"unemployment_data.groupby('week_number','race_asian')['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,29,"unemployment_data.groupby(('week_number','race_asian'))['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,30,"unemployment_data.groupby[('week_number','race_asian')]['total_claims'].mean()",31,"unemployment_data.groupby(['week_number','race_asian'])['total_claims'].mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file101.ipynb,33,unemployment_data.groupby('uuid')['total_claims'].mean(),34,unemployment_data.groupby('uu_id')['total_claims'].mean(),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file101.ipynb,42,prediction_list.head(,43,prediction_list.head(),[0;36m
./executed/file101.ipynb,76,unemplyment_data.value_counts(),77,unemployment_data.value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file102.ipynb,55,"for col in X.columns[2:]:
    li = []
    for i in prediction_list['uu_id']:
        li.append(X.loc[X['uu_id'] == i, col].mean())
    pred_list[col] = li
prediction_list",56,"for col in X.columns[2:]:
    li = []
    for i in prediction_list['uu_id']:
        li.append(X.loc[X['uu_id'] == i, col].mean())
    prediction_list[col] = li
prediction_list",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file103.ipynb,53,"df1 = df.head(25)
df1.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()",60,"df1 = df.head(25)
df1.plot(kind='bar',figsize=(16,10))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file104.ipynb,15,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
data.iloc[X_test.index(),'uu_id']",19,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
temp",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file104.ipynb,16,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
data.iloc[X_test.index,'uu_id']",19,"X_test = X_test.loc[X_test[""week_number""]==34]
X_test.head(10)
temp",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file104.ipynb,20,"a=pd.Dataframe(data=temp)
a",21,"a=pd.DataFrame(data=temp)
a",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file104.ipynb,22,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a",24,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file104.ipynb,25,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a[count].astype('int')
a",26,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file104.ipynb,27,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;36m
./executed/file104.ipynb,28,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a.loc[:,columns]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file104.ipynb,29,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a.iloc[:,columns]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file104.ipynb,30,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[[columns]]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file104.ipynb,31,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week','count']]
a",32,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
columns=['uu_id','week','count']
a=a[['uu_id','week_number','count']]
a",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file104.ipynb,34,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a.rename('week_number':'week')
a",35,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a.rename(columns={'week_number':'week'})
a",[0;36m
./executed/file104.ipynb,39,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a['week_number']=a['week_number+1']
a=a.rename(columns={'week_number':'week'})
a.to_string(index=False)",40,"a=pd.DataFrame(data=temp)
a['count']=y_pred
a['count']=a['count'].astype('int')
a=a[['uu_id','week_number','count']]
a['week_number']=a['week_number']+1
a=a.rename(columns={'week_number':'week'})
a.to_string(index=False)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file105.ipynb,25,"dopred(37,41)",30,"dopred(37,41)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file106.ipynb,17,"pred = pd.DataFrame({
    'uu_id': pred.index,
    'week_number': 39,
    'total_claims': pred.to_numpy()
})",27,"df = pd.DataFrame({
    'uu_id': pred.index,
    'week_number': 39,
    'total_claims': pred.to_numpy().astype(np.int)
})",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file106.ipynb,18,"pred = pd.DataFrame({
    'uu_id': pred.index,
    'week_number': 39,
    'total_claims': pred.to_list()
})",27,"df = pd.DataFrame({
    'uu_id': pred.index,
    'week_number': 39,
    'total_claims': pred.to_numpy().astype(np.int)
})",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file106.ipynb,26,"df = pd.DataFrame({
    'uu_id': pred.index,
    'week_number': 39,
    'total_claims': pred.to_numpy().as_type(np.int)
})",27,"df = pd.DataFrame({
    'uu_id': pred.index,
    'week_number': 39,
    'total_claims': pred.to_numpy().astype(np.int)
})",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file107.ipynb,39,"pred = results.get_prediction(start=1, dynamic=False)
pred_ci = pred.conf_int()
y = df3[""total_claims""]
ax = y.plot(label='observed')
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)
ax.set_xlabel('Date')
ax.set_ylabel('Total Claims')
plt.legend()
plt.show()",40,"pred = results.get_prediction(start=1, dynamic=False)
pred_ci = pred.conf_int()
y = df3[""total_claims""]
ax = y.plot(label='observed')
print(pred.predicted_mean)
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)
ax.set_xlabel('Date')
ax.set_ylabel('Total Claims')
plt.legend()
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file107.ipynb,40,"pred = results.get_prediction(start=1, dynamic=False)
pred_ci = pred.conf_int()
y = df3[""total_claims""]
ax = y.plot(label='observed')
print(pred.predicted_mean)
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)
ax.set_xlabel('Date')
ax.set_ylabel('Total Claims')
plt.legend()
plt.show()",41,"pred = results.get_prediction(start=1, dynamic=False)
pred_ci = pred.conf_int()
y = df3[""total_claims""]
ax = y.plot(label='observed')
print(pred.predicted_mean)
df3[""pred""]=pred.predicted_mean
pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
ax.fill_between(pred_ci.index,
                pred_ci.iloc[:, 0],
                pred_ci.iloc[:, 1], color='k', alpha=.2)
ax.set_xlabel('Date')
ax.set_ylabel('Total Claims')
plt.legend()
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file108.ipynb,28,predition_data,29,prediction_data,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file109.ipynb,7,"query_job = bigquery_client.query(query)
print(query_job)
unemployment_data = query_job.to_dataframe()
print(unemployment_cases_data)",9,"query_job = bigquery_client.query(query)
print(query_job)
unemployment_data = query_job.to_dataframe()
print(unemployment_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file109.ipynb,43,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",49,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file109.ipynb,45,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",49,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file109.ipynb,47,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",49,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file109.ipynb,53,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",57,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file109.ipynb,55,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",57,"query_job = bigquery_client.query(query)
test = query_job.to_dataframe()
test.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file109.ipynb,65,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",67,"query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file109.ipynb,82,"# Drop duplicate columns
relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis=1,thresh=1).info()",83,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis=1,thresh=1).info()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file109.ipynb,84,"# Drop duplicate columns
relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns').info()",85,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns').info()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file109.ipynb,86,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True).info()",87,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True)
relevant_unemployment_df",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file109.ipynb,89,relevant_unemployment_df[1],93,relevant_unemployment_df['top_category_employer1'],[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file109.ipynb,90,relevant_unemployment_df['top_category_employer1'][0,91,relevant_unemployment_df['top_category_employer1'][0],[0;36m
./executed/file109.ipynb,92,relevant_unemployment_df['top_category_employer1'][0].dtype,93,relevant_unemployment_df['top_category_employer1'],[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file109.ipynb,96,relevant_unemployment_df['uu_id'][0,97,relevant_unemployment_df['uu_id'][0],[0;36m
./executed/file109.ipynb,105,relevant_unemployment_df.convert_dtypes(inplace=True),106,relevant_unemployment_df.convert_dtypes(),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file109.ipynb,107,"# Drop duplicate columns
relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True)
relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()",113,"# Drop duplicate columns
# relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
# Drop columns with excessive null values
# NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
relevant_unemployment_df.dropna(axis='columns', inplace=True)
relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].apply(str)
relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].apply(str)
relevant_unemployment_df.info()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file110.ipynb,12,"sns.heatmap(unemployment_data.fillna(0).corr(), annot = True)
plt.show()",18,"sns.heatmap(unemployment_data.fillna(0).corr(), annot = True)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file110.ipynb,28,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",31,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file111.ipynb,18,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file111.ipynb,19,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file111.ipynb,21,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",24,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_cor,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file111.ipynb,38,"plt.figure(figsize=(12,10))
cor = X.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file111.ipynb,39,"plt.figure(figsize=(12,10))
cor = pd.dataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file111.ipynb,40,"plt.figure(figsize=(12,10))
cor = pd.dataframe(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",41,"plt.figure(figsize=(12,10))
cor = pd.DataFrame(X).corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file112.ipynb,0,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",11,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file112.ipynb,1,"X=data.drop([""date"",""potential_water_deficit""],1)
y=data[""mean_temperature""]
# print(X)
# print(y)
data.head()",8,"X=data.drop([""date"",""potential_water_deficit""],1)
y=data[""mean_temperature""]
# print(X)
# print(y)
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file112.ipynb,18,"ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file112.ipynb,19,"ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file112.ipynb,20,"from matplotlib import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file112.ipynb,21,"from matplotlib import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax.figure(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file112.ipynb,22,"from matplotlib import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax.figure(figsize=(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file112.ipynb,23,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax.figure(figsize=(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file112.ipynb,31,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(10000,10000))",32,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1000,1000))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file112.ipynb,32,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1000,1000))",33,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(100,100))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file113.ipynb,5,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file113.ipynb,7,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",9,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file113.ipynb,26,"X = data.drop(""total_claims"")   #Feature Matrix
y = data[""total_claims""]          #Target Variable",27,"X = data.drop(""total_claims"",1)   #Feature Matrix
y = data[""total_claims""]          #Target Variable",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file114.ipynb,38,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
final_data",39,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')
final_data",[0;36m
./executed/file114.ipynb,47,fin.shape(),48,fin.shape,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file114.ipynb,100,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file114.ipynb,101,"tt = pred_data.merge(fin,how = 'left', final_data= 'uu_id')
tt                     ",102,"tt = pred_data.merge(final_data,how = 'left', on= 'uu_id')
tt                     ",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file115.ipynb,20,"!pip install pandas-profiling[notebook]
from pandas_profiling import ProfileReport
profile = ProfileReport(df3, title=""Report"")
profile",34,"!pip install pandas-profiling[notebook]
from pandas_profiling import ProfileReport
profile = ProfileReport(df3, title=""Report"")
profile",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,21,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df)
analyze_report.show_html(report.html', open_browser=False)",22,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=False)",[0;36m
./executed/file115.ipynb,22,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=False)",26,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=False)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,23,"!pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class
AV = AutoViz_Class()
df_av = AV.AutoViz('parking.csv')",24,"!pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class
AV = AutoViz_Class()
df3.to_csv(""rt.csv"")
df_av = AV.AutoViz('rt.csv')",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,24,"!pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class
AV = AutoViz_Class()
df3.to_csv(""rt.csv"")
df_av = AV.AutoViz('rt.csv')",25,"!pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class
AV = AutoViz_Class()
df3.to_csv(""rt.csv"")
df_av = AV.AutoViz('rt.csv')",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,26,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=False)",28,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,28,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",30,"!pip install sweetviz
import sweetviz as sv
import HBoxModel
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,30,"!pip install sweetviz
import sweetviz as sv
import HBoxModel
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",31,"!pip install sweetviz
get_ipython().system('pip install HBoxModel')
import sweetviz as sv
import HBoxModel
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,31,"!pip install sweetviz
get_ipython().system('pip install HBoxModel')
import sweetviz as sv
import HBoxModel
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",32,"!pip install sweetviz
import sweetviz as sv
import HBoxModel
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,32,"!pip install sweetviz
import sweetviz as sv
import HBoxModel
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",33,"!pip install sweetviz
import sweetviz as sv
analyze_report = sv.analyze(df3)
analyze_report.show_html('report.html', open_browser=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file115.ipynb,34,"!pip install pandas-profiling[notebook]
from pandas_profiling import ProfileReport
profile = ProfileReport(df3, title=""Report"")
profile",35,"!pip install pandas-profiling[notebook]
from pandas_profiling import ProfileReport
profile = ProfileReport(df3, title=""Report"")
profile",[0;31m---------------------------------------------------------------------------[0m[0;31mModuleNotFoundError[0m
./executed/file116.ipynb,20,"data[""top_category_employer2""].map(lambda x: '32' if x =='31-33')",21,"data[""top_category_employer2""].map(lambda x: '32' if x =='31-33' else x)",[0;36m
./executed/file116.ipynb,39,"df[""top_category_employer1""].astype(int) ",42,"data[""top_category_employer1""].astype(float) ",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file116.ipynb,40,"data[""top_category_employer1""].astype(int) ",42,"data[""top_category_employer1""].astype(float) ",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file116.ipynb,48,data.fillna('0'),49,data.fillna(0),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file116.ipynb,67,"data[""test""] = data.groupby(by=[""uu_id""]).sum()",68,"data.groupby(by=[""uu_id""]).mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file117.ipynb,15,"predictions = regressor.predict(test_features).astype(int)
predictions = np.round(predictions,decimals = 0, out = None)
print(predictions)",16,"predictions = regressor.predict(x_test).astype(int)
predictions = np.round(predictions,decimals = 0, out = None)
print(predictions)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file118.ipynb,1,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",6,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file118.ipynb,12,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",17,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file118.ipynb,41,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",53,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file118.ipynb,53,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",54,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file118.ipynb,54,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",55,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file118.ipynb,55,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",58,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file119.ipynb,1,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",6,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file119.ipynb,12,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']= pd.to_datetime(data['date'])
data.head()",17,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head(3)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file119.ipynb,41,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",53,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file119.ipynb,53,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",54,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file119.ipynb,54,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",55,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file119.ipynb,55,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",58,"plt.figure(figsize=(12,10))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file120.ipynb,58,tract[0].head(),63,tract_dict[0].head(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file120.ipynb,82,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file120.ipynb,83,"#compare the previous predictor to the new predictor
tract_dict[1][""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file120.ipynb,84,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",85,"#compare the previous predictor to the new predictor
tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file120.ipynb,86,"#compare the previous predictor to the new predictor
tract_dict[1[[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",87,"#compare the previous predictor to the new predictor
tract_dict[1][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",[0;36m
./executed/file121.ipynb,8,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
#rn that error is expected, they r still tryna fix it",21,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file121.ipynb,17,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
#rn that error is expected, they r still tryna fix it",21,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
wage_data = query_job.to_dataframe()
wage_data.head(5)",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file121.ipynb,25,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
unemployment_data.columns()",27,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
unemployment_data.columns",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file121.ipynb,40,"pip install google-cloud-bigquery
get_ipython().system('pip install google-cloud-bigquery[pandas]')",41,"get_ipython().system('pip install google-cloud-bigquery')
get_ipython().system('pip install google-cloud-bigquery[pandas]')",[0;36m
./executed/file121.ipynb,46,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
print(unemployment_data.columns)
dtype(unemployment_data)",48,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
#unemployment_data.head(5)
print(unemployment_data.columns)
unemployment_data.dtypes",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file121.ipynb,123,"gender = edu_level.groupby(['total_claims'])['gender_female'].count().reset_index(
  name='Count').sort_values(['Count'], ascending=True)",135,"genders = gender.groupby(['total_claims', 'uu_id'])['gender_female', 'gender_male'].count().reset_index(
  name='Count').sort_values(['Count'], ascending=True)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file121.ipynb,162,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",166,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,163,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",166,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,165,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df2",166,"df2 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,176,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",184,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file121.ipynb,180,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",184,"regressor = LinearRegression()  
regressor.fit(X_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file121.ipynb,188,"df2 = pd.DataFrame({'Actual': y_test.flatten() , 'Predicted': y_pred.flatten()})
df2",190,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,189,"df2 = pd.DataFrame({'Actual': y_test.flatten , 'Predicted': y_pred.flatten()})
df2",190,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,192,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",198,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file121.ipynb,194,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",198,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file121.ipynb,196,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",198,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file121.ipynb,284,"#fit the model to the data
model = LinearRegression()  
model.fit(X_train, y_train)",291,"#fit the model to the data
model = LinearRegression()  
model.fit(X_train, y_train)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file121.ipynb,285,"#To retrieve the intercept:
print(model.intercept_)",292,"#To retrieve the intercept:
print(model.intercept_)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,286,"#For retrieving the slope:
print(model.coef_)",292,"#To retrieve the intercept:
print(model.intercept_)",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file121.ipynb,298,"df2 = pd.DataFrame({'Actual': y_test.flatten() , 'Predicted': y_pred.flatten()})
df2",307,"df2 = pd.DataFrame({'Actual': y_test , 'Predicted': y_pred.flatten()})
df2",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file122.ipynb,62,"predictions = dict()
actual = dict()
for pred_id in ids[:1]:
    predictions[pred_id] = []
    actual[pred_id] = []
    
    for wk in test_weeks:
        # Train test split
        if id not in bad_wage_ids:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
        else:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
        train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
        test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
        actual[pred_id].append(test_y[0])
        
        ## Prediction
        # Linear Model
        reg = LinearRegression().fit(train_x, train_y)
        prediction = reg.predict(test_x)
        predictions[pred_id].append(prediction[0])",77,"predictions = dict()
actual = dict()
for pred_id in ids[:1]:
    predictions[pred_id] = []
    actual[pred_id] = []
    
    for wk in test_weeks:
        # Train test split
        if id not in bad_wage_ids:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
        else:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
        print(train_x)
        train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
        test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
        actual[pred_id].append(test_y.values[0])
        
        ## Prediction
        # Linear Model
        reg = LinearRegression().fit(train_x, train_y)
        prediction = reg.predict(test_x)
        predictions[pred_id].append(prediction[0])",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file122.ipynb,71,"predictions = dict()
actual = dict()
for pred_id in ids[:1]:
    predictions[pred_id] = []
    actual[pred_id] = []
    
    for wk in test_weeks:
        # Train test split
        if id not in bad_wage_ids:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
        else:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
            print(train_x
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
        train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
        test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
        actual[pred_id].append(test_y.values[0])
        
        ## Prediction
        # Linear Model
        reg = LinearRegression().fit(train_x, train_y)
        prediction = reg.predict(test_x)
        predictions[pred_id].append(prediction[0])",77,"predictions = dict()
actual = dict()
for pred_id in ids[:1]:
    predictions[pred_id] = []
    actual[pred_id] = []
    
    for wk in test_weeks:
        # Train test split
        if id not in bad_wage_ids:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
        else:
            train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
            test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
                               [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
        print(train_x)
        train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
        test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
        actual[pred_id].append(test_y.values[0])
        
        ## Prediction
        # Linear Model
        reg = LinearRegression().fit(train_x, train_y)
        prediction = reg.predict(test_x)
        predictions[pred_id].append(prediction[0])",[0;36m
./executed/file122.ipynb,100,bad_ids,101,bad_wage_ids,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file123.ipynb,55,"for col in X.columns[2:]:
    li = []
    for i in prediction_list['uu_id']:
        li.append(X.loc[X['uu_id'] == i, col].mean())
    pred_list[col] = li
prediction_list",56,"for col in X.columns[2:]:
    li = []
    for i in prediction_list['uu_id']:
        li.append(X.loc[X['uu_id'] == i, col].mean())
    prediction_list[col] = li
prediction_list",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file124.ipynb,132,"crime_model = ols(""claim ~ uu_id + tract + age + single"", data=dta).fit()
print(crime_model.summary())",133,"crime_model = ols(""murder ~ urban + poverty + hs_grad + single"", data=dta).fit()
print(crime_model.summary())",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file125.ipynb,34,week_available.shape,36,len(week_available),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file125.ipynb,35,week_available.shape(),36,len(week_available),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file126.ipynb,25,"dopred(37,41)",30,"dopred(37,41)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file127.ipynb,6,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",8,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file127.ipynb,33,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.interpolate(method='linear', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file127.ipynb,34,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.interpolate(method='linear',limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file127.ipynb,40,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data[:,3:-1].interpolate(method='linear',limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file127.ipynb,41,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.iloc[:,3:-1].interpolate(method='linear',limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file127.ipynb,42,"#Fill missing data using columnwise linear interpolation in unemployment data
num_data=unemployment_data.iloc[:,3:-1].astype(int32)
num_data.interpolate(method='linear', limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file127.ipynb,43,"#Fill missing data using columnwise linear interpolation in unemployment data
num_data=unemployment_data.iloc[:,3:-1].astype('int32')
num_data.interpolate(method='linear', limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file127.ipynb,44,"#Fill missing data using columnwise linear interpolation in unemployment data
num_data=unemployment_data.iloc[:,3:-1]
num_data.interpolate(method='linear', limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file127.ipynb,78,"lm.score(X,y)",79,"lm.score(X_data,y_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file128.ipynb,46,"pdf = pdf.drop([""Unnamed: 0""], axis = 1)
sorted(pdf[""uu_id""])
pdf.value_counts()",57,"pdf[""uu_id""]=sorted(pdf[""uu_id""])
pdf.value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file128.ipynb,54,"sorted(f_df[""uu_id""])
fdf.value_counts()",55,"sorted(f_df[""uu_id""])
f_df.value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file128.ipynb,61,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1"" and f_df[""week_number""] == 3]
# f2_df[""week_number""].value_counts()",66,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f2_df = f2_df[f2_df[""week_number""] == 3]
f2_df
# f2_df[""week_number""].value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file128.ipynb,62,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1"", f_df[""week_number""] == 3]
# f2_df[""week_number""].value_counts()",66,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f2_df = f2_df[f2_df[""week_number""] == 3]
f2_df
# f2_df[""week_number""].value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file128.ipynb,63,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1"" f_df[""week_number""] == 3]
# f2_df[""week_number""].value_counts()",66,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f2_df = f2_df[f2_df[""week_number""] == 3]
f2_df
# f2_df[""week_number""].value_counts()",[0;36m
./executed/file128.ipynb,65,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f2_df = f2_df[f2_df[""week_number""] == 3]\
f2_df
# f2_df[""week_number""].value_counts()",66,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f2_df = f2_df[f2_df[""week_number""] == 3]
f2_df
# f2_df[""week_number""].value_counts()",[0;36m
./executed/file128.ipynb,81,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.value_count()",82,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file128.ipynb,83,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.values_count",86,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file128.ipynb,84,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.values_count()",86,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file128.ipynb,85,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.value_count()",86,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
f3_df = f2_df.drop_duplicates()
# f3_df = f2_df[f2_df[""week_number""] == 33]
f3_df.value_counts()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file128.ipynb,101,"data1 = f1_df.set_index('week_number')
data1.index",103,"# data1 = f2_df.set_index('week_number')
# data1.index
f1_df.index",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file129.ipynb,16,"print(""Tables contained in '{}':"".format(dataset_id))
for table in tables:
    print(""{}.{}.{}"".format(table.project, table.dataset_id, table.table_id))",21,"print(""Tables contained in '{}':"".format(dataset_id))
for table in tables:
    print(""{}.{}.{}"".format(table.project, table.dataset_id, table.table_id))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file130.ipynb,12,"sns.heatmap(unemployment_data.fillna(0).corr(), annot = True)
plt.show()",18,"sns.heatmap(unemployment_data.fillna(0).corr(), annot = True)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file130.ipynb,28,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",31,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file131.ipynb,0,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",11,"from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file131.ipynb,1,"X=data.drop([""date"",""potential_water_deficit""],1)
y=data[""mean_temperature""]
# print(X)
# print(y)
data.head()",8,"X=data.drop([""date"",""potential_water_deficit""],1)
y=data[""mean_temperature""]
# print(X)
# print(y)
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file131.ipynb,18,"ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file131.ipynb,19,"ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file131.ipynb,20,"from matplotlib import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file131.ipynb,21,"from matplotlib import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax.figure(figsize(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file131.ipynb,22,"from matplotlib import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax.figure(figsize=(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file131.ipynb,23,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
ax.figure(figsize=(1,1))",24,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1,1))",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file131.ipynb,31,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(10000,10000))",32,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1000,1000))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file131.ipynb,32,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(1000,1000))",33,"from matplotlib.pyplot import figure
ax = y.plot(alpha=0.5)
ax = y_pred.plot(ax=ax, linewidth=1)
figure(figsize=(100,100))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file132.ipynb,67,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    rows.append([uuid, 39, predict_claims(uuid, 39)])",74,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    print(uuid)
    claims = predict_claims(uuid, 39)
    rows.append([uuid, 39, claims])",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file132.ipynb,70,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    claims = predict_claims(uuid, 39)
    rows.append([uuid, 39, claims])",74,"fields = ['uu_id', 'week_number', 'total_claims']
rows = []
for uuid in uuids:
    print(uuid)
    claims = predict_claims(uuid, 39)
    rows.append([uuid, 39, claims])",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file132.ipynb,85,unemployment_data.uu_ids.count_values(),87,unemployment_data.uu_id.value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file132.ipynb,86,unemployment_data.uu_id.count_values(),87,unemployment_data.uu_id.value_counts(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file132.ipynb,88,unemployment_data.uu_id.value_counts().sort(),89,unemployment_data.uu_id.value_counts().tolist(),[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file132.ipynb,104,"# 005be9532fd717dc36d4be318fd9ad25
unemployment_data.groupby('uu_id').median().get_group('005be9532fd717dc36d4be318fd9ad25')",105,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median()
groupby_id",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file132.ipynb,106,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median()
groupby_id.get_group('005be9532fd717dc36d4be318fd9ad25')",108,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median()
groupby_id",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file132.ipynb,107,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median()
groupby_id.first()",108,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median()
groupby_id",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file132.ipynb,112,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median().total_claims
groupby_id[groupby_id.uu_id == '005be9532fd717dc36d4be318fd9ad25']",113,"# 005be9532fd717dc36d4be318fd9ad25
groupby_id = unemployment_data.groupby('uu_id').median().total_claims
groupby_id[groupby_id == '005be9532fd717dc36d4be318fd9ad25']",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file133.ipynb,3,"query = """"""
SELECT *
FROM `ironhacks-data.ironhacks_competition.unemployment_data`
""""""
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
covid19_cases_data.head()",13,"query = """"""
SELECT *
FROM `ironhacks-data.ironhacks_competition.unemployment_data`
""""""
query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
covid19_cases_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file134.ipynb,17,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
head(df_three_col)",19,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
df_three_col.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file134.ipynb,21,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
df_three_col[0:21,]",23,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
df_three_col.head(21)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file135.ipynb,4,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment = query_job.to_dataframe()
unemployment.head()",8,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
covid19_cases_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file135.ipynb,37,wage.column,38,wage.columns,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file136.ipynb,7,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",11,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file136.ipynb,9,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data['date']=pd.to_datetime(data['date'])
data.head()",11,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file136.ipynb,27,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.dtypes()
# data = data.drop([""tract_name"",""edu_8th_or_less"",",33,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'],axis=1)
data",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file136.ipynb,29,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop([""tract_name"",""edu_8th_or_less""])
data",33,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'],axis=1)
data",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file136.ipynb,31,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'])
data",33,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data = data.drop(['tract_name','edu_8th_or_less'],axis=1)
data",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file137.ipynb,42,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
pylab.show()",44,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
pylab.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file137.ipynb,52,"scaled.plot.hist(subplots=True, legend=True, layout=(8, 2))",53,"scaled.plot.hist(subplots=True, legend=True, layout=(9, 2))",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file137.ipynb,55,scale_log,56,scale_logs,[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file137.ipynb,67,sns.distplot(unemp_sqrt[edu_grades_9_11]),68,"sns.distplot(unemp_sqrt[""edu_grades_9_11""])",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file137.ipynb,83,"print('statistics=%.3f, p=%.3f\n' %(statisticss, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",85,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
if pvalue>0.05:
    print(""Probably Normal"")
else:
    print(""Probably not Normal"")",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file138.ipynb,2,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()",5,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file138.ipynb,7,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()
head(unemp, n=3)",9,"#QUERY DATA
query_job = bigquery_client.query(query)
print(query_job)
unemp = query_job.to_dataframe()
unemp.head(2)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file138.ipynb,12,unemp['tract_name'].value_counts()['tract_name'],13,print(unemp['tract_name'].value_counts()),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file138.ipynb,14,print(sort(unemp['tract_name'].value_counts())),15,print(unemp['tract_name'].value_counts()),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file139.ipynb,6,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",8,"# QUERY THE DATA ONCE
query_job = bigquery_client.query(query)
unemployment_data = query_job.to_dataframe()
unemployment_data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mBadRequest[0m
./executed/file139.ipynb,33,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.interpolate(method='linear', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file139.ipynb,34,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.interpolate(method='linear',limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file139.ipynb,40,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data[:,3:-1].interpolate(method='linear',limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file139.ipynb,41,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.iloc[:,3:-1].interpolate(method='linear',limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file139.ipynb,42,"#Fill missing data using columnwise linear interpolation in unemployment data
num_data=unemployment_data.iloc[:,3:-1].astype(int32)
num_data.interpolate(method='linear', limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file139.ipynb,43,"#Fill missing data using columnwise linear interpolation in unemployment data
num_data=unemployment_data.iloc[:,3:-1].astype('int32')
num_data.interpolate(method='linear', limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file139.ipynb,44,"#Fill missing data using columnwise linear interpolation in unemployment data
num_data=unemployment_data.iloc[:,3:-1]
num_data.interpolate(method='linear', limit_direction='both', axis=0)",45,"#Fill missing data using columnwise linear interpolation in unemployment data
unemployment_data.fillna(method='pad', axis=0)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file139.ipynb,78,"lm.score(X,y)",79,"lm.score(X_data,y_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file140.ipynb,1,print(X_train),8,print(X_train),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file141.ipynb,1,print(X_train),8,print(X_train),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file142.ipynb,20,"data[""top_category_employer2""].map(lambda x: '32' if x =='31-33')",21,"data[""top_category_employer2""].map(lambda x: '32' if x =='31-33' else x)",[0;36m
./executed/file142.ipynb,39,"df[""top_category_employer1""].astype(int) ",42,"data[""top_category_employer1""].astype(float) ",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file142.ipynb,40,"data[""top_category_employer1""].astype(int) ",42,"data[""top_category_employer1""].astype(float) ",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file142.ipynb,48,data.fillna('0'),49,data.fillna(0),[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file142.ipynb,67,"data[""test""] = data.groupby(by=[""uu_id""]).sum()",68,"data.groupby(by=[""uu_id""]).mean()",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file143.ipynb,14,"for x in c:
    xCount.append(xValues.count(x))",28,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file143.ipynb,22,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(xValues,xCount)",25,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file143.ipynb,31,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xxValues)
plt.bar(c,xCount)",34,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xValues))
plt.bar(c,xCount)",[0;36m
./executed/file143.ipynb,42,"xValues = []
xCount = []
for i in uuid[:2]:
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",44,"for i in uuid[:2]:
    xValues = []
    xCount = []
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file144.ipynb,14,"for x in c:
    xCount.append(xValues.count(x))",28,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file144.ipynb,22,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(xValues,xCount)",25,"for x in c:
    xCount.append(xValues.count(x))
print(xValues)
print(xCount)
plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file144.ipynb,31,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xxValues)
plt.bar(c,xCount)",34,"for x in c:
    xCount.append(xValues.count(x))
print(len(xValues))
print(statistics.mean(xValues))
plt.bar(c,xCount)",[0;36m
./executed/file144.ipynb,42,"xValues = []
xCount = []
for i in uuid[:2]:
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",44,"for i in uuid[:2]:
    xValues = []
    xCount = []
    for k in b:
        if (k[0] == i):
            xValues.append(k[6])
    c = list(dict.fromkeys(xValues))

    for x in c:
        xCount.append(xValues.count(x))
    print(statistics.mean(xValues))
    print(statistics.median(xValues))
    plt.bar(c,xCount)",[0;31m---------------------------------------------------------------------------[0m[0;31mValueError[0m
./executed/file145.ipynb,3,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",9,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file145.ipynb,4,"covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",9,"query_job = bigquery_client.query(query)
covid19_cases_data = query_job.to_dataframe()
print(covid19_cases_data)",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file145.ipynb,18,"query_job = bigquery_client.query(query)
import db_dtypes
date_dtype_name = db.DateDtype.name
data = query_job.to_dataframe()
data.head()",22,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file145.ipynb,20,"query_job = bigquery_client.query(query)
date_dtype_name = db.DateDtype.name
data = query_job.to_dataframe()
data.head()",22,"query_job = bigquery_client.query(query)
data = query_job.to_dataframe()
data.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file146.ipynb,0,"visual_data = data.groupby(['uu_id'])['total_claims'].sum().reset_index().merge(
    data_dict['wage_data'],
    on=['uu_id'],
    how='inner',
)
sns.scatterplot(data=visual_data, x='total_claims', y='average_wage', marker='+')",14,"visual_data = data.groupby(['uu_id'])['total_claims'].sum().reset_index().merge(
    data_dict['wage_data'],
    on=['uu_id'],
    how='inner',
)
sns.scatterplot(data=visual_data, x='total_claims', y='average_wage', marker='+')",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file147.ipynb,0,"visual_data = data.groupby(['uu_id'])['total_claims'].sum().reset_index().merge(
    data_dict['wage_data'],
    on=['uu_id'],
    how='inner',
)
sns.scatterplot(data=visual_data, x='total_claims', y='average_wage', marker='+')",14,"visual_data = data.groupby(['uu_id'])['total_claims'].sum().reset_index().merge(
    data_dict['wage_data'],
    on=['uu_id'],
    how='inner',
)
sns.scatterplot(data=visual_data, x='total_claims', y='average_wage', marker='+')",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file148.ipynb,19,"cdf[""week_number'].unique()",20,"cdf[""week_number""].unique()",[0;36m
./executed/file148.ipynb,22,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
cdf = query1_job.to_dataframe()
cdf.head()",24,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
cdf = query1_job.to_dataframe()
cdf.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file148.ipynb,32,udf.columns(),33,udf.columns,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file148.ipynb,44,"udf[""tract_name""][10].unique()",45,"udf[""tract_name""][10]",[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file148.ipynb,46,"udf[udf[""tract_name""][10]]",48,"udf[[""tract_name""]]",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file148.ipynb,47,"udf[[""tract_name""][10]]",48,"udf[[""tract_name""]]",[0;31m---------------------------------------------------------------------------[0m[0;31mIndexError[0m
./executed/file148.ipynb,61,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
Mdf = query1_job.to_dataframe()
Mdf.head()",63,"# QUERY THE DATA ONCE
query1_job = bigquery_client.query(query1)
Mdf = query1_job.to_dataframe()
Mdf.head()",[0;31m---------------------------------------------------------------------------[0m[0;31mNotFound[0m
./executed/file148.ipynb,74,"mdf.drop(['Unnamed:', 'uu_id_1'], axis=1)",76,"mdf.drop(['Unnamed: 0', 'uu_id_1'], axis=1)",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file148.ipynb,102,"plt.figure(figsize=(12,10))
cor = mdf.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",104,"plt.figure(figsize=(12,10))
cor = mdf.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file148.ipynb,125,mdf.columns.type,126,mdf.columns,[0;31m---------------------------------------------------------------------------[0m[0;31mAttributeError[0m
./executed/file148.ipynb,129,"mdf[""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]",130,"mdf[[""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]]",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file148.ipynb,203,f_df.groupby(['Animal']),204,f_df.groupby(['uu_id']),[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file148.ipynb,220,"data1 = data1.set_index('week_number')
data1.index",222,"data1 = data1.set_index('week_number')
data1.index",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file148.ipynb,235,"data1 = f1_df.set_index('week_number')
data1.index",237,"# data1 = f1_df.set_index('week_number')
data1.index",[0;31m---------------------------------------------------------------------------[0m[0;31mKeyError[0m
./executed/file149.ipynb,20,"plt.figure(figsize=(20,15))
cor = data.corr()
sns.heatmap(cor,mask=True annot=True, cmap=plt.cm.Reds)
plt.show()",21,"plt.figure(figsize=(20,15))
cor = data.corr()
sns.heatmap(cor,mask=True,annot=True, cmap=plt.cm.Reds)
plt.show()",[0;36m
./executed/file149.ipynb,22,"plt.figure(figsize=(20,15))
cor = data.corr()
sns.heatmap(cor,mask=mask,annot=True, cmap=plt.cm.Reds)
plt.show()",23,"plt.figure(figsize=(20,15))
cor = data.corr()
sns.heatmap(cor,annot=True, cmap=plt.cm.Reds)
plt.show()",[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file149.ipynb,25,data.shape(),26,data.shape,[0;31m---------------------------------------------------------------------------[0m[0;31mTypeError[0m
./executed/file149.ipynb,29,sum(data[race_white].isnull()),30,sum(data['race_white'].isnull()),[0;31m---------------------------------------------------------------------------[0m[0;31mNameError[0m
./executed/file149.ipynb,31,"for i in data.columns:
    print('The null values in',i,'are',sum(data[i].isnull())",33,"for i in data.columns:
    print('The null values in',i,'are',sum(data[i].isnull()))",[0;36m
./executed/file149.ipynb,32,"for i in data.columns:
    print('The null values in',i,'are',sum(data[i].isnull())",33,"for i in data.columns:
    print('The null values in',i,'are',sum(data[i].isnull()))",[0;36m
