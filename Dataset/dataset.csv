filename,cell number,code,Primary-label,Secondary-label,
file1,0,"print(""Hello World :)"")",comment only,none,
file1,1,get_ipython().system('pip install db-dtypes'),helper functions,none,
file1,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,none,390
file1,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file1,4,"query_get_tables = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 query_job.to_dataframe().head()",load data,,
file1,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file1,6,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(2)",load data,,
file1,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id IN (SELECT uu_id
  FROM `ironhacks-data.ironhacks_competition.prediction_list`
  WHERE week_number = (SELECT MAX(week_number) FROM `ironhacks-data.ironhacks_competition.prediction_list`))
 ORDER BY uu_id,week_number
 """"""",load data,,
file1,8,relevant_unemployment_df.head(3),data exploration,,
file1,9,"relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.isna().sum()
 relevant_unemployment_df.info()",data preprocessing,,
file1,10,"# Drop duplicate columns
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
 # Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].astype(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].astype(str)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df.info()",data preprocessing,data exploration,
file1,11,"print(relevant_unemployment_df['uu_id'].value_counts())
 print(relevant_unemployment_df['top_category_employer1'].value_counts())",data exploration,,
file1,12,wage_data,data exploration,,
file1,13,len(set(relevant_unemployment_df['uu_id'])),data exploration,,
file1,14,"relevant_unemployment_df.join(wage_data, how='left', on='uu_id')",data preprocessing,,
file1,15,relevant_unemployment_df['wage_data'] = relevant_unemployment_df['uu_id'].map(wage_data.set_index('uu_id')['average_wage']),data preprocessing,,
file1,16,wage_data.order_by('uu_id'),data preprocessing,,
file1,17,wage_data.sort_values('uu_id'),data preprocessing,,
file1,18,relevant_unemployment_df[relevant_unemployment_df.isna()],data preprocessing,,
file1,19,"relevant_unemployment_df.loc[relevant_unemployment_df['uu_id'] == ""a5c6dcff737e183f7931b472f10c3235""]",data exploration,,
file1,20,"set(relevant_unemployment_df.loc[relevant_unemployment_df['wage_data'].isna(),'uu_id'])",data preprocessing,,
file1,21,"# Drop duplicate columns
 relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)",data preprocessing,,
file1,22,"# Add Wage data
 relevant_unemployment_df['wage_data'] = relevant_unemployment_df['uu_id'].map(wage_data.set_index('uu_id')['average_wage'])
 relevant_unemployment_df.info()
 bad_wage_uu_ids = list(set(relevant_unemployment_df.loc[relevant_unemployment_df['wage_data'].isna(),'uu_id']))",comment only,,
file1,23,relevant_unemployment_df['uu_id'].value_counts(),data exploration,,
file1,24,"relevant_unemployment_df.loc[relevant_unemployment_df['uu_id']=='fec479d0202d6e1e3f051a9ee902ff5d',]",data exploration,,
file1,25,"relevant_unemployment_df.loc[relevant_unemployment_df['uu_id']=='e851e672d9f0f6700711449f8426d3b4',]",data exploration,,
file1,26,"relevant_unemployment_df.loc[relevant_unemployment_df['uu_id']=='5e819ecea31bac6db64c0ccf48818fa8',]",data exploration,,
file1,27,relevant_unemployment_df['week_number'].value_counts().plot.bar(),result visualization,,
file1,28,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import matplotlib.pyplot as plt",helper functions,,
file1,29,"relevant_unemployment_df['week_number'].value_counts().plot.bar()
 plt.plot(relevant_unemployment_df)",result visualization,,
file1,30,plt.plot(relevant_unemployment_df['week_number']),result visualization,,
file1,31,"plt.plot(relevant_unemployment_df['week_number'],relevant_unemployment_df['week_number'].value_counts())",result visualization,,
file1,32,relevant_unemployment_df.groupby(['week_number'])['week_number'].count(),data exploration,,
file1,33,"# Constants
 PRED_WEEK = 39",helper functions,,
file1,34,relevant_unemployment_df.columns,data exploration,,
file1,35,"# Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].astype(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].astype(str)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()",data preprocessing,,
file1,36,"temp_df = pd.DataFrame(columns=list(relevant_unemployment_df.columns))
 temp_df",data preprocessing,data exploration,
file1,37,"# for uu_id in set(relevant_unemployment_df['uu_id']):
  # for week in range(PRED_WEEK):",comment only,,
file1,38,"relevant_unemployment_df.loc[relevant_unemployment_df.uu_id==""5e819ecea31bac6db64c0ccf48818fa8"" & relevant_unemployment_df.week_number == 1,]",data exploration,,
file1,39,row,data exploration,,
file1,40,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==uu_id) 
  & (relevant_unemployment_df.week_number == week)),]
 row",data preprocessing,data exploration,
file1,41,"row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==""d5e819ecea31bac6db64c0ccf48818fa8"") 
  & (relevant_unemployment_df.week_number == 1)),]
 row.append([None, None, None, None, None, None, None, None, None])",data preprocessing,,
file2,0,"print(""Hello World :)"")",comment only,,
file2,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file2,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file2,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file2,4,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file2,5,"query_job = bigquery_client.query(query)
 print(query_job)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file2,6,"query_job = bigquery_client.query(query)
 print(query_job)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)
 unemployment_data",load data,data exploration,
file2,7,"query_get_tables = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 print(query_job)",load data,data exploration,
file2,8,"query_get_tables = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 query_job.to_dataframe().head()",load data,data exploration,
file2,9,"query_job = bigquery_client.query(query)
 prediction_list_data = query_job.to_dataframe()
 prediction_list_data.head(2)",load data,data exploration,
file2,10,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id IN (SELECT uu_id 
  FROM `ironhacks-data.ironhacks_competition.prediction_list`
  WHERE week_number == MAX(week_number))
 """"""",load data,,
file2,11,"query = """"""
 SELECT uu_id 
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 WHERE week_number = MAX(week_number)
 """"""",load data,,
file2,12,"query = """"""
 SELECT uu_id
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 HAVING week_number = (SELECT MAX(week_number) FROM `ironhacks-data.ironhacks_competition.prediction_list`)
 """"""",load data,,
file2,13,relevant_unemployment_df,data exploration,,
file2,14,relevant_unemployment_df.drop_duplicates(),data preprocessing,,
file2,15,"relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df",data preprocessing,data exploration,
file2,16,"relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.isna().sum()
 relevant_unemployment_df.info()",data preprocessing,data exploration,
file2,17,"# Drop duplicate columns
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
 # Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis=1,thresh=1)",data preprocessing,,
file2,18,relevant_unemployment_df['top_category_employer1'][0,data exploration,,
file2,19,relevant_unemployment_df['uu_id'][0],data exploration,,
file2,20,relevant_unemployment_df.convert_dtypes(),data preprocessing,,
file2,21,"# Drop duplicate columns
 # relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
 # Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].apply(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].apply(str)
 relevant_unemployment_df.info()",data preprocessing,data exploration,
file2,22,print(relevant_unemployment_df['uu_id'].value_counts()),data exploration,,
file2,23,"print(relevant_unemployment_df['uu_id'].value_counts())
 print(relevant_unemployment_df['top_category_employer1'].value_counts())",data exploration,,
file3,0,"from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error
 from sklearn.metrics import mean_absolute_error",helper functions,,
file3,1,"results = dict()
 predictions = dict()
 actual = dict()
 MSE = dict()
 MAE = dict()
 for pred_id in ids:
  predictions[pred_id] = []
  actual[pred_id] = []
  MSE[pred_id] = []
  MAE[pred_id] = []
  
  for wk in test_weeks:
  # Train test split
  if pred_id not in bad_wage_ids:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  else:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
  test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
  actual[pred_id].append(test_y.values[0])
  
  ## Prediction
  # Linear Model
  reg = LinearRegression().fit(train_x, train_y)
  prediction = reg.predict(test_x)
  predictions[pred_id].append(prediction[0])
  results[pred_id] = [actual[pred_id], predictions[pred_id]]
  
  # Error metrics
  # MSE[pred_id].append(mean_squared_error(actual[pred_id], predictions[pred_id]))
  # MAE[pred_id].append(mean_absolute_error(actual[pred_id], predictions[pred_id]))",modeling,prediction,
file3,2,results,evaluation,,
file3,3,"print(""Hello World :)"")",comment only,,
file3,4,get_ipython().system('pip install db-dtypes'),helper functions,,
file3,5,"import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import matplotlib.pyplot as plt
 import numpy as np",helper functions,,
file3,6,"# Constants
 PRED_WEEK = 40",helper functions,,
file3,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file3,8,"query_get_tables = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 query_job.to_dataframe().head()",load data,data exploration,
file3,9,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file3,10,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(2)",load data,data exploration,
file3,11,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id IN (SELECT uu_id
  FROM `ironhacks-data.ironhacks_competition.prediction_list`
  WHERE week_number = (SELECT MAX(week_number) FROM `ironhacks-data.ironhacks_competition.prediction_list`))
 ORDER BY uu_id,week_number
 """"""",load data,,
file3,12,relevant_unemployment_df.head(3),data exploration,,
file3,13,"# Drop duplicate columns
 relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)",data preprocessing,,
file3,14,"# Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].astype(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].astype(str)
 relevant_unemployment_df['real_data'] = True
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()",data preprocessing,,
file3,15,"# Add Wage data
 relevant_unemployment_df['wage_data'] = relevant_unemployment_df['uu_id'].map(wage_data.set_index('uu_id')['average_wage'])
 relevant_unemployment_df.info()
 bad_wage_uu_ids = list(set(relevant_unemployment_df.loc[relevant_unemployment_df['wage_data'].isna(),'uu_id']))",comment only,,
file3,16,"for uuid in set(relevant_unemployment_df[""uu_id""]):
  num_tracts = 0
  tracts=[]
  for tract in set(relevant_unemployment_df.loc[relevant_unemployment_df[""uu_id""]==uuid,'wage_data']):
  num_tracts += 1
  tracts.append(tract)
  if num_tracts > 1:
  print(uuid,num_tracts,tracts)
 relevant_unemployment_df",data preprocessing,data exploration,
file3,17,"import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)",helper functions,,
file3,18,"temp_df = pd.DataFrame(columns=list(relevant_unemployment_df.columns))
 my_ids = list(set(relevant_unemployment_df['uu_id']))
 for UUid in my_ids:
  ctyfips = None
  wage_data = None
  tract_vote = [None, 0]
  top_cat_1_vote = [None, 0]
  top_cat_2_vote = [None, 0]
  top_cat_3_vote = [None, 0]
  total_claims_avg = []
  total_claims_num = 0
  
  for week in range(1,PRED_WEEK):
  row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==UUid) 
  & (relevant_unemployment_df.week_number == week)),]
  if len(row) == 1:
  temp_df = pd.concat([temp_df, row])
  
  ### Store information about values to infer for rows without data
  ## Consistent within uu_id values
  ctyfips = row['countyfips'].values[0]
  wage_data = row['wage_data'].values[0]
  
  ## Inconsistent within uu_id values
  # Categorical 
 

  if ((tract_vote[0] == None) or ((tract_vote[0] != row['tract'].values[0]) and (tract_vote[1] == 1))):
  tract_vote = [row['tract'].values[0], 1]
  elif tract_vote[0] == row['tract'].values[0]:
  tract_vote[1] += 1
  else:
  tract_vote[1] -= 1
  
  if ((top_cat_1_vote[0] == None) or ((top_cat_1_vote[0] != row['top_category_employer1'].values[0]) and (top_cat_1_vote[1] == 1))):
  top_cat_1_vote = [row['top_category_employer1'].values[0], 1]
  elif top_cat_1_vote[0] == row['top_category_employer1'].values[0]:
  top_cat_1_vote[1] += 1
  else:
  top_cat_1_vote[1] -= 1
  
  if ((top_cat_2_vote[0] == None) or ((top_cat_2_vote[0] != row['top_category_employer2'].values[0]) and (top_cat_2_vote[1] == 1))):
  top_cat_2_vote = [row['top_category_employer2'].values[0], 1]
  elif top_cat_2_vote[0] == row['top_category_employer2'].values[0]:
  top_cat_2_vote[1] += 1
  else:
  top_cat_2_vote[1] -= 1
  
  if ((top_cat_3_vote[0] == None) or ((top_cat_3_vote[0] != row['top_category_employer3'].values[0]) and (top_cat_3_vote[1] == 1))):
  top_cat_3_vote = [row['top_category_employer3'].values[0], 1]
  elif top_cat_3_vote[0] == row['top_category_employer3'].values[0]:
  top_cat_3_vote[1] += 1
  else:
  top_cat_3_vote[1] -= 1
  
  # Numerical
  total_claims_avg.append(row['total_claims'].values[0])
  if len(total_claims_avg) > 0:
  total_claims_num = np.mean(total_claims_avg)
 

  continue
  temp_df = pd.concat([temp_df, pd.DataFrame.from_records([{'uu_id': UUid, 
  'week_number': week, 
  'countyfips': ctyfips, 
  'tract': tract_vote[0],
  'total_claims': total_claims_num, 
  'top_category_employer1': top_cat_1_vote[0],
  'top_category_employer2': top_cat_2_vote[0], 
  'top_category_employer3': top_cat_3_vote[0],
  'real_data': False, 
  'wage_data': wage_data}])])",data preprocessing,,
file3,19,"temp_df.reset_index(inplace=True)
 temp_df.drop('index', axis=1,inplace=True)
 temp_df.info()",data preprocessing,data exploration,
file3,20,temp_df.head(),data exploration,,
file3,21,"non_null_cols = [""countyfips"", ""tract"", ""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3"", ""wage_data""]",data preprocessing,,
file3,22,"for UUid2 in my_ids[::-1]:
  for week_num in range(PRED_WEEK-1,0,-1):
  row = temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num)),]
  if row['countyfips'].values[0] == None:
  for col in non_null_cols:
  temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num)), col] = temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num+1)), col].values[0]",data preprocessing,,
file3,23,temp_df.info(),data exploration,,
file3,24,"temp_df = temp_df.convert_dtypes()
 temp_df.info()",data preprocessing,data exploration,
file3,25,"bad_wage_ids=set(temp_df.loc[temp_df['wage_data'].isna(),""uu_id""])",data preprocessing,,
file3,26,"data = temp_df
 data.rename(columns={""uu_id"": ""id"", ""week_number"": ""week"", ""countyfips"": ""fips"", ""top_category_employer1"": ""emp1""}, inplace=True)
 data.rename(columns={""top_category_employer2"": ""emp2"", ""top_category_employer3"": ""emp3"", ""real_data"": ""real"", ""wage_data"":""wages""}, inplace=True)
 for col in ['fips', 'tract', 'emp1', 'emp2', 'emp3']:
  data[col] = pd.factorize(data[col])[0]",data preprocessing,,
file3,27,data.head(),data exploration,,
file3,28,data.info(),data exploration,,
file3,29,abs(data.corr()['total_claims']),data exploration,,
file3,30,"ids = list(set(data['id']))
 weeks = list(set(data['week']))
 test_weeks = weeks[PRED_WEEK-10:]",data preprocessing,,
file3,31,"from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error
 from sklearn.metrics import mean_absolute_error",helper functions,,
file3,32,"results = dict()
 predictions = dict()
 actual = dict()
 MSE = dict()
 MAE = dict()
 for pred_id in ids:
  predictions[pred_id] = []
  actual[pred_id] = []
  MSE[pred_id] = []
  MAE[pred_id] = []
  
  for wk in test_weeks:
  # Train test split
  if pred_id not in bad_wage_ids:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  else:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
  test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
  actual[pred_id].append(test_y.values[0])
  
  ## Prediction
  # Linear Model
  reg = LinearRegression().fit(train_x, train_y)
  prediction = reg.predict(test_x)
  predictions[pred_id].append(prediction[0])
  results[pred_id] = [actual[pred_id], predictions[pred_id]]
  
  # Error metrics
  # MSE[pred_id].append(mean_squared_error(actual[pred_id], predictions[pred_id]))
  # MAE[pred_id].append(mean_absolute_error(actual[pred_id], predictions[pred_id]))",modeling,prediction,
file3,33,results,evaluation,none,
file3,34,pd.DataFrame.from_dict(results),data exploration,,
file3,35,"pd.DataFrame.from_dict(results, orient='index')",data exploration,,
file3,36,"pd.DataFrame.from_dict(results, orient='index').columns = ['actual', 'prediction']",data exploration,,
file3,37,"print(""Hello World :)"")",comment only,,
file3,38,get_ipython().system('pip install db-dtypes'),helper functions,,
file3,39,"import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import matplotlib.pyplot as plt
 import numpy as np",helper functions,,
file3,40,"# Constants
 PRED_WEEK = 40",helper functions,,
file3,41,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file3,42,"query_get_tables = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 query_job.to_dataframe().head()",load data,data exploration,
file3,43,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file3,44,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(2)",load data,data exploration,
file3,45,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id IN (SELECT uu_id
  FROM `ironhacks-data.ironhacks_competition.prediction_list`
  WHERE week_number = (SELECT MAX(week_number) FROM `ironhacks-data.ironhacks_competition.prediction_list`))
 ORDER BY uu_id,week_number
 """"""",load data,,
file3,46,relevant_unemployment_df.head(3),data exploration,,
file3,47,"# Drop duplicate columns
 relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)",data preprocessing,,
file3,48,"# Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].astype(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].astype(str)
 relevant_unemployment_df['real_data'] = True
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()",data preprocessing,,
file3,49,"# Add Wage data
 relevant_unemployment_df['wage_data'] = relevant_unemployment_df['uu_id'].map(wage_data.set_index('uu_id')['average_wage'])
 relevant_unemployment_df.info()
 bad_wage_uu_ids = list(set(relevant_unemployment_df.loc[relevant_unemployment_df['wage_data'].isna(),'uu_id']))",data preprocessing,,
file3,50,"for uuid in set(relevant_unemployment_df[""uu_id""]):
  num_tracts = 0
  tracts=[]
  for tract in set(relevant_unemployment_df.loc[relevant_unemployment_df[""uu_id""]==uuid,'wage_data']):
  num_tracts += 1
  tracts.append(tract)
  if num_tracts > 1:
  print(uuid,num_tracts,tracts)
 relevant_unemployment_df",data preprocessing,data exploration,
file3,51,"import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)",helper functions,,
file3,52,"temp_df = pd.DataFrame(columns=list(relevant_unemployment_df.columns))
 my_ids = list(set(relevant_unemployment_df['uu_id']))
 for UUid in my_ids:
  ctyfips = None
  wage_data = None
  tract_vote = [None, 0]
  top_cat_1_vote = [None, 0]
  top_cat_2_vote = [None, 0]
  top_cat_3_vote = [None, 0]
  total_claims_avg = []
  total_claims_num = 0
  
  for week in range(1,PRED_WEEK):
  row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==UUid) 
  & (relevant_unemployment_df.week_number == week)),]
  if len(row) == 1:
  temp_df = pd.concat([temp_df, row])
  
  ### Store information about values to infer for rows without data
  ## Consistent within uu_id values
  ctyfips = row['countyfips'].values[0]
  wage_data = row['wage_data'].values[0]
  
  ## Inconsistent within uu_id values
  # Categorical 
 

  if ((tract_vote[0] == None) or ((tract_vote[0] != row['tract'].values[0]) and (tract_vote[1] == 1))):
  tract_vote = [row['tract'].values[0], 1]
  elif tract_vote[0] == row['tract'].values[0]:
  tract_vote[1] += 1
  else:
  tract_vote[1] -= 1
  
  if ((top_cat_1_vote[0] == None) or ((top_cat_1_vote[0] != row['top_category_employer1'].values[0]) and (top_cat_1_vote[1] == 1))):
  top_cat_1_vote = [row['top_category_employer1'].values[0], 1]
  elif top_cat_1_vote[0] == row['top_category_employer1'].values[0]:
  top_cat_1_vote[1] += 1
  else:
  top_cat_1_vote[1] -= 1
  
  if ((top_cat_2_vote[0] == None) or ((top_cat_2_vote[0] != row['top_category_employer2'].values[0]) and (top_cat_2_vote[1] == 1))):
  top_cat_2_vote = [row['top_category_employer2'].values[0], 1]
  elif top_cat_2_vote[0] == row['top_category_employer2'].values[0]:
  top_cat_2_vote[1] += 1
  else:
  top_cat_2_vote[1] -= 1
  
  if ((top_cat_3_vote[0] == None) or ((top_cat_3_vote[0] != row['top_category_employer3'].values[0]) and (top_cat_3_vote[1] == 1))):
  top_cat_3_vote = [row['top_category_employer3'].values[0], 1]
  elif top_cat_3_vote[0] == row['top_category_employer3'].values[0]:
  top_cat_3_vote[1] += 1
  else:
  top_cat_3_vote[1] -= 1
  
  # Numerical
  total_claims_avg.append(row['total_claims'].values[0])
  if len(total_claims_avg) > 0:
  total_claims_num = np.mean(total_claims_avg)
 

  continue
  temp_df = pd.concat([temp_df, pd.DataFrame.from_records([{'uu_id': UUid, 
  'week_number': week, 
  'countyfips': ctyfips, 
  'tract': tract_vote[0],
  'total_claims': total_claims_num, 
  'top_category_employer1': top_cat_1_vote[0],
  'top_category_employer2': top_cat_2_vote[0], 
  'top_category_employer3': top_cat_3_vote[0],
  'real_data': False, 
  'wage_data': wage_data}])])",data preprocessing,,
file3,53,"temp_df.reset_index(inplace=True)
 temp_df.drop('index', axis=1,inplace=True)
 temp_df.info()",data preprocessing,data exploration,
file3,54,temp_df.head(),data exploration,,
file3,55,"non_null_cols = [""countyfips"", ""tract"", ""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3"", ""wage_data""]",data preprocessing,,
file3,56,"for UUid2 in my_ids[::-1]:
  for week_num in range(PRED_WEEK-1,0,-1):
  row = temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num)),]
  if row['countyfips'].values[0] == None:
  for col in non_null_cols:
  temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num)), col] = temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num+1)), col].values[0]",data preprocessing,,
file3,57,temp_df.info(),data exploration,,
file3,58,"temp_df = temp_df.convert_dtypes()
 temp_df.info()",data preprocessing,data exploration,
file3,59,"bad_wage_ids=set(temp_df.loc[temp_df['wage_data'].isna(),""uu_id""])",data preprocessing,,
file3,60,"data = temp_df
 data.rename(columns={""uu_id"": ""id"", ""week_number"": ""week"", ""countyfips"": ""fips"", ""top_category_employer1"": ""emp1""}, inplace=True)
 data.rename(columns={""top_category_employer2"": ""emp2"", ""top_category_employer3"": ""emp3"", ""real_data"": ""real"", ""wage_data"":""wages""}, inplace=True)
 for col in ['fips', 'tract', 'emp1', 'emp2', 'emp3']:
  data[col] = pd.factorize(data[col])[0]",data preprocessing,,
file3,61,data.head(),data exploration,,
file3,62,data.info(),data exploration,,
file3,63,abs(data.corr()['total_claims']),data exploration,,
file3,64,"ids = list(set(data['id']))
 weeks = list(set(data['week']))
 test_weeks = weeks[PRED_WEEK-10:]",data preprocessing,,
file3,65,"from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error
 from sklearn.metrics import mean_absolute_error",helper functions,,
file3,66,"results = dict()
 predictions = dict()
 actual = dict()
 MSE = dict()
 MAE = dict()
 for pred_id in ids:
  predictions[pred_id] = []
  actual[pred_id] = []
  MSE[pred_id] = []
  MAE[pred_id] = []
  
  for wk in test_weeks:
  # Train test split
  if pred_id not in bad_wage_ids:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  else:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
  test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
  actual[pred_id].append(test_y.values[0])
  
  ## Prediction
  # Linear Model
  reg = LinearRegression().fit(train_x, train_y)
  prediction = reg.predict(test_x)
  predictions[pred_id].append(prediction[0])
  results[pred_id] = [actual[pred_id], predictions[pred_id]]
  
  # Error metrics
  # MSE[pred_id].append(mean_squared_error(actual[pred_id], predictions[pred_id]))
  # MAE[pred_id].append(mean_absolute_error(actual[pred_id], predictions[pred_id]))",modeling,prediction,
file3,67,"results_df = pd.DataFrame.from_dict(results, orient='index')
 results_df.columns = ['actual', 'prediction']
 results_df",data preprocessing,data exploration,
file3,68,"results_df = pd.DataFrame.from_dict(results, orient='index')
 results_df.columns = ['actual', 'prediction']
 results_df=results_df.explode(['actual','prediction'])
 #results_df['week'] = []
 1:10",data preprocessing,,
file4,0,"print(""Hello World"")",comment only,,
file4,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file4,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file4,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file4,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file4,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file5,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file5,1,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data exploration,
file5,2,"import os
 import pandas
 import pandas as pd
 import numpy as np
 import matplotlib
 import itertools
 import matplotlib.pyplot as plt
 import seaborn as sns
 # import lightgbm as lgb
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file5,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file5,4,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` d
 LEFT JOIN `ironhacks-data.ironhacks_competition.wage_data` i
 ON d.uu_id = i.uu_id;
 """"""",load data,,
file5,5,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file5,6,len(pdf),data exploration,,
file5,7,"wdf.to_csv(""wage.csv"")
 udf.to_csv(""unemploy.csv"")
 Mdf.to_csv(""mixed.csv"")
 pdf.to_csv(""pred.csv"")
 # udf = pd.read_csv(""unemploy.csv"")
 # wdf = pd.read_csv(""wage.csv"")
 # mdf = pd.read_csv(""mixed.csv"")
 # pdf = pd.read_csv(""pred.csv"")",data preprocessing,,
file5,8,"udf[""""week_number].unique()
 # udf.nunique()",data exploration,,
file5,9,"print(udf[""week_number""].unique())
 print(wdf[""week_number""].unique())
 print(mdf[""week_number""].unique())
 print(pdf[""week_number""].unique())
 # udf.nunique()",data exploration,,
file5,10,"# cleaning the data by imputing null and nan with 0
 mdf = mdf.replace(np.nan, 0)
 udf = udf.replace(np.nan, 0)",data preprocessing,,
file5,11,"mdf_1 = mdf.groupby(""uu_id"").mean()
 mdf_1",data preprocessing,data exploration,
file5,12,mdf.dtypes,data exploration,,
file5,13,"# removing unwanted columns
 mdf_1 = mdf.drop([""uu_id_1"",""countyfips_1"",""tract_1"",""tract_name_1"",""tract_name""])
 mdf_1.columns",data preprocessing,data exploration,
file5,14,"emp_cols = [""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('31-33', 31.6)
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('48-49', 48.5)
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('44-45', 44.5)",data preprocessing,,
file5,15,"mdf_1[""top_category_employer1""] = pd.to_numeric(mdf_1[""top_category_employer1""])
 mdf_1[""top_category_employer2""] = pd.to_numeric(mdf_1[""top_category_employer2""])
 mdf_1[""top_category_employer3""] = pd.to_numeric(mdf_1[""top_category_employer3""])",data preprocessing,,
file5,16,"# understanding the datatypes for each columns of the dataframe
 mdf.dtypes",data exploration,,
file5,17,"mdf_1[""top_category_employer1""]",data exploration,,
file5,18,mdf_1.describe(),data exploration,,
file5,19,mdf_1.info(),data exploration,,
file5,20,"plt.figure(figsize=(15,5))
 plt.plot(df['uu_id'],df['total_claims'])
 plt.show()",result visualization,,
file5,21,"plt.figure(figsize=(15,5))
 plt.plot(mdf_2['total_claims'],mdf_2['average_wage'])
 plt.show()",result visualization,,
file5,22,"mdf_3 = mdf_1.groupby(""week_number"").agg(pd.Series.mode)
 mdf_3",data preprocessing,data exploration,
file5,23,"mdf_3 = mdf_1.groupby(""week_number"").median()
 mdf_3.head(5)",data preprocessing,data exploration,
file5,24,"k = 0
 for x in mdf_1[""uu_id""].unique():
  if 36 in mdf_1[[""uu_id"" == x,""week_number""]]:
  k+=1
 k",data preprocessing,data exploration,
file5,25,"k = 0
 u_uuid = mdf_1[""uu_id""].unique()
 for x in mdf_1:
  if mdf_1[""uu_id""] in u_uuid and mdf_1[""week_number""] == 36:
  k+=1
  u_uuid.remove(mdf_1[""uu_id""])
 print(u_uuid)
 k",data preprocessing,data exploration,
file5,26,"k = 0
 u_uuid = []
 for i, row in mdf_1.iterrows():
  if row[""uu_id""] not in u_uuid and row[""week_number""] == 36:
  k+=1
  u_uuid.append(row[""uu_id""])
 print(u_uuid)
 len(k)",data preprocessing,data exploration,
file5,27,"print(len(u_uuid))
 k",data exploration,,
file5,28,"k = 0
 u_uuid = []
 for i, row in mdf_1.iterrows():
  if row[""uu_id""] not in u_uuid:
  if row[""week_number""] == 36 or row[""week_number""] == 37 or row[""week_number""] == 35 or row[""week_number""] == 34:
  k+=1
  su_uuid.append(row[""uu_id""])",data preprocessing,,
file5,29,"k = 0
 u_uuid = []
 for i, row in mdf_1.iterrows():
  if row[""uu_id""] not in u_uuid:
  if row[""week_number""] in [37,36,35,34,33,32,31,30,29,28,27]:
  k+=1
  u_uuid.append(row[""uu_id""])",data preprocessing,,
file5,30,"mdf_5 = mdf_1.groupby(['uu_id'], sort=False)['week_number'].max()
 mdf_5",data preprocessing,data exploration,
file5,31,"mdf_5 = mdf_1.groupby(['uu_id'], sort=False)['week_number'].max()
 u_1uid = []
 for i, row in mdf_5.iterrows():
  if row[""week_number""] != 37:
  u_1uid.append(row[""uu_id""])",data preprocessing,,
file5,32,"print(len(u_1uid))
 len(u_1uid)",data exploration,,
file5,33,"mdf_5 = mdf_1.groupby(['uu_id'], sort=False)['week_number'].max()
 u_1uid = mdf_5.index
 print(mdf_5.index)
 for row in mdf_5:
  print(row.index)
  if row != 37:
  u_1uid.remove(row[""uu_id""])",data preprocessing,,
file5,34,"for row in mdf_5.index:
  if mdf_5[row] != 37:
  u_1uid.append(rows)",data preprocessing,,
file5,35,mdf_1.columns(),data exploration,,
file5,36,"arr = [0]*93
 dct = {}
 coll = mdf_1.columns",data preprocessing,,
file5,37,"for cal in coll:
  if call == 'uu_id':
  dct[cal] = u_1uid
  else:
  dct[cal] = arr",data preprocessing,,
file5,38,dct,data exploration,,
file5,39,"for cal in coll:
  if cal == 'uu_id':
  dct[cal] = u_1uid
  elif cal == 'week_number':
  dct[cal] = arr_week
  else:
  dct[cal] = arr",data preprocessing,,
file5,40,"mdf_6 = pd.DataFrame(dct)
 mdf_6",data preprocessing,data exploration,
file5,41,"mdf_6 = pd.DataFrame(dct)
 mdf_6 = mdf_6.append(mdf_1, ignore_index = True)",data preprocessing,,
file5,42,"mdf_p = mdf_1[mdf_1[""week_number"" == 37]]
 mdf_r = mdf_1[mdf_1[""week_number"" != 37]]
 len(mdf_p)",data preprocessing,data exploration,
file5,43,"mdf_p = mdf_6[mdf_6[""week_number""] == 37]
 mdf_r = mdf_6[mdf_6[""week_number""] != 37]
 mdf_p = mdf_p.groupby(""uu_id"").max()",data preprocessing,,
file5,44,"mdf1_r = mdf_r[""week_number""] - 7
 mdf2_r = mdf1_r[mdf1_r[""week_number""] > 0]
 mdf2_r",data preprocessing,data exploration,
file5,45,"f1_r = mdf_r[""week_number""] - 7
 mdf1_r = mdf_r
 mdf1_r[""week_number""] = f1_r
 # mdf2_r = mdf1_r[mdf1_r[""week_number""] > 0]
 # mdf2_r",data preprocessing,,
file5,46,"import os
 import pandas
 import pandas as pd
 import numpy as np
 import matplotlib
 import itertools
 import matplotlib.pyplot as plt
 import seaborn as sns
 # import lightgbm as lgb
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split",helper functions,,
file5,47,"mdf2_r[""tract""].nunique()",data exploration,,
file5,48,"# mdf2_r[""tract""].nunique()
 mdf2_r[""uu_id""].nunique()",data exploration,,
file5,49,"mdf_p = mdf_6[mdf_6[""week_number""] == 37]
 mdf_r = mdf_6[mdf_6[""week_number""] != 37]
 print(mdf_r[""uu_id""].nunique())
 mdf_r = mdf_r.drop_duplicates()
 mdf_p = mdf_p.groupby(""uu_id"").max()
 mdf_p",data preprocessing,data exploration,
file5,50,"mdf_p = mdf_6[mdf_6[""week_number""] == 37]
 mdf_r = mdf_6[mdf_6[""week_number""] != 37]
 mdf_r = mdf_r.drop_duplicates(subset=[""uu_id"", ""week_number"", ""tract""])
 print(len(mdf_r))
 mdf_p = mdf_p.groupby(""uu_id"").max()
 mdf_p",data preprocessing,data exploration,
file5,51,"k = mdf_r[""week_number""].max()
 k",data preprocessing,data exploration,
file5,52,"print(mdf_r[""week_number""].max())
 f1_r = mdf_r[""week_number""] - 7
 mdf1_r = mdf_r
 mdf1_r[""week_number""] = f1_r
 mdf1_r[""week_number""] 
 mdf2_r = mdf1_r[mdf1_r[""week_number""] > 0]
 mdf2_r.head(5)",data preprocessing,data exploration,
file5,53,"k = mdf_r[""week_number""].max()+1
 dct1 = {}
 inx = []
 for x in mdf_p.index:
  k+=1
  dct1[x] = k
  inx.append(k)",data preprocessing,,
file5,54,inx,data exploration,,
file5,55,len(inx),data exploration,,
file5,56,"mdf_p[""int_uu_id""]= inx
 mdf_p.head(5)",data preprocessing,data exploration,
file5,57,"mdf_p[""int_uu_id""]= inx
 mdf_p[""nwk""]= mdf_p[""int_uu_id""] + mdf_p[""week_number""]
 mdf_p.head(5)",data preprocessing,data exploration,
file5,58,"inx1 = []
 for i,rows in mdf_r.iterrows:
  inx1.append(dct1[rows[""uu_id""]])",data preprocessing,,
file5,59,"# train test split
 X = mdf_r.drop([""uu_id"",""week_number"",""total_claims""], axis = 0)
 y = mdf_r[""total_claims""]",data preprocessing,,
file5,60,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)",data preprocessing,,
file5,61,"# training and fitting the model
 regr = linear_model.LinearRegression()
 regr.fit(X_train, y_train)
 regr.score(x_test,y_test)",modeling,evaluation,
file5,62,"# prepare the data to predict
 pre = mdf_p.drop([""uu_id"",""week_number"",""total_claims""], axis = 1)
 predict = regr.predict(pre)
 print(len(predict))
 predict",prediction,data exploration,
file5,63,"d = {'uu_id': mdf_p.index, ""total_claims"": mdf_p[""total_claims""], 'week_number': mdf_p[""week_number""]}
 rmdf = pd.DataFrame(data=d)
 rmdf.head(5)",data preprocessing,data exploration,
file5,64,"ulist = mdf_p.index
 tc = mdf_p[""total_claims""]
 wn = mdf_p[""week_number""]
 d = {'uu_id': ulist, ""total_claims"": tc, 'week_number': wn}
 rmdf = pd.DataFrame(d)
 rmdf.head(5)",data preprocessing,data exploration,
file5,65,"lxt1 = [i for i in range(1,501)]
 d = {'uu_id': mdf_p.index, ""total_claims"": predict, 'week_number': mdf_p[""week_number""]}
 rmdf = pd.DataFrame(d)
 rmdf.index = lxt1
 print(len(rmdf))
 rmdf.head(5)",data preprocessing,data exploration,
file5,66,"rmdf.to_csv(""submission_prediction_output.csv"")",data exploration,,
file5,67,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file6,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file6,1,from google.cloud import bigquery,helper functions,,
file6,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file6,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file6,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file6,5,unemployment_data,data exploration,,
file6,6,unemployment_data['total_claims'].head,data exploration,,
file6,7,unemployment_data['week_number'].head,data exploration,,
file6,8,unemployment_data.describe(),data exploration,,
file6,9,unmeployment_data[['total_claims']].groupby('week_number').mean(),data exploration,,
file6,10,unemployment_data.groupby('week_number')['total_claims'].mean(),data exploration,,
file6,11,wage_data.head,data exploration,,
file6,12,prediction_list.head(,data exploration,,
file6,13,"merged_data = prediction_list.merge(unemployment_data, on='uu_id',how='left')",data preprocessing,,
file6,14,merged_data,data exploration,,
file6,15,"merged_data.shape,prediction_list.shape",data exploration,,
file6,16,"merged_data.shape,prediction_list.shape,unemployment_data.shape",data exploration,,
file6,17,import pandas as pd,helper functions,,
file6,18,merged_data.shape,data exploration,,
file6,19,merged_data.head(),data exploration,,
file6,20,unemplyment_data.value_counts(),data exploration,,
file7,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file7,1,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",helper functions,,
file7,2,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",helper functions,data exploration,
file8,0,"# wdf.to_csv(""wage.csv"")
 # udf.to_csv(""unemploy.csv"")
 # Mdf.to_csv(""mixed.csv"")
 udf = pd.read_csv(""unemploy.csv"")
 wdf = pd.read_csv(""wage.csv"")
 mdf = pd.read_csv(""mixed.csv"")",save results,,
file8,1,"import pandas as pd
 import numpy as np",helper functions,,
file8,2,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file8,3,"# QUERY THE DATA ONCE
 query1_job = bigquery_client.query(query1)
 pdf = query1_job.to_dataframe()
 pdf.head()",load data,,
file8,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file8,5,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file8,6,get_ipython().system('pip install db-dtypes'),helper functions,,
file9,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file9,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file9,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file9,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file9,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file10,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file10,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file10,2,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file10,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",data preprocessing,data exploration,
file10,4,"covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",data preprocessing,data exploration,
file10,5,print(covid19_cases_data),data exploration,,
file10,6,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file10,7,"import os
 import pandas",helper functions,,
file10,8,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file10,9,"query_job = bigquery_client.query(query)
 import db_dtypes
 date_dtype_name = db.DateDtype.name
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file11,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file11,1,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file12,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file12,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file12,2,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.unemployment_data`
 """"""",load data,,
file12,3,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file13,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file13,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 un_data = querydb(query)",load data,,
file13,2,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file13,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",data preprocessing,data exploration,
file13,4,"get_ipython().run_cell_magic('capture', '', '%logstop\n%logstart -t -r -q ipython_command_log.py global\n')",helper functions,,
file13,5,"import os
 from datetime import datetime
 import IPython.core.history as history",helper functions,,
file13,6,"ha = history.HistoryAccessor()
 ha_tail = ha.get_tail(1)
 ha_cmd = next(ha_tail)
 session_id = str(ha_cmd[0])
 command_id = str(ha_cmd[1])
 timestamp = datetime.utcnow().isoformat()
 history_line = ','.join([session_id, command_id, timestamp]) + '\n'
 logfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')
 logfile.write(history_line)
 logfile.close()",helper functions,,
file13,7,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file13,8,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file13,9,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file13,10,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head()",data preprocessing,data exploration,
file14,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file14,1,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file14,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file14,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file14,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file14,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file14,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file14,7,"X = data.drop(""total_claims"",1)  #Feature Matrix
 X = data.drop(""week_number"",1)
 y = data[""total_claims""] #Target Variable",data preprocessing,,
file14,8,"plt.figure(figsize=(24,20))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data exploration,
file14,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file14,10,"data.plot(subplots=True, figsize=(20,24))",result visualization,data exploration,
file14,11,"min(data['week_number']),max(data['week_number'])",data exploration,,
file14,12,data.dtypes,data exploration,,
file14,13,data = data.set_index('week_number'),data preprocessing,,
file14,14,data.index,data exploration,,
file14,15,"data.sample(37, random_state=0)",data exploration,,
file14,16,data.loc[37],data exploration,,
file14,17,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file14,18,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file15,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file15,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file15,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file15,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file15,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file15,5,"query = """"""
 SELECT date, max_rel_humidity
 FROM ironhacks-data.ironhacks_training.weather_data
 WHERE date='2020-06-16'
 """"""
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",data preprocessing,,
file15,6,"query = """"""
 

 Select 
 a.*,
 b.cases 
 

 FROM 
 

 (SELECT 
 extract(week(Monday) from date) as week_number,
 AVG(mean_temperature) as mean_temperature_week,
 date as start_date,
 AVG(wind_speed) as mean_wind_speed_week
 FROM `ironhacks_training.weather_data`
 group by week_number,start_date) a
 

 JOIN `ironhacks-data.ironhacks_training.covid19_cases` b 
 ON a.week_number=b.week_number
 order by week_number
 

 

 

 """"""",data preprocessing,,
file16,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file16,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file16,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file16,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file16,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data` ) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",data preprocessing,,
file16,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",data preprocessing,,
file16,6,empdata.head().transpose(),data exploration,,
file16,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data exploration,
file16,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file16,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data exploration,,
file16,10,empdata[['total_claims']].describe(),data exploration,,
file16,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,data exploration,
file16,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",data preprocessing,result visualization,
file16,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,data exploration,
file16,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file16,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file16,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",data preprocessing,prediction,
file16,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` order by uu_id
 """"""",load data,,
file16,18,uupred.head(),data exploration,,
file16,19,"last_week = int(empdata['week_number'].max())
 last_week",data preprocessing,data exploration,
file16,20,"pred_week = int(uupred['week_number'].max())
 pred_week",data preprocessing,data exploration,
file16,21,"uupred['total_claims'] = 0
 uupred.head()",data preprocessing,data exploration,
file16,22,"for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,last_week+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values, seasonal=False, error_action='ignore')
  pred = int(m.predict(n_periods=pred_week-last_week)[pred_week-last_week-1])
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",data preprocessing,data exploration,
file16,23,"uupred = uupred[['uu_id', 'total_claims', 'week_number']]
 uupred.to_csv('submission_prediction_output.csv', index=False)",save results,,
file16,24,1,comment only,,
file17,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file17,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file17,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file17,3,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file17,4,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('pip install numpy scikit-learn statsmodels')",helper functions,,
file17,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file17,6,"query = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file17,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data preprocessing,data exploration,
file18,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file18,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file18,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file18,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file18,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",data preprocessing,data exploration,
file18,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file18,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",data preprocessing,,
file18,7,print(covid19_cases_data),data exploration,,
file18,8,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,data preprocessing,
file19,0,"get_ipython().system('pip install transformers')
 get_ipython().system('pip install torch')
 get_ipython().system('pip install -r requirements.txt')",helper functions,,
file19,1,"import torch
 from transformers import AutoTokenizer, AutoModel
 tokenizer = AutoTokenizer.from_pretrained(""GanjinZero/UMLSBert_ENG"")
 model = AutoModel.from_pretrained(""GanjinZero/UMLSBert_ENG"")
 import pandas as pd",helper functions,,
file19,2,"# read in scraped mayo symptoms
 with open(""mayo_data.json"", 'r') as j:
  data = json.loads(j.read())",data preprocessing,,
file19,3,"symptoms=[]
 specialties=[]
 diseases=[]",data preprocessing,,
file19,4,"for disease in list(data.keys()):
  list_of_symptoms=data[disease]['symptoms']
  if len(list_of_symptoms)==0:
  pass
  symptoms.append(list_of_symptoms)
  diseases.append([disease]*len(list_of_symptoms))
  list_of_specialties=data[disease]['specialties']
  specialtystr=','.join(list_of_specialties)
  specialties.append([specialtystr]*len(list_of_symptoms))",data preprocessing,,
file19,5,"def flatten(l):
  return [item for sublist in l for item in sublist]",data preprocessing,,
file19,6,"diseases=flatten(diseases)
 symptoms=flatten(symptoms)
 specialties=flatten(specialties)",data preprocessing,,
file19,7,"lst = [diseases,symptoms,specialties]
 df = pd.DataFrame(
  {'disease': diseases,
  'symptoms': symptoms,
  'specialties': specialties
  })",data preprocessing,,
file19,8,"def get_pooled_embedding(text, model, tokenizer):
  tokenized_input = tokenizer(text, return_tensors='pt')
  output = model(**tokenized_input)
  
  return output.pooler_output.detach().numpy()[0]",data preprocessing,,
file19,9,"def create_pooled_embedding_df(model, tokenizer, df):
  df['pooled_embedding'] = df.symptoms.apply(get_pooled_embedding, args=(model, tokenizer))
  
  return df",data preprocessing,,
file19,10,"# add embedding column and save to pickle obj
 df= create_pooled_embedding_df(model, tokenizer, df)
 print(df)
 #df.to_pickle(""mayo_symptoms_embeddings.plk"")",data preprocessing,data exploration,
file19,11,import torch,helper functions,,
file20,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file20,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file20,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file20,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY-PROJECT)",load data,,
file20,4,"query = """"""
 select * from 'ironhacks-data.ironhacks_training.weather_data'
 """"""",load data,,
file20,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']=pd.to_dataframe(data['date'])
 data.head()",data preprocessing,data exploration,
file20,6,print(data['precipitation_data']),data exploration,,
file20,7,"X=data.drop(""potential_water_deficit"",1)",data preprocessing,,
file20,8,"X=data.drop(""potential_water_deficit"",1)
 X = data.drop(""date"",1)
 data",data preprocessing,data exploration,
file20,9,"X=data.drop(""potential_water_deficit"",1)
 X=data.drop(""date"",1)
 y=data[""mean_temperature""]
 data.head()",data preprocessing,data exploration,
file20,10,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data exploration,
file20,11,"X=data.drop(""date"",1)
 X=data.drop(""potential_water_deficit"",1)
 y=data[""mean_temperature""]
 print(X)
 print(y)
 # data.head()",data preprocessing,data exploration,
file20,12,"cor_target = abs(cor[""mean_temperature""])
 relevant_features = cor_target[cor_target>0.75]
 print(relevant_features)",data preprocessing,data exploration,
file21,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file21,1,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file21,2,get_ipython().system('pip3 install git+https://github.com/ourownstory/neural_prophet.git#egg=neuralprophet'),helper functions,,
file21,3,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file21,4,get_ipython().system('pip3 install neuralprophet'),helper functions,,
file21,5,"import pandas as pd 
 from neuralprophet import NeuralProphet
 from matplotlib import pyplot as plt
 import pickle",helper functions,,
file21,6,"!pip3 install neuralprophet
 get_ipython().system('pip uninstall neuralprophet')",helper functions,,
file21,7,get_ipython().system('pip install db-dtypes'),helper functions,,
file21,8,get_ipython().system('pip install torch==1.6.0'),helper functions,,
file21,9,"# Let's look at the wage_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file21,10,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head(3)",data preprocessing,data exploration,
file21,11,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file21,12,"#Number of rows in wage_data
 len(wage_data.index)",data exploration,,
file21,13,"#Let's merge the unemployment_data and wage_data
 # We will merge only cols uu_id and average_wage from wage_data into unemployment_data
 Merged_unemployment_wage = unemployment_data.merge(wage_data[['uu_id', 'average_wage']])
 Merged_unemployment_wage.columns",data preprocessing,data exploration,
file21,14,Merged_unemployment_wage.head(3),data exploration,,
file21,15,"#checking for duplicates rows
 print(Merged_unemployment_wage.duplicated().sum())",data exploration,,
file21,16,"#Drop duplicate rows
 drop_duplicates = Merged_unemployment_wage.drop_duplicates()",data preprocessing,,
file21,17,print(drop_duplicates.duplicated().sum()),data exploration,,
file21,18,"#Checking for nulls 
 drop_duplicates.isnull().sum()",data exploration,,
file21,19,"#Replaced the remaining null values with 0s
 cleaned_df = drop_duplicates.fillna(0)",data preprocessing,,
file21,20,cleaned_df.isnull().sum(),data exploration,,
file21,21,"# Is there a column with all 0's?
 (cleaned_df == 0).all()",data exploration,,
file21,22,"#Let's drop the column with no data
 Final_df=cleaned_df.drop(['race_hawaiiannative'], axis=1)",data preprocessing,,
file21,23,Final_df.shape,data exploration,,
file21,24,Final_df.dtypes,data exploration,,
file21,25,Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).sum().add_prefix('Sum_of_'),data preprocessing,,
file21,26,"plt.rcParams[""figure.figsize""] = (12,4)
 ax = Claims_by_week.plot(title='Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,data exploration,
file21,27,"Mean_Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).mean().add_prefix('Mean_of_')
 plt.rcParams[""figure.figsize""] = (12,4)
 ax = Mean_Claims_by_week.plot(title='Mean Of Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,data exploration,
file21,28,"matrix = Final_df.corr().round(2)
 sns.heatmap(matrix, annot=True)
 plt.show()",data preprocessing,result visualization,
file21,29,"#Top 3 UUID with the max number of claims
 Top_10_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False)
 Top_10_claimers.head(3)",data preprocessing,data exploration,
file21,30,"#UUIDs with the least number of claims
 Top_10_claimers.tail(3)",data exploration,,
file21,31,"#Do people with a certain degree file more claims 
 y=[Final_df.edu_8th_or_less.sum(),
  Final_df.edu_grades_9_11.sum(),
  Final_df.edu_hs_grad_equiv.sum(),
  Final_df.edu_post_hs.sum(),
  Final_df.edu_unknown.sum()
  ]",data preprocessing,,
file21,32,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(17, 7))
 plt.title(""Total Claims by Education"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['8th grade or less education','9 through 11','high school diploma or equivalent','completed a degree beyond high school','Education Unknown'],color='black')",data preprocessing,,
file21,33,"# To display sum values
 for index, value in enumerate(y):
  plt.text(value, index,
  str(value))
 plt.show()",result visualization,data exploration,
file21,34,"#Do people with a certain gender file more claims 
 y=[Final_df.gender_female.sum(),
  Final_df.gender_male.sum(),
  Final_df.gender_na.sum()
  ]",data preprocessing,,
file21,35,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(15,7))
 plt.title(""Total Claims by Gender"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['Female', 'Male' , 'Gender Unknown'],color='black')",result visualization,data exploration,
file21,36,"#Do people with a certain gender file more claims 
 y=[Final_df.race_amerindian.sum(),
  Final_df.race_asian.sum(),
  Final_df.race_black.sum(),
  Final_df.race_noanswer.sum(),
  Final_df.race_other.sum(),
  Final_df.race_white.sum()
  ]",data exploration,,
file21,37,"pd.set_option('float_format', '{:f}'.format)
 Final_df[['average_wage']].describe()",data exploration,,
file21,38,"Final_df[['top_category_employer1', 'top_category_employer2', 'top_category_employer3']].describe()",data exploration,,
file21,39,"sns.relplot(x =""edu_8th_or_less"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_grades_9_11"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_hs_grad_equiv"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_post_hs"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_unknown"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer1"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer2"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer3"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_female"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_male"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_na"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_amerindian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_asian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_black"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_noanswer"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_other"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_white"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""average_wage"", y =""total_claims"",
  data = Final_df);",result visualization,data exploration,
file21,40,"Final_df['timeperiod'] = pd.to_datetime(Final_df['timeperiod'],
  format='%Y%m%d')",data preprocessing,,
file21,41,"d = dict.fromkeys(Final_df.select_dtypes(np.int64).columns, np.int32)
 Final_df = Final_df.astype(d)
 Final_df.dtypes",data preprocessing,data exploration,
file21,42,Final_df.uu_id.unique(),data exploration,,
file21,43,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 uuid1 = Final_df[Final_df['uu_id']=='f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 #uuid1['timeperiod'] = pd.to_datetime(uuid1['timeperiod'],
 # format='%Y%m%d') 
 uuid1",data preprocessing,data exploration,
file21,44,"plt.plot(uuid1['timeperiod'], uuid1['total_claims'])
 plt.show()",result visualization,data preprocessing,
file21,45,"data = uuid1[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data preprocessing,data exploration,
file21,46,m = NeuralProphet(),data preprocessing,,
file21,47,"from neuralprophet import NeuralProphet
 m = NeuralProphet()",helper functions,,
file21,48,get_ipython().system('pip3 install neuralprophet[live]'),helper functions,,
file21,49,from neuralprophet import NeuralProphet,helper functions,,
file21,50,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file21,51,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file21,52,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",data preprocessing,data exploration,
file21,53,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file21,54,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 prediction_list.head(3)",data preprocessing,data exploration,
file21,55,"#Number of rows in wage_data
 len(prediction_list.index)",data exploration,,
file21,56,"#Let's merge the unemployment_data and wage_data
 # We will merge only cols uu_id and average_wage from wage_data into unemployment_data
 Merged_unemployment_wage = unemployment_data.merge(wage_data[['uu_id', 'average_wage']])
 Merged_unemployment_wage.columns",data preprocessing,data exploration,
file21,57,Merged_unemployment_wage.head(3),data exploration,,
file21,58,"#checking for duplicates rows
 print(Merged_unemployment_wage.duplicated().sum())",data exploration,,
file21,59,"#Drop duplicate rows
 drop_duplicates = Merged_unemployment_wage.drop_duplicates()",data preprocessing,,
file21,60,print(drop_duplicates.duplicated().sum()),data exploration,,
file21,61,"#Checking for nulls 
 drop_duplicates.isnull().sum()",data exploration,,
file21,62,"#Replaced the remaining null values with 0s
 cleaned_df = drop_duplicates.fillna(0)",data preprocessing,,
file21,63,cleaned_df.isnull().sum(),data exploration,,
file21,64,"# Is there a column with all 0's?
 (cleaned_df == 0).all()",data exploration,,
file21,65,"Final_df['timeperiod'] = pd.to_datetime(Final_df['timeperiod'],
  format='%Y%m%d')",data preprocessing,,
file21,66,"d = dict.fromkeys(Final_df.select_dtypes(np.int64).columns, np.int32)
 Final_df = Final_df.astype(d)
 Final_df.dtypes",data preprocessing,data exploration,
file21,67,Final_df.uu_id.unique(),data exploration,,
file21,68,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 uuid1 = Final_df[Final_df['uu_id']=='f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 #uuid1['timeperiod'] = pd.to_datetime(uuid1['timeperiod'],
 # format='%Y%m%d') 
 uuid1",data preprocessing,data exploration,
file21,69,"plt.plot(uuid1['timeperiod'], uuid1['total_claims'])
 plt.show()",result visualization,data exploration,
file21,70,"data = uuid1[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data preprocessing,data exploration,
file21,71,m = NeuralProphet(),helper functions,,
file21,72,"model = m.fit(data, freq='W')",modeling,,
file21,73,"future = m.make_future_dataframe(data, periods=20)
 forecast = m.predict(future)
 forecast",prediction,data exploration,
file21,74,plot1 = m.plot(forecast),result visualization,data exploration,
file21,75,"#21957d5517323845818d87623589e1ba
 uuid2 = Final_df[Final_df['uu_id']=='21957d5517323845818d87623589e1ba'].sort_values('week_number')
 uuid2.head()",data preprocessing,data exploration,
file21,76,"#c82ff1f157906a6e9c37a08989544676  
 #0ad94f09274e2c9cb0ef5cb77eb334b4  
 #cb304c84e572423d939db1dbb2009609
 uuid2 = Final_df[Final_df['uu_id']=='c82ff1f157906a6e9c37a08989544676'].sort_values('week_number')
 uuid2.head()",data preprocessing,data exploration,
file21,77,"df_numerics_only = Final_df.select_dtypes(include=np.number)
 df_numerics_only",data preprocessing,data exploration,
file21,78,"#df =df_numerics_only.drop(['week_number'], axis=1)
 df = Final_df[['week_number','edu_8th_or_less',
  'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_other', 'race_white', 'average_wage', 'total_claims']].sort_values('week_number')
 df",data preprocessing,data exploration,
file21,79,"features = df.iloc[:,:-1]
 features.head()",data preprocessing,data exploration,
file21,80,"X = features.values
 X",data preprocessing,data exploration,
file21,81,"y = df['total_claims'].values
 y",data preprocessing,data exploration,
file21,82,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file21,83,"X_train, X_test, y_train, y_test",data exploration,,
file21,84,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train) #training the algorithm",modeling,,
file21,85,"y_train_pred = regressor.predict(X_train)
 y_train_pred",prediction,,
file21,86,"regressor.score(X_train, y_train)",evaluation,,
file21,87,"y_test_pred = regressor.predict(X_test)
 y_test_pred",prediction,,
file21,88,"regressor.score(X_test, y_test)",evaluation,,
file21,89,"df_actual_pred = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})",data preprocessing,,
file21,90,"df1 = df_actual_pred.head(50)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,data exploration,
file21,91,"print(f'Mean Absolute Error: {round(metrics.mean_absolute_error(y_test, y_test_pred),2)}')  
 print(f'Mean Squared Error: {round(metrics.mean_squared_error(y_test, y_test_pred),2)}')  
 print(f'Root Mean Squared Error: {round(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)),2)}')",data exploration,,
file21,92,regressor.predict(X_test),prediction,,
file21,93,"Predicted_Values = regressor.predict(X)
 Predicted_Values",prediction,data exploration,
file21,94,"Final_df['prediction_total_claims'] = Predicted_Values.tolist()
 Final_df.head()",data preprocessing,data exploration,
file21,95,"final_cols = Final_df[['uu_id','timeperiod','week_number','total_claims','prediction_total_claims']].sort_values('week_number')
 final_cols",data preprocessing,data exploration,
file21,96,final_cols[Final_df['uu_id']=='c82ff1f157906a6e9c37a08989544676'].sort_values('week_number'),data preprocessing,,
file21,97,"groupby_uuid = final_cols.groupby('uu_id', as_index=False)['prediction_total_claims'].agg(['mean'])
 len(groupby_uuid.index)",data preprocessing,data exploration,
file21,98,"total_claims_prediction = pd.merge(prediction_list, groupby_uuid, left_on='uu_id', right_index=True)
 total_claims_prediction.head()",data preprocessing,data exploration,
file21,99,"total_claims_prediction.rename(columns = {'mean':'total_claims'}, inplace = True)
 total_claims_prediction.head()",data preprocessing,data exploration,
file21,100,"total_claims_prediction = total_claims_prediction[['uu_id', 'total_claims', 'week_number']]
 total_claims_prediction",data preprocessing,data exploration,
file21,101,total_claims_prediction.round(0),data preprocessing,,
file21,102,total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676'],data exploration,,
file21,103,"#c82ff1f157906a6e9c37a08989544676  
 #0ad94f09274e2c9cb0ef5cb77eb334b4  
 #cb304c84e572423d939db1dbb2009609
 uuid2 = Final_df[Final_df['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4'].sort_values('week_number')
 uuid2",data preprocessing,data exploration,
file21,104,"data = uuid3[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data preprocessing,data exploration,
file21,105,"m = NeuralProphet()
 model = m.fit(data, freq='W')",modeling,,
file21,106,"future = m.make_future_dataframe(data, periods=20)
 forecast = m.predict(future)
 forecast",prediction,,
file21,107,plot1 = m.plot(forecast),result visualization,evaluation,
file21,108,"total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676']
 total_claims_prediction[total_claims_prediction['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4']",data exploration,,
file21,109,"#c82ff1f157906a6e9c37a08989544676  
 #0ad94f09274e2c9cb0ef5cb77eb334b4  
 #cb304c84e572423d939db1dbb2009609
 uuid4 = Final_df[Final_df['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4'].sort_values('week_number')
 uuid4
 data = uuid4[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()
 m = NeuralProphet()
 model = m.fit(data, freq='W')
 future = m.make_future_dataframe(data, periods=20)
 forecast = m.predict(future)
 forecast
 plot1 = m.plot(forecast)",modeling,prediction,
file21,110,"#f7f087af0599e6b2eaa4045ba1a0be50  
 #747f8bc2b0c8c0a04d29caa4cfe327d2  
 #6fbb60a508283bc1fb30c13ac419941a  
 uuid5 = Final_df[Final_df['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4'].sort_values('week_number')
 uuid5",data preprocessing,data exploration,
file21,111,Final_df.groupby('uu_id').apply(len),data exploration,,
file21,112,"Final_df.groupby('uu_id').apply(len)
 Final_df.groupby('uu_id').value_counts()",data exploration,,
file21,113,"#Top 3 UUID with the max number of claims
 Top_10_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False)
 Top_10_claimers.head(3)",data preprocessing,data exploration,
file21,114,"#Number of Claims filed
 Final_df['uu_id'].value_counts(ascending=True)",data exploration,,
file21,115,"total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676']
 total_claims_prediction[total_claims_prediction['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4']
 total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676']
 total_claims_prediction[total_claims_prediction['uu_id']=='f7f087af0599e6b2eaa4045ba1a0be50']
 #f7f087af0599e6b2eaa4045ba1a0be50  
 #747f8bc2b0c8c0a04d29caa4cfe327d2  
 #6fbb60a508283bc1fb30c13ac419941a",data exploration,,
file21,116,"total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676']
 total_claims_prediction[total_claims_prediction['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4']
 total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676']
 total_claims_prediction[total_claims_prediction['uu_id']=='0ad94f09274e2c9cb0ef5cb77eb334b4']
 f013068de98db1470bd986137a0c6d23 - 0
 #c82ff1f157906a6e9c37a08989544676 - 75  
 #0ad94f09274e2c9cb0ef5cb77eb334b4 - 70  
 #cb304c84e572423d939db1dbb2009609 - 33
 #f7f087af0599e6b2eaa4045ba1a0be50 - 0",data exploration,,
file21,117,,,,
file21,118,"total_claims_prediction[total_claims_prediction['uu_id']=='f013068de98db1470bd986137a0c6d23']
 #f013068de98db1470bd986137a0c6d23 - 0
 #c82ff1f157906a6e9c37a08989544676 - 75  
 #0ad94f09274e2c9cb0ef5cb77eb334b4 - 70  
 #cb304c84e572423d939db1dbb2009609 - 33
 #f7f087af0599e6b2eaa4045ba1a0be50 - 0
 #df.loc[df['Age'] < 30, 'Age Category'] = 'Under 30'
 #df.loc[df[""gender""] == ""male"", ""gender""] = 1",data exploration,,
file22,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file22,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file22,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file22,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file22,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file22,5,global df3,helper functions,,
file22,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file22,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file22,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file22,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,data exploration,
file22,10,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file22,11,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file22,12,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file22,13,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,data exploration,
file22,14,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file22,15,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file22,16,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file22,17,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,data exploration,
file22,18,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,data exploration,
file22,19,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file22,20,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file22,21,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file22,22,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file22,23,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,data exploration,
file22,24,"temp = df3[[k for k in features]]
 temp",data preprocessing,data exploration,
file22,25,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,,
file22,26,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",modeling,,
file22,27,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 24
 test_df[""month""]=10",data preprocessing,,
file22,28,test_df.drop_duplicates(),data preprocessing,,
file22,29,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file22,30,"from statsmodels.tsa.stattools import adfuller
 adfuller(df3[""total_claims""])",helper functions,data preprocessing,
file22,31,"from pandas.plotting import autocorrelation_plot
 autocorrelation_plot(df3[""total_claims""])",helper functions,data preprocessing,
file22,32,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,prediction,
file22,33,test_df,data exploration,save results,
file22,34,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data preprocessing,data exploration,
file22,35,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")",save results,,
file23,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file23,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file23,2,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file23,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file23,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file23,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",data preprocessing,,
file23,6,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file23,7,unemployment_data,data exploration,,
file23,8,unemployment_data.isnull().sum(),data exploration,,
file23,9,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file23,10,"unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file23,11,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file23,12,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid]
 

  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]
 

  y = df[['price']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
  plt.title('Tract: '+uuid)
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",modeling,prediction,
file23,13,,,,
file23,14,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",data exploration,,
file24,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file24,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file24,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file24,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file24,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",load data,data preprocessing,
file24,5,"query_pred_list = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query = bigquery_client.query(query_pred_list)
 df_pred_list = query.to_dataframe()
 #df_pred_list.head()",load data,data preprocessing,
file25,0,"from google.cloud import bigquery
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file25,1,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/.config/gcloud/application_default_credentials.json'
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file26,0,gcloud auth login,helper functions,,
file27,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file27,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file27,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file27,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file27,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline",helper functions,,
file27,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file27,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file27,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file27,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file27,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",data preprocessing,,
file27,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file27,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 37
 """"""",load data,,
file27,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file27,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file27,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file27,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file27,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file27,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file27,18,"data = data.drop(['tract_name', 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file27,19,"plt.figure(figsize=(8,6))
 cor = data.corr().round(2)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={""size"": 6})
 plt.show()",result visualization,data exploration,
file27,20,"data = data.drop(['timeperiod'], axis=1)",data preprocessing,,
file27,21,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file27,22,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file27,23,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file27,24,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file27,25,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file27,26,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,,
file27,27,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",modeling,,
file27,28,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file27,29,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file27,30,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",modeling,,
file27,31,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=50)",modeling,,
file27,32,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file27,33,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,,
file27,34,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",prediction,,
file27,35,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,data exploration,
file27,36,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",evaluation,,
file27,37,plt.show(),result visualization,evaluation,
file27,38,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file27,39,unemployment_prediction_data = query_from_statement(prediction_query),data preprocessing,,
file27,40,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file27,41,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data preprocessing,data exploration,
file27,42,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",data preprocessing,data exploration,
file27,43,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,data exploration,
file27,44,"unemployment_prediction_data.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file28,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file28,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file28,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file28,3,"last_week = 37
 pred_week = 40",helper functions,,
file28,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file28,5,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
  WHERE week_number <= """""" + str(last_week) + """""" ) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",data preprocessing,,
file28,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",data preprocessing,,
file28,7,empdata.head().transpose(),data preprocessing,,
file28,8,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",data exploration,,
file28,9,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file28,10,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",result visualization,,
file28,11,empdata[['total_claims']].describe(),data exploration,,
file28,12,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file28,13,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",data preprocessing,result visualization,
file28,14,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file28,15,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data preprocessing,,
file28,16,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,last_week+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data exploration,data preprocessing,
file28,17,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=pred_week-last_week)[pred_week-last_week-1]",prediction,modeling,
file28,18,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file28,19,uupred.head(),data exploration,,
file28,20,"uupred['total_claims'] = 0
 uupred.head()",data exploration,,
file28,21,"for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,last_week+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
  m = pm.auto_arima(allweeks['total_claims'].values, seasonal=False, error_action='ignore')
  pred = int(m.predict(n_periods=pred_week-last_week)[pred_week-last_week-1])
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",data preprocessing,data exploration,
file29,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file29,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file29,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file29,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file29,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file29,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file29,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,,
file29,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file29,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file29,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file29,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",helper functions,,
file29,11,"def example_function():
  print('Hello World')",comment only,,
file29,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file29,13,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",modeling,,
file29,14,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file29,15,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",data exploration,evaluation,
file29,16,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",data exploration,modeling,
file29,17,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file29,18,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",data exploration,prediction,
file29,19,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",data exploration,modeling,
file29,20,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file29,21,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",data exploration,prediction,
file29,22,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file29,23,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file29,24,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file29,25,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file29,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file29,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file29,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file29,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file29,30,"print(f""slope: {new_model.coef_}"")",data exploration,,
file29,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file29,32,"# Test Linear Regression
 x",data exploration,,
file29,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file29,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file29,35,"# Test Linear Regression
 print(results.summary())",data exploration,,
file29,36,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file29,37,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file29,38,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file29,39,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file29,40,"def plot_regression(d):  
  e = d.loc[:, COL_MAP['edu'] + ['total_claims']].reset_index(0, drop=True)
  e = e.fillna(0)
  X = e[COL_MAP['edu']]
  y = e['total_claims']
  model = sklearn.linear_model.LinearRegression()
  model.fit(X, y)
  fig, ax = plt.subplots()
  ax.plot(X.index, model.predict(X), label='predict')
  ax.plot(X.index, y, label='observed', ls='--')
  ax.legend()
  ax.set_xlabel('index')
  ax.set_ylabel('total_claims')",result visualization,,
file29,41,plot_regression(d),result visualization,,
file29,42,"def load_imp_industry(csv_name='4_imp_industry.csv'):
  if not os.path.isfile(csv_name):
  d = load_clean()
  d = d.groupby('uu_id').apply(impute_industry).reset_index(0, drop=True)
  d.to_csv(csv_name, index=False)
  else:
  d = pd.read_csv(csv_name)
  return d",data preprocessing,save results,
file29,43,"sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",modeling,,
file29,44,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",data preprocessing,,
file29,45,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]
  
  if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model = 
  sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",data preprocessing,modeling,
file29,46,"# Importing necessary libraries
 import numpy as np
 import pandas as pd",helper functions,,
file29,47,"housing = pd.read_csv(""Housing.csv"")
 housing.head()",data exploration,,
file29,48,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]",data preprocessing,,
file29,49,"mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]",data preprocessing,,
file29,50,"if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model =
  
  sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",modeling,,
file29,51,"if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]",data preprocessing,,
file29,52,"d = load_imp_industry()
 d",data exploration,,
file29,53,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file29,54,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file29,55,get_ipython().system('pip install db-dtypes'),helper functions,,
file29,56,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file29,57,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file29,58,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file29,59,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,,
file29,60,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file29,61,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file29,62,"import numpy as np
 from sklearn.linear_model import LinearRegression",modeling,,
file29,63,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file29,64,"def example_function():
  print('Hello World')",comment only,,
file29,65,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file29,66,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",data exploration,prediction,
file29,67,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file29,68,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file29,69,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",data exploration,modeling,
file29,70,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file29,71,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",data exploration,prediction,
file29,72,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",data exploration,modeling,
file29,73,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file29,74,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",data exploration,prediction,
file29,75,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file29,76,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file29,77,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file29,78,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file29,79,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file29,80,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file29,81,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",data exploration,evaluation,
file29,82,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,data exploration,
file29,83,"print(f""slope: {new_model.coef_}"")",data exploration,,
file29,84,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file29,85,"# Test Linear Regression
 x",data exploration,,
file29,86,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file29,87,"# Test Linear Regression
 results = model.fit()",modeling,,
file29,88,"# Test Linear Regression
 print(results.summary())",data exploration,,
file29,89,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file29,90,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file29,91,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file29,92,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file29,93,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]",data preprocessing,,
file29,94,"mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]",data preprocessing,,
file29,95,"if y_train.shape[0] == 0:
  g[ycol] = None
 return g[ycol]
  
  classes = y_train[ycol].unique()
 if len(classes) == 1:
  yhat = [classes[0]]
 else:
  model =
 sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
 return g[ycol]",data preprocessing,modeling,
file29,96,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",data preprocessing,,
file29,97,"def load_imp_industry(csv_name='4_imp_industry.csv'):
  if not os.path.isfile(csv_name):
  d = load_clean()
  d = d.groupby('uu_id').apply(impute_industry).reset_index(0, drop=True)
  d.to_csv(csv_name, index=False)
  else:
  d = pd.read_csv(csv_name)
  return d",data preprocessing,save results,
file29,98,"d = load_imp_industry()
 d",data exploration,,
file29,99,"def plot_regression(d):  
  e = d.loc[:, COL_MAP['edu'] + ['total_claims']].reset_index(0, drop=True)
  e = e.fillna(0)
  X = e[COL_MAP['edu']]
  y = e['total_claims']
  model = sklearn.linear_model.LinearRegression()
  model.fit(X, y)
  fig, ax = plt.subplots()
  ax.plot(X.index, model.predict(X), label='predict')
  ax.plot(X.index, y, label='observed', ls='--')
  ax.legend()
  ax.set_xlabel('index')
  ax.set_ylabel('total_claims')",data preprocessing,result visualization,
file29,100,plot_regression(d),result visualization,,
file30,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file30,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file30,2,"BIGQUERY_PROJECT = 'ironhacks-covid19-data'
 BIGQUERY_KEYPATH = 'service-account.json'",load data,,
file30,3,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = BIGQUERY_KEYPATH
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file30,4,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file30,5,"min(data['date']),max(data['date'])",data exploration,,
file30,6,data.dtypes,data exploration,,
file30,7,data = data.set_index('date'),data preprocessing,,
file30,8,data.index,data exploration,,
file30,9,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,,
file30,10,data.loc['2019-08'],data exploration,,
file30,11,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file30,12,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file30,13,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file30,14,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",helper functions,result visualization,
file30,15,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file30,16,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file30,17,"y_pred = pd.Series(model.predict(X), index=X.index)",data preprocessing,,
file30,18,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file30,19,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file30,20,data['lag_1']=lag_1,data preprocessing,,
file31,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file31,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file31,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file31,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file31,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",helper functions,,
file31,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",helper functions,,
file31,6,data.describe(),data exploration,,
file31,7,data.shape,data exploration,,
file31,8,data.info(),data exploration,,
file31,9,y=data['total_claims'],data preprocessing,,
file31,10,"l=[]
 for i in data.columns:
  if sum(data[i].isnull())>0:
  l.append(i)
  print('The null values in',i,'are',sum(data[i].isnull()))",data exploration,data preprocessing,
file31,11,data.isnull().sum(axis=0),data preprocessing,,
file31,12,"for i in data.columns:
  if data[i].isnull().sum()>= 0.4*len(data):
  data=data.drop(i,axis=1)",data preprocessing,,
file31,13,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print('The value counts of feature',i)
  print(data[i].value_counts(),'\n')",data preprocessing,data exploration,
file31,14,"for i in data.columns:
  print('The unique values in',i,'are',len(data[i].value_counts()))",data exploration,,
file31,15,data.corr(),data exploration,,
file31,16,"plt.figure(figsize=(20,15))
 cor = data.corr()
 sns.heatmap(cor,annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file32,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file32,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file32,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file32,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file32,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file32,5,print(covid19_cases_data),data exploration,,
file32,6,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file32,7,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data exploration,,
file33,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file33,1,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file33,2,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file33,3,"get_ipython().system('pip install localpip')
 get_ipython().system('localpip install neuralprophet')",helper functions,,
file34,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file34,1,"import pandas as pd
 import numpy as np
 from google.cloud import bigquery",helper functions,,
file34,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file34,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file34,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file34,5,unemployment_data.shape,data exploration,,
file34,6,unemployment_data.columns,data exploration,,
file34,7,"unemployment_data.drop(['uu_id', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data preprocessing,,
file34,8,"unemployment_data.drop_duplicates(inplace=True)
 unemployment_data.sort_values(['week_number'])",data preprocessing,,
file34,9,"def add_missing_weeks(df):
  # Fill in missing weeks by taking the ceil of the average of prev and next
  for week in range(1, 37):",data preprocessing,,
file34,10,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file34,11,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file34,12,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day', 'week_number'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file34,13,"unemployment_data.sort_values(['ds'], inplace=True)",data preprocessing,,
file34,14,from prophet import Prophet,helper functions,,
file34,15,ud = unemployment_data,helper functions,,
file34,16,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file34,17,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",prediction,,
file34,18,ud_train,data exploration,,
file34,19,"m = Prophet(weekly_seasonality=False,
  daily_seasonality=False,
  interval_width=0.95, 
  mcmc_samples = 500)",modeling,,
file34,20,m.fit(ud_train),prediction,,
file34,21,"future = m.make_future_dataframe(periods=20, freq='W')",data preprocessing,,
file34,22,forecast = m.predict(df=future),prediction,,
file34,23,"m.fit(ud_train, show_console=True)",prediction,,
file35,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file35,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file35,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file35,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file35,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline",helper functions,,
file35,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file35,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file35,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file35,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file35,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,,
file35,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file35,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 37
 """"""",load data,,
file35,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file35,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file35,14,unemployment_wage_data = query_from_statement(wage_query),load data,,
file35,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file35,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file35,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file35,18,data['tract_name'],data exploration,,
file35,19,"data['tract_name'] = [i.split(',')[1].split(' ')[0] for i in data['tract_name']]",data preprocessing,,
file35,20,"tract_name_encoder = OneHotEncoder(sparse=False)
 data['tract_name'] = tract_name_encoder.fit_transform(data['tract_name'])",data preprocessing,,
file35,21,data['tract_name'].unique(),data exploration,,
file35,22,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.categories_",data exploration,data preprocessing,
file35,23,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(list(data['tract_name'].values.reshape(-1, 1)))
 tract_name_encoder.categories",data preprocessing,data exploration,
file35,24,"data['tract_name'] = tract_name_encoder.transform(list(data['tract_name'].values.reshape(-1, 1))).toarray()",data preprocessing,,
file35,25,tract_name_encoder.transform(data['tract_name'].values),data exploration,data preprocessing,
file35,26,data,data exploration,,
file35,27,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(data['tract_name'].values.reshape(-1, 1))
 #tract_name_encoder.categories_[0]
 data[tract_name_encoder.categories_[0]] = tract_name_encoder.transform(data['tract_name'].values.reshape(-1, 1))
 data = data.drop(['tract_name'], axis=1)",data preprocessing,,
file35,28,"data = data.drop(['top_category_employer1', 'top_category_employer2',
  'top_category_employer3'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file35,29,"plt.figure(figsize=(5,5))
 cor = data.corr().round(2)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={""size"": 6})
 plt.show()",result visualization,,
file35,30,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file35,31,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",prediction,,
file35,32,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file35,33,"data = pd.concat([data, tract_name_dataset])
 display(data)",data preprocessing,data exploration,
file35,34,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file35,35,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file35,36,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,data exploration,
file35,37,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",helper functions,,
file35,38,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",prediction,,
file35,39,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file35,40,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",modeling,data exploration,
file35,41,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=10)",prediction,,
file35,42,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file35,43,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,,
file35,44,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",prediction,,
file35,45,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,,
file35,46,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",result visualization,,
file35,47,plt.show(),result visualization,,
file35,48,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",,,
file35,49,unemployment_prediction_data = query_from_statement(prediction_query),load data,,
file35,50,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file35,51,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data exploration,data preprocessing,
file35,52,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",data exploration,prediction,
file35,53,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,data exploration,
file36,0,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file36,1,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,,
file36,2,"from google.cloud import bigquery
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file36,3,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/.config/gcloud/application_default_credentials.json'
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file37,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file37,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file37,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file37,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file37,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",helper functions,load data,
file37,5,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,,
file37,6,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]",data preprocessing,,
file37,7,"df_three_col.to_csv(""three_col.txt"",index=False,sep=""\t"")",save results,,
file37,8,"df_three_col = df_three_col.drop_duplicates()
 df_three_col.shape",data preprocessing,,
file37,9,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
 head(df_three_col)",data exploration,data preprocessing,
file37,10,"res = pd.DataFrame(columns = ['uu_id', 'total_claims', 'week_number'])",data preprocessing,,
file37,11,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  test_data = test_data.tail(3)
  y_pred = int(test_data[""total_claims""].mean())
  cur_row = pd.DataFrame([[cur_uu_id, y_pred, 41]], columns=['uu_id', 'total_claims', 'week_number'] )
  res = pd.concat([res,cur_row] ,ignore_index = True)",data preprocessing,,
file37,12,"res.to_csv(""Nov19_submission_prediction_output.csv"", index=False)",save results,,
file38,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file38,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file38,2,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file38,3,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import plotly.express as px
 import csv",helper functions,,
file38,4,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file38,5,"def querydb(request):
  query_job = bigquery_client.query(request)
  data = query_job.to_dataframe()
  return data",load data,,
file38,6,"def createSubmission():
  return",data preprocessing,,
file38,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file38,8,"#Unemployment Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 unemployment_data = querydb(query)",load data,,
file38,9,"# Prediction Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 prediction_data = querydb(query)",load data,,
file38,10,"querydb(""""""SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`"""""")",load data,,
file38,11,df_wage = wage_data,data preprocessing,,
file38,12,"#should add another column called previous week claims, this would probably be a good predictor for the next week of claims
 df_un = unemployment_data",data preprocessing,,
file38,13,df_pred = prediction_data,data preprocessing,,
file38,14,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""]).drop_duplicates()",data preprocessing,,
file38,15,"df_pred[""uu_id""]",data exploration,,
file38,16,"prediction_dict = {}
 for i, tract in enumerate(df_pred[""uu_id""]):
  df_working = df_un[df_un[""uu_id""] == tract].sort_values(by = [""week_number""]).drop_duplicates()
  mean = df_working[""total_claims""].mean()
  prediction_dict[tract] = mean",data preprocessing,,
file38,17,"header = [""uu_id"",""total_claims"",""week_number""]
 week = 38
 fname = ""submission_prediction_output.csv""
 with open(fname, 'w', encoding = ""UTF8"", newline="""") as f:
  writer = csv.writer(f)
  writer.writerow(header)
  for k, v in prediction_dict.items():
  writer.writerow([k, v, week])",data preprocessing,,
file38,18,total/count,data exploration,,
file38,19,7.5/21.5,data exploration,,
file38,20,"# how can I pull this from just pandas
 tract_dict = {}",helper functions,,
file38,21,"for i, tract in enumerate(df_pred[""uu_id""]):
  tract_dict[i] = df_un[df_un[""uu_id""] == tract].sort_values(by = [""week_number""]).drop_duplicates()",data preprocessing,,
file38,22,"df_wage = wage_data
 print(df_wage)",data exploration,data preprocessing,
file38,23,"# dictionary of df's for each uu_id
 tract_dict = {}",helper functions,,
file38,24,tract[0].head(),data exploration,,
file38,25,tract[0],data exploration,,
file38,26,tract_dict[1].head(38),data exploration,,
file38,27,"# compute the simple moving average over 4 weeks for each frame
 for key, val in tract_dict.items():
  tract_dict[key][""SMA4""] = tract_dict[key][""total_claims""].rolling(4).mean()",data preprocessing,,
file38,28,"tract_dict[0][[""SMA4"", ""total_claims""]].plot()",result visualization,,
file38,29,"tract_dict[0][[""total_claims"", ""SMA4"", ""y_bar""]].plot()",result visualization,,
file38,30,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import plotly.express as px
 import csv
 import random",helper functions,,
file38,31,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file38,32,"# compute the simple moving average over 4 weeks for each frame
 for key, val in tract_dict.items():
  tract_dict[key][""SMA4""] = tract_dict[key][""total_claims""].rolling(4).mean()
  tract_dict[key][""EMA4""] = tract_dict[key][""total_claims""].ewm(4).mean()
  tract_dict[key][""y_bar""] = tract_dict[key][""total_claims""].mean()",data preprocessing,,
file38,33,"#compare the previous predictor to the new predictor
 tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",result visualization,,
file38,34,"tract_dict[0][""EMA4""].iat[-1]",data exploration,,
file39,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file39,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file39,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file39,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file39,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file39,5,global df3,data exploration,,
file39,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file39,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file39,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,,
file39,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",load data,helper functions,
file39,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file39,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,data exploration,
file39,12,"for k in range(1,10):
  temp = df3[df3.month ==k]
  ax= sns.lineplot( x= ""day"", y= ""total_claims"", hue = ""month"", data =temp )
  plt.show()",result visualization,,
file39,13,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file39,14,df4[df4.total_claims>0.2],data exploration,,
file39,15,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(x = df3[feature], y = df3[""total_claims""])
  plt.show()",result visualization,data preprocessing,
file39,16,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(y = df3[feature], x = ""week_number"", data = df3)
  ax= sns.lineplot(y = df3[feature], x = ""week_number"", data = df3)
  plt.show()",result visualization,data preprocessing,
file39,17,"df3 = df.copy()
 df3.columns",data exploration,data preprocessing,
file39,18,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data exploration,data preprocessing,
file39,19,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file39,20,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file39,21,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file39,22,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,data exploration,
file39,23,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,data exploration,
file39,24,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file39,25,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file39,26,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file39,27,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,helper functions,
file39,28,"temp = df3[[k for k in features]]
 temp",data exploration,data preprocessing,
file39,29,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,data preprocessing,
file39,30,"import statsmodels.api as sm
 from statsmodels import tsa",helper functions,,
file39,31,"mod = sm.tsa.statespace.SARIMAX(np.array(df3[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 12),
  enforce_stationarity=False,
  enforce_invertibility=False)
 results = mod.fit()
 print(results.summary().tables[1])",data exploration,data preprocessing,
file39,32,"results.plot_diagnostics(figsize=(16, 8))
 plt.show()",result visualization,,
file39,33,"pred = results.get_prediction(start=1, dynamic=False)
 pred_ci = pred.conf_int()
 y = df3[""total_claims""]
 ax = y.plot(label='observed')
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Total Claims')
 plt.legend()
 plt.show()",result visualization,data preprocessing,
file40,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file40,1,"import matplotlib.pyplot as plt
 import os
 import pandas as pd
 import numpy as np",helper functions,,
file40,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics",helper functions,,
file40,3,"import csv 
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file40,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file40,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file40,6,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file40,7,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file40,8,"wage_data = wage_data.drop_duplicates()
 wage_data.fillna(0, inplace=True)
 wage_data = wage_data.filter(['uu_id', 'average_wage'])",data preprocessing,,
file40,9,"unemployment_data=unemployment_data.filter(['uu_id', 'week_number','total_claims','edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs','race_amerindian', 'race_asian', 'race_black','race_white'])",data preprocessing,,
file40,10,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()
 unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file40,11,"a = pd.merge(unemployment_data, wage_data, on='uu_id')
 a",data exploration,data preprocessing,
file40,12,"data = unemployment_data[unemployment_data.uu_id == '06c78e49b4daedfeb808c42e58fb25e4']
 data",data exploration,data preprocessing,
file40,13,unemployment_data.uu_id.unique()[:20],data exploration,,
file40,14,"from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler",helper functions,,
file40,15,x_axis = result.week_number,data preprocessing,,
file40,16,"# Build scatterplot
 plt.plot(x_axis, result['total_claims'], c = 'b', alpha = 0.5, marker = '.', label = 'Real')
 plt.plot(x_axis, result['prediction'], c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')",result visualization,,
file40,17,"plt.xlabel('Week')
 plt.ylabel('Claims')",result visualization,,
file40,18,"plt.grid(color = '#D3D3D3', linestyle = 'solid')",result visualization,,
file40,19,plt.legend(loc = 'lower right'),result visualization,,
file40,20,plt.show(),result visualization,,
file40,21,"def predict_claim(uuid):
  data = unemployment_data[unemployment_data.uu_id == uuid]
  y = data['total_claims']
  X = data.drop(['total_claims','uu_id'], axis = 1)
 

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
 

  regr = RandomForestRegressor(n_estimators = 1000, max_depth = 5, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  predictions = regr.predict(X_test)
 

  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  result = result.sort_values('week_number')
  result = result[['week_number','total_claims', 'prediction']]
  
  lm = LinearRegression()
  lm.fit(np.array(result['week_number']).reshape(-1,1), np.array(result['total_claims']))
 

  intercept = lm.intercept_
  slope = lm.coef_
  
  return 40*slope + intercept",data preprocessing,,
file40,22,uuids = prediction_list.uu_id.tolist(),data preprocessing,,
file40,23,len(uuids),data exploration,,
file40,24,"data_count = unemployment_data.uu_id.value_counts().to_dict()
 print(len(data_count))",data exploration,data preprocessing,
file40,25,"lessthanten = []
 for uuid, cnt in data_count.items():
  if cnt <10:
  lessthanten.append(uuid)
 print(len(lessthanten))",data preprocessing,,
file40,26,"for i in lessthanten:
  uuids.remove(i)",data preprocessing,,
file40,27,"rows = []
 for uuid in uuids:
  rows.append([uuid, 44, int(predict_claim(uuid))])",data preprocessing,,
file40,28,"for uuid in lessthanten:
  rows.append([uuid, 44, int(unemployment_data.groupby('uu_id').median().total_claims[uuid])])",data preprocessing,,
file40,29,len(rows),data exploration,,
file40,30,"filename = 'submission_prediction_output.csv'
 fields = ['uu_id', 'week_number', 'total_claims']",data preprocessing,,
file40,31,"with open(filename, 'w') as csvfile: 
  
  csvwriter = csv.writer(csvfile) 
 

  csvwriter.writerow(fields) 
 

  csvwriter.writerows(rows)",data preprocessing,,
file41,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file41,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file41,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file41,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file41,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,,
file41,5,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file41,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file41,7,"#Correlation with output variable
 cor_target = abs(cor[""potential_water_deficit""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file41,8,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file41,9,"min(data['date']),max(data['date'])",data preprocessing,,
file41,10,data.dtypes,data exploration,,
file41,11,data = data.set_index('date'),data preprocessing,,
file41,12,data.index,data exploration,,
file41,13,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,,
file41,14,data.loc['2019-08'],data exploration,,
file41,15,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file41,16,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file41,17,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file41,18,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",result visualization,,
file41,19,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file41,20,"sns.boxplot(data=data, x='Month', y='wind_speed');",result visualization,,
file41,21,"from statsmodels.graphics.tsaplots import plot_acf
 plot_acf(x=data['max_temperature'], lags=50)",result visualization,,
file41,22,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,prediction,
file41,23,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file41,24,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file41,25,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file41,26,data['lag_1']=lag_1,data preprocessing,,
file42,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file42,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file42,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima
 from statsmodels.tsa.holtwinters import ExponentialSmoothing",helper functions,,
file42,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file42,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data` ) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file42,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,,
file42,6,empdata.head().transpose(),data exploration,,
file42,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file42,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data exploration,,
file42,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data preprocessing,,
file42,10,empdata[['total_claims']].describe(),data exploration,,
file42,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file42,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",result visualization,,
file42,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file42,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file42,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file42,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",prediction,modeling,
file42,17,m.summary(),data exploration,,
file43,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file43,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file43,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file43,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file43,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file43,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,,
file43,6,empdata.head().transpose(),data exploration,,
file43,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file43,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file43,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data preprocessing,,
file43,10,empdata[['total_claims']].describe(),data exploration,,
file43,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file43,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",result visualization,,
file43,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file43,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file43,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file43,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",prediction,modeling,
file43,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file43,18,uupred.head(),data exploration,,
file43,19,"uupred['total_claims'] = 0
 uupred.head()",data exploration,data preprocessing,
file43,20,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values[:lastw], seasonal=False, error_action='ignore')
  pred = m.predict(n_periods=predw-lastw)
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",data exploration,data preprocessing,
file43,21,"dopred(37,41)",data exploration,,
file43,22,"uupred = uupred[['uu_id', 'total_claims', 'week_number']]
 uupred.to_csv('submission_prediction_output.csv', index=False)",data preprocessing,save results,
file44,0,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file44,1,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file44,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file44,3,"from google.cloud import bigquery
 import pandas as pd",helper functions,,
file44,4,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file44,5,get_ipython().system('pip install db-dtypes'),helper functions,,
file45,0,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file45,1,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file45,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file45,3,pip install neuralprophet,helper functions,,
file45,4,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file45,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file45,6,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file45,7,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",data exploration,load data,
file45,8,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file45,9,"#Number of rows in wage_data
 len(wage_data.index)",data exploration,,
file45,10,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 prediction_list.head(3)",data exploration,load data,
file45,11,"pip install localpip 
 localpip install neuralprophet",helper functions,,
file45,12,"get_ipython().system('pip install localpip')
 get_ipython().system('localpip install neuralprophet')",helper functions,,
file46,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file46,1,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 6
 test_df[""month""]=11",load data,helper functions,
file46,2,test_df.drop_duplicates(),data preprocessing,,
file46,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file46,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file46,5,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file46,6,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file46,7,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file46,8,global df3,data exploration,,
file46,9,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file46,10,"query_job1 = bigquery_client.query(query)
 query_job1",load data,,
file46,11,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",helper functions,data exploration,
file46,12,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,data exploration,
file46,13,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,data exploration,
file46,14,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file46,15,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,data exploration,
file46,16,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file46,17,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file46,18,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file46,19,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,data exploration,
file46,20,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,data exploration,
file46,21,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file46,22,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file46,23,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file46,24,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file46,25,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",helper functions,result visualization,
file46,26,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,,
file46,27,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",data preprocessing,,
file46,28,"temp = df3[[k for k in features]]
 temp",data preprocessing,data exploration,
file46,29,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file46,30,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 6
 test_df[""month""]=11",load data,helper functions,
file46,31,test_df.drop_duplicates(),data preprocessing,,
file46,32,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",comment only,,
file46,33,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file46,34,"""""""test_df1=test_df.copy()
 for col in ['total_claims', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in extra:
  #test_df.loc[test_df.uu_id_enc==k,col] =0
  temp=df[df.uu_id_enc == k]
  temp[""average_wage""]=-9999
  temp =temp.replace("""",0)
  feature_test_pred = np.array(test_df1[test_df1.uu_id_enc==k])
  #print(k, temp)
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  idk = float(val.predict(np.array(feature_test_pred))[0])
  print(idk)
  test_df.loc[test_df.uu_id_enc==k,col].value = idk
 test_df""""""",comment only,,
file46,35,df3,data exploration,,
file46,36,"from statsmodels.tsa.stattools import adfuller
 adfuller(df3[""total_claims""])",helper functions,data exploration,
file46,37,"from pandas.plotting import autocorrelation_plot
 autocorrelation_plot(df3[""total_claims""])",helper functions,result visualization,
file46,38,"df3 =df3.dropna()
 for col in features:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,col] = val.predict(np.array(feature_test_pred))[0]
 test_df",data preprocessing,data exploration,
file46,39,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,modeling,
file46,40,test_df,data exploration,,
file46,41,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data preprocessing,data exploration,
file46,42,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")",data preprocessing,,
file47,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file47,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file47,2,"import matplotlib.pyplot as plt
 import os
 import pandas as pd
 import numpy as np",helper functions,,
file47,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics",helper functions,,
file47,4,"import csv 
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file47,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file47,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file47,7,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file47,8,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file47,9,"wage_data = wage_data.drop_duplicates()
 wage_data.fillna(0, inplace=True)
 wage_data = wage_data.filter(['uu_id', 'average_wage'])",data preprocessing,,
file47,10,"unemployment_data=unemployment_data.filter(['uu_id', 'week_number','total_claims','edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs','race_amerindian', 'race_asian', 'race_black','race_white'])",data preprocessing,,
file47,11,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()
 unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file47,12,"a = pd.merge(unemployment_data, wage_data, on='uu_id')
 a",data preprocessing,data exploration,
file47,13,"data = unemployment_data[unemployment_data.uu_id == '06c78e49b4daedfeb808c42e58fb25e4']
 data",data preprocessing,data exploration,
file47,14,unemployment_data.uu_id.unique()[:20],data preprocessing,,
file47,15,"from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler",helper functions,,
file47,16,x_axis = result.week_number,data preprocessing,,
file47,17,"# Build scatterplot
 plt.plot(x_axis, result['total_claims'], c = 'b', alpha = 0.5, marker = '.', label = 'Real')
 plt.plot(x_axis, result['prediction'], c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')",result visualization,,
file47,18,"plt.xlabel('Week')
 plt.ylabel('Claims')",result visualization,,
file47,19,"plt.grid(color = '#D3D3D3', linestyle = 'solid')",result visualization,,
file47,20,plt.legend(loc = 'lower right'),result visualization,,
file47,21,plt.show(),result visualization,,
file47,22,"def predict_claim(uuid):
  data = unemployment_data[unemployment_data.uu_id == uuid]
  y = data['total_claims']
  X = data.drop(['total_claims','uu_id'], axis = 1)
 

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
 

  regr = RandomForestRegressor(n_estimators = 1000, max_depth = 5, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  predictions = regr.predict(X_test)
 

  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  result = result.sort_values('week_number')
  result = result[['week_number','total_claims', 'prediction']]
  
  lm = LinearRegression()
  lm.fit(np.array(result['week_number']).reshape(-1,1), np.array(result['total_claims']))
 

  intercept = lm.intercept_
  slope = lm.coef_
  
  return 40*slope + intercept",modeling,data preprocessing,
file47,23,uuids = prediction_list.uu_id.tolist(),data preprocessing,,
file47,24,len(uuids),data exploration,,
file47,25,"data_count = unemployment_data.uu_id.value_counts().to_dict()
 print(len(data_count))",data preprocessing,data exploration,
file47,26,"lessthanten = []
 for uuid, cnt in data_count.items():
  if cnt <10:
  lessthanten.append(uuid)
 print(len(lessthanten))",data preprocessing,data exploration,
file47,27,"for i in lessthanten:
  uuids.remove(i)",data preprocessing,,
file47,28,"rows = []
 for uuid in uuids:
  rows.append([uuid, 40, int(predict_claim(uuid))])",data preprocessing,,
file47,29,"for uuid in lessthanten:
  rows.append([uuid, 40, int(unemployment_data.groupby('uu_id').median().total_claims[uuid])])",data preprocessing,,
file47,30,len(rows),data exploration,,
file47,31,"filename = 'submission_prediction_output.csv'
 fields = ['uu_id', 'week_number', 'total_claims']",data preprocessing,,
file47,32,"with open(filename, 'w') as csvfile: 
  
  csvwriter = csv.writer(csvfile) 
 

  csvwriter.writerow(fields) 
 

  csvwriter.writerows(rows)",data preprocessing,,
file48,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file48,1,import bigquery,helper functions,,
file48,2,from google.cloud import bigquery,helper functions,,
file48,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file48,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file48,5,"from google.cloud import bigquery
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file48,6,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/service-account.json'
 bigquery_client = bigquery.Client(project='ironhacks-covid19-data')
 bigquery_client = bigquery.Client()",load data,,
file48,7,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/.config/gcloud/application_default_credentials.json'
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file48,8,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.unemployement_data`
 """"""",load data,,
file49,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file49,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file49,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file49,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file49,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import OneHotEncoder",helper functions,,
file49,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file49,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file49,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",result visualization,,
file49,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file49,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,,
file49,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file49,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 39
 """"""",load data,,
file49,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file49,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file49,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file49,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file49,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file49,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file49,18,"data['tract_name'] = [i.split(',')[1].strip().split(' ')[0] for i in data['tract_name']]",data preprocessing,,
file49,19,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(data['tract_name'].values.reshape(-1, 1))
 #tract_name_encoder.categories_[0]
 tract_name_dataset = pd.DataFrame(tract_name_encoder.transform(data['tract_name'].values.reshape(-1, 1)), index=data.index, columns= tract_name_encoder.categories_[0])
 data = data.drop(['tract_name'], axis=1)",data preprocessing,,
file49,20,"data = data.drop(['top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'timeperiod'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,,
file49,21,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file49,22,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,prediction,
file49,23,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file49,24,"data = pd.concat([data, tract_name_dataset], axis=1)
 display(data)",data preprocessing,,
file49,25,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file49,26,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file49,27,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,data exploration,
file49,28,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",data preprocessing,,
file49,29,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file49,30,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file49,31,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",modeling,data exploration,
file49,32,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=10)",modeling,,
file49,33,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file49,34,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,,
file49,35,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",prediction,,
file49,36,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,,
file49,37,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",result visualization,,
file49,38,plt.show(),result visualization,,
file49,39,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file49,40,unemployment_prediction_data = query_from_statement(prediction_query),data preprocessing,,
file49,41,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file49,42,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data exploration,data preprocessing,
file49,43,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",prediction,data exploration,
file49,44,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,data exploration,
file49,45,"unemployment_prediction_data.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file49,46,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file49,47,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file49,48,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file50,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file50,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file50,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file50,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file50,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file50,5,"query = """"""
 

 SELECT date, max_rel_humidity
 FROM ironhacks-data.ironhacks_training.weather_data
 WHERE date='2020-06-16'
 

 

 

 

 """"""",load data,,
file50,6,"query_job = bigquery_client.query(query)
 get_ipython().system('python3 -m pip install pandas')
 import pandas
 data = query_job.to_dataframe()
 data.head()",load data,,
file51,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file51,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file51,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file51,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file51,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file51,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file51,6,data.describe(),data exploration,,
file51,7,data.shape,data exploration,,
file51,8,data.info(),data exploration,,
file51,9,y=data['total_claims'],data preprocessing,,
file51,10,"l=[]
 for i in data.columns:
  if sum(data[i].isnull())>0:
  l.append(i)
  print('The null values in',i,'are',sum(data[i].isnull()))",data preprocessing,data exploration,
file51,11,data.isnull().sum(axis=0),data exploration,,
file51,12,"data=data.drop('total_claims',axis=1)",data preprocessing,,
file51,13,"for i in data.columns:
  print('The unique values in',i,'are',len(data[i].value_counts()))",data exploration,,
file51,14,data.corr(),data exploration,,
file51,15,"plt.figure(figsize=(20,15))
 cor = data.corr()
 sns.heatmap(cor,annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file51,16,"for i in data.columns:
  if data[i].isnull().sum()>= 0.5*len(data)
  data=data.drop(i,axis=1)",data preprocessing,,
file51,17,data.columns,data exploration,,
file51,18,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print(data[i].value_counts())",data preprocessing,data exploration,
file51,19,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print('The value counts of feature',i)
  print(data[i].value_counts())",data exploration,data preprocessing,
file52,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file52,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file52,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file52,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file52,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file52,5,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file52,6,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file53,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file53,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file53,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file53,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file53,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data` ) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file53,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,,
file53,6,empdata.head().transpose(),data exploration,,
file53,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file53,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data exploration,data preprocessing,
file53,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data preprocessing,,
file53,10,empdata[['total_claims']].describe(),data exploration,,
file53,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file53,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",result visualization,,
file53,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file53,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data preprocessing,,
file53,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file53,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",result visualization,prediction,
file53,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` order by uu_id
 """"""",load data,,
file53,18,uupred.head(),data exploration,,
file53,19,"last_week = int(empdata['week_number'].max())
 last_week",data preprocessing,data exploration,
file53,20,"pred_week = int(uupred['week_number'].max())
 pred_week",data preprocessing,data exploration,
file53,21,"uupred['total_claims'] = 0
 uupred.head()",data preprocessing,data exploration,
file53,22,len(empdata.loc[empdata['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d'][['total_claims']]),data exploration,,
file53,23,"for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  if len(testuu_id['total_claims']) == 0:
  print(uu, ""NOT FOUND"")
  continue
  allweeks = pd.DataFrame({'week_number':range(1,last_week+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values, seasonal=False, error_action='ignore')
  pred = int(m.predict(n_periods=pred_week-last_week)[pred_week-last_week-1])
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",data preprocessing,data exploration,
file53,24,"uupred = uupred[['uu_id', 'total_claims', 'week_number']]
 uupred.to_csv('submission_prediction_output.csv', index=False)",save results,,
file53,25,1,,,
file54,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file54,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file54,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file54,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 # covid19_cases_data = query_job.to_dataframe()",load data,,
file55,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file55,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file55,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file55,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file55,4,"print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file55,5,print(covid19_cases_data),data exploration,,
file55,6,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file55,7,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file56,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file56,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file56,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file56,3,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file56,4,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file56,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file56,6,"query = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file56,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data exploration,
file56,8,"X = data.drop(""total_claims"",1)
 X = data.drop(""week_number"",1)
 y = data[""total_claims""]",data preprocessing,,
file56,9,"plt.figure(figsize=(20,20))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file56,10,"cor_target = abs(cor[""total_claims""])
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features) # prints the correllation of columns to the total_claims column",data preprocessing,data exploration,
file56,11,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file56,12,"min(data['week_number']),max(data['week_number'])",data exploration,,
file56,13,data.dtypes #shows data types of columns,data exploration,,
file56,14,data = data.set_index('week_number'),data preprocessing,,
file56,15,data.index,data exploration,,
file56,16,"data.sample(5, random_state=0)",data exploration,,
file56,17,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",helper functions,modeling,
file56,18,"get_ipython().run_line_magic('pip', 'install scikit-learn')",helper functions,,
file56,19,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file56,20,data['unemployment_data'].plot(linewidth=0.5);,result visualization,,
file56,21,data['ironhacks-data.ironhacks_competition.unemployment_data'].plot(linewidth=0.5);,result visualization,,
file56,22,"sns.set(rc={'figure.figsize':(11, 4)})
 data['precipitation_data'].plot(linewidth=0.5);",result visualization,,
file57,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file57,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file57,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file57,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file57,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file57,5,get_ipython().system('pip install db-dtypes'),helper functions,,
file58,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file58,1,"#import cell
 import pandas as pd
 import numpy as np
 import statistics
 import csv
 import matplotlib.pyplot as plt
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file58,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file58,3,"#Gets the master unemployed table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file58,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemploymentData = query_job.to_dataframe()
 print(unemploymentData.shape)
 pd.set_option('display.max_columns', None)
 unemploymentData.head(3)",load data,data exploration,
file58,5,"#Gets each tracks mean and std dev
 #unlist has the master unemployment list
 #b becomes the filtered list
 unList = unemploymentData.values.tolist()
 b_set = set(tuple(x) for x in unList)
 b = [ list(x) for x in b_set ]",data preprocessing,,
file58,6,"uuid = []
 #makes a list of the unique uuid
 for x in b:
  if(uuid.count(x[0]) == 0):
  uuid.append(x[0])",data preprocessing,,
file58,7,"#setup for extract  
 values = []
 export = []",helper functions,,
file58,8,"#for each value make a list of each weeks claims
 for y in uuid:
  temp = [y]
  for x in b:
  if (x[0] == y):
  temp.append(x[6])
  values.append(temp)",data preprocessing,,
file58,9,"for x in values:
  name = x[0]
  mean = statistics.mean(x[1:])
  if (len(x) > 2):
  stdev = statistics.stdev(x[1:])
  else:
  print(""short"")
  export.append([name, mean, stdev])
 #Everything below this is testing",data preprocessing,,
file58,10,"#Make bar charts
 #unique list as guide to count
 x1 = []
 y1 = []
 for i in uuid[:1]:
  for k in b:
  if (k[0] == i):
  x1.append(k[2])
  y1.append(k[6])",data preprocessing,,
file58,11,"plt.bar(x1,y1)",result visualization,,
file58,12,"xValues = []
 xCount = []
 for i in uuid[:1]:
  for k in b:
  if (k[0] == i):
  xValues.append(k[6])
 c_set = set(tuple(x) for x in xValues)
 c = [ list(x) for x in c_set ]",data preprocessing,,
file58,13,"for x in c:
  xCount.append(xValues.count(x))",data preprocessing,,
file58,14,"plt.bar(xValues,xCount)",result visualization,,
file58,15,"for x in c:
  xCount.append(xValues.count(x))
 print(xValues)
 print(xCount)
 plt.bar(xValues,xCount)",result visualization,data exploration,
file58,16,"for x in c:
  xCount.append(xValues.count(x))
 print(statistics.mean(xValues))
 print(statistics.median(xValues))
 plt.bar(c,xCount)",result visualization,data exploration,
file58,17,"xValues = []
 xCount = []
 for i in uuid[:2]:
  for k in b:
  if (k[0] == i):
  xValues.append(k[6])
  c = list(dict.fromkeys(xValues))
 

  for x in c:
  xCount.append(xValues.count(x))
  print(statistics.mean(xValues))
  print(statistics.median(xValues))
  plt.bar(c,xCount)",result visualization,data exploration,
file59,0,y_true=test.loc[['week_number'==35]],data preprocessing,,
file60,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file60,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file60,2,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file60,3,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import plotly.express as px
 import csv",helper functions,,
file60,4,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file60,5,"def querydb(request):
  query_job = bigquery_client.query(request)
  data = query_job.to_dataframe()
  return data",load data,,
file60,6,"def createSubmission():
  return",helper functions,,
file60,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file60,8,"#Unemployment Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 unemployment_data = querydb(query)",load data,,
file60,9,"# Prediction Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 prediction_data = querydb(query)",load data,,
file60,10,"querydb(""""""SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`"""""")",load data,,
file60,11,df_wage = wage_data,data preprocessing,,
file60,12,"#should add another column called previous week claims, this would probably be a good predictor for the next week of claims
 df_un = unemployment_data",data preprocessing,,
file60,13,df_pred = prediction_data,data preprocessing,,
file60,14,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""]).drop_duplicates()",data preprocessing,,
file60,15,"df_pred[""uu_id""]",data exploration,,
file61,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file61,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file61,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file61,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file61,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file61,5,global df3,data exploration,,
file61,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file61,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file61,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file61,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",helper functions,data exploration,
file61,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file61,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,data exploration,
file61,12,"import statsmodels.api as sm
 from statsmodels import tsa",helper functions,,
file61,13,"mod = sm.tsa.statespace.SARIMAX(np.array(df3[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 12),
  enforce_stationarity=False,
  enforce_invertibility=False)
 results = mod.fit()
 print(results.summary().tables[1])",modeling,data exploration,
file61,14,df3 =df3.dropna(),data preprocessing,,
file61,15,"for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 12),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  pred = results.get_prediction(start=2, dynamic=False)
  pred_ci = pred.conf_int()
  y = temp[""total_claims""]
  ax = y.plot(label='observed')
  print(pred.predicted_mean)
  val = pred.predicted_mean
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = pred.predicted_mean
 test_df",data preprocessing,prediction,
file61,16,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",data preprocessing,,
file61,17,test_df.drop_duplicates(),data exploration,,
file61,18,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data exploration,data preprocessing,
file61,19,"adfuller(df3.[""total_claims""))",data exploration,,
file61,20,"import statsmodels.tsa.stattools.adfuller
 adfuller(df3[""total_claims""])",helper functions,data exploration,
file61,21,from pandas.tools.plotting import autocorrelation_plot,helper functions,,
file61,22,"from pandas.tools.plotting import autocorrelation_plot
 autocorrelation_plot(df3[""total_claims""])",helper functions,,
file61,23,test_df,data exploration,,
file61,24,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data exploration,save results,
file61,25,"import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat(temp,test_df)
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  pred = results.get_prediction(start=28, end =40, dynamic=False)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = (pred.predicted_mean)
  df3_.loc[df3_.uu_id_enc==k,""predicted_total_claims""] = (pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,,
file61,26,"df3_[[""total_claims"", ""predicted_total_claims""]].plot(figsize=(12, 8))",data preprocessing,,
file61,27,df3_,data exploration,,
file61,28,df3_.columns,data exploration,,
file61,29,"#Checking ARIMA
 df3_",data exploration,,
file61,30,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file61,31,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file61,32,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file61,33,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file61,34,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file61,35,global df3,data exploration,,
file61,36,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file61,37,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file61,38,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file61,39,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data exploration,helper functions,
file61,40,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file61,41,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,data exploration,
file61,42,"#df3_[""predicted_total_claims""]=li
 #df3_[[""total_claims"", ""predicted_total_claims""]].plot(figsize=(12, 8)) 
 df3_",data exploration,result visualization,
file61,43,"#df3_.loc[df3_[""week_number""]==28,""predicted_total_claims""]=li
 #df3_[[""total_claims"", ""predicted_total_claims""]].plot(figsize=(12, 8)) 
 df3_.loc[df3_[""week_number""]==28,""predicted_total_claims""]",result visualization,data exploration,
file61,44,test_df,data exploration,,
file61,45,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data exploration,save results,
file61,46,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  pred = results.get_prediction(start=28, end =28, dynamic=True)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = (pred.predicted_mean)
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,helper functions,
file61,47,"pd.merge(test_df, df3, on= [""uu_id"",""week_number""])",data preprocessing,,
file61,48,"res = pd.merge(test_df, df3, on= [""uu_id"",""week_number""])
 res = [[""uu_id"",""week_number"",""total_claims_x"",""total_claims_y""]]
 res",data exploration,,
file61,49,df3,data exploration,,
file61,50,test_df.astype(float),data preprocessing,,
file61,51,test_df[test_df.week_number==28],data exploration,,
file61,52,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",data preprocessing,load data,
file61,53,test_df.drop_duplicates(),data exploration,,
file61,54,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",comment only,,
file61,55,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data exploration,data preprocessing,
file61,56,"df3[df3.week_number==28].sort_values(""uu_id_enc"")",data exploration,,
file61,57,"test_df[test_df.week_number==28].sort_values(""uu_id_enc"")
 #test_df.dtypes",data exploration,,
file61,58,submission_prediction_output.uu_id.nunique(),data exploration,,
file61,59,df3.uu_id_enc.nunique(),data exploration,,
file61,60,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file61,61,"pred = results.get_prediction(start=39, dynamic=False)
 pred_ci = pred.conf_int()
 ax = y.plot(label='observed')
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Furniture Sales')
 plt.legend()
 plt.show()",result visualization,,
file61,62,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 test_df.isna()",data exploration,,
file61,63,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output.isnull().values.any()",data exploration,,
file61,64,"""""""test_df1=test_df.copy()
 for col in ['total_claims', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in extra:
  #test_df.loc[test_df.uu_id_enc==k,col] =0
  temp=df[df.uu_id_enc == k]
  temp[""average_wage""]=-9999
  temp =temp.replace("""",0)
  feature_test_pred = np.array(test_df1[test_df1.uu_id_enc==k])
  #print(k, temp)
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  idk = float(val.predict(np.array(feature_test_pred))[0])
  print(idk)
  test_df.loc[test_df.uu_id_enc==k,col].value = idk
 test_df""""""",comment only,,
file61,65,test_df,data exploration,,
file61,66,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data exploration,save results,
file61,67,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)",data preprocessing,,
file61,68,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0]",data exploration,data preprocessing,
file61,69,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k][""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()",data exploration,data preprocessing,
file61,70,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k][""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data exploration,save results,
file61,71,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=True)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,,
file61,72,submission_prediction_output[submission_prediction_output.total_claims<=0],data exploration,,
file62,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file62,1,print(X_train),data exploration,,
file62,2,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file62,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file62,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data exploration,
file62,5,"X = data['min_temperature'].values.reshape(-1,1)
 y = data['max_temperature'].values.reshape(-1,1)",,,
file62,6,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",,,
file62,7,class(X_train),data exploration,,
file63,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file63,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file63,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file63,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file64,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file64,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file64,2,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install plotly')
 get_ipython().system('pip install plotly-geo')
 get_ipython().system('pip install geopandas')
 get_ipython().system('pip install shapely')",helper functions,,
file64,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file64,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file64,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file64,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file64,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file64,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file64,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file64,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file64,11,"def example_function():
  print('Hello World')",comment only,,
file64,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file64,13,"model.fit(x,y)
 LinearRegression",modeling,,
file64,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file64,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,data exploration,
file64,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file64,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file64,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",data exploration,modeling,
file64,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file64,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",data exploration,prediction,
file64,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file64,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file64,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file64,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file64,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file64,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file64,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file64,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file64,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file64,30,"print(f""slope: {new_model.coef_}"")",data exploration,,
file64,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file64,32,"# Test Linear Regression
 x",data exploration,,
file64,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file64,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file64,35,"# Test Linear Regression
 print(results.summary())",data exploration,,
file64,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file64,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file64,38,dta = sm.datasets.statecrime.load_pandas().data,load data,,
file64,39,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file64,40,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file64,41,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",load data,helper functions,
file64,42,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",result visualization,,
file64,43,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file64,44,"# Defining MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",evaluation,,
file64,45,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file64,46,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file64,47,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,load data,
file64,48,"X = data['total_claims'].values.reshape(-1,1)
 y = data['week_number'].values.reshape(-1,1)",data preprocessing,,
file64,49,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",modeling,,
file64,50,"df = pd.DataFrame(np.random.rand(10, 3), columns =['a', 'b', 'c'])",data preprocessing,,
file64,51,print(df),data exploration,,
file64,52,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file64,53,"# importing matplotlib
 import matplotlib.pyplot",helper functions,,
file64,54,"# importing pandas as pd
 import pandas as pd",helper functions,,
file64,55,"# importing numpy as np
 import numpy as np",helper functions,,
file64,56,"# creating a dataframe
 df = pd.DataFrame(np.random.rand(10, 10),
  columns =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])",data preprocessing,,
file64,57,df,data exploration,,
file64,58,"# using a function df.plot.bar()
 df.plot.bar()",result visualization,,
file64,59,"from sklearn.metrics import mean_absolute_error
 Y_actual = [1,2,3,4,5]
 Y_Predicted = [1,2.5,3,4.1,4.9]
 mape = mean_absolute_error(Y_actual, Y_Predicted)*100
 print(mape)",data exploration,evaluation,
file64,60,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(20)",data exploration,load data,
file64,61,"import numpy as np
 numbers = np.linspace(5, 50, 24, dtype=int).reshape(4, -1)
 numbers",data exploration,helper functions,
file64,62,"mask = numbers % 4 == 0
 mask",data exploration,data preprocessing,
file64,63,"# Input 4 creates the mask
 numbers[mask]",data exploration,,
file64,64,"by_four = numbers[numbers % 4 == 0]
 by_four",data exploration,data preprocessing,
file64,65,"# USING MATPLOTLIB - image
 # A WAY TO COMMUNICATE DATA AND RESULTS
 # UNEMPLOYMENT RATES OCTOBER 2022 - Indiana Image
 # SOURCE: http://www.stats.indiana.edu/maptools/laus.asp
 # insall image
 # using opencv
 path = ""http://www.stats.indiana.edu/maptools/laus.asp""",helper functions,,
file64,66,"import numpy as np
 import matplotlib.image as mpimg",helper functions,,
file64,67,"img = mpimg.imread (http://www.stats.indiana.edu/maptools/laus.asp)
 print(type(img) )
 print(img.shape )",data exploration,,
file64,68,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file64,69,"def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data",load data,,
file64,70,"def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d",data preprocessing,,
file64,71,def load_raw(csv_name='0_raw.csv'):,load data,,
file64,72,"def load_raw(csv_name='0_raw.csv'):
  '''
 

 # IMPORT IRONHACKS DATA
 

 def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data
 

 def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d
 

 # IMPORT IRONHACKS DATA
 

 def query(table):
  Bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data
 

 def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d
 

 # IMPORT THE LINEAR REGRESSION MODEL
 

 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 

 # IMPORT IRONHACKS DATA
 

 def query(table):
  Bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data
 

 def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d
 

 # IMPORT THE LINEAR REGRESSION MODEL
 

 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 

 # IMPORT PACKAGE THE LINEAR REGRESSION MODEL- SCIKIT-LEARN 
 # SOURCE: https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 # NUMPY IS A FUNDAMENTAL PUYTHON SCIENTIFIC PACKAGE THAT ALLOWS MANY HIGH PEROFORMATION OPERATION SON A SINGLE AND MULTI DIMENSIONAL ARRAYS.
 # SOURCE: https://realpython.com/linear-regression-in-python/#python-packages-for-linear-regression
 

 import numpy as np
 from sklearn.linear_model import LinearRegression
 

 # DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])
 

 # Test Linear Regression
 model = LinearRegression().fit(x, y)
 

 f load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def get_week_number_map(g, colname):
  '''
  Creates a dictionary that maps from week number to an existing value in a given `colname`
  '''
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map
 

 def get_county(tract_name):
  m = re.search('Census Tract \S+, (.+) County, Indiana', tract_name)
  county = m.group(1)
  return county
 

 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def get_week_number_map(g, colname):
  '''
  
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map",,,
file64,73,"def get_county(tract_name):
  m = re.search('Census Tract \S+, (.+) County, Indiana', tract_name)
  county = m.group(1)
  return county",,,
file64,74,"def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c",load data,data preprocessing,
file64,75,"def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]",data preprocessing,,
file64,76,load_county().head(),data exploration,,
file64,77,"def get_week_number_map(g, colname):
  '''
  Creates a dictionary that maps from week number to an existing value in a given `colname`
  '''
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map",data preprocessing,,
file64,78,"def week_number_to_date(week_number, first_week_date='20220101'):
  '''
 

 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def get_week_number_map(g, colname):
  '''
  Creates a dictionary that maps from week number to an existing value in a given `colname`
  '''
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map
 

 def get_county(tract_name):
  m = re.search('Census Tract \S+, (.+) County, Indiana', tract_name)
  county = m.group(1)
  return county
 

 def week_number_to_date(week_number, first_week_date='20220101'):
  '''
  Prepare a date column for ARIMA
  '''
 def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c
 

 def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]
 

 load_county().head()
 

 def rf_industry(g):
  uu_id = g.uu_id.values[0]
  gg = g[COL_MAP['industry'] + ['week_number', 'total_claims']]  
  gg = gg.dropna()
  gg = pd.get_dummies(gg)
  
  if gg.shape[0] == 0:
  print(g.uu_id.values[0])
  mean = g.total_claims.mean()
  return pd.DataFrame([{'uu_id': uu_id, 'week_number': 38, 'total_claims': mean, 'predicted': mean}])
  x = gg.drop(['total_claims'], axis=1)  
  y = gg['total_claims']  
  max_avail_week_number = int(x.week_number.max())
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x, y.values.ravel())
  last = x.loc[x.week_number == max_avail_week_number, :].copy()
  last['week_number'] = max_avail_week_number + 1
  x_test = pd.concat([x, last], ignore_index=True)
  x_test['predicted'] = rf.predict(x_test)
  result = x_test.copy()
  result['total_claims'] = y.reset_index(0, drop=True)
  result['uu_id'] = uu_id
  return result[['uu_id', 'week_number', 'total_claims', 'predicted']]
  
 def plot_industry(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i in range(6):
  ax = axs[i]
  dd = subset(d, i)
  result = rf_industry(dd)
  ax.plot(result.week_number, result.total_claims, 'o-', label='original')
  ax.plot(result.week_number, result.predicted, 'o-', label='predict')
  ax.set_xlabel('week_number')
  ax.set_title(f'uu_id: {i}')
  ax.set_xlim(0, 42)
  
  axs[-1].legend(frameon=False)
  axs[0].set_ylabel('claims')
  plt.show()
 

 plot_industry(load_featured())
 

 def rf_industry(g):
  uu_id = g.uu_id.values[0]
  gg = g[COL_MAP['industry'] + ['week_number', 'total_claims']]  
  gg = gg.dropna()
  gg = pd.get_dummies(gg)
  
  if gg.shape[0] == 0:
  print(g.uu_id.values[0])
  mean = g.total_claims.mean()
  return pd.DataFrame([{'uu_id': uu_id, 'week_number': 38, 'total_claims': mean, 'predicted': mean}])
  x = gg.drop(['total_claims'], axis=1)  
  y = gg['total_claims']  
  max_avail_week_number = int(x.week_number.max())
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x, y.values.ravel())
  last = x.loc[x.week_number == max_avail_week_number, :].copy()
  last['week_number'] = max_avail_week_number + 1
  x_test = pd.concat([x, last], ignore_index=True)
  x_test['predicted'] = rf.predict(x_test)
  result = x_test.copy()
  result['total_claims'] = y.reset_index(0, drop=True)
  result['uu_id'] = uu_id
  return result[['uu_id', 'week_number', 'total_claims', 'predicted']]
  
 def plot_industry(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i in range(6):
  ax = axs[i]
  dd = subset(d, i)
  result = rf_industry(dd)
  ax.plot(result.week_number, result.total_claims, 'o-', label='original')
  ax.plot(result.week_number, result.predicted, 'o-', label='predict')
  ax.set_xlabel('week_number')
  ax.set_title(f'uu_id: {i}')
  ax.set_xlim(0, 42)
  
  axs[-1].legend(frameon=False)
  axs[0].set_ylabel('claims')
  plt.show()
 

 plot_industry(load_featured())
 

 def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data
 

 def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d
 

 def load_raw(csv_name='0_raw.csv'):
  '''",,,
file64,79,"def load_raw(csv_name='0_raw.csv'):
  '''
  def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data
 

 def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d
 

 def load_raw(csv_name='0_raw.csv'):
  '''",data preprocessing,,
file64,80,"def load_raw(csv_name='0_raw.csv'):
  '''
  Loads the unemployment and wage data and does some basic cleaning
  '''
  if not os.path.isfile(csv_name):
  u = query('unemployment_data')
  w = query('wage_data')
  raw = combine(u, w)
  raw.to_csv(csv_name, index=False)
  else:
  raw = pd.read_csv(csv_name)
  raw = raw.drop(['tract', 'timeperiod'], axis=1)
  raw = raw.sort_values(by=['uu_id', 'week_number'])
  raw = raw.drop_duplicates()
  raw = raw.replace({np.nan: None})
  raw = raw.reset_index(0, drop=True)
  return raw",save results,data preprocessing,
file64,81,load_raw().to_dict('records')[0],data exploration,,
file64,82,"X = data['total_claims'].values.reshape(-1,1)
 y = data['week_number'].values.reshape(-1,1)",data preprocessing,,
file64,83,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",modeling,,
file64,84,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file64,85,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file64,86,"df = pd.DataFrame(np.random.rand(10, 3), columns =['a', 'b', 'c'])",data preprocessing,,
file64,87,print(df),data exploration,,
file64,88,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file64,89,"from sklearn.metrics import mean_absolute_error
 Y_actual = [1,2,3,4,5]
 Y_Predicted = [1,2.5,3,4.1,4.9]
 mape = mean_absolute_error(Y_actual, Y_Predicted)*100
 print(mape)",data exploration,evaluation,
file64,90,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(20)",data exploration,load data,
file64,91,"# USING MATPLOTLIB - image
 # A WAY TO COMMUNICATE DATA AND RESULTS
 # UNEMPLOYMENT RATES OCTOBER 2022 - Indiana Image
 # SOURCE: http://www.stats.indiana.edu/maptools/laus.asp
 # insall image
 # using opencv
 path = ""http://www.stats.indiana.edu/maptools/laus.asp""",helper functions,,
file64,92,"import numpy as np
 import matplotlib.image as mpimg",helper functions,,
file64,93,"img = mpimg.imread (http://www.stats.indiana.edu/maptools/laus.asp)
 print(type(img) )
 print(img.shape )",data exploration,,
file64,94,"def rf_industry(g):
  uu_id = g.uu_id.values[0]
  gg = g[COL_MAP['industry'] + ['week_number', 'total_claims']]  
  gg = gg.dropna()
  gg = pd.get_dummies(gg)
  
  if gg.shape[0] == 0:
  print(g.uu_id.values[0])
  mean = g.total_claims.mean()
  return pd.DataFrame([{'uu_id': uu_id, 'week_number': 38, 'total_claims': mean, 'predicted': mean}])
  x = gg.drop(['total_claims'], axis=1)  
  y = gg['total_claims']  
  max_avail_week_number = int(x.week_number.max())
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x, y.values.ravel())
  last = x.loc[x.week_number == max_avail_week_number, :].copy()
  last['week_number'] = max_avail_week_number + 1
  x_test = pd.concat([x, last], ignore_index=True)
  x_test['predicted'] = rf.predict(x_test)
  result = x_test.copy()
  result['total_claims'] = y.reset_index(0, drop=True)
  result['uu_id'] = uu_id
  return result[['uu_id', 'week_number', 'total_claims', 'predicted']]",modeling,data preprocessing,
file64,95,"def plot_industry(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i in range(6):
  ax = axs[i]
  dd = subset(d, i)
  result = rf_industry(dd)
  ax.plot(result.week_number, result.total_claims, 'o-', label='original')
  ax.plot(result.week_number, result.predicted, 'o-', label='predict')
  ax.set_xlabel('week_number')
  ax.set_title(f'uu_id: {i}')
  ax.set_xlim(0, 42)
  
  axs[-1].legend(frameon=False)
  axs[0].set_ylabel('claims')
  plt.show()",result visualization,,
file64,96,plot_industry(load_featured()),data exploration,,
file64,97,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",result visualization,,
file64,98,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file64,99,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file64,100,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file64,101,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",data exploration,,
file64,102,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file64,103,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",data preprocessing,,
file65,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file65,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file65,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file65,3,"# get data information
 query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file65,4,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",load data,,
file65,5,"unemployment = query_job1.to_dataframe()
 wage = query_job2.to_dataframe()
 pred = query_job3.to_dataframe()",data preprocessing,,
file65,6,"# combine all data
 query4 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.uu_id
 """"""
 query_job4 = bigquery_client.query(query4)
 data = query_job4.to_dataframe()",data preprocessing,,
file65,7,"# drop duplicate columns
 dropcol = ['uu_id_1','countyfips_1','tract_1','tract_name_1']
 for col in dropcol:
  data = data.drop(col, axis=1)",data preprocessing,,
file65,8,data.head(),data exploration,,
file65,9,data1 = data.dropna(),data preprocessing,,
file65,10,data1,data exploration,,
file66,0,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file66,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file66,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file66,3,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",data preprocessing,,
file66,4,unemployment_data = unemployment_data.drop_duplicates(),data preprocessing,,
file66,5,"unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file66,6,"unemployment_data = unemployment_data.filter(['uu_id', 'week_number','total_claims','edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs','race_amerindian', 'race_asian', 'race_black','race_white'])",data preprocessing,,
file66,7,unemployment_data.isnull().sum(),data exploration,,
file66,8,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file66,9,unemployment_data.head(),data exploration,,
file66,10,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file66,11,uuids[:10],data exploration,,
file66,12,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid]
  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  # new = pd.DataFrame({'uu_id':uuid, 'week_number': 39, 'total_claims': data.total_claims.median(), 'edu_8th_or_less':0, 'edu_grades_9_11': 0, 'edu_hs_grad_equiv':0, 'edu_post_hs':0,
  #  'race_amerindian':0, 'race_asian':0, 'race_black':0, 'race_white':0}, index=[0])
  
  data.loc[len(data.index)] = [uuid, week, data.total_claims.median(), 0,0,0,0,0,0,0,0]
  data
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
  plt.title('Tract: '+uuid)
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  
  return result.sort_values(by = 'week_number')",modeling,prediction,
file66,13,,,,
file66,14,"predict_claims('005be9532fd717dc36d4be318fd9ad25',39)",data exploration,,
file66,15,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",data exploration,,
file66,16,prediction_list,data exploration,,
file66,17,uuids = prediction_list.tolist(),data preprocessing,,
file66,18,len(uuids),data exploration,,
file66,19,import csv,helper functions,,
file66,20,"fields = ['uu_id', 'week_number', 'total_claims']
 rows = []
 for uuid in uuids:
  rows.append(list(uuid, 39, predict_claims(uuid, 39))",data preprocessing,,
file66,21,filename = 'submission_prediction_output.csv',data preprocessing,,
file66,22,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid].copy()
  data.loc[len(data.index)] = [uuid, week, data.total_claims.median(), 0,0,0,0,0,0,0,0]
  # plt.plot(data.week_number, data.total_claims)
  # plt.show()
  
  
  data
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number",modeling,prediction,
file66,23,"result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",data preprocessing,,
file67,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file67,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file67,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file67,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file67,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file67,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file67,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file67,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file67,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file67,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file67,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file67,11,"def example_function():
  print('Hello World')",data preprocessing,,
file67,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file67,13,"model.fit(x,y)
 LinearRegression",modeling,,
file67,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file67,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,evaluation,
file67,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file67,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file67,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file67,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file67,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,evaluation,
file67,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file67,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file67,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file67,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file67,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file67,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file67,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file67,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file67,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file67,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file67,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file67,32,"# Test Linear Regression
 x",evaluation,,
file67,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file67,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file67,35,"# Test Linear Regression
 print(results.summary())",evaluation,,
file67,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file67,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file67,38,prestige.head(),data exploration,,
file67,39,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file67,40,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file67,41,get_ipython().system('pip install db-dtypes'),helper functions,,
file67,42,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file67,43,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file67,44,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file67,45,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file67,46,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file67,47,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file67,48,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file67,49,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file67,50,"def example_function():
  print('Hello World')",data preprocessing,,
file67,51,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file67,52,"model.fit(x,y)
 LinearRegression",modeling,,
file67,53,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file67,54,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,evaluation,
file67,55,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file67,56,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file67,57,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file67,58,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file67,59,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,evaluation,
file67,60,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file67,61,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file67,62,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file67,63,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file67,64,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file67,65,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file67,66,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file67,67,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file67,68,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file67,69,"print(f""slope: {new_model.coef_}"")",evaluation,,
file67,70,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file67,71,"# Test Linear Regression
 x",data exploration,,
file67,72,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file67,73,"# Test Linear Regression
 results = model.fit()",modeling,,
file67,74,"# Test Linear Regression
 print(results.summary())",evaluation,,
file67,75,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file67,76,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file67,77,prestige.head(),data exploration,,
file67,78,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,,
file67,79,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file67,80,print(prestige_model.summary()),evaluation,,
file67,81,"crime_model = ols(""claim ~ uu_id + tract + age + single"", data=dta).fit()
 print(crime_model.summary())",modeling,evaluation,
file68,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file68,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file68,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file68,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file68,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import OneHotEncoder",helper functions,,
file68,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file68,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file68,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file68,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file68,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,data preprocessing,
file68,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file68,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 39
 """"""",load data,,
file68,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file68,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file68,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file68,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 print(duplicated_rows)
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,data exploration,
file68,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file69,0,"ud_test
 ud_train",data exploration,,
file69,1,ud_test,data exploration,,
file69,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file70,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file70,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file70,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file70,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file70,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file70,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,data preprocessing,
file70,6,empdata.head().transpose(),data exploration,,
file70,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",data exploration,,
file70,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file70,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data exploration,,
file70,10,empdata[['total_claims']].describe(),data exploration,,
file70,11,plt.plot(np.sort(empdata['total_claims'].values)),data exploration,,
file70,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",data preprocessing,data exploration,
file70,13,"empdata.plot.box('week_number', figsize=(25,15))",data exploration,,
file70,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data preprocessing,,
file70,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file70,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=6)[5]",modeling,prediction,
file70,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file70,18,uupred.head(),data exploration,,
file70,19,"uupred['total_claims'] = 0
 uupred.head()",data preprocessing,data exploration,
file70,20,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values[:lastw], seasonal=False, error_action='ignore')
  pred = int(m.predict(n_periods=predw-lastw)[predw-lastw-1])
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",data preprocessing,,
file70,21,"dopred(37,43)",data exploration,,
file70,22,"uupred = uupred[['uu_id', 'total_claims', 'week_number']]
 uupred.to_csv('submission_prediction_output.csv', index=False)",save results,,
file70,23,1,,,
file71,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file71,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file71,2,"import db_dtypes
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file71,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file71,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file72,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file72,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file72,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file72,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file72,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",load data,data preprocessing,
file72,5,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,data preprocessing,
file72,6,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]
 #df_three_col.shape",data preprocessing,,
file72,7,"df_three_col = df_three_col.drop_duplicates()
 #print(df_three_col.shape)",data preprocessing,,
file72,8,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
 #df_three_col.head(21)
 #df_three_col.tail(3)
 #uu_id week_number total_claims
 #1876 001cd9ae23064d7f0fd3cd327c873d8d 31 34",data preprocessing,,
file72,9,"tmp_df = df_three_col['week_number']
 tmp_df = tmp_df.drop_duplicates()
 print(tmp_df)",data preprocessing,data exploration,
file72,10,"res = pd.DataFrame(columns = ['uu_id', 'total_claims', 'week_number'])",data preprocessing,,
file72,11,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  #test_data = test_data.tail(3)
  week_list = test_data['week_number'].tolist()
  y_pred = -1
  count = 0
  sum = 0
  for week_id in range(33,38):
  if week_id in week_list:
  count+=1
  tmp_row = test_data[test_data['week_number'] == week_id ]
  sum+= int( tmp_row['total_claims'])
  if count > 0:
  y_pred = int(sum/count)
  else:
  max_week = max(week_list)
  tmp_row = test_data[test_data['week_number'] == max_week ] 
  y_pre = tmp_row['total_claims']
  cur_row = pd.DataFrame([[cur_uu_id, y_pred, 42]], columns=['uu_id', 'total_claims', 'week_number'] )
  res = pd.concat([res,cur_row] ,ignore_index = True)",data preprocessing,,
file72,12,"res.to_csv(""Nov28_submission_prediction_output.csv"", index=False)",save results,,
file73,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file73,1,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file73,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file73,3,"#QUERY DATA
 query_job = bigquery_client.query(query)
 print(query_job)
 unemp = query_job.to_dataframe()",data preprocessing,data exploration,
file74,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file74,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file74,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file74,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file74,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file74,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file74,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",load data,,
file74,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file74,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file74,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file74,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file74,11,"def example_function():
  print('Hello World')",helper functions,,
file74,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file74,13,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",modeling,,
file74,14,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file74,15,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file74,16,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,evaluation,
file74,17,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file74,18,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file74,19,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file74,20,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file74,21,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,evaluation,
file74,22,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file74,23,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file74,24,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file74,25,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file74,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file74,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file74,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file74,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file74,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file74,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file74,32,"# Test Linear Regression
 x",evaluation,,
file74,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file74,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file74,35,"# Test Linear Regression
 print(results.summary())",evaluation,,
file74,36,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,data exploration,
file74,37,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file74,38,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file74,39,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file74,40,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]",data preprocessing,,
file74,41,"mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]",data preprocessing,,
file74,42,"if y_train.shape[0] == 0:
  g[ycol] = None
 return g[ycol]
  
  classes = y_train[ycol].unique()
 if len(classes) == 1:
  yhat = [classes[0]]
 else:
  model =
 sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
 return g[ycol]",data preprocessing,modeling,
file74,43,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",data preprocessing,,
file74,44,"def load_imp_industry(csv_name='4_imp_industry.csv'):
  if not os.path.isfile(csv_name):
  d = load_clean()
  d = d.groupby('uu_id').apply(impute_industry).reset_index(0, drop=True)
  d.to_csv(csv_name, index=False)
  else:
  d = pd.read_csv(csv_name)
  return d",data preprocessing,save results,
file74,45,"d = load_imp_industry()
 d",data preprocessing,data exploration,
file74,46,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]",data preprocessing,,
file74,47,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]
  
  if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model =
 sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",data preprocessing,modeling,
file74,48,"def get_avg_total_claims(g):
  return pd.DataFrame([{'uu_id': g.uu_id.values[0], 'average_wage': g.average_wage.values[0], 'avg_total_claims': g.total_claims.mean()}])",data preprocessing,,
file74,49,"def plot_avg_total_claims(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i, week_number in enumerate([1, 6, 11, 16, 31, 36]):  
  ax = axs[i]
  dd = d.loc[d.week_number == week_number, ['average_wage', 'total_claims']]
  ax.plot(dd.average_wage, dd.total_claims, 'o')
  ax.set_title(f'week_number: {week_number}')
  ax.set_xlabel('average_wage')
  axs[0].set_ylabel('total_claims')
  plt.show()
  plt.close()
  
  avg = d.groupby('uu_id').apply(get_avg_total_claims).reset_index(0, drop=True)
  fig, ax = plt.subplots()
  ax.plot(avg.average_wage, avg.avg_total_claims, 'o')  
  ax.set_xlabel('average_wage')
  ax.set_ylabel('average_total_claims')
  plt.show()
  plt.close()
  
  print(avg[['average_wage', 'avg_total_claims']].corr())",result visualization,,
file74,50,plot_avg_total_claims(d),result visualization,,
file74,51,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file74,52,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file74,53,get_ipython().system('pip install db-dtypes'),helper functions,,
file74,54,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file74,55,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file74,56,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file74,57,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file74,58,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file74,59,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file74,60,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file74,61,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file74,62,"def example_function():
  print('Hello World')",helper functions,,
file74,63,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file74,64,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",modeling,,
file74,65,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file74,66,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file74,67,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file74,68,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file74,69,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file74,70,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file74,71,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file74,72,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,evaluation,
file74,73,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file74,74,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file74,75,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file74,76,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file74,77,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file74,78,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file74,79,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file74,80,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file74,81,"print(f""slope: {new_model.coef_}"")",evaluation,,
file74,82,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file74,83,"# Test Linear Regression
 x",evaluation,,
file74,84,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file74,85,"# Test Linear Regression
 results = model.fit()",modeling,,
file74,86,"# Test Linear Regression
 print(results.summary())",evaluation,,
file74,87,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file74,88,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file74,89,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file74,90,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file74,91,"def get_avg_total_claims(g):
  return pd.DataFrame([{'uu_id': g.uu_id.values[0], 'average_wage': g.average_wage.values[0], 'avg_total_claims': g.total_claims.mean()}])",data preprocessing,,
file74,92,"def plot_avg_total_claims(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i, week_number in enumerate([1, 6, 11, 16, 31, 36]):  
  ax = axs[i]
  dd = d.loc[d.week_number == week_number, ['average_wage', 'total_claims']]
  ax.plot(dd.average_wage, dd.total_claims, 'o')
  ax.set_title(f'week_number: {week_number}')
  ax.set_xlabel('average_wage')
  axs[0].set_ylabel('total_claims')
  plt.show()
  plt.close()
  
  avg = d.groupby('uu_id').apply(get_avg_total_claims).reset_index(0, drop=True)
  fig, ax = plt.subplots()
  ax.plot(avg.average_wage, avg.avg_total_claims, 'o')  
  ax.set_xlabel('average_wage')
  ax.set_ylabel('average_total_claims')
  plt.show()
  plt.close()
  
  print(avg[['average_wage', 'avg_total_claims']].corr())",result visualization,,
file74,93,plot_avg_total_claims(d),result visualization,,
file74,94,"def plot_regression(d):  
  e = d.loc[:, COL_MAP['edu'] + ['total_claims']].reset_index(0, drop=True)
  e = e.fillna(0)
  X = e[COL_MAP['edu']]
  y = e['total_claims']
  model = sklearn.linear_model.LinearRegression()
  model.fit(X, y)
  fig, ax = plt.subplots()
  ax.plot(X.index, model.predict(X), label='predict')
  ax.plot(X.index, y, label='observed', ls='--')
  ax.legend()
  ax.set_xlabel('index')
  ax.set_ylabel('total_claims')",result visualization,,
file74,95,plot_regression(d),result visualization,,
file74,96,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file74,97,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file74,98,get_ipython().system('pip install db-dtypes'),helper functions,,
file74,99,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file74,100,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file74,101,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file74,102,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file74,103,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file74,104,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file74,105,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file74,106,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file74,107,"def example_function():
  print('Hello World')",helper functions,,
file74,108,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file74,109,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",modeling,,
file74,110,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file74,111,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file74,112,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,evaluation,
file74,113,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file74,114,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file74,115,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file74,116,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file74,117,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file74,118,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file74,119,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file74,120,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file74,121,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file74,122,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file74,123,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file74,124,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file74,125,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file74,126,"print(f""slope: {new_model.coef_}"")",evaluation,,
file74,127,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file74,128,"# Test Linear Regression
 x",evaluation,,
file74,129,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file74,130,"# Test Linear Regression
 results = model.fit()",evaluation,,
file74,131,"# Test Linear Regression
 print(results.summary())",evaluation,,
file74,132,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file74,133,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file74,134,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file74,135,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file74,136,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file74,137,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file74,138,"prestige = sm.datasets.get_rdataset(""Duncan"", ""carData"", cache=True).data",data preprocessing,,
file74,139,prestige.head(),data exploration,,
file74,140,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,evaluation,
file74,141,print(prestige_model.summary()),evaluation,,
file74,142,"fig = sm.graphics.influence_plot(prestige_model, criterion=""cooks"")
 fig.tight_layout(pad=1.0)",result visualization,,
file74,143,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file74,144,"crime_model = ols(""murder ~ urban + poverty + hs_grad + single"", data=dta).fit()
 print(crime_model.summary())",modeling,evaluation,
file74,145,img.show(),result visualization,,
file74,146,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file74,147,"df = pd.DataFrame(Actual.y_test(), columns = Predicted.y_pred())
 print(df)",data preprocessing,data exploration,
file74,148,"df = pd.DataFrame(np.random.rand(10, 3), columns =['a', 'b', 'c'])",data preprocessing,,
file74,149,print(df),data exploration,,
file74,150,y_pred = regressor.predict(X_test),prediction,,
file74,151,"# DATA VISUALIZATION - COMPARISON RESULTS AS A SCATTER
 plt.scatter(X_test, y_test, color='gray')
 plt.plot(X_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file74,152,"plt.scatter(X_test, y_test, color='gray')
 plt.plot(X_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file74,153,"# importing matplotlib
 import matplotlib.pyplot",helper functions,,
file74,154,"# importing pandas as pd
 import pandas as pd",helper functions,,
file74,155,"# importing numpy as np
 import numpy as np",helper functions,,
file74,156,"# creating a dataframe
 df = pd.DataFrame(np.random.rand(10, 10),
  columns =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])",data preprocessing,,
file74,157,df,data exploration,,
file74,158,"# using a function df.plot.bar()
 df.plot.bar()",data preprocessing,,
file74,159,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file74,160,"# Print the shape of the image
 print(img.shape)
 img.show()",result visualization,,
file74,161,"# Print the shape of the image
 plt.show()
 plt.close()",result visualization,,
file74,162,"# Print the shape of the image
 plt.show()
 plt.close()
 

  print(avg[['average_wage', 'avg_total_claims']].corr())",result visualization,evaluation,
file74,163,plot_avg_total_claims(d),result visualization,,
file74,164,"regressor = LinearRegression()
 regressor.fit (x_train, y_train)",modeling,evaluation,
file75,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file76,0,2+2 #this is a code cell,helper functions,,
file76,1,2^2,helper functions,,
file76,2,"import numpy as np
 import matplotlib.pyplot as plt
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file76,3,"x = np.linspace(0, 10, 500)
 y = np.cumsum(np.random.randn(500, 6), 0)",data preprocessing,,
file76,4,"plt.figure(figsize=(12, 7))
 plt.plot(x, y)
 plt.legend('ABCDEF', ncol=2, loc='upper left')",result visualization,,
file77,0,"get_ipython().system('pip install pandas-profiling[notebook]')
 from pandas_profiling import ProfileReport
 profile = ProfileReport(df3, title=""Report"")
 profile",helper functions,,
file77,1,"get_ipython().system('pip install sweetviz')
 import sweetviz as sv
 analyze_report = sv.analyze(df)
 analyze_report.show_html(report.html', open_browser=False)",helper functions,data preprocessing,
file77,2,"get_ipython().system('pip install autoviz')
 from autoviz.AutoViz_Class import AutoViz_Class
 AV = AutoViz_Class()
 df_av = AV.AutoViz('parking.csv')",helper functions,data preprocessing,
file77,3,get_ipython().system('pip install sweetviz'),helper functions,,
file77,4,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file77,5,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file77,6,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file77,7,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file77,8,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file77,9,global df3,helper functions,,
file77,10,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file77,11,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file77,12,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data preprocessing,
file77,13,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file77,14,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file77,15,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file77,16,analyze_report,data exploration,,
file77,17,"import codecs
 file = codecs.open(""report.html"", ""r"", ""utf-8"")
 print(file.read())",data preprocessing,data exploration,
file78,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file78,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file78,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file78,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file78,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file78,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file78,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file78,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file78,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file78,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file78,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file78,11,"def example_function():
  print('Hello World')",helper functions,,
file78,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file78,13,"model.fit(x,y)
 LinearRegression",evaluation,,
file78,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file78,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file78,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file78,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file78,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file78,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file78,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file78,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file78,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file78,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file78,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file78,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file78,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file78,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",evaluation,,
file78,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file78,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file78,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file78,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file78,32,"# Test Linear Regression
 x",evaluation,,
file78,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file78,34,"# Test Linear Regression
 results = model.fit()",evaluation,,
file78,35,"# Test Linear Regression
 print(results.summary())",evaluation,,
file78,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file78,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file78,38,print(prestige_model.summary()),evaluation,,
file78,39,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file78,40,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file78,41,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file78,42,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",data preprocessing,,
file78,43,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file78,44,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file78,45,get_ipython().system('pip install db-dtypes'),helper functions,,
file78,46,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file78,47,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file78,48,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file78,49,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file78,50,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file78,51,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file78,52,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file78,53,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file78,54,"def example_function():
  print('Hello World')",helper functions,,
file78,55,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file78,56,"model.fit(x,y)
 LinearRegression",evaluation,,
file78,57,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file78,58,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file78,59,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file78,60,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file78,61,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file78,62,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file78,63,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file78,64,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file78,65,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file78,66,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file78,67,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file78,68,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",evaluation,,
file78,69,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file78,70,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",evaluation,,
file78,71,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file78,72,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file78,73,"print(f""slope: {new_model.coef_}"")",evaluation,,
file78,74,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file78,75,"# Test Linear Regression
 x",evaluation,,
file78,76,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file78,77,"# Test Linear Regression
 results = model.fit()",evaluation,,
file78,78,"# Test Linear Regression
 print(results.summary())",evaluation,,
file78,79,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file78,80,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file78,81,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file78,82,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file78,83,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file78,84,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",data preprocessing,,
file78,85,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",result visualization,,
file78,86,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file78,87,"# Defining MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",data preprocessing,,
file78,88,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file78,89,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file78,90,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file78,91,"X = data['total_claims'].values.reshape(-1,1)
 y = data['week_number'].values.reshape(-1,1)",data preprocessing,,
file78,92,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file78,93,"df = pd.DataFrame(np.random.rand(10, 3), columns =['a', 'b', 'c'])",data preprocessing,,
file78,94,print(df),data exploration,,
file78,95,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",data exploration,result visualization,
file78,96,"# importing matplotlib
 import matplotlib.pyplot",helper functions,,
file78,97,"# importing pandas as pd
 import pandas as pd",helper functions,,
file78,98,"# importing numpy as np
 import numpy as np",helper functions,,
file78,99,"# creating a dataframe
 df = pd.DataFrame(np.random.rand(10, 10),
  columns =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])",data preprocessing,,
file78,100,df,data exploration,,
file78,101,"# using a function df.plot.bar()
 df.plot.bar()",data exploration,result visualization,
file78,102,"from sklearn.metrics import mean_absolute_error
 Y_actual = [1,2,3,4,5]
 Y_Predicted = [1,2.5,3,4.1,4.9]
 mape = mean_absolute_error(Y_actual, Y_Predicted)*100
 print(mape)",data preprocessing,data exploration,
file78,103,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(20)",load data,data exploration,
file78,104,"import numpy as np
 numbers = np.linspace(5, 50, 24, dtype=int).reshape(4, -1)
 numbers",data preprocessing,data exploration,
file78,105,"mask = numbers % 4 == 0
 mask",data preprocessing,data exploration,
file78,106,"# Input 4 creates the mask
 numbers[mask]",data preprocessing,,
file78,107,"by_four = numbers[numbers % 4 == 0]
 by_four",data preprocessing,data exploration,
file78,108,"# USING MATPLOTLIB - image
 # A WAY TO COMMUNICATE DATA AND RESULTS
 # UNEMPLOYMENT RATES OCTOBER 2022 - Indiana Image
 # SOURCE: http://www.stats.indiana.edu/maptools/laus.asp
 # insall image
 # using opencv
 path = ""http://www.stats.indiana.edu/maptools/laus.asp""",data preprocessing,,
file78,109,"import numpy as np
 import matplotlib.image as mpimg",helper functions,,
file78,110,"img = mpimg.imread (http://www.stats.indiana.edu/maptools/laus.asp)
 print(type(img) )
 print(img.shape )",data preprocessing,data exploration,
file79,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file79,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file79,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file79,3,"def example_function():
  print('Hello World')",helper functions,,
file79,4,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file79,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file79,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file79,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file79,8,"#Unemployment Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.unemployment_data`
 """"""",load data,,
file79,9,"# Wage Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file79,10,wage_data,data exploration,,
file79,11,predition_data,data exploration,,
file79,12,unemployment_data,data exploration,,
file80,0,import bigquery,helper functions,,
file80,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file80,2,import Google.Cloud.BigQuery,helper functions,,
file80,3,import pandas-gbq,helper functions,,
file80,4,from pandas import bigquery,helper functions,,
file80,5,"from pandas.io import gbq
 import pandas_qbg",helper functions,,
file80,6,"import pandas as pd
 from pandas.io import gbq",helper functions,,
file80,7,"import pandas as pd
 import json
 import requests
 from pandas.io import gbq
 import pandas_gbq
 import gcsfc",helper functions,,
file80,8,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file80,9,from google.cloud import bigquery,helper functions,,
file80,10,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training`
 """"""",load data,,
file80,11,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file81,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file81,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file81,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file81,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file81,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file81,5,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('pip install db-dtypes')",helper functions,,
file82,0,"y_pred = model.predict(X_test)
 y_pred.shape()",prediction,evaluation,
file82,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file82,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file82,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file82,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file82,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file82,6,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file82,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",data exploration,result visualization,
file82,8,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file82,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file82,10,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data preprocessing,data exploration,
file82,11,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data preprocessing,data exploration,
file82,12,"from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]
 temp=test.loc[test[""week_number""]==34]",modeling,data preprocessing,
file82,13,"model.fit(X_train,y)",evaluation,,
file82,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data preprocessing,data exploration,
file83,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file83,1,"#import cell
 import pandas as pd
 import numpy as np
 import statistics
 import csv
 import matplotlib.pyplot as plt
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file83,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file83,3,"#Gets the master unemployed table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file83,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemploymentData = query_job.to_dataframe()
 print(unemploymentData.shape)
 pd.set_option('display.max_columns', None)
 unemploymentData.head(3)",load data,data preprocessing,
file84,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file84,1,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file84,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file84,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file84,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file84,5,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file84,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",data exploration,result visualization,
file84,7,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file84,8,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file84,9,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data preprocessing,data exploration,
file84,10,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data preprocessing,data exploration,
file84,11,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]",modeling,data preprocessing,
file84,12,"model.fit(X_train,y)",evaluation,,
file84,13,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 data.iloc[X_test.index(),'uu_id']",data preprocessing,data exploration,
file84,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data preprocessing,data exploration,
file84,15,"a=pd.Dataframe(data=temp)
 a",data preprocessing,data exploration,
file84,16,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a",data preprocessing,data exploration,
file84,17,"y_pred = model.predict(X_test)
 y_pred",prediction,,
file84,18,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a[count].astype('int')
 a",data preprocessing,data exploration,
file84,19,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 columns=['uu_id','week','count']
 a=a.loc[:,columns]
 a",data preprocessing,data exploration,
file84,20,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 a=a[['uu_id','week_number','count']]
 a['week_number'].replace('week')
 a",data preprocessing,data exploration,
file84,21,"a.to_csv(""submission.csv"",index=False)
 a",save results,data exploration,
file84,22,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 a=a[['uu_id','week_number','count']]
 a['week_number']=a['week_number+1']
 a=a.rename(columns={'week_number':'week'})
 a.to_string(index=False)",data preprocessing,,
file85,0,from google.cloud import bigquery,helper functions,,
file85,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file85,2,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training`
 """"""",load data,,
file85,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file86,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file86,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file86,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file86,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file86,4,"query = """"""
 select * from `ironhacks-data.ironhacks_competition`
 """"""",load data,,
file86,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']=pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file86,6,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file86,7,data['edu_8th_or_less'],data exploration,,
file86,8,data['edu_8th_or_less'].value_counts(),data exploration,,
file86,9,data['edu_grades_9_11'].value_counts(),data exploration,,
file86,10,data['edu_grades_9_11'].isna().sum(),data exploration,,
file86,11,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.dtypes()
 # data = data.drop([""tract_name"",""edu_8th_or_less"",",load data,data preprocessing,
file86,12,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 pd.set_option('display.max_columns', None)
 data = data.drop(['tract_name','edu_8th_or_less','edu_unknown'],axis=1)
 data",load data,data preprocessing,
file87,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file87,1,"import pandas as pd
 import numpy as np
 from google.cloud import bigquery",helper functions,,
file87,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file87,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file87,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file87,5,unemployment_data.shape,data exploration,,
file87,6,unemployment_data.sort_values(['week_number']),data exploration,,
file87,7,unemployment_data.columns,data exploration,,
file87,8,"unemployment_data.drop(['uu_id', 'week_number', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data preprocessing,,
file87,9,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file87,10,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file87,11,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file87,12,"unemployment_data.sort_values(['ds'], inplace=True)",data preprocessing,,
file87,13,unemployment_data.drop_duplicates(inplace=True),data preprocessing,,
file87,14,from prophet import Prophet,helper functions,,
file87,15,ud = unemployment_data,data preprocessing,,
file87,16,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file87,17,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",data preprocessing,,
file87,18,m = Prophet(),modeling,,
file87,19,m.fit(ud_train),evaluation,,
file87,20,"future = m.make_future_dataframe(periods=ud_test.shape[0], freq='W')",data preprocessing,,
file87,21,forecast = m.predict(df=future),prediction,,
file87,22,forecast,evaluation,,
file87,23,plot1 = m.plot(forecast),result visualization,,
file87,24,ud_test,data exploration,,
file87,25,"ud_test
 ud_train",data exploration,,
file87,26,"print(ud_train, ud_test)",data exploration,,
file87,27,ud_train,data exploration,,
file87,28,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file87,29,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file87,30,unemployment_data.shape,data exploration,,
file87,31,unemployment_data.sort_values(['week_number']),data preprocessing,,
file87,32,unemployment_data.columns,data exploration,,
file87,33,"unemployment_data.drop(['uu_id', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data preprocessing,,
file87,34,"unemployment_data.drop_duplicates(inplace=True)
 unemployment_data.sort_values(['week_number'])",data preprocessing,,
file87,35,"m = Prophet(weekly_seasonality=False,
  daily_seasonality=False,
  interval_width=0.95, 
  mcmc_samples = 500)",modeling,,
file87,36,"m.add_seasonality(
  name='monthly', 
  period=30.5, 
  fourier_order=5
  )",modeling,,
file87,37,forecast['ds'],data exploration,,
file87,38,forecast['ds' == pd.to_datetime('2022-05-14')]).yhat,data exploration,,
file87,39,ud = unemployment_data,data preprocessing,,
file87,40,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file87,41,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",data preprocessing,,
file87,42,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file87,43,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file87,44,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day', 'week_number'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file87,45,"unemployment_data.sort_values(['ds'], inplace=True)",data preprocessing,,
file87,46,m.fit(ud_train),evaluation,,
file87,47,"future = m.make_future_dataframe(periods=35, freq='W')",evaluation,,
file87,48,forecast = m.predict(df=future),evaluation,,
file87,49,ud_train,data exploration,,
file88,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file88,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file88,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file88,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file88,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file88,5,global df3,data exploration,,
file88,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file88,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file88,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file88,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file88,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file88,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file88,12,"!pip install pandas-profiling[notebook]
 from pandas_profiling import ProfileReport
 profile = ProfileReport(df3, title=""Report"")
 profile",helper functions,data preprocessing,
file88,13,"!pip install sweetviz
 import sweetviz as sv
 analyze_report = sv.analyze(df3)
 analyze_report.show_html('report.html', open_browser=True)
 profile = ProfileReport(df3, title=""report"")
 profile",helper functions,data preprocessing,
file88,14,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt
 import warnings
 warnings.filterwarnings(""ignore"")
 import pandas as pd
 import csv
 import numpy as np",helper functions,,
file88,15,analyze_report.show,data exploration,,
file88,16,"show_html( filepath='report.html', 
  open_browser=True, 
  layout='widescreen', 
  scale=None)",save results,,
file88,17,"for feature in p:
  sns.violinplot(data=houseprices_num[feature].values, 
  inner='quartile', color='white')
  plt.show()",result visualization,,
file88,18,"p=[]
 from pandas.api.types import is_string_dtype
 from pandas.api.types import is_numeric_dtype
 for feature in features:
  if (is_numeric_dtype(df3[feature])==True):
  p.append(feature)
 num_df = df3[[k for k in p]]",helper functions,data preprocessing,
file88,19,num_df,data exploration,,
file88,20,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file88,21,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file88,22,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,,
file88,23,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file88,24,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file88,25,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file88,26,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,data exploration,
file88,27,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,,
file88,28,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file88,29,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file88,30,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file88,31,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,,
file88,32,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file88,33,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file88,34,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file88,35,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file88,36,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file88,37,global df3,helper functions,,
file88,38,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file88,39,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file88,40,"query_job1 = bigquery_client.query(query)
 query_job1",load data,,
file88,41,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file88,42,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file88,43,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file88,44,"for k in range(1,10):
  temp = df3[df3.month ==k]
  ax= sns.lineplot( x= ""day"", y= ""total_claims"", hue = ""month"", data =temp )
  plt.show()",data preprocessing,,
file88,45,df3,data exploration,,
file88,46,df4[df4.total_claims>0.2],data exploration,,
file88,47,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(x = df3[feature], y = df3[""total_claims""])
  plt.show()",data preprocessing,result visualization,
file88,48,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(y = df3[feature], x = ""week_number"", data = df3)
  ax= sns.lineplot(y = df3[feature], x = ""week_number"", data = df3)
  plt.show()",data preprocessing,result visualization,
file88,49,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file88,50,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,data exploration,
file88,51,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file88,52,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file88,53,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file88,54,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,,
file88,55,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,,
file88,56,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file88,57,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file88,58,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file88,59,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file88,60,"import itertools
 get_ipython().run_line_magic('matplotlib', 'inline')
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,,
file88,61,"p=[]
 from pandas.api.types import is_string_dtype
 from pandas.api.types import is_numeric_dtype
 for feature in features:
  if (is_numeric_dtype(df3[feature])==True):
  p.append(feature)
 num_df = df3[[k for k in p]]",data preprocessing,,
file88,62,num_df,data exploration,,
file88,63,"query_job1.plot(kind=""bar"", x=""country_name"", y=""num_regions"", figsize=(15, 10))",result visualization,,
file88,64,query_job1.result(),evaluation,,
file88,65,"for row in query_job1.result():
  print(row.term)",helper functions,,
file89,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file89,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file89,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file89,3,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file89,4,"def createSubmission():
  return",helper functions,,
file89,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file89,6,"#Unemployment Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file89,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file90,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file90,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file90,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,data preprocessing,
file90,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file90,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data preprocessing,
file90,5,"print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file90,6,"query = """"""
 SELECT *
 FROM `ironhacks-data:ironhacks_competition.unemployment_data`
 """"""",load data,,
file91,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file91,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file92,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file92,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file92,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file92,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file92,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file92,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,data preprocessing,
file92,6,empdata.head().transpose(),data exploration,,
file92,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file92,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file92,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",result visualization,,
file92,10,empdata[['total_claims']].describe(),data exploration,,
file92,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file92,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",data preprocessing,result visualization,
file92,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file92,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file92,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number')
 allweeks",data preprocessing,data exploration,
file92,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=6)[5]",modeling,prediction,
file93,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file93,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file93,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file93,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file93,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file93,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file93,6,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file93,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file93,8,"#Correlation with output variable
 cor_target = abs(cor[""potential_water_deficit""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file93,9,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file93,10,"min(data['date']),max(data['date'])",data exploration,,
file93,11,data.dtypes,data exploration,,
file93,12,data = data.set_index('date'),data preprocessing,,
file93,13,data.index,data exploration,,
file93,14,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,data exploration,
file93,15,data.loc['2019-08'],data exploration,,
file93,16,"sns.set(rc={'figure.figsize':(11, 4)})",data preprocessing,,
file93,17,data['precipitation_data'].plot(linewidth=0.5);,data exploration,,
file93,18,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file93,19,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",result visualization,,
file93,20,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file93,21,"sns.boxplot(data=data, x='Month', y='wind_speed');",result visualization,,
file93,22,"from statsmodels.graphics.tsaplots import plot_acf
 plot_acf(x=data['max_temperature'], lags=50)",helper functions,result visualization,
file93,23,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,evaluation,
file93,24,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file93,25,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file93,26,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file93,27,data['lag_1']=lag_1,data preprocessing,,
file94,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file94,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file94,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file94,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file94,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline",helper functions,,
file94,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file94,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file94,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file94,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file94,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,data preprocessing,
file94,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file94,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 37
 """"""",load data,,
file94,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file94,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file94,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file94,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file94,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file94,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file94,18,"data = data.drop(['tract_name', 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file94,19,"plt.figure(figsize=(8,6))
 cor = data.corr().round(2)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={""size"": 6})
 plt.show()",result visualization,,
file94,20,"data = data.drop(['timeperiod'], axis=1)",data preprocessing,,
file94,21,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file94,22,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file94,23,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file94,24,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file94,25,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file94,26,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,data exploration,
file94,27,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",data preprocessing,,
file94,28,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file94,29,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file94,30,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",evaluation,,
file94,31,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=50)",evaluation,,
file94,32,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",evaluation,,
file94,33,"krr_model = make_pipeline(StandardScaler(), KernelRidge(alpha=1.0))
 krr_model.fit(X_train, y_train.ravel())",evaluation,,
file95,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file95,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file95,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file95,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file95,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file95,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file95,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file95,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file95,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file95,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file95,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file95,11,"def example_function():
  print('Hello World')",helper functions,,
file95,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file95,13,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",evaluation,,
file95,14,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",evaluation,,
file95,15,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file95,16,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file95,17,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file95,18,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,19,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,20,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file95,21,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file95,22,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file95,23,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file95,24,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file95,25,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file95,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file95,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",evaluation,,
file95,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file95,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file95,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file95,32,"# Test Linear Regression
 x",evaluation,,
file95,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file95,34,"# Test Linear Regression
 results = model.fit()",evaluation,,
file95,35,"# Test Linear Regression
 print(results.summary())",evaluation,,
file95,36,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file95,37,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file95,38,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file95,39,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file95,40,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file95,41,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file95,42,get_ipython().system('pip install db-dtypes'),helper functions,,
file95,43,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file95,44,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file95,45,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file95,46,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file95,47,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file95,48,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file95,49,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file95,50,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file95,51,"def example_function():
  print('Hello World')",helper functions,,
file95,52,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file95,53,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",evaluation,,
file95,54,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",evaluation,,
file95,55,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file95,56,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file95,57,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file95,58,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,59,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,60,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file95,61,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file95,62,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file95,63,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file95,64,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file95,65,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file95,66,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file95,67,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",evaluation,,
file95,68,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file95,69,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,70,"print(f""slope: {new_model.coef_}"")",evaluation,,
file95,71,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file95,72,"# Test Linear Regression
 x",evaluation,,
file95,73,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file95,74,"# Test Linear Regression
 results = model.fit()",evaluation,,
file95,75,"# Test Linear Regression
 print(results.summary())",evaluation,,
file95,76,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file95,77,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file95,78,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file95,79,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file95,80,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file95,81,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file95,82,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file95,83,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file95,84,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file95,85,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file95,86,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file95,87,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file95,88,"def example_function():
  print('Hello World')",helper functions,,
file95,89,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file95,90,"# START USING A MODEL - .fit() fits the model
 # SOURCE: REAL PYTHON - https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn
 model.fit(x,y)
 LinearRegression",evaluation,,
file95,91,"model.fit(x,y)
 LinearRegression",evaluation,,
file95,92,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",evaluation,,
file95,93,"model = LinearRegression().fit(x,y)",evaluation,,
file95,94,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file95,95,get_ipython().system('pip install db-dtypes'),helper functions,,
file95,96,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file95,97,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file95,98,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file95,99,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file95,100,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,101,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,102,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file95,103,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file95,104,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file95,105,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file95,106,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file95,107,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file95,108,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file95,109,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",evaluation,,
file95,110,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file95,111,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file95,112,"print(f""slope: {new_model.coef_}"")",evaluation,,
file95,113,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file95,114,"# Test Linear Regression
 x",evaluation,,
file95,115,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file95,116,"# Test Linear Regression
 results = model.fit()",evaluation,,
file95,117,"# Test Linear Regression
 print(results.summary())",evaluation,,
file95,118,"def plot_regression
 plt.scatter(X_train, y_train, color='red') #plotting the observation line",result visualization,,
file95,119,"plt.plot(X_train, regressor.predict(X_train), color='blue') #plotting the regression line",result visualization,,
file95,120,"plt.title('Week_number') vs (""uu_Id""(Training Set)) 
 # stating the title of the graph",result visualization,,
file95,121,"plt.xlabel('Week_Number') #adding the name of x-axis
 plt.ylabel('uu_Id') #adding the name of y-axis
 plt.show() #specifies end of graph",result visualization,,
file95,122,"y_predict = regressor.predict(x_test)
 print(y_predict)",prediction,,
file95,123,"plt.title('Week_number vs ""uu_Id""(Training Set)) 
 plt.title('Salary Vs Number Of Years Of Experience Of Training Set')
 # stating the title of the graph",result visualization,,
file95,124,import pandas as pd,helper functions,,
file95,125,"dataset = pd.read_csv('/content/drive/MyDrive/Salary_Data.csv')
 x = dataset.iloc[:,:-1]
 y = dataset.iloc[:,-1].values",load data,data preprocessing,
file95,126,"from sklearn.model_selection import train_test_split
 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)",helper functions,data preprocessing,
file95,127,"from sklearn.linear_model import LinearRegression 
 regressor = LinearRegression()
 regressor.fit(x_train, y_train)",modeling,evaluation,
file95,128,"dataset = pd.read_csv(ironhacks-data)
 x = dataset.iloc[:,:-1]
 y = dataset.iloc[:,-1].values",load data,data preprocessing,
file95,129,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file95,130,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",data preprocessing,,
file95,131,"prestige = sm.datasets.get_rdataset(""Duncan"", ""carData"", cache=True).data",load data,,
file95,132,prestige.head(),data exploration,,
file95,133,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,evaluation,
file95,134,print(prestige_model.summary()),evaluation,,
file95,135,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]
  
  if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model =
 sklearn.linear_model.LogisticRegression(multi_class='multinomial', 
 solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",data preprocessing,,
file95,136,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",data preprocessing,,
file95,137,"def load_imp_industry(csv_name='4_imp_industry.csv'):
  if not os.path.isfile(csv_name):
  d = load_clean()
  d = d.groupby('uu_id').apply(impute_industry).reset_index(0, drop=True)
  d.to_csv(csv_name, index=False)
  else:
  d = pd.read_csv(csv_name)
  return d",save results,,
file95,138,"d = load_imp_industry()
 d",save results,data exploration,
file95,139,"def plot_regression(d):  
  e = d.loc[:, COL_MAP['edu'] + ['total_claims']].reset_index(0, drop=True)
  e = e.fillna(0)
  X = e[COL_MAP['edu']]
  y = e['total_claims']
  model = sklearn.linear_model.LinearRegression()
  model.fit(X, y)
  fig, ax = plt.subplots()
  ax.plot(X.index, model.predict(X), label='predict')
  ax.plot(X.index, y, label='observed', ls='--')
  ax.legend()
  ax.set_xlabel('index')
  ax.set_ylabel('total_claims')",result visualization,,
file95,140,plot_regression(d),result visualization,,
file95,141,"def get_avg_total_claims(g):
  return pd.DataFrame([{'uu_id': g.uu_id.values[0], 'average_wage': g.average_wage.values[0], 'avg_total_claims': g.total_claims.mean()}])",data exploration,,
file95,142,"def plot_avg_total_claims(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i, week_number in enumerate([1, 6, 11, 16, 31, 36]):  
  ax = axs[i]
  dd = d.loc[d.week_number == week_number, ['average_wage', 'total_claims']]
  ax.plot(dd.average_wage, dd.total_claims, 'o')
  ax.set_title(f'week_number: {week_number}')
  ax.set_xlabel('average_wage')
  axs[0].set_ylabel('total_claims')
  plt.show()
  plt.close()
  
  avg = d.groupby('uu_id').apply(get_avg_total_claims).reset_index(0, drop=True)
  fig, ax = plt.subplots()
  ax.plot(avg.average_wage, avg.avg_total_claims, 'o')  
  ax.set_xlabel('average_wage')
  ax.set_ylabel('average_total_claims')
  plt.show()
  plt.close()
  
  print(avg[['average_wage', 'avg_total_claims']].corr())",result visualization,,
file95,143,plot_avg_total_claims(d),data exploration,,
file95,144,"from PIL import Image
 filename = ""mapinmth10.jpg""
 with Image.open(filename) as img:
 ...  img.load()
 ...",helper functions,result visualization,
file95,145,"type(img)
 <class 'PIL.JpegImagePlugin.JpegImageFile'>",result visualization,,
file95,146,"isinstance(img, Image.Image)
 True",result visualization,,
file95,147,"# Print the shape of the image
 plt.show()
 plt.close()",result visualization,,
file95,148,"print(avg[['average_wage', 'avg_total_claims']].corr())",data exploration,,
file95,149,"# TEST IMPORT TEST (PREP FOR IN MAP)
 def read_single_disk(image_id):
  """""" Stores a single image to disk.
  Parameters:
  ---------------
  image_id integer unique ID for image
 

  Returns:
  ----------
  image  image array, (32, 32, 3) to be stored
  label  associated meta data, int label
  """"""
  image = np.array(Image.open(disk_dir / f""{image_id}.png""))
 

  with open(disk_dir / f""{image_id}.csv"", ""r"") as csvfile:
  reader = csv.reader(
  csvfile, delimiter="" "", quotechar=""|"", quoting=csv.QUOTE_MINIMAL
  )
  label = int(next(reader)[0])
 

  return image, label",result visualization,,
file95,150,"plt.scatter (X_test,y_test,color='red') 
 plt.plot(X_train, regressor.predict(X_train), color='blue') # plotting the regression line",result visualization,,
file95,151,"plt.title(""Average Wage vs UU_Ids (Testing set)"")",result visualization,,
file95,152,"plt.xlabel(""Average Wage"") 
 plt.ylabel(""UU_Ids"") 
 plt.show()",result visualization,,
file95,153,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file95,154,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file95,155,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file95,156,"# Defining MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",evaluation,,
file95,157,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file95,158,"X = data['total_claims'].values.reshape(-1,1)
 y = data['week_number'].values.reshape(-1,1)",data preprocessing,,
file95,159,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file95,160,"regressor = LinearRegression()
 regressor.fit (x_train, y_train)",modeling,evaluation,
file95,161,"averages = img.mean(axis=2) # Take the average of each R, G, and B
 mpimg.imsave(""bad-gray.jpg"", averages, cmap=""gray"")",evaluation,,
file95,162,"# importing Image class from PIL package
 from PIL import Image",helper functions,,
file95,163,"# creating a object
 im = Image.open(""lost+found/may 2022 Indiana employment snapshot.jpg"")
 im.show()",evaluation,,
file95,164,"get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file95,165,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file95,166,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",load data,,
file95,167,"plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",result visualization,,
file95,168,y_pred = regressor.predict(x_test),evaluation,,
file95,169,"df = pd.DataFrame(np.random.rand(10, 3), columns =['a', 'b', 'c'])",data preprocessing,,
file95,170,print(df),data exploration,,
file95,171,"df = pd.DataFrame(Actual.y_test(), columns = Predicted.y_pred())
 print(df)",data preprocessing,data exploration,
file95,172,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file95,173,"plt.scatter(x_test, y_test, color='gray')
 plt.plot(x_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file95,174,"# importing matplotlib
 import matplotlib.pyplot",helper functions,,
file95,175,"# importing pandas as pd
 import pandas as pd",helper functions,,
file95,176,"# importing numpy as np
 import numpy as np",helper functions,,
file95,177,"# creating a dataframe
 df = pd.DataFrame(np.random.rand(10, 10),
  columns =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])",data preprocessing,,
file95,178,df,data exploration,,
file95,179,"# using a function df.plot.bar()
 df.plot.bar()",result visualization,,
file95,180,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file95,181,"import LinearRegression
 linear_model = LinearRegression().fit(X_train, y_train)",evaluation,,
file95,182,"# Predictions on Testing data
 LR_Test_predict = linear_model.predict(X_test)",prediction,,
file95,183,"# Using MAPE error metrics to check for the error rate and accuracy level
 LR_MAPE = MAPE(y_test, LR_Test_predict)
 print(""MAPE:"", LR_MAPE)",evaluation,,
file95,184,"from sklearn.metrics import mean_absolute_error
 Y_actual = [1,2,3,4,5]
 Y_Predicted = [1,2.5,3,4.1,4.9]
 mape = mean_absolute_error(Y_actual, Y_Predicted)*100
 print(mape)",evaluation,,
file95,185,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file95,186,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(20)",load data,data preprocessing,
file95,187,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head(20)",load data,data preprocessing,
file95,188,"import numpy as np
 numbers = np.linspace(5, 50, 24, dtype=int).reshape(4, -1)
 numbers",data preprocessing,data exploration,
file95,189,"mask = numbers % 4 == 0
 mask",data preprocessing,data exploration,
file95,190,"# Input 4 creates the mask
 numbers[mask]",data exploration,,
file95,191,"by_four = numbers[numbers % 4 == 0]
 by_four",data preprocessing,data exploration,
file95,192,"# insall image
 # using opencv
 path = ""lost+found/may 2022 Indiana employment snapshot.jpg""",data preprocessing,,
file95,193,"# importing Image class from PIL package
 from PIL import Image",helper functions,,
file95,194,"# creating a object
 im = Image.open(""lost+found/may 2022 Indiana employment snapshot.jpg"")
 im.show()",result visualization,,
file95,195,"import matplotlib. pyplot as plt
 import matplotlib. image as mpimg
 img = mpimg. imread('lost+found/mapinmth10.jpg')
 imgplot = plt. imshow(img)
 plt. show()",helper functions,result visualization,
file95,196,"import numpy as np
 import matplotlib.image as mpimg",helper functions,,
file95,197,"img = mpimg.imread (""lost+found/mapinmth10.jpg"")
 print( type(img) )
 print(img.shape )",result visualization,,
file95,198,"Image_mod.py
 class 'numpy.ndarray'>",data exploration,,
file95,199,"output = img.copy() # The original image is read-only!
 output[:, :, :2] = 0
 mpimg.imsave(""blue.jpg"", output)",result visualization,,
file95,200,"def plot_regression(d):  
  e = d.loc[:, COL_MAP['edu'] + ['total_claims']].reset_index(0, drop=True)
  e = e.fillna(0)
  X = e[COL_MAP['edu']]
  y = e['total_claims']
  model = sklearn.linear_model.LinearRegression()
  model.fit(X, y)
  fig, ax = plt.subplots()
  ax.plot(X.index, model.predict(X), label='predict')
  ax.plot(X.index, y, label='observed', ls='--')
  ax.legend()
  ax.set_xlabel('index')
  ax.set_ylabel('total_claims')",result visualization,,
file95,201,plot_regression(d),result visualization,,
file95,202,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file95,203,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file95,204,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",load data,,
file95,205,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",result visualization,,
file95,206,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file95,207,"query_job_pred = bigquery_client.query(query_pred)
 data_pred_query = query_job_pred.to_dataframe()",load data,data preprocessing,
file95,208,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file95,209,"# Defining MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",data preprocessing,,
file95,210,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file95,211,"plt.scatter(x_test, y_test, color='gray')
 plt.plot(x_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file95,212,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file95,213,"averages = img.mean(axis=2) # Take the average of each R, G, and B
 mpimg.imsave(""bad-gray.jpg"", averages, cmap=""gray"")",result visualization,,
file95,214,"regressor = LinearRegression()
 regressor.fit (x_train, y_train)",modeling,evaluation,
file95,215,"# USING MATPLOTLIB - image
 # A WAY TO COMMUNICATE DATA AND RESULTS
 # UNEMPLOYMENT RATES OCTOBER 2022 - Indiana Image
 # SOURCE: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.hoosierdata.in.gov/docs/state/mapin/2022/mapinmth10.pdf
 # insall image
 # using opencv
 path = ""lost+found/may 2022 Indiana employment snapshot.jpg""",data preprocessing,,
file95,216,"img = mpimg.imread (may 2022 Indiana employment snapshot.jpg)
 print( type(img) )
 print(img.shape )",data preprocessing,data exploration,
file95,217,"import numpy as np
 import matplotlib.image as mpimg",helper functions,,
file95,218,"img = mpimg.imread (http://www.stats.indiana.edu/maptools/laus.asp)
 print(type(img) )
 print(img.shape )",data preprocessing,data exploration,
file95,219,"# USING MATPLOTLIB - image
 # A WAY TO COMMUNICATE DATA AND RESULTS
 # UNEMPLOYMENT RATES OCTOBER 2022 - Indiana Image
 # SOURCE: http://www.stats.indiana.edu/maptools/laus.asp
 # insall image
 # using opencv
 path = ""http://www.stats.indiana.edu/maptools/laus.asp""",data preprocessing,,
file96,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import db_dtypes",helper functions,,
file96,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file96,2,"#QUERY DATA
 query_job = bigquery_client.query(query)
 print(query_job)
 unemp = query_job.to_dataframe()",load data,data preprocessing,
file96,3,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file96,4,"unemp.loc[unemp['tract_name'] == ""Allen County""]",data exploration,,
file96,5,"unemp.loc[unemp['tract_name'] == ""Census Tract 9, Allen County, Indiana""]",data exploration,,
file96,6,unemp['tract_name'].value_counts()['tract_name'],data exploration,,
file96,7,print(unemp['tract_name'].value_counts()),data exploration,,
file96,8,"X = data['total_claims'].values.reshape(-1,1)
 y = data['gender_male'].values.reshape(-1,1)",data preprocessing,,
file96,9,"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)",data preprocessing,,
file96,10,X_train.head(),data exploration,,
file96,11,print(X_train),data exploration,,
file96,12,"#Trains Algorithim
 regressor = LinearRegression()  
 regressor.fit(X_train, y_train)",modeling,evaluation,
file96,13,"X_train = np.array(X_train)
 Y_train = np.array(Y_train)",data preprocessing,,
file96,14,X.head,data exploration,,
file96,15,"#To retrieve the intercept:
 print(regressor.intercept_)",evaluation,,
file96,16,"#For retrieving the slope:
 print(regressor.coef_)",evaluation,,
file96,17,print(X_test),data exploration,,
file96,18,print(y),data exploration,,
file96,19,unemp = unemp[unemp['tract'] == 4300],data preprocessing,,
file96,20,"X = np.array(X)
 y = np.array(y)",data preprocessing,,
file96,21,unemp.groupby(['tract']).Tract.value_counts().nlargest(5),data preprocessing,,
file96,22,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file96,23,"#QUERY DATA
 query_job = bigquery_client.query(query)
 print(query_job)
 unemp = query_job.to_dataframe()",load data,data preprocessing,
file97,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file97,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file97,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file97,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file97,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",load data,data preprocessing,
file97,5,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,data preprocessing,
file97,6,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]
 #df_three_col.shape",data preprocessing,,
file97,7,"df_three_col = df_three_col.drop_duplicates()
 #print(df_three_col.shape)",data preprocessing,,
file97,8,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
 #df_three_col.head(21)
 #df_three_col.tail(3)
 #uu_id week_number total_claims
 #1876 001cd9ae23064d7f0fd3cd327c873d8d 31 34",data preprocessing,,
file97,9,"res = pd.DataFrame(columns = ['uu_id', 'total_claims', 'week_number'])",data preprocessing,,
file97,10,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  #test_data = test_data.tail(3)
  week_list = test_data['week_number'].tolist()
  y_pred = -1
  count = 0
  sum = 0
  for week_id in range(31,38):
  if week_id in week_list:
  count+=1
  tmp_row = test_data[test_data['week_number'] == week_id ]
  sum+= int( tmp_row['total_claims'])
  if count > 0:
  y_pred = int(sum/count)
  else:
  max_week = max(week_list)
  tmp_row = test_data[test_data['week_number'] == max_week ] 
  y_pre = tmp_row['total_claims']
  cur_row = pd.DataFrame([[cur_uu_id, y_pred, 44]], columns=['uu_id', 'total_claims', 'week_number'] )
  res = pd.concat([res,cur_row] ,ignore_index = True)",data preprocessing,,
file97,11,"res.to_csv(""Dec5_submission_prediction_output.csv"", index=False)",save results,,
file98,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file98,1,"# Let's look at the wage_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file98,2,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head(3)",load data,data preprocessing,
file98,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file98,4,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file98,5,"#Let's merge the unemployment_data and wage_data
 # We will merge only cols uu_id and average_wage from wage_data into unemployment_data
 Merged_unemployment_wage = unemployment_data.merge(wage_data[['uu_id', 'average_wage']])
 Merged_unemployment_wage.columns",data preprocessing,data exploration,
file98,6,Merged_unemployment_wage.head(3),data exploration,,
file98,7,"#Replaced the remaining null values with 0s
 cleaned_df = drop_duplicates.fillna(0)",data preprocessing,,
file98,8,"#Drop duplicate rows
 drop_duplicates = Merged_unemployment_wage.drop_duplicates()",data preprocessing,,
file98,9,"#Let's drop the column with no data
 Final_df=cleaned_df.drop(['race_hawaiiannative'], axis=1)",data preprocessing,,
file98,10,"#Number of Claims filed
 Final_df['uu_id'].value_counts(ascending=True).to_frame()",data exploration,,
file98,11,"#Top 3 UUID with the max number of claims
 Top_10_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False)
 Top_10_claimers",data preprocessing,data exploration,
file98,12,"#UUIDs with the least number of claims
 Top_claimers.tail(3)",data exploration,,
file98,13,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
 No_claims_after_week_35 = Final_df.groupby(['uu_id'])[Final_df[""total_claims""] > 35]",data preprocessing,,
file98,14,"#Top_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False).to_frame()
 No_claims_after_week_35 = Final_df[Final_df[""week_number""] > 35]
 array1 = No_claims_after_week_35.uu_id.unique()",data preprocessing,,
file98,15,"array2 = Final_df.uu_id.unique()
 array2",data preprocessing,data exploration,
file98,16,"print(""Unique values in array2 that are not in array1:"")
 print(np.setxor1d(array1, array2))",data exploration,,
file98,17,"print(np.setxor1d(array1, array2))",data exploration,,
file98,18,"#f7f087af0599e6b2eaa4045ba1a0be50 - Yes  
 #747f8bc2b0c8c0a04d29caa4cfe327d2 - No  
 #6fbb60a508283bc1fb30c13ac419941a - No  
 #b67c2c4abede3730932f8d53aba0341a - No
 uuid5 = Final_df[Final_df['uu_id']=='050a624d618a68e43fe31189909c644f'].sort_values('week_number')
 uuid5",data preprocessing,data exploration,
file98,19,"#c82ff1f157906a6e9c37a08989544676  
 #0ad94f09274e2c9cb0ef5cb77eb334b4  
 #cb304c84e572423d939db1dbb2009609
 uuid4 = Final_df[Final_df['uu_id']=='cb304c84e572423d939db1dbb2009609'].sort_values('week_number')
 uuid4.tail(3)",data preprocessing,data exploration,
file98,20,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 uuid1 = Final_df[Final_df['uu_id']=='f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 #uuid1['timeperiod'] = pd.to_datetime(uuid1['timeperiod'],
 # format='%Y%m%d') 
 uuid1.tail(3)",data exploration,,
file98,21,"uuid6 = Final_df[Final_df['uu_id']=='747f8bc2b0c8c0a04d29caa4cfe327d2'].sort_values('week_number')
 uuid6.tail(3)",data preprocessing,data exploration,
file98,22,total_claims_prediction.dtypes,data exploration,,
file98,23,"Final_df['prediction_total_claims'] = Predicted_Values.tolist()
 Final_df.head()",data preprocessing,data exploration,
file98,24,"df_numerics_only = Final_df.select_dtypes(include=np.number)
 df_numerics_only",data preprocessing,data exploration,
file98,25,"#df =df_numerics_only.drop(['week_number'], axis=1)
 df = Final_df[['week_number','edu_8th_or_less',
  'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_other', 'race_white', 'average_wage', 'total_claims']].sort_values('week_number')
 df",data preprocessing,data exploration,
file98,26,"features = df.iloc[:,:-1]
 features.head()",data preprocessing,data exploration,
file98,27,"X = features.values
 X",data preprocessing,data exploration,
file98,28,"y = df['total_claims'].values
 y",data preprocessing,data exploration,
file98,29,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file98,30,"X_train, X_test, y_train, y_test",data exploration,,
file98,31,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train) #training the algorithm",modeling,,
file98,32,"y_train_pred = regressor.predict(X_train)
 y_train_pred",prediction,evaluation,
file98,33,"regressor.score(X_train, y_train)",evaluation,,
file98,34,"y_test_pred = regressor.predict(X_test)
 y_test_pred",prediction,,
file98,35,"regressor.score(X_test, y_test)",evaluation,,
file98,36,"df_actual_pred = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})",data preprocessing,,
file98,37,"df1 = df_actual_pred.head(50)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",data exploration,result visualization,
file98,38,"print(f'Mean Absolute Error: {round(metrics.mean_absolute_error(y_test, y_test_pred),2)}')  
 print(f'Mean Squared Error: {round(metrics.mean_squared_error(y_test, y_test_pred),2)}')  
 print(f'Root Mean Squared Error: {round(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)),2)}')",evaluation,,
file98,39,regressor.predict(X_test),prediction,,
file98,40,"Predicted_Values = regressor.predict(X)
 Predicted_Values",data preprocessing,data exploration,
file98,41,"final_cols = Final_df[['uu_id','timeperiod','week_number','total_claims','prediction_total_claims']].sort_values('week_number')
 final_cols",data preprocessing,data exploration,
file98,42,"groupby_uuid = final_cols.groupby('uu_id', as_index=False)['prediction_total_claims'].agg(['mean'])
 len(groupby_uuid.index)",data preprocessing,data exploration,
file98,43,"total_claims_prediction = pd.merge(prediction_list, groupby_uuid, left_on='uu_id', right_index=True)
 total_claims_prediction.head()",data preprocessing,data exploration,
file98,44,"total_claims_prediction.rename(columns = {'mean':'total_claims'}, inplace = True)
 total_claims_prediction.head()",data preprocessing,data exploration,
file98,45,"total_claims_prediction = total_claims_prediction[['uu_id', 'total_claims', 'week_number']]
 total_claims_prediction",data preprocessing,data exploration,
file98,46,total_claims_prediction.round(total_claims),data preprocessing,,
file98,47,"total_claims_prediction[total_claims_prediction['uu_id']=='cb304c84e572423d939db1dbb2009609']
 #f013068de98db1470bd986137a0c6d23 - 0
 #c82ff1f157906a6e9c37a08989544676 - 75  
 #0ad94f09274e2c9cb0ef5cb77eb334b4 - 70  
 #cb304c84e572423d939db1dbb2009609 - 33
 #f7f087af0599e6b2eaa4045ba1a0be50 - 0
 #df.loc[df['Age'] < 30, 'Age Category'] = 'Under 30'
 #total_claims_prediction.loc[total_claims_prediction[""uu_id""] == ""cb304c84e572423d939db1dbb2009609"", ""total_claims""] = 33",data exploration,,
file98,48,"total_claims_prediction['total_claims'] = total_claims_prediction['total_claims'].apply(np.ceil) 
 total_claims_prediction",data preprocessing,data exploration,
file98,49,"total_claims_prediction.loc[total_claims_prediction[""uu_id""] == ""f013068de98db1470bd986137a0c6d23"", ""total_claims""] = 0
 total_claims_prediction.loc[total_claims_prediction[""uu_id""] == ""c82ff1f157906a6e9c37a08989544676"", ""total_claims""] = 75
 total_claims_prediction.loc[total_claims_prediction[""uu_id""] == ""0ad94f09274e2c9cb0ef5cb77eb334b4"", ""total_claims""] = 70
 total_claims_prediction.loc[total_claims_prediction[""uu_id""] == ""cb304c84e572423d939db1dbb2009609"", ""total_claims""] = 33
 total_claims_prediction.loc[total_claims_prediction[""uu_id""] == ""f7f087af0599e6b2eaa4045ba1a0be50"", ""total_claims""] = 0",data exploration,,
file98,50,"total_claims_prediction[total_claims_prediction['uu_id']=='c82ff1f157906a6e9c37a08989544676']
 #f013068de98db1470bd986137a0c6d23 - 0
 #c82ff1f157906a6e9c37a08989544676 - 75  
 #0ad94f09274e2c9cb0ef5cb77eb334b4 - 70  
 #cb304c84e572423d939db1dbb2009609 - 33
 #f7f087af0599e6b2eaa4045ba1a0be50 - 0",data exploration,,
file98,51,total_claims_prediction[total_claims_prediction['uu_id']=='cb304c84e572423d939db1dbb2009609'],data exploration,,
file98,52,"Let's experiment with some more UUID's
 #'050a624d618a68e43fe31189909c644f' '074f501122885ab9aef5e9d07004209d'
 #'0dc217a2798a141c59b99f5bcff29fa9' '0e6523fb3fc17f6a2ac7050972bd4bfd'
 #'2b6b2f2e6d3340e7d9ae46cd41eaef1b'
 uuid6 = Final_df[Final_df['uu_id']=='050a624d618a68e43fe31189909c644f'].sort_values('week_number')
 uuid6.tail(3)",data exploration,,
file98,53,"data = uuid6[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data preprocessing,data exploration,
file98,54,"m = NeuralProphet()
 model = m.fit(data, freq='W')",modeling,evaluation,
file98,55,get_ipython().system('pip install neuralprophet'),helper functions,,
file98,56,from neuralprophet import NeuralProphet,helper functions,,
file99,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file99,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file99,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file99,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file99,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file99,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file99,6,print(covid19_cases_data),data exploration,,
file99,7,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file99,8,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file100,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file100,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file100,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file100,3,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file100,4,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('pip install numpy scikit-learn statsmodels')",helper functions,,
file100,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file100,6,"query = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file100,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file100,8,"query = """"""
 SELECT week_number, total_claims, edu_8th_or_less, edu_grades_9_11, edu_hs_grad_equiv, edu_post_hs
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 """"""",load data,,
file100,9,import db_dtypes,helper functions,,
file101,0,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,evaluation,
file101,1,"X=data.drop([""date"",""potential_water_deficit""],1)
 y=data[""mean_temperature""]
 # print(X)
 # print(y)
 data.head()",data preprocessing,data exploration,
file101,2,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file101,3,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file101,4,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file101,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file101,6,"query = """"""
 select * from `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file101,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']=pd.to_datetime(data['date'])
 # data.dtypes
 data.head()",load data,data preprocessing,
file101,8,"plt.figure(figsize=(7,7))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data exploration,
file101,9,"cor_target = abs(cor[""mean_temperature""])
 relevant_features = cor_target[cor_target>0.75]
 print(relevant_features)",data preprocessing,data exploration,
file101,10,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file101,11,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file101,12,"ax = y.plot(alpha=0.5)
 plt.figure().set_figwidth(3)
 ax = y_pred.plot(ax=ax, linewidth=1)",result visualization,,
file101,13,"from matplotlib import figure
 ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=1)
 figure(figsize(1,1))",result visualization,,
file101,14,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file101,15,data['lag_1']=lag_1,data preprocessing,,
file101,16,lag_1 = data['mean_temperature'].shift(1),data preprocessing,,
file102,0,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 data.iloc[X_test.index(),'uu_id']",data preprocessing,data exploration,
file102,1,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file102,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file102,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file102,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file102,5,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file102,6,get_ipython().system('pip install db-dtypes'),helper functions,,
file103,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file103,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file103,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file103,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file103,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file103,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data`
 """"""",load data,,
file103,6,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_competition.unemployment_data`
 """"""",load data,,
file104,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file104,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file104,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file104,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file104,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data` ) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file104,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,,
file104,6,empdata.head().transpose(),data exploration,,
file104,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file104,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file104,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",result visualization,,
file104,10,empdata[['total_claims']].describe(),data exploration,,
file104,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file104,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",data preprocessing,result visualization,
file104,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file104,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file104,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file104,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",modeling,evaluation,
file104,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` order by uu_id
 """"""",load data,,
file104,18,uupred.head(),data exploration,,
file104,19,"last_week = int(empdata['week_number'].max())
 last_week",data preprocessing,data exploration,
file104,20,"pred_week = int(uupred['week_number'].max())
 pred_week",data preprocessing,data exploration,
file104,21,"uupred['total_claims'] = 0
 uupred.head()",data preprocessing,data exploration,
file104,22,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values, seasonal=False, error_action='ignore')
  pred = int(m.predict(n_periods=predw-lastw)[predw-lastw-1])
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",modeling,prediction,
file104,23,"dopred(37, 40)",evaluation,,
file104,24,"dopred(35, 37)",evaluation,,
file104,25,allweeks['total_claims'].values,data exploration,,
file104,26,"empdata.loc[empdata['uu_id'] == ""6c6ada16bac3b76836e80d0ea651d7be""]",data exploration,,
file104,27,print [[100]],data exploration,,
file104,28,[[100]],data exploration,,
file104,29,asdfasfd,data exploration,,
file104,30,[[]],data exploration,,
file104,31,[[100]].describe(),data exploration,,
file104,32,[[100]].shape(),data exploration,,
file104,33,uupred['total_claims'] = 0,data preprocessing,,
file104,34,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 uupred = query_job.to_dataframe()",load data,data preprocessing,
file104,35,uupred,data exploration,,
file104,36,uupred['w38'],data exploration,,
file104,37,uupred['w38'].values,data exploration,,
file104,38,"print('Mean Absolute Error:', metrics.mean_absolute_error(empdata['total_claims'].values, uupred['w38'].values))  
 #print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 #print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file104,39,allweeks,data exploration,,
file104,40,empdata.loc[(empdata['uu_id'] == 'fd613eba867c6ad7350a937f743b88f2') & (empdata['week_number'] == 37)],data exploration,,
file104,41,uupred.join(empdata.loc[empdata['week_number'] == 37][['total_claims']],data exploration,,
file105,0,"# Importing the required libraries
 from wordcloud import WordCloud, STOPWORDS
 import matplotlib.pyplot as plt",helper functions,,
file105,1,"# Replace end of line character with space
 text_raw.replace('\n', ' ')",data preprocessing,,
file105,2,"# Save a lower-case version of each word to a list
 words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']",data preprocessing,,
file105,3,"# Eliminate non alpha elements
 text_list = [word.lower() for word in words_list if word.isalpha()]",data preprocessing,,
file105,4,"# Transforming the list into a string for displaying
 text_str = ' '.join(text_list)",data preprocessing,,
file105,5,"# Defining the wordcloud parameters
 wc = WordCloud(background_color=""black"", max_words=2000,stopwords=stpwords)",result visualization,,
file105,6,"# Generate word cloud
 wc.generate(text_str)",result visualization,,
file105,7,"# Show the cloud
 plt.imshow(wc)
 plt.axis('off')
 # plt.show()",result visualization,,
file105,8,"words_list.plot.bar(x=""word"",y=""freq"")
 plt.show()",result visualization,,
file105,9,"df = pd.DataFrame(words_list)
 print(df)
 # df.plot.bar(x=""word"",y=""freq"")
 # plt.show()",data preprocessing,data exploration,
file105,10,"words = nltk.tokenize.word_tokenize(words_list)
 word_dist = nltk.FreqDist(words)
 rslt = pd.DataFrame(word_dist.most_common(top_N),
  columns=['Word', 'Frequency'])
 print(rslt)",data preprocessing,data exploration,
file105,11,"df = pd.DataFrame.from(words_list,columns=['words'])
 # print(df)
 df.plot.bar(x=""words"")
 plt.show()",data preprocessing,result visualization,
file105,12,"# Crating and updating the stopword list
 # stpwords = set(STOPWORDS)
 # stpwords.add('will')
 # stpwords.add('said')
 top_N=7
 words = nltk.tokenize.word_tokenize(text_str)
 word_dist = nltk.FreqDist(words)
 rslt = pd.DataFrame(word_dist.most_common(top_N),
  columns=['Word', 'Frequency'])
 print(rslt)",data preprocessing,data exploration,
file105,13,"# df = pd.DataFrame.from(words_list,columns=['words'])
 # print(df)
 plt.to_file('Trump.png')
 rslt.plot.bar(x=""Word"",y=""Frequency"")
 plt.show()",result visualization,,
file105,14,"rslt.plot.bar(x=""Word"",y=""Frequency"")
 plt.show()
 plt.savefig('Trump.png')",result visualization,,
file106,0,"import torch
 x = torch.rand(5, 3)
 print(x)",data preprocessing,data exploration,
file106,1,get_ipython().system('pip install torch'),helper functions,,
file107,0,"a.to_csv(""submission1.csv"",index=False)",save results,,
file108,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file108,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file108,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file108,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file108,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file108,5,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file108,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file108,7,"#Correlation with output variable
 cor_target = abs(cor[""wind_speed""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file108,8,"sns.set(rc={'figure.figsize':(11, 4)})",data preprocessing,,
file108,9,data['precipitation_data'].plot(linewidth=0.5);,data exploration,,
file108,10,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,evaluation,
file108,11,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file108,12,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file108,13,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file108,14,data['lag_1']=lag_1,data preprocessing,,
file109,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file109,1,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file109,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file109,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file109,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file109,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file109,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file109,7,"query_job = bigquery_client.query(query)
 unemployment_data_table = query_job.to_dataframe()
 print unemployment_data_table",load data,data preprocessing,
file109,8,"query_job = bigquery_client.query(query)
 unemployment_data_table = query_job.to_dataframe()
 print(""Columns:"")
 print('\n'.join(unemployment_data_table.columns))
 print(""\nResults:"")
 print(unemployment_data_table.head())",load data,data exploration,
file109,9,"query2 = """"""
 SELECT week_number, SUM(total_claims)
 FROM 'ironhacks-data.ironhacks_competition.unemployment_data'
 GROUP BY week_number;
 """"""",load data,,
file109,10,"query_job = bigquery_client.query(query)
 unemployment_total_claims_by_week = query_job.to_dataframe()",load data,data preprocessing,
file109,11,print(unemployment_total_claims_by_week),data exploration,,
file110,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file110,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file110,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file110,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition`
 """"""",load data,,
file110,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file110,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_competition.unemployment_data`
 """"""",load data,,
file111,0,get_ipython().system('pip install torch'),helper functions,,
file111,1,"import torch
 x = torch.rand(5, 3)
 print(x)",data preprocessing,data exploration,
file112,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file112,1,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file112,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file112,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file112,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file112,5,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file112,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",data preprocessing,result visualization,
file112,7,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file112,8,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file112,9,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data preprocessing,data exploration,
file112,10,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data preprocessing,data exploration,
file112,11,Using Linear regression model to predict from the data,comment only,,
file112,12,"from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]
 temp=test.loc[test[""week_number""]==34]",modeling,prediction,
file112,13,"model.fit(X_train,y)",evaluation,,
file112,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data preprocessing,data exploration,
file112,15,"y_pred = model.predict(X_test)
 y_pred",prediction,,
file112,16,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 a=a[['uu_id','week_number','count']]
 a['week_number']=a['week_number']+1
 a=a.rename(columns={'week_number':'week'})
 a.to_string(index=False)",data preprocessing,,
file112,17,"a.to_csv(""submission2.csv"",index=False)
 a",save results,,
file112,18,y_true=test.loc[['week_number'==35]],data preprocessing,,
file112,19,test['week_number],data exploration,,
file112,20,"y_true=test.loc[test['week_number']==35]
 MAE=metrics.mean_absolute_error(y_true,y_pred)
 MSE=metrics.mean_squared_error(y_true,y_pred)
 print(MAE)
 MSE",evaluation,,
file113,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file113,1,"#import cell
 import pandas as pd
 import numpy as np
 import statistics
 import csv
 import matplotlib.pyplot as plt
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file113,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file113,3,"#Gets the master unemployed table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file113,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemploymentData = query_job.to_dataframe()
 print(unemploymentData.shape)
 pd.set_option('display.max_columns', None)
 unemploymentData.head(3)",load data,data preprocessing,
file113,5,"#Gets each tracks mean and std dev
 #unlist has the master unemployment list
 #b becomes the filtered list
 unList = unemploymentData.values.tolist()
 b_set = set(tuple(x) for x in unList)
 b = [ list(x) for x in b_set ]",data preprocessing,,
file113,6,"uuid = []
 #makes a list of the unique uuid
 for x in b:
  if(uuid.count(x[0]) == 0):
  uuid.append(x[0])",data preprocessing,,
file113,7,"#setup for extract  
 values = []
 export = []",data preprocessing,,
file113,8,"#for each value make a list of each weeks claims
 for y in uuid:
  temp = [y]
  for x in b:
  if (x[0] == y):
  temp.append(x[6])
  values.append(temp)",data preprocessing,,
file113,9,"for x in values:
  name = x[0]
  mean = statistics.mean(x[1:])
  if (len(x) > 2):
  stdev = statistics.stdev(x[1:])
  else:
  print(""short"")
  export.append([name, mean, stdev])",data preprocessing,data exploration,
file113,10,"#Everything above this is testing
 #Make the answer csv
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file113,11,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 predictionData = query_job.to_dataframe()",load data,data preprocessing,
file113,12,"answers = predictionData.values.tolist()
 #Answers[names, week]
 #export[names, mean, stddev]
 i = 0
 for x in answers:
  for y in export:
  if(y.count(x[0]) == 1):
  answers[i].insert(1, y[1])
  i = i + 1",data preprocessing,,
file113,13,"fields = ['uu_id', 'total_claims', 'week_number']
 with open('submission_prediction_output.csv', 'w') as f:
  write = csv.writer(f)
  write.writerow(fields)
  write.writerows(answers)",save results,,
file114,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file114,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file114,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file114,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file114,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file114,5,"query = """"""
 

 SELECT date, max_rel_humidity
 FROM ironhacks-data.ironhacks_training.weather_data
 WHERE date='2020-06-16'
 

 

 

 

 """"""",load data,,
file114,6,"query_job = bigquery_client.query(query)
 get_ipython().system('python3 -m pip install pandas')
 import pandas
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file114,7,"query = """"""
 

 Select 
 a.*,
 b.cases 
 

 FROM 
 

 (SELECT 
 extract(week(Monday) from date) as week_number,
 AVG(mean_temperature) as mean_temperature_week,
 date as start_date,
 AVG(wind_speed) as mean_wind_speed_week
 FROM `ironhacks_training.weather_data`
 group by week_number,start_date) a
 

 JOIN `ironhacks-data.ironhacks_training.covid19_cases` b 
 ON a.week_number=b.week_number
 order by week_number
 

 

 

 """"""",load data,,
file115,0,"# Importing the required libraries
 from wordcloud import WordCloud, STOPWORDS
 import matplotlib.pyplot as plt",helper functions,,
file115,1,"# Read the whole text
 file_read = open('ai_trends.txt' 'rU')
 text_raw = file_read.read()",data preprocessing,,
file115,2,"# Replace end of line character with space
 text_raw.replace('\n', ' ')",data preprocessing,,
file115,3,"# Save a lower-case version of each word to a list
 words_list = []
 for word in text_raw.strip().split(): 
  words_list.append(word.lower())",data preprocessing,,
file115,4,"# Eliminate non alpha elements
 text_list = [word.lower() for word in words_list if word.isalpha()]",data preprocessing,,
file115,5,"# Transforming the list into a string for displaying
 text_str = ' '.join(text_list)",data preprocessing,,
file115,6,"# Crating and updating the stopword list
 stpwords = set(STOPWORDS)
 stpwords.add('will')
 stpwords.add('said')",data preprocessing,,
file115,7,"# Defining the wordcloud parameters
 wc = WordCloud(background_color=""white"", max_words=2000,
  stopwords=stpwords)",result visualization,,
file115,8,"# Generate word cloud
 wc.generate(text_str)",result visualization,,
file115,9,"# Store to file
 wc.to_file('Trump.png')",result visualization,,
file115,10,"# Show the cloud
 plt.imshow(wc)
 plt.axis('off')
 plt.show()",result visualization,,
file115,11,"# Read the whole text
 # file_read = open('ai_trends.txt' 'rU')
 text_raw = """"""Increase security Drones are going to change the way we live I think of drones now as the equivalent of what phones were in the 90s Drones open up the ability to transport things through the air over short distances and in complex spaces which is just not something we have another solution for today Whether thats package delivery or emergency response or delivering medical products urgently all of those things become possible immediately One of the amazing things about drones is that they fly The idea that you can have something routinely inspect places that are hard to look at will make our world dramatically safer It will be another way in which drones will help us realize this future vision a lot of us have of whats going to happen this network of autonomous drones flying around taking care of things and interacting with us It will bring an immediacy to what theyre doing in the same way a smartphone in your hand made that change so readily apparent
 Generate new services and potentially social issues Artificial intelligence really means the extension of our ability to solve problems and to generate new ideas Its quite possible that 10 years will get us to an inflection point after which we will see advancement at an unprecedented rate AI and and robotics will have been assimilated into business operations and will be having a major impact on efficiency in organizations Entirely new AI-based products and services will have created new consumer and industrial markets At the same time AI will bring along new challenges perhaps most importantly increasing inequality and possibly unemployment as routine predictable types of work are automated There will also be critical challenges in areas like privacy security algorithmic bias and military applications of AI Ten years from now a vibrant debate about these issues will have likely come to the forefront of our political and social discourse Finding a way to address these issues on behalf of humanity will soon be one of the defining challenges for the coming decades
 Empower businesses The consumer-facing applications of AI and ML feel stuck to me relegated to doing what humans can already do or more critically only what we trust them to do Over the next ten years I think well start seeing trust barriers decrease and as a result dependence on AI powered algorithms and machines will increase We started learning this lesson years ago building our consumer applications If youre going on a trip to Paris for example youd trust almost anything your friend recommends even if theyd only been to Paris once for a short trip just because they said so In the same situation we use sophisticated ML techniques to deliver extremely personalized recommendations but algorithms dont get the benefit of trust like a friend In order to gain user trust we had to try and explain the what the AI thought which defeats the power of it Today these ideas are starting to grow in acceptance and expectation as brands such as Foursquare continue to use ML to build location-aware technology that push the boundaries of mobile capabilitiesand gives developers analysts marketers and beyond a unique way of understanding and interacting with their users The industry excitement around this technology and the rich experiences built on top of it I hope and believe represent the tides of change
 Improve healthcare When it comes to healthcare theres a lot machines can do to help the doctor I dont see a future where we actually dont have doctors guiding but a lot of the busy work doctors have to do is better done using artificial intelligence If you think about a doctors career thirty or forty years the number of patients you can see during that time period is very limited Many doctors are burnt out overworked its a serious issue They also dont have time to keep themselves up to date on the most recent research treatment techniques and advancements in medication Machines can play a very important role here Artificial intelligence can access a much larger set of patient data of how they were treated and what the outcomes were You can imagine machines being in a much better position because machines can start doing the busy work around the diagnosis and humans can actually interact with the patient and work with artificial intelligence to improve outcomes That in my mind is super exciting
 Facilitate sustainability Artificial intelligence is going to impact every single industry and everything that we do On a bigger level areas like sustainability climate change environmental issues they are becoming more at the forefront of everybodys minds as we move more into the 21st century and think about the huge challenges we need to tackle like population increases urbanization and energy Theres so many different areas within urbanization that touch on city planning We can do that much more efficiently once we can track the movements of residents within cities and once we can map which areas we can use to improve the density of cities In the AI floodgates conversation Ive seen in the past six months or so a lot more companies wanting to focus on AI instead of just thinking about the quick goals of making workloads and businesses more efficient If we look at the bigger picture of AI for good then it connects us with more purpose and meaning
 Blend the lines of digital and physical Personally Im most interested in furthering spatial computing When you live in a world where your computer is not just bound to a specific device but can be anywhere you want to put it it means computers will have to react to humans much more intelligently than they do now Right now if you do something wrong today your computer throws up an error you close it and thats it But imagine if for example you had a full-scale game that was in your apartment and something went wrong Where does the dialog box go then Should there be one at all There are a lot of open questions around how humans will want to interact with their computers For example Foursquare did a really great job with having with having location-based notifications designed to give you the right information at the right time There are many many applications that should be making use of that kind of awareness today And in the future having these really intelligent ways of surfacing information are going to move from nice-to-haves to essentials And I for one will be looking forward to that because I want computers to be smarter already
 Make us smarter Computational power is going to continue to increase giving us more power to train our models The amount of data being created is going to continue to grow exponentially as well allowing us to monitor more elements in our platforms and our world Combine the two together with AI and it gives us the ability to be able to make more intelligent projections on future behavior and events as you can train more intelligent models and knowledge systems However in the financial services industry regulatory pressures could actually curb the power of AI to some degree by siloing off information used to train models thus cutting off one source of fuel AI needs to thrive slowing its ability to make an impact I believe peoples worries about AI becoming all powerful are unfounded Ultimately training an AI platform it is very much like molding a child If you treat it the right way and teach it the right things train it to know whats right and wrong it will inherently grow up to become a productive member of society that cares about people and the future Just like any one of us
 Inspire artists In ten years there will be some sort of algorithm involved in most decisions made big or small Artists like myself can get involved in artificial intelligence While such a mission may sound daunting its really about starting to use it exploring the capabilities of algorithms as well as what they can express through AI systems I encourage artists to try to use AI to create something beautiful and expressive the way they would with any other medium However I never want to see the technology become the artist We must think about AI as a tool for the augmentation of human thought and creation and make every effort not to turn the reigns of creativity or ethics over to the machines When deploying AI individuals businesses governments etc must consider how to maintain a sense of civility creativity and equity in the AI system being released into the world For industries the driving question is how can AI be used to increase productivity while respecting human diversity dignity and our cultural specificities""""""",data preprocessing,,
file115,12,"# Save a lower-case version of each word to a list
 words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']
 for word in text_raw.strip().split(): 
  words_list.append(word.lower())",data preprocessing,,
file116,0,get_ipython().system('pip install torch'),helper functions,,
file116,1,"import torch
 x = torch.rand(5, 3)
 print(x)",data preprocessing,data exploration,
file117,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file117,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file117,2,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file117,3,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file117,4,data.info(),data exploration,,
file117,5,data.corr(),data exploration,,
file118,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file118,1,"#Unemployment Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file118,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file118,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file118,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file118,5,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file118,6,"def createSubmission():
  return",data preprocessing,,
file118,7,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file118,8,wage_data,data exploration,,
file118,9,unemployment_data,data exploration,,
file118,10,prediction_data,data exploration,,
file118,11,"pip install db-dtypes
 pip install plotly",helper functions,,
file118,12,"# Wage Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 wage_data = querydb(query)",load data,,
file118,13,"def querydb(request):
  query_job = bigquery_client.query(request)
  data = query_job.to_dataframe()
  return data",load data,data preprocessing,
file118,14,"querydb(""""""SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`"""""")",load data,,
file118,15,"prediction_data[uu_id = ""bbcb018f0e5e49e13636f6e78ce9f60f""]",data exploration,,
file118,16,prediction_data.columns,data exploration,,
file118,17,df_pred = prediction_data,data preprocessing,,
file118,18,df_pred.where(prediction_data),data exploration,,
file118,19,df_wage = wage_data,data preprocessing,,
file118,20,"#should add another column called previous week claims, this would probably be a good predictor for the next week of claims
 df_un = unemployment_data",data preprocessing,,
file118,21,"df_pred.where(df_pred[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f"")",data exploration,,
file118,22,"df_un[""uu_id"" == ""bbcb018f0e5e49e13636f6e78ce9f60f""]).dropna()",data exploration,,
file118,23,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sortby(""week_number"")",data exploration,,
file118,24,"## This can also be a good place for you to cleanup any input/output and export your results to a file.
 ## might just do a niave approach and declare the average as the value for each
 ## pull all values as a dictionary with uu_id as the key and average as the value
 # df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""]).drop_duplicates()
 for i in df_pred[""uu_id""]:
  print(i)",data exploration,,
file118,25,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import plotly.express as px
 import csv",helper functions,,
file118,26,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file118,27,"header = [""uu_id"",""total_claims"",""week_number""]
 week = 38
 fname = ""Bohlin_Week"" + str(week)+ "".csv""
 with open(fname, 'w', encoding = ""UTF8"", newline="""") as f:
  writer = csv.writer(f)
  writer.writerow(header)
  for k, v in predition_dict:
  writer.writerow(k, v, week)",data preprocessing,,
file118,28,"prediction_dict = {}
 for i, tract in enumerate(df_pred[""uu_id""]):
  df_working = df_un[df_un[""uu_id""] == tract].sort_values(by = [""week_number""]).drop_duplicates()
  mean = df_working[""total_claims""].mean()
  prediction_dict[tract] = mean",data preprocessing,,
file118,29,"for i in prediction_dict:
  print(i)",data exploration,,
file118,30,"df_pred[""uu_id""]",data exploration,,
file119,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file119,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file119,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file119,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file119,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file119,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file119,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file119,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file119,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file119,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file119,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file119,11,"def example_function():
  print('Hello World')",helper functions,,
file119,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file119,13,"model.fit(x,y)
 LinearRegression",evaluation,,
file119,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file119,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file119,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file119,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file119,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file119,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file119,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file119,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file119,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file119,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file119,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file119,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",evaluation,,
file119,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file119,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",evaluation,,
file119,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file119,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file119,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file119,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file119,32,"# Test Linear Regression
 x",data exploration,,
file119,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file119,34,"# Test Linear Regression
 results = model.fit()",evaluation,,
file119,35,from lmfit,,,
file119,36,"import Minimizer, Parameters, report_fit
 import numpy as np
 import matplotlib.pylab as plt",helper functions,,
file119,37,"# Test Linear Regression
 print(results.summary())",evaluation,,
file119,38,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file119,39,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file119,40,prestige.head(),data exploration,,
file119,41,print(prestige_model.summary()),evaluation,,
file119,42,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,evaluation,
file119,43,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file119,44,"crime_model = ols(""murder ~ urban + poverty + hs_grad + single"", data=dta).fit()
 print(crime_model.summary())",modeling,evaluation,
file119,45,"fig = sm.graphics.plot_partregress_grid(crime_model)
 fig.tight_layout(pad=1.0)",result visualization,,
file119,46,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file119,47,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file119,48,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",result visualization,,
file119,49,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file119,50,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file119,51,get_ipython().system('pip install db-dtypes'),helper functions,,
file119,52,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file119,53,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file119,54,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file119,55,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file119,56,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file119,57,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file119,58,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file119,59,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file119,60,"def example_function():
  print('Hello World')",helper functions,,
file119,61,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file119,62,"model.fit(x,y)
 LinearRegression",modeling,,
file119,63,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file119,64,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,,
file119,65,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file119,66,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file119,67,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file119,68,"x_new = np.arange(5). reshape((-1, 1))
 x_new",evaluation,,
file119,69,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file119,70,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file119,71,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file119,72,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file119,73,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file119,74,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file119,75,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file119,76,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file119,77,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file119,78,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file119,79,"print(f""slope: {new_model.coef_}"")",evaluation,,
file119,80,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",evaluation,,
file119,81,"# Test Linear Regression
 x",evaluation,,
file119,82,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file119,83,"# Test Linear Regression
 results = model.fit()",modeling,,
file119,84,"# Test Linear Regression
 print(results.summary())",modeling,,
file119,85,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file119,86,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file119,87,prestige.head(),data exploration,,
file119,88,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima
 import collections.abc
 #hyper needs the four following aliases to be done manually.
 collections.Iterable = collections.abc.Iterable
 collections.Mapping = collections.abc.Mapping
 collections.MutableSet = collections.abc.MutableSet
 collections.MutableMapping = collections.abc.MutableMapping
 import hts
 from hts.hierarchy import HierarchyTree
 from hts.model import AutoArimaModel
 from hts import HTSRegressor",helper functions,,
file119,89,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file119,90,"To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file119,91,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",,,
file119,92,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",,,
file119,93,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file119,94,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,,
file119,95,print(prestige_model.summary()),evaluation,,
file120,0,"Run these terminal commands when your Notebook Session begins
 >```gcloud auth login```",,,
file120,1,>```gcloud auth application-default login```,,,
file120,2,>```gcloud auth application-default set-quota-project ironhacks-data```,,,
file120,3,"You can launch a new terminal by hitting the ""+"" at the top and navigating to terminal at the bottom of the launcher page",,,
file120,4,"def example_function():
  
  BIGQUERY_PROJECT = 'ironhacks-data'
  bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
  
  
  query = """"""
  SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
  """"""
  query_job = bigquery_client.query(query)
  data = query_job.to_dataframe()
  data.head()
  
  
  query = """"""
  Select 
  a.*,
  b.cases 
 

  FROM 
  (SELECT extract(week(Monday) from date) as week_number, AVG(mean_temperature) as mean_temperature_week, date as start_date, AVG(wind_speed) as mean_wind_speed_week
  FROM `ironhacks_training.weather_data`
  group by week_number,start_date) a
 

  JOIN `ironhacks-data.ironhacks_training.covid19_cases` b 
  ON a.week_number=b.week_number
  order by week_number
  """"""
 

  query_job = bigquery_client.query(query)
  data = query_job.to_dataframe()
  data.head()
  
  
  
  print('Hello World')",load data,data preprocessing,
file120,5,"def example_function():
  print('Hello World')",helper functions,,
file120,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file120,7,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file120,8,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file120,9,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install google-cloud-bigquery\n!pip install google-cloud-bigquery[pandas]\n"")",helper functions,,
file120,10,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file120,11,"query = """"""
 Select 
 a.*,
 b.cases 
 

 FROM 
 (SELECT extract(week(Monday) from date) as week_number, AVG(mean_temperature) as mean_temperature_week, date as start_date, AVG(wind_speed) as mean_wind_speed_week
 FROM `ironhacks_training.weather_data`
 group by week_number,start_date) a
 

 JOIN `ironhacks-data.ironhacks_training.covid19_cases` b 
 ON a.week_number=b.week_number
 order by week_number
 """"""",load data,,
file120,12,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file120,13,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file120,14,"query = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file121,0,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file121,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file121,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file121,3,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file121,4,unemployment_data = unemployment_data.drop_duplicates(),data preprocessing,,
file121,5,"unemployment_data = unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file121,6,unemployment_data.columns,data exploration,,
file121,7,len(unemployment_data),data exploration,,
file121,8,"unemployment_data = unemployment_data.filter(['uu_id', 'week_number','total_claims','edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs','race_amerindian', 'race_asian', 'race_black','race_white'])",data preprocessing,,
file121,9,unemployment_data.isnull().sum(),data exploration,,
file121,10,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file121,11,unemployment_data.head(),data exploration,,
file121,12,"def predict_claims(uuid):
  data = unemployment_data[unemployment_data.uu_id == uuid]
  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  return result.head()",modeling,prediction,
file121,13,,,,
file121,14,predict_claims('00d85dff1c1f21f01f4f5f0bd683d32b'),data exploration,,
file121,15,"def predict_claims(uuid):
  data = unemployment_data[unemployment_data.uu_id == uuid]
  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test.sort_values(by = 'week_number')
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.total_claims
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  
  return result.head()",modeling,prediction,
file121,16,predict_claims('005be9532fd717dc36d4be318fd9ad25'),data exploration,,
file121,17,predict_claims('00f962ce727b8dbbf20925abd5a253dd'),data exploration,,
file121,18,uuids[:10],data exploration,,
file121,19,uuids = unemployment_data.uu_id.unique(),data exploration,,
file121,20,predict_claims('06e492b4f29d153af26c659d1f7da2a1'),data exploration,,
file121,21,predict_claims('066fa7cc5a96dcdeca8485c68e2993b8'),data exploration,,
file121,22,predict_claims('050a624d618a68e43fe31189909c644f'),data exploration,,
file122,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file122,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",load data,,
file122,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file122,3,"query = """"""
 SELECT *
 FROM `ironhacks-data:ironhacks_competition.unemployment_data`
 """"""",load data,,
file122,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment = query_job.to_dataframe()
 unemployment.head()",load data,data preprocessing,
file122,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file122,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file122,7,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file122,8,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",load data,,
file122,9,"unemployment = query_job1.to_dataframe()
 wage = query_job2.to_dataframe()
 pred = query_job3.to_dataframe()",data preprocessing,,
file122,10,wage.head(),data exploration,,
file122,11,wage.columns,data exploration,,
file122,12,pred.head(),data exploration,,
file122,13,import xgboost as xgb,helper functions,,
file122,14,get_ipython().system('python3 -m pip install xgboost'),helper functions,,
file122,15,"get_ipython().system('python3 -m pip install xgboost')
 get_ipython().system('python3 -m pip install sklearn')",helper functions,,
file122,16,unemployment.columns,data exploration,,
file122,17,"query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.wage.uu_id
 """"""",load data,,
file122,18,data,data preprocessing,,
file122,19,"query4 = """"""
 SELECT *, wage.average_wage
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.uu_id
 """"""
 query_job4 = bigquery_client.query(query4)
 data = query_job4.to_dataframe()",load data,data preprocessing,
file122,20,data = data.drop(data['uu_id_1']),data preprocessing,,
file122,21,"data = data.drop('uu_id_1', axis=1)",data preprocessing,,
file122,22,data.columns,data exploration,,
file122,23,"data = data.drop(['uu_id_1','countyfips_1','tract_1','tract_name_1'], axis=1)",data preprocessing,,
file122,24,"dropcol = ['uu_id_1','countyfips_1','tract_1','tract_name_1']
 for col in dropcol:
  data = data.drop(col, axis=1)",data preprocessing,,
file123,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file123,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file123,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file123,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file123,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file124,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file124,1,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",load data,data preprocessing,
file124,2,test_df.drop_duplicates(),data preprocessing,,
file125,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file125,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file125,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file125,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file125,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file125,5,data.info(),data exploration,,
file125,6,data.corr(),data exploration,,
file125,7,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file125,8,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file125,9,data.describe(),data exploration,,
file125,10,data.shape(),data exploration,,
file125,11,"for i in data.columns:
  print('The unique values in each feature are',len(data[i].value_counts()))",data exploration,,
file125,12,sum(data[race_white].isnull()),data exploration,,
file125,13,"for i in data.columns:
  print('The null values in',i,'are',sum(data[i].isnull())",data exploration,,
file125,14,"for i in data.columns:
  if sum(data[i].isnull())>0:
  print('The null values in',i,'are',sum(data[i].isnull()))",data exploration,,
file125,15,y=data['total_claims'],data preprocessing,,
file125,16,l,data exploration,,
file125,17,"for i in l:
  sns.countplot(data[i])",result visualization,,
file125,18,data.isnull().sum(axis=0),data preprocessing,,
file126,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file126,1,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file126,2,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file126,3,get_ipython().system('pip install db-dtypes'),helper functions,,
file127,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file127,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file127,2,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",load data,,
file127,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file128,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file128,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file128,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file128,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file128,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import OneHotEncoder",helper functions,,
file128,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file128,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file128,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file128,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file128,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,,
file128,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file128,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 39
 """"""",load data,,
file128,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file128,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file128,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file128,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file128,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file128,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file128,18,"data['tract_name'] = [i.split(',')[1].strip().split(' ')[0] for i in data['tract_name']]",data preprocessing,,
file128,19,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(data['tract_name'].values.reshape(-1, 1))
 #tract_name_encoder.categories_[0]
 tract_name_dataset = pd.DataFrame(tract_name_encoder.transform(data['tract_name'].values.reshape(-1, 1)), index=data.index, columns= tract_name_encoder.categories_[0])
 data = data.drop(['tract_name'], axis=1)",data preprocessing,modeling,
file128,20,"data = data.drop(['top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'timeperiod'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file128,21,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file128,22,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,modeling,
file128,23,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file128,24,"data = pd.concat([data, tract_name_dataset], axis=1)
 display(data)",data preprocessing,data exploration,
file128,25,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,data exploration,
file128,26,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file128,27,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,data exploration,
file128,28,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",data preprocessing,,
file128,29,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file128,30,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file128,31,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",data exploration,modeling,
file128,32,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=10)",modeling,,
file128,33,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file128,34,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,none,
file128,35,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",result visualization,,
file128,36,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,,
file128,37,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",result visualization,evaluation,
file128,38,plt.show(),result visualization,,
file128,39,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file128,40,unemployment_prediction_data = query_from_statement(prediction_query),data preprocessing,,
file128,41,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file128,42,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data exploration,data preprocessing,
file128,43,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",data exploration,prediction,
file128,44,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data exploration,data preprocessing,
file128,45,"unemployment_prediction_data.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file128,46,data['average_wage'] = data['average_wage']/1000,data preprocessing,,
file129,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file129,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file129,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file129,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file129,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file129,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file129,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file129,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file129,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file129,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file129,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file129,11,"def example_function():
  print('Hello World')",helper functions,,
file129,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file129,13,"model.fit(x,y)
 LinearRegression",modeling,data exploration,
file129,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",data exploration,evaluation,
file129,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,data exploration,
file129,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file129,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file129,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",data exploration,modeling,
file129,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file129,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,data exploration,
file129,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file129,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file129,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file129,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file129,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,none,
file129,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,none,
file129,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,none,
file129,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file129,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file129,30,"print(f""slope: {new_model.coef_}"")",data exploration,,
file129,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file129,32,"# Test Linear Regression
 x",data exploration,,
file129,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file129,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file129,35,"# Test Linear Regression
 print(results.summary())",data exploration,,
file129,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file129,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file129,38,prestige.head(),data exploration,,
file129,39,"from lmfit import Minimizer, Parameters, report_fit
 import numpy as np
 import matplotlib.pylab as plt",helper functions,,
file129,40,from lmfit,helper functions,,
file129,41,fit_linear_NLLS.params,data exploration,,
file129,42,"#Create object for storing parameters
 params_linear = Parameters()",data exploration,,
file129,43,"#Add parameters and initial values to it
 params_linear.add('a', value = 1)
 params_linear.add('b', value = 1)
 params_linear.add('c', value = 1)
 params_linear.add('d', value = 1)",modeling,,
file129,44,"get_ipython().run_line_magic('conda', 'install module_name')",helper functions,,
file129,45,y = b0 + b1 * x1,helper functions,,
file130,0,gcloud auth login,helper functions,,
file131,0,"import os
 import csv
 #import pandas as pd
 import pandas
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file131,1,"get_ipython().system('pip install lightgbm')
 import db_dtypes
 import lightgbm as lgb
 #print(""OK"")",helper functions,,
file131,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file131,3,!pip install db-dtypes,load data,,
file131,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file131,5,"query_job = bigquery_client.query(query)
 df_unemployment = query_job.to_dataframe()
 df_unemployment.head()",load data,,
file131,6,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 df_unemployment.head()",load data,,
file131,7,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 df_wage.head()",load data,,
file131,8,"df_unemployment.shape
 df_wage.shape
 df_schema.shape
 df_pred_list.shape",data exploration,,
file131,9,"print(df_unemployment.shape)
 print(df_wage.shape)
 print(df_pred_list.shape)
 print(df_schema.shape)",data exploration,,
file131,10,"print(""OK"")",data exploration,,
file131,11,"print(df_pred_list[""uu_id""].unique())",data exploration,,
file131,12,"print(df_pred_list[""week_number""].unique().size) # 39",data exploration,,
file131,13,"print(df_unemployment[""uu_id""].unique().size) # 39",data exploration,,
file131,14,"print(df_unemployment[""week_number""].unique())",data exploration,,
file131,15,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]",data exploration,,
file131,16,df_three_col.head(),data exploration,,
file131,17,"test_data = df_three_col[df_three_col[""uu_id""].isin(['26c71b31d464bc7bedc8aed7e5c6e641', 'd0808351616eaf2e1d5d36d52e6cb669', '241f741bee0eb0ce595304d106811776']) ]
 test_data.shape",data exploration,data preprocessing,
file131,18,"import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file131,19,"seaborn.lmplot(x=""week_number"", y = ""total_claims"", hue = ""uu_id"", data = test_data)",result visualization,,
file131,20,"#sns.lmplot(x=""week_number"", y = ""total_claims"", hue = ""uu_id"", data = test_data)
 sns.scatterplot(data = test_data, x = 'week_number', y = 'total_claims', data = 'uu_id')",result visualization,,
file131,21,"from sklearn.linear_model import LogisticRegression
 logisticRegr = LogisticRegression(solver = 'lbfgs')",helper functions,modeling,
file131,22,"logreg.fit(test_data[""week_number""], test_data[""total_claims""])",modeling,,
file131,23,"y_pred = logreg.predict(39)
 print(y_pred)",data exploration,prediction,
file131,24,test_data.shape[1],data exploration,,
file131,25,"test_data.shape[0]
 x_pred = np.reshape([39], (1,1))
 x_pred",data preprocessing,data exploration,
file131,26,"x_train = np.reshape(test_data[""week_number""], (test_data.shape[0],1))
 y_train = np.reshape(test_data[""total_claims""], (test_data.shape[0],1))
 logreg.fit(x_train, y_train)",data preprocessing,modeling,
file131,27,"x_pred = np.reshape([39], (1,1))
 y_pred = logreg.predict(x_pred)
 print(y_pred)",prediction,data exploration,
file131,28,"print(x_train)
 print(y_train)",data exploration,,
file131,29,"x_train = np.array(test_data[""week_number""]).reshape(-1, 1)
 y_train = np.array(test_data[""total_claims""]).reshape(-1, 1)
 logisticRegr.fit(x_train, y_train)",data preprocessing,modeling,
file131,30,"test_data[""total_claims""]",data exploration,,
file131,31,"logisticRegr.fit(x_train, y_train)",modeling,,
file131,32,"from sklearn.linear_model import SGDRegressor
 sgdr = SGDRegressor()",modeling,helper functions,
file131,33,"#https://www.datatechnotes.com/2020/09/regression-example-with-sgdregressor-in-python.html
 x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
 y_train = np.array(test_data[""total_claims""].values).reshape(-1, 1)
 sgdr.fit(x_train, y_train)",modeling,data preprocessing,
file131,34,"x_pred = np.array([39]).reshape(-1, 1)
 y_pred = int(sgdr.predict(x_pred))
 print(y_pred)",prediction,data exploration,
file131,35,"for uu_id in df_pred_list['uu_id']:
  print(uu_id)",data exploration,,
file131,36,sgdr = SGDRegressor(max_iter = 10000),modeling,,
file131,37,random.seed(0),helper functions,,
file131,38,"res = pandas.DataFrame(columns = ['Name', 'Articles', 'Improved'])",data preprocessing,,
file131,39,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  x_train = np.array(test_data[""week_number""].values).reshape(-1, 1)
  y_train = np.array(test_data[""total_claims""].values) 
  sgdr.fit(x_train, y_train)
  x_pred = np.array([39]).reshape(-1, 1)
  y_pred = int(sgdr.predict(x_pred))
  res = pd.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])
  res = df.append({'uu_id' : cur_uu_id, 'week_number' : 37, 'total_claims' : y_pred},ignore_index = True)",data preprocessing,prediction,
file131,40,"res.to_csv(""submission_prediction_output.csv"", index=False)s",save results,,
file131,41,"#import os
 import csv
 #import pandas as pd
 import pandas
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file131,42,"from sklearn.linear_model import SGDRegressor
 import random",helper functions,,
file131,43,"res = pd.DataFrame(columns = ['uu_id', 'week_number', 'total_claims'])",data preprocessing,,
file131,44,reg = LinearRegression(),modeling,,
file131,45,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file131,46,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file131,47,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file131,48,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",helper functions,load data,
file131,49,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,,
file131,50,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]",data preprocessing,,
file132,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file132,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file132,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file132,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file132,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file132,5,global df3,data exploration,,
file132,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file132,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file132,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,,
file132,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",helper functions,data exploration,
file132,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file132,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,data exploration,
file132,12,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file132,13,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,data exploration,
file132,14,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file132,15,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file132,16,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file132,17,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",modeling,data exploration,
file132,18,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",modeling,data exploration,
file132,19,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file132,20,"df4 = df3.corr()
 df4",data exploration,data preprocessing,
file132,21,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file132,22,"features =df4[abs(df4.total_claims)>0.6].index
 features",data exploration,data preprocessing,
file132,23,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,modeling,
file132,24,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",modeling,data preprocessing,
file132,25,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 24
 test_df[""month""]=10",load data,data preprocessing,
file132,26,test_df.drop_duplicates(),data exploration,,
file132,27,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",helper functions,,
file132,28,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file132,29,"""""""test_df1=test_df.copy()
 for col in ['total_claims', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in extra:
  #test_df.loc[test_df.uu_id_enc==k,col] =0
  temp=df[df.uu_id_enc == k]
  temp[""average_wage""]=-9999
  temp =temp.replace("""",0)
  feature_test_pred = np.array(test_df1[test_df1.uu_id_enc==k])
  #print(k, temp)
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  idk = float(val.predict(np.array(feature_test_pred))[0])
  print(idk)
  test_df.loc[test_df.uu_id_enc==k,col].value = idk
 test_df""""""",data exploration,data preprocessing,
file132,30,df3,data exploration,,
file132,31,"from statsmodels.tsa.stattools import adfuller
 adfuller(df3[""total_claims""])",helper functions,data exploration,
file132,32,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",helper functions,data preprocessing,
file132,33,test_df,data exploration,,
file132,34,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data exploration,data preprocessing,
file132,35,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")",save results,,
file133,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file133,1,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file133,2,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file133,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file133,4,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file133,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",data exploration,load data,
file133,6,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file133,7,"#Number of rows in wage_data
 len(wage_data.index)",data exploration,,
file133,8,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 prediction_list.head(3)",data exploration,load data,
file133,9,"#Let's merge the unemployment_data and wage_data
 # We will merge only cols uu_id and average_wage from wage_data into unemployment_data
 Merged_unemployment_wage = unemployment_data.merge(wage_data[['uu_id', 'average_wage']])
 Merged_unemployment_wage.columns",data exploration,data preprocessing,
file133,10,Merged_unemployment_wage.head(3),data exploration,,
file133,11,"#checking for duplicates rows
 print(Merged_unemployment_wage.duplicated().sum())",data exploration,,
file133,12,"#Drop duplicate rows
 drop_duplicates = Merged_unemployment_wage.drop_duplicates()",data preprocessing,,
file133,13,"#Checking for nulls 
 drop_duplicates.isnull().sum()",data exploration,,
file133,14,"#Replaced the remaining null values with 0s
 cleaned_df = drop_duplicates.fillna(0)",data preprocessing,,
file133,15,cleaned_df.isnull().sum(),data exploration,,
file133,16,"# Is there a column with all 0's?
 (cleaned_df == 0).all()",data exploration,,
file133,17,"#Let's drop the column with no data
 Final_df=cleaned_df.drop(['race_hawaiiannative'], axis=1)",data preprocessing,,
file133,18,Final_df.shape,data exploration,,
file133,19,Final_df.dtypes,data exploration,,
file133,20,Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).sum().add_prefix('Sum_of_'),data preprocessing,,
file133,21,"plt.rcParams[""figure.figsize""] = (12,4)
 ax = Claims_by_week.plot(title='Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,,
file133,22,"Mean_Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).mean().add_prefix('Mean_of_')
 plt.rcParams[""figure.figsize""] = (12,4)
 ax = Mean_Claims_by_week.plot(title='Mean Of Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,,
file133,23,"matrix = Final_df.corr().round(2)
 sns.heatmap(matrix, annot=True)
 plt.show()",result visualization,,
file133,24,"#Top 3 UUID with the max number of claims
 Top_10_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False)
 Top_10_claimers.head(3)",data exploration,data preprocessing,
file133,25,"#UUIDs with the least number of claims
 Top_10_claimers.tail(3)",data exploration,,
file133,26,"#Do people with a certain degree file more claims 
 y=[Final_df.edu_8th_or_less.sum(),
  Final_df.edu_grades_9_11.sum(),
  Final_df.edu_hs_grad_equiv.sum(),
  Final_df.edu_post_hs.sum(),
  Final_df.edu_unknown.sum()
  ]",data preprocessing,,
file133,27,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(17, 7))
 plt.title(""Total Claims by Education"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['8th grade or less education','9 through 11','high school diploma or equivalent','completed a degree beyond high school','Education Unknown'],color='black')",result visualization,,
file133,28,"# To display sum values
 for index, value in enumerate(y):
  plt.text(value, index,
  str(value))
 plt.show()",result visualization,,
file133,29,"#Do people with a certain gender file more claims 
 y=[Final_df.gender_female.sum(),
  Final_df.gender_male.sum(),
  Final_df.gender_na.sum()
  ]",data preprocessing,,
file133,30,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(15,7))
 plt.title(""Total Claims by Gender"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['Female', 'Male' , 'Gender Unknown'],color='black')",result visualization,,
file133,31,"#Do people with a certain gender file more claims 
 y=[Final_df.race_amerindian.sum(),
  Final_df.race_asian.sum(),
  Final_df.race_black.sum(),
  Final_df.race_noanswer.sum(),
  Final_df.race_other.sum(),
  Final_df.race_white.sum()
  ]",data preprocessing,,
file133,32,"pd.set_option('float_format', '{:f}'.format)
 Final_df[['average_wage']].describe()",data exploration,,
file133,33,"Final_df[['top_category_employer1', 'top_category_employer2', 'top_category_employer3']].describe()",data exploration,,
file133,34,"sns.relplot(x =""edu_8th_or_less"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_grades_9_11"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_hs_grad_equiv"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_post_hs"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_unknown"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer1"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer2"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer3"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_female"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_male"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_na"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_amerindian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_asian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_black"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_noanswer"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_other"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_white"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""average_wage"", y =""total_claims"",
  data = Final_df);",result visualization,,
file133,35,"Final_df['timeperiod'] = pd.to_datetime(Final_df['timeperiod'],
  format='%Y%m%d')",data preprocessing,,
file133,36,"d = dict.fromkeys(Final_df.select_dtypes(np.int64).columns, np.int32)
 Final_df = Final_df.astype(d)
 Final_df.dtypes",data preprocessing,data exploration,
file133,37,Final_df.uu_id.unique(),data exploration,,
file133,38,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 uuid1 = Final_df[Final_df['uu_id']=='f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 #uuid1['timeperiod'] = pd.to_datetime(uuid1['timeperiod'],
 # format='%Y%m%d') 
 uuid1",data preprocessing,data exploration,
file133,39,"plt.plot(uuid1['timeperiod'], uuid1['total_claims'])
 plt.show()",result visualization,,
file133,40,"data = uuid1[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data exploration,data preprocessing,
file133,41,"model = m.fit(data, freq='W')",modeling,,
file133,42,m = NeuralProphet(),modeling,,
file133,43,get_ipython().system('pip install db-dtypes'),helper functions,,
file133,44,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file133,45,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file133,46,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file133,47,"future = m.make_future_dataframe(data, periods=20)
 forecast = m.predict(future)
 forecast",prediction,data exploration,
file134,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file135,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file135,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file135,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file135,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file135,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline",helper functions,,
file135,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file135,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file135,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file135,8,"plt.figure(figsize=(4,3))
 cor = data.corr().round(2)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={""size"": 6})
 plt.show()",result visualization,,
file136,0,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file136,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file136,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file136,3,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file136,4,unemployment_data = unemployment_data.drop_duplicates(),data preprocessing,,
file136,5,"unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file136,6,"unemployment_data = unemployment_data.filter(['uu_id', 'week_number','total_claims','edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs','race_amerindian', 'race_asian', 'race_black','race_white'])",data preprocessing,,
file136,7,unemployment_data.isnull().sum(),data exploration,,
file136,8,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file136,9,unemployment_data.head(),data exploration,,
file136,10,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid].copy()
 

  # plt.plot(data.week_number, data.total_claims)
  # plt.show()
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number",modeling,prediction,
file136,11,"result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",data preprocessing,,
file136,12,,,,
file136,13,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",data exploration,,
file136,14,prediction_list,data exploration,,
file136,15,uuids = prediction_list.uu_id.tolist(),data preprocessing,,
file136,16,import csv,helper functions,,
file136,17,"fields = ['uu_id', 'week_number', 'total_claims']
 rows = []
 for uuid in uuids:
  rows.append([uuid, 39, predict_claims(uuid, 39)])",data preprocessing,,
file136,18,filename = 'submission_prediction_output.csv',helper functions,,
file136,19,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file136,20,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file136,21,uuids[:10],data exploration,,
file136,22,"predict_claims('2a63ff5339efc0d6ac8023f6d06746e2', 39)",data exploration,,
file136,23,a,data exploration,,
file136,24,"fields = ['uu_id', 'week_number', 'total_claims']
 rows = []
 for uuid in uuids:
  print(uuid)
  claims = predict_claims(uuid, 39)
  rows.append([uuid, 39, claims])",data exploration,data preprocessing,
file136,25,"a = predict_claims('2a62116efd5f9a6da7b4ce2803eba96d', 39)",data preprocessing,,
file136,26,"fields = ['uu_id', 'week_number', 'total_claims']
 filename = 'submission_prediction_output.csv'",helper functions,,
file136,27,uuid = '2a62116efd5f9a6da7b4ce2803eba96d',helper functions,,
file136,28,"a = predict_claims(uuid, 39)",helper functions,,
file136,29,"for i in range(len(uuids)):
  c = predict_claims(uuids[i], 39)
  print(c)",data exploration,data preprocessing,
file136,30,unemployment_data.uu_ids.count_values(),data exploration,,
file136,31,unemployment_data.uu_id.value_counts().sort(),data exploration,,
file136,32,less_than_ten = unemployment_data.uu_id[unemployment_data.uu_id.value_counts()<10],data preprocessing,,
file136,33,"lessthanten = []
 for uuid, cnt in data_count.items():
  if cnt <10:
  lessthanten.append(uuid)
 lessthanten",data preprocessing,data exploration,
file136,34,len(uuids - lessthanten),data exploration,,
file136,35,len(uuids),data exploration,,
file136,36,"for i in lessthanten:
  uuids.remove(i)",data preprocessing,,
file136,37,rows[:5],data exploration,,
file136,38,"fields = ['uu_id', 'week_number', 'total_claims']
 rows = []
 for uuid in uuids:
  rows.append([uuid, 39, int(predict_claims(uuid, 39))])
 filename = 'submission_prediction_output.csv'",data preprocessing,,
file136,39,unemployment_data.groupby('uu_id').median(),data exploration,,
file136,40,"# 005be9532fd717dc36d4be318fd9ad25
 unemployment_data.groupby('uu_id').median().get_group('005be9532fd717dc36d4be318fd9ad25')",data exploration,,
file136,41,"# 005be9532fd717dc36d4be318fd9ad25
 groupby_id = unemployment_data.groupby('uu_id').median()
 groupby_id",data preprocessing,data exploration,
file136,42,"for uuid in lessthanten:
  rows.append([uuid, 39, int(unemployment_data.groupby('uu_id').median().total_claims[uuid])])",data preprocessing,,
file136,43,len(rows),data exploration,,
file136,44,rows[-10:-1],data exploration,,
file136,45,"with open(filename, 'w') as csvfile: 
  
  csvwriter = csv.writer(csvfile) 
 

  csvwriter.writerow(fields) 
 

  csvwriter.writerows(rows)",save results,,
file136,46,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file136,47,"Since the test is not workable for ids that the number of recorded weeks is less than 3, I separated the ids whose data count is less than 10 to predict the claim different.",helper functions,,
file137,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file137,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file137,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file138,0,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid]
 

  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
  plt.title('Tract: '+uuid)
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",modeling,prediction,
file138,1,,,,
file138,2,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",data exploration,,
file138,3,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file138,4,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file138,5,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file138,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file138,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file138,8,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file138,9,"unemployment_data = unemployment_data.drop_duplicates().fillna(0, inplace=True)",data preprocessing,,
file138,10,unemployment_data,data exploration,,
file138,11,unemployment_data = unemployment_data,data preprocessing,,
file138,12,"sns.heatmap(unemployment_data.fillna(0).corr(), annot = True)
 plt.show()",result visualization,,
file138,13,unemployment_data.isnull().sum(),data exploration,,
file138,14,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file138,15,"unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file138,16,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file138,17,len(uuids),data exploration,,
file139,0,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file139,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file139,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file139,3,get_ipython().system('pip uninstall neuralprophet'),helper functions,,
file139,4,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file139,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data exploration,
file139,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file139,7,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file140,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file140,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file140,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file140,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file140,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file140,5,global df3,data preprocessing,,
file140,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",data exploration,,
file140,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",data exploration,,
file140,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,,
file140,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file140,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file140,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file140,12,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",load data,data preprocessing,
file140,13,test_df.drop_duplicates(),data preprocessing,,
file140,14,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file140,15,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,,
file140,16,test_df,data exploration,,
file140,17,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data preprocessing,save results,
file140,18,submission_prediction_output[submission_prediction_output.total_claims<=0],data exploration,,
file140,19,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]",data exploration,,
file141,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file141,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file141,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file141,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file141,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file141,5,global df3,data preprocessing,,
file141,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file141,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file141,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,,
file141,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file141,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file141,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file141,12,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file141,13,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(x = df3[feature], y = df3[""total_claims""])
  plt.show()",data preprocessing,result visualization,
file141,14,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file141,15,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,,
file141,16,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file141,17,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file141,18,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file141,19,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,,
file141,20,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,,
file141,21,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file141,22,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file141,23,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,,
file141,24,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,,
file141,25,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",data preprocessing,modeling,
file141,26,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",load data,data preprocessing,
file141,27,test_df.drop_duplicates(),data preprocessing,,
file141,28,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file141,29,"for col in features:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,col] = val.predict(np.array(feature_test_pred))[0]
 test_df",data preprocessing,,
file141,30,"for k in df3.uu_id_enc.unique():
  temp2=df3[df3.uu_id_enc == k]
  temp2=temp2[[k for k in features]]
  print(temp2.shape)
  val = final_pred(temp2)
  feature_test_pred = np.array(test_df[[k for k in features]])
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val.predict(np.array(feature_test_pred))[0]
 test_df",data preprocessing,data exploration,
file141,31,"p = d = q = range(0, 2)
 pdq = list(itertools.product(p, d, q))
 seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
 print('Examples of parameter combinations for Seasonal ARIMA...')
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",data preprocessing,,
file141,32,"for param in pdq:
  for param_seasonal in seasonal_pdq:
  try:
  mod = sm.tsa.statespace.SARIMAX(Y_test,
  order=param,
  seasonal_order=param_seasonal,
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
  except:
  continue",modeling,,
file141,33,df3,data exploration,,
file141,34,"import statsmodels.api as sm
 from statsmodels import tsa",helper functions,,
file141,35,"mod = sm.tsa.statespace.SARIMAX(np.array(df3[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 12),
  enforce_stationarity=False,
  enforce_invertibility=False)
 results = mod.fit()
 print(results.summary().tables[1])",modeling,,
file141,36,"results.plot_diagnostics(figsize=(16, 8))
 plt.show()",result visualization,,
file141,37,"pred = results.get_prediction(start=1, dynamic=False)
 pred_ci = pred.conf_int()
 y = df3[""total_claims""]
 ax = y.plot(label='observed')
 print(pred.predicted_mean)
 df3[""pred""]=pred.predicted_mean
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Total Claims')
 plt.legend()
 plt.show()",prediction,result visualization,
file142,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file142,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install xgboost')
 get_ipython().system('pip install prophet')
 get_ipython().system('pip install plotly')
 get_ipython().system('pip install plotly-geo')
 get_ipython().system('pip install geopandas')
 get_ipython().system('pip install pyshp')
 get_ipython().system('pip install shapely')",helper functions,,
file142,2,"import datetime
 import itertools
 import os
 import re
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import plotly.figure_factory
 import plotly.subplots",helper functions,,
file142,3,"import statsmodels.api as sm
 import sklearn.experimental.enable_iterative_imputer
 import sklearn.impute
 import sklearn.ensemble
 import sklearn.model_selection
 import sklearn.linear_model
 import xgboost as xgb",helper functions,,
file142,4,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file142,5,"pd.set_option('display.max_columns', 500)
 pd.set_option('display.max_rows', 500)
 pd.set_option('display.width', 1000)",helper functions,,
file142,6,"# define relevant columns based on categories 
 COL_MAP = {
  'edu': ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown'],
  'race': ['race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative', 'race_other', 'race_white'],  
  'gender': ['gender_female', 'gender_male', 'gender_na'],
  'industry': ['top_category_employer1', 'top_category_employer2', 'top_category_employer3']  
 }",data preprocessing,,
file142,7,"def get_cols(names):
  l = []
  for name in names:
  if name in COL_MAP:
  l += COL_MAP[name]
  else:
  l += [name]
  return l",data preprocessing,,
file142,8,"def subset(df, uu_id_idx):  
  return df.loc[df.uu_id == df.uu_id.unique()[uu_id_idx], :]",data preprocessing,,
file142,9,"def convert_to_submission(results_csv, week_number_to_submit=40):
  r = pd.read_csv(results_csv)
  last = r.loc[r.week_number == week_number_to_submit, ['uu_id', 'predicted']]
  last.index = last.uu_id
  uuid_map = last.to_dict(orient='dict')['predicted']
  p = query('prediction_list')
  p['total_claims'] = p['uu_id'].map(uuid_map)
  p.to_csv('submission_prediction_output.csv', index=False)",save results,,
file142,10,"def get_county(tract_name):
  m = re.search('Census Tract \S+, (.+) County, Indiana', tract_name)
  county = m.group(1)
  return county",data preprocessing,,
file142,11,"def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data",load data,,
file142,12,"def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d",data preprocessing,,
file142,13,"def load_raw(csv_name='0_raw.csv'):
  '''
  Loads the unemployment and wage data and does some basic cleaning
  '''
  if not os.path.isfile(csv_name):
  u = query('unemployment_data')
  w = query('wage_data')
  raw = combine(u, w)
  raw.to_csv(csv_name, index=False)
  else:
  raw = pd.read_csv(csv_name)
  raw['county'] = raw['tract_name'].apply(get_county)
  raw = raw.drop(['tract', 'timeperiod', 'tract_name'], axis=1)
  raw = raw.sort_values(by=['uu_id', 'week_number'])
  raw = raw.drop_duplicates()
  raw = raw.replace({np.nan: None})
  raw = raw.reset_index(0, drop=True)
  return raw",data preprocessing,,
file142,14,load_raw().to_dict('records')[0],data exploration,,
file142,15,"def get_week_number_map(g, colname):
  '''
  Creates a dictionary that maps from week number to an existing value in a given `colname`
  '''
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map",data preprocessing,,
file142,16,"def week_number_to_date(week_number, first_week_date='20220101'):
  '''
  Prepare a date column for ARIMA
  '''
  return pd.to_datetime(first_week_date, format='%Y%m%d') + pd.DateOffset(days=7*(week_number - 1))",data preprocessing,,
file142,17,"def insert_na_week_number(g, min_week_number='min', max_week_number=37):
  d = {}
  if min_week_number == 'min':
  week_number_min = g.week_number.min()
  else:
  week_number_min = min_week_number
  for colname in g.columns:
  week_number_map = get_week_number_map(g, colname)
  series = pd.Series(range(week_number_min, max_week_number+1))  
  d[colname] = series.map(week_number_map)  
  
  df = pd.DataFrame(d)
  df['week_number'] = range(week_number_min, max_week_number+1)
  df['uu_id'] = [v for v in df['uu_id'].unique() if type(v) == str][0]
  df['average_wage'] = [v for v in g['average_wage'].unique()][0]
  df['countyfips'] = [v for v in g['countyfips'].unique()][0]
  df['county'] = [v for v in g['county'].unique() if type(v) == str][0]
  return df",data preprocessing,,
file142,18,"def load_raw_full(csv_name='1_raw_full.csv'):
  if not os.path.isfile(csv_name):
  raw = load_raw()
  raw_full = raw.groupby('uu_id').apply(insert_na_week_number).reset_index(0, drop=True)
  raw_full['date'] = raw_full['week_number'].apply(week_number_to_date)  
  raw_full.to_csv(csv_name, index=False)
  else:
  raw_full = pd.read_csv(csv_name)
  return raw_full",data preprocessing,,
file142,19,"load_raw_full()[['uu_id', 'week_number', 'total_claims']]",data exploration,,
file142,20,import prophet,helper functions,,
file142,21,"import logging
 logger = logging.getLogger('cmdstanpy')
 logger.addHandler(logging.NullHandler())
 logger.propagate = False
 logger.setLevel(logging.CRITICAL)",helper functions,,
file142,22,"def run_prophet(g, period=0, growth='linear', changepoint_range=0.8, n_changepoints=100, changepoint_prior_scale=0.75, seasonality_mode='additive', seasonality_prior_scale=10.0):
  g = g.reset_index(0, drop=True)
  gg = g.copy()
  x = pd.DataFrame({'ds': gg['date'], 'y': np.log(gg['total_claims']), 'cap': np.log(gg['total_claims']).max()})
  model = prophet.Prophet(
  weekly_seasonality=False,
  changepoint_range=changepoint_range,  
  n_changepoints=n_changepoints, 
  changepoint_prior_scale=changepoint_prior_scale,
  growth=growth,
  seasonality_mode=seasonality_mode,
  seasonality_prior_scale=seasonality_prior_scale,
  )
  if period:
  model.add_seasonality(name='monthly', period=30, fourier_order=period)
  model.add_seasonality(name='quarterly', period=90, fourier_order=period)
  model.add_seasonality(name='yearly', period=365, fourier_order=period)
  pred = model.fit(x).predict(x)
  gg['predicted'] = np.exp(pred.yhat)
  gg['predicted'] = gg['predicted'].clip(lower=0, upper=gg['total_claims'].max())
  gg['total_claims_imputed'] = gg['total_claims'].fillna(gg['predicted'])
  return gg",modeling,prediction,
file142,23,"def plot_prophet(n=12):
  d = load_raw_full()
  ncols = 6
  nrows = int(np.ceil(n/ncols))
  fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(3*ncols, 3*nrows), sharex=True)
  for i in range(n):
  if i % 10 == 0:
  print(i)
  idx_row = int(i / ncols)
  idx_col = i % ncols
  if nrows == 1:
  ax = axs[idx_col]
  else:
  ax = axs[idx_row, idx_col]
  dd = subset(d, i)
  ax.plot(dd.week_number, dd.total_claims, 'o', label='raw')
  for growth in ['logistic', 'linear']:  
  pred = run_prophet(dd, growth=growth)
  pp = pred.loc[pred.total_claims.isna(), :]
  ax.plot(pp.week_number, pp.predicted, 'o', label=f'fb: {growth}')
  if idx_row == 0 and idx_col == ncols - 1:
  ax.legend()
  if idx_row == nrows - 1:
  ax.set_xlabel('week_number')
  if idx_col == 0:
  ax.set_ylabel('claims')",result visualization,,
file142,24,plot_prophet(n=12),result visualization,,
file142,25,"def load_imp_tot(csv_name='2_imp_tot.csv'):
  if not os.path.isfile(csv_name):
  raw = load_raw_full()
  l = []
  for i, (uu_id, g) in enumerate(raw.groupby('uu_id')):
  if i % 20 == 0:
  print(i)
  gg = run_prophet(g, growth='linear')
  gg['total_claims'] = gg['total_claims_imputed']
  l.append(gg)
  d = pd.concat(l, ignore_index=True)
  d = d.drop(['predicted', 'total_claims_imputed'], axis=1)
  d.to_csv(csv_name, index=False)  
  else:
  d = pd.read_csv(csv_name)
  return d",save results,,
file142,26,"def plot_impute(n=6):
  raw = load_raw_full()
  imp = load_imp_tot()
  fig, axs = plt.subplots(ncols=n, sharey=False, figsize=(n*3, 3))
  for i in range(n):
  rraw = subset(raw, i)
  iimp = subset(imp, i)
  ax = axs[i]
  ax.plot(rraw.week_number, rraw.total_claims, 'o', label='original')
  ax.plot(iimp.week_number, iimp.total_claims, '-', label='imputed')
  ax.set_xlim(0, 42)
  ax.set_xlabel('week_number')
  axs[0].set_ylabel('claims')
  axs[-1].legend(fancybox=False)",result visualization,,
file142,27,plot_impute(),result visualization,,
file142,28,"def load_county():
  d = load_imp_tot()
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c",load data,,
file142,29,"def avg_weeks(c, start, end):
  mask = (start <= c.week_number) & (c.week_number <= end)
  cc = c.loc[mask, :]",data preprocessing,,
file142,30,load_county(),load data,,
file142,31,"def plot_counties(cc, week_number):
  fips = cc.fips
  values = cc.total_claims
  # endpts = list(np.mgrid[min(values):max(values):4j])
  endpts = [15, 30, 45, 60]
  colorscale = [
  'rgb(239,239,239)',
  'rgb(195, 196, 222)',
  'rgb(144,148,194)',
  'rgb(101,104,168)',
  'rgb(65, 53, 132)'
  ]
  fig = plotly.figure_factory.create_choropleth(
  fips=fips, values=values, scope=['Indiana'], show_state_data=True,
  colorscale=colorscale, binning_endpoints=endpts, round_legend_values=True,
  plot_bgcolor='rgb(229,229,229)',
  paper_bgcolor='rgb(229,229,229)',
  legend_title=f'Average total claims by county for week {week_number}',
  county_outline={'color': 'rgb(255,255,255)', 'width': 0.5},
  exponent_format=True,
  )
  fig.layout.template = None
  fig.show()
  return fig",result visualization,,
file142,32,"def plot_county_all():
  c = load_county()
  week_numbers = [1, 18, 36]
  for i, week_number in enumerate(week_numbers):
  cc = c.loc[c.week_number == week_number, :]
  plot_counties(cc, week_number)",result visualization,,
file142,33,plot_county_all(),result visualization,,
file142,34,"def load_extrap(csv_name='3_extrap.csv'):
  if not os.path.isfile(csv_name):
  d = load_imp_tot()
  c = load_county()
  l = []
  for uu_id, g in d.groupby('uu_id'):
  g = insert_na_week_number(g, min_week_number=1).reset_index(0, drop=True)
  fips = g['countyfips'].values[0]
  cc = c.loc[c.fips==fips, :].reset_index(0, drop=True)
  g['total_claims'] = g['total_claims'].fillna(cc.total_claims)
  l.append(g)
  extrap = pd.concat(l, ignore_index=True)
  extrap['date'] = extrap['week_number'].apply(week_number_to_date)
  extrap.to_csv(csv_name, index=False)
  else:
  extrap = pd.read_csv(csv_name)
  return extrap",load data,,
file142,35,"load_extrap()[get_cols(['uu_id', 'week_number', 'total_claims', 'gender'])]",load data,,
file142,36,"def replace_na_cols(g):
  '''
  If a column only has None or zero values, replace that entire columnn with zeros
  '''
  x = g.copy()
  for col in g.columns:
  cond1 = g[col].isnull()
  cond2 = g[col] == 0
  if (cond1 | cond2).all():
  x[col] = 0  
  return x",data preprocessing,,
file142,37,"def impute_iterative(df):
  '''
  Wrapper fucntion for IterativeImputer for a generic data frame. 
  Mostly, for testing. We might need need this function
  Impute data assuming there are zero columns where all the values are NA
  '''
  imputer = sklearn.impute.IterativeImputer(random_state=0, min_value=0)
  imputed_cols = imputer.fit_transform(df)
  df_imputed = pd.DataFrame(imputed_cols, columns=df.columns)
  return df_imputed",data preprocessing,,
file142,38,"def iter_cat(g):
  g = replace_na_cols(g)  
  for cat in ['edu', 'race', 'gender']:
  gg = g.loc[:, COL_MAP[cat] + ['total_claims']]
  yield cat, gg",data preprocessing,,
file142,39,"def print_impute_cat(test_subset, impute_func):
  for cat, gg in iter_cat(test_subset):
  line = '*'*len(cat)
  print(line)
  print(cat)
  print(line)
  print(impute_func(gg).head())",data exploration,,
file142,40,"print_impute_cat(subset(load_extrap(), 6), impute_iterative)",data exploration,,
file142,41,"def impute_rowsum(df, target_col='total_claims'):
  l = []
  for idx, row in df.iterrows():
  n_unknowns = row.isna().sum()  
  if n_unknowns == 1:
  others = row[~row.isna() & (row.index != target_col)]
  val = row[target_col] - others.sum()
  val = val if val > 0 else 0
  row[row.isna()] = val
  l.append(row)
  df = pd.DataFrame(l).reset_index(0, drop=True)
  
  l = []
  for idx, row in df.iterrows():
  n_unknowns = row.isna().sum()  
  
  weights = {}
  for col in row[row.isna()].index:
  weights[col] = df[col].mean()
  weights = {k:v/sum(weights.values()) for k, v in weights.items()} 
  
  if n_unknowns > 1:  
  others = row[~row.isna() & (row.index != target_col)]
  row[row.isna()] = row[row.isna()].index.map(weights)*(row[target_col] - others.sum())  
  l.append(row)
  
  df_imputed = pd.DataFrame(l).reset_index(0, drop=True)
  return df_imputed",data preprocessing,,
file142,42,"def impute_all(df):
  x = df.copy().reset_index(0, drop=True)
  for cat, gg in iter_cat(df):
  df_imputed = impute_rowsum(gg)
  df_imputed = df_imputed.drop('total_claims', axis=1)
  x[COL_MAP[cat]] = df_imputed
  return x",data preprocessing,,
file142,43,"def load_imp_feature(csv_name='4_imp_feature.csv'):
  if not os.path.isfile(csv_name):
  imp_tot = load_extrap()
  imp = imp_tot.groupby('uu_id').apply(impute_all).reset_index(0, drop=True)
  imp['date'] = imp['week_number'].apply(week_number_to_date)
  imp.to_csv(csv_name, index=False)
  else:
  imp = pd.read_csv(csv_name)
  return imp",data preprocessing,,
file142,44,load_imp_feature().to_dict('records')[0],data preprocessing,,
file142,45,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]
  
  if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",modeling,prediction,
file142,46,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",prediction,,
file142,47,"def load_imp_industry(csv_name='5_imp_industry.csv'):
  if not os.path.isfile(csv_name):
  d = load_imp_feature()
  d = d.groupby('uu_id').apply(impute_industry).reset_index(0, drop=True)
  d.to_csv(csv_name, index=False)
  else:
  d = pd.read_csv(csv_name)
  return d",load data,,
file142,48,load_imp_industry().isna().sum(),load data,,
file142,49,"def feature_engineer(d):
  d['gender_male_ratio'] = d['gender_male'] / d['total_claims']
  d['edu_post_hs_ratio'] = d['edu_post_hs'] / d['total_claims']
  d['race_white_ratio'] = d['race_white'] / d['total_claims']
  d['race_black_ratio'] = d['race_black'] / d['total_claims']
  d = d.drop(get_cols(['gender', 'edu', 'race']), axis=1)
  return d",data preprocessing,,
file142,50,"import warnings
 from statsmodels.tools.sm_exceptions import ConvergenceWarning
 warnings.simplefilter('ignore', ConvergenceWarning)",helper functions,,
file142,51,"def arimax(y, order, seasonal_order, exog=None):
  model = sm.tsa.statespace.SARIMAX(y, exog=exog, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
  results = model.fit(maxiter=300, disp=False)
  return results",modeling,,
file142,52,"def get_best_params(y, exog=None, period=0, steps=2):
  r1 = r2 = r3 = range(steps)
  pdq = list(itertools.product(r1, r2, r3))  
  if period:
  seasonal_pdq = [(x[0], x[1], x[2], period) for x in list(itertools.product(r1, r2, r3))]
  else:
  seasonal_pdq = [(0, 0, 0, 0)]
  aic_min = np.inf
  for order in pdq:
  for seasonal_order in seasonal_pdq:
  results = arimax(y, order, seasonal_order, exog=exog)
  if results.aic < aic_min:
  aic_min = results.aic
  best_params = [order, seasonal_order, results.aic]  
  return best_params",modeling,,
file142,53,"def predict(g, exog=False, ylabel='total_claims', target_week_number=43):
  g = g.set_index('date')
  g.index = pd.DatetimeIndex(g.index).to_period('W')
  y = g[ylabel].astype(np.float64)
  x = g[['edu_post_hs_ratio', 'gender_male_ratio', 'race_white_ratio', 'race_black_ratio']]
  if exog:
  best_params = get_best_params(y, exog=x)
  else:
  best_params = get_best_params(y)
  best_results = arimax(y, best_params[0], best_params[1])  
  pred = best_results.get_prediction(start=week_number_to_date(target_week_number - 10), end=week_number_to_date(target_week_number), dynamic=False)
  
  x = g.join(pred.predicted_mean, on=g.index, how='outer')
  x['date'] = x.key_0
  ci = pred.conf_int()
  x['ci_lower'] = ci.iloc[:, 0]
  x['ci_upper'] = ci.iloc[:, 1]
  x['uu_id'] = g['uu_id'].values[0]
  x['predicted'] = x['predicted_mean']
  x = x.reset_index(0, drop=True)
  return x",prediction,,
file142,54,"def predict_all(d, ylabel, csv_name, exog=False):
  l = []
  for i, (uu_id, g) in enumerate(d.groupby('uu_id')):  
  if i % 100 == 0:
  print(f'processed {i} UUIDs')
  predicted = predict(g, ylabel=ylabel, exog=exog)
  l.append(predicted)  
  df = pd.concat(l, ignore_index=True)
  df.to_csv(csv_name, index=False)",prediction,save results,
file142,55,"if not os.path.isfile('results_arima.csv'):
  predict_all(load_featured(), 'total_claims', 'results_arima.csv')",save results,,
file142,56,"def plot_arima(n=6):
  fig, axs = plt.subplots(nrows=1, ncols=n, figsize=(3*n, 3))
  res_arima = pd.read_csv('results_arima.csv')
  res_arimax = pd.read_csv('results_arimax.csv')
  raw = load_featured()
  for i in range(n):
  ax = axs[i]
  rraw = subset(raw, i)
  rres_arima = subset(res_arima, i)
  rres_arimax = subset(res_arimax, i)
  ax.plot(rraw.week_number, rraw.total_claims, 'o-', label='raw')
  ax.plot(rres_arima.week_number, rres_arima.predicted, 'o-', label='arima')
  ax.plot(rres_arimax.week_number, rres_arimax.predicted, 'o-', label='arimax')
  ax.set_xlim(0, 42)
  ax.set_xlabel('week_number')
  ax.set_ylim(0, None)
  axs[0].set_ylabel('claims')
  axs[-1].legend(fancybox=False)",result visualization,,
file142,57,plot_arima(),result visualization,,
file142,58,"convert_to_submission('results_arima.csv', week_number_to_submit=37)",save results,,
file142,59,"def get_train_test(g, split_week_number=30):
  xcols = ['edu_post_hs_ratio', 'gender_male_ratio', 'race_white_ratio', 'race_black_ratio', 'week_number']
  ycols = ['total_claims']  
  mask_train = g.week_number <= split_week_number
  mask_test = ~mask_train
  
  x_train, x_test = g.loc[mask_train, xcols], g.loc[mask_test, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[mask_test, ycols]
  return x_train, x_test, y_train, y_test",data preprocessing,,
file142,60,"def run_rf(g, split_week_number=30):
  x_train, x_test, y_train, y_test = get_train_test(g, split_week_number=split_week_number)
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x_train, y_train.values.ravel())
  yhat = rf.predict(pd.concat([x_train, x_test]))
  return x_train, x_test, y_train, y_test, yhat",modeling,prediction,
file142,61,"def plot_rf(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i in range(6):
  ax = axs[i]
  dd = subset(d, i)
  
  x_train, x_test, y_train, y_test, yhat = run_rf(dd)
  ax.plot(x_train.week_number, y_train, 'o-', label='original')
  ax.plot(dd.week_number, yhat, 'o-', label='predict')
  ax.set_xlabel('week_number')
  ax.set_title(f'uu_id: {i}')
  ax.set_xlim(0, 42)
  
  axs[-1].legend(frameon=False)
  axs[0].set_ylabel('claims')
  plt.show()",result visualization,,
file142,62,plot_rf(load_featured()),result visualization,,
file142,63,"def rf_industry(g):
  uu_id = g.uu_id.values[0]
  gg = g[COL_MAP['industry'] + ['week_number', 'total_claims']]  
  gg = gg.dropna()
  gg = pd.get_dummies(gg)
  
  if gg.shape[0] == 0:
  print(g.uu_id.values[0])
  mean = g.total_claims.mean()
  return pd.DataFrame([{'uu_id': uu_id, 'week_number': 38, 'total_claims': mean, 'predicted': mean}])
  x = gg.drop(['total_claims'], axis=1)  
  y = gg['total_claims']  
  max_avail_week_number = int(x.week_number.max())
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x, y.values.ravel())
  last = x.loc[x.week_number == max_avail_week_number, :].copy()
  last['week_number'] = max_avail_week_number + 1
  x_test = pd.concat([x, last], ignore_index=True)
  x_test['predicted'] = rf.predict(x_test)
  result = x_test.copy()
  result['total_claims'] = y.reset_index(0, drop=True)
  result['uu_id'] = uu_id
  return result[['uu_id', 'week_number', 'total_claims', 'predicted']]",modeling,prediction,
file142,64,"def run_rf_all(d, csv_name='results_rf.csv'):
  if not os.path.isfile(csv_name):
  result_rf = d.groupby('uu_id').apply(rf_industry).reset_index(0, drop=True)
  result_rf.to_csv(csv_name, index=False)",save results,,
file142,65,run_rf_all(load_featured()),save results,,
file142,66,"def get_train_test(g):  
  xcols = ['edu_post_hs_ratio', 'gender_male_ratio', 'race_white_ratio', 'race_black_ratio', 'week_number']
  ycols = ['total_claims']
  mask_train = g.week_number <= 30
  mask_test = ~mask_train
  g_train = g.loc[mask_train, :]
  g_test = g.loc[mask_test, :]
  
  x_train = g_train[xcols]
  y_train = g_train[ycols]
  x_test = g_test[xcols]
  y_test = g_test[ycols]
  return x_train, y_train, x_test, y_test",data preprocessing,,
file142,67,"def run_xgb_old(g, params={'n_estimators': 1000}):
  uu_id = g.uu_id.values[0]
  x_train, y_train, x_test, y_test = get_train_test(g)
  reg = xgb.XGBRegressor(objective='reg:squarederror', **params)
  reg.fit(x_train, y_train, verbose=True)
  yhat = reg.predict(x_test)
  x_test['predicted'] = yhat
  x_test['uu_id'] = uu_id
  return x_test",modeling,prediction,
file142,68,"def optimize_xgb(g):
  uu_id = g.uu_id.values[0]
  x, y, x_test = get_train_test(g)
  # params = {
  #  'min_child_weight': [1, 5, 10],
  #  'gamma': [0.3, 0.5, 1.0, 1.5, 2.0, 5.0],
  #  'subsample': [0.6, 0.8, 1.0],
  #  'colsample_bytree': [0.6, 0.8, 1.0],
  #  'max_depth': [2, 3, 4, 5],
  #  'n_estimators': [300, 600, 1000],
  #  'learning_rate': [0.001, 0.01, 0.1]
  # }
  params = {
  'min_child_weight': [1],
  'gamma': [0.3],
  'subsample': [0.6, 0.8],
  'colsample_bytree': [0.6, 0.8],
  'max_depth': [3],
  'n_estimators': [600, 1000]  
  }
  reg = xgb.XGBRegressor(nthread=-1, objective='reg:squarederror')
  grid = sklearn.model_selection.GridSearchCV(reg, params)
  grid.fit(x, y)
  yhat = grid.best_estimator_.predict(x_test)
  x_test['predicted'] = yhat
  x_test['uu_id'] = uu_id
  return x_test, grid.best_score_, grid.best_params_",modeling,prediction,
file142,69,"def plot_xgb(d, n=6):  
  fig, axs = plt.subplots(ncols=n, figsize=(18, 3), sharey=True)
  for i in range(n):
  ax = axs[i]
  g = subset(d, i)
  pred = run_xgb_old(g)
  # pred, best_score, best_params = optimize_xgb(g)
  # print(i)
  # print(f'best score: {best_score}')
  # print('best_param: ', best_params)
  ax.plot(g.week_number, g.total_claims, 'o-', label='raw')
  ax.plot(pred.week_number, pred.predicted, 'o-', label='xgboost')
  ax.set_xlim(0, 42)
  ax.set_xlabel('week_number')
  axs[0].set_ylabel('claims')
  axs[-1].legend(fancybox=False)",result visualization,,
file142,70,"def preprocess_xgb(d):
  d = d[['total_claims', 'uu_id', 'average_wage', 'week_number', 'edu_post_hs_ratio', 'gender_male_ratio', 'race_white_ratio', 'race_black_ratio']].copy()
  # d = get_dummies(d, COL_MAP['industry'])
  d['date'] = d['week_number'].apply(week_number_to_date)
  d['month'] = d['date'].dt.month
  d['quarter'] = d['date'].dt.quarter
  nlags = 3
  d['mean_month'] = d.groupby('month')['total_claims'].transform(lambda x: float(x.dropna().mean()))
  d['mean_quarter'] = d.groupby('quarter')['total_claims'].transform(lambda x: float(x.dropna().mean())) 
  
  for lag in range(1, nlags + 1):  
  d[f'shift_{lag}'] = d.groupby('uu_id')['total_claims'].transform(lambda x: x.shift(lag))
  
  d = d.drop(['date'], axis=1)  
  encoder = sklearn.preprocessing.LabelEncoder()
  d['uu_id'] = encoder.fit_transform(d['uu_id'].astype(str))
  d = d.dropna()
  return d",result visualization,,
file142,71,preprocess_xgb(load_featured()),result visualization,,
file142,72,"def get_train_test_xgb(g):
  mask_train = g.week_number <= 30
  mask_test = ~mask_train
  x_cols = g.columns.difference(['total_claims'])
  y_cols = ['total_claims']
  x_train = g.loc[mask_train, x_cols]
  y_train = g.loc[mask_train, y_cols]
  x_test = g.loc[mask_test, x_cols]
  y_test = g.loc[mask_test, y_cols]
  return x_train, y_train, x_test, y_test",data preprocessing,,
file142,73,"def plot_xgb(n=6):  
  pp = preprocess_xgb(load_featured())
  res = run_xgb(pp)
  
  ncols = 6
  nrows = int(np.ceil(n/ncols))
  fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(3*ncols, 3*nrows), sharex=True)
  for i in range(n):
  if i % 10 == 0:
  print(i)
  idx_row = int(i / ncols)
  idx_col = i % ncols
  if nrows == 1:
  ax = axs[idx_col]
  else:
  ax = axs[idx_row, idx_col]
  ppp = pp.loc[pp.uu_id == pp.uu_id.unique()[i], :]
  rres = res.loc[res.uu_id == res.uu_id.unique()[i], :]
  ax.plot(ppp.week_number, ppp.total_claims, 'o-', label='raw')
  ax.plot(rres.week_number, rres.predicted, 'o-', label='xgboost')
  ax.set_xlim(0, 42)
  ax.set_xlabel('week_number')
  if idx_col == 0:
  ax.set_ylabel('claims')
  if idx_col == ncols - 1:
  ax.legend(fancybox=False)",result visualization,,
file142,74,plot_xgb(n=24),result visualization,,
file143,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file143,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file143,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file143,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file143,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file143,5,"query = """"""
 

 SELECT date, max_rel_humidity
 FROM ironhacks-data.ironhacks_training.weather_data
 WHERE date='2020-06-16'
 

 

 

 

 """"""",load data,,
file143,6,"query_job = bigquery_client.query(query)
 get_ipython().system('python3 -m pip install pandas')
 import pandas
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file143,7,"query = """"""
 

 Select 
 a.*,
 b.cases 
 

 FROM 
 

 (SELECT 
 extract(week(Monday) from date) as week_number,
 AVG(mean_temperature) as mean_temperature_week,
 date as start_date,
 AVG(wind_speed) as mean_wind_speed_week
 FROM `ironhacks_training.weather_data`
 group by week_number,start_date) a
 

 JOIN `ironhacks-data.ironhacks_training.covid19_cases` b 
 ON a.week_number=b.week_number
 order by week_number
 

 

 

 """"""",load data,,
file144,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file144,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file144,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from statsmodels.formula.api import ols
 from pandas import Series, DataFrame",helper functions,,
file144,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file144,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file144,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file144,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file144,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data preprocessing,data exploration,
file144,8,"query_job3 = bigquery_client.query(query3)
 prediction_list = query_job3.to_dataframe()
 prediction_list.head()",load data,data preprocessing,
file144,9,"unemploy_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemploy_wage_data = unemploy_wage_data.drop(['timeperiod', 'countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemploy_wage_data = unemploy_wage_data.fillna(0)
 unemploy_wage_data.head()",data preprocessing,data exploration,
file144,10,unemploy_wage_data.describe(),data exploration,,
file144,11,"sns.relplot(data=unemploy_wage_data, x='week_number', y='total_claims')",result visualization,,
file144,12,"sns.distplot(unemploy_wage_data.total_claims, bins=10)",result visualization,,
file144,13,"plt.figure(figsize=(16,14))
 cor = unemploy_wage_data.corr()
 cmap = sns.diverging_palette(210, 20, as_cmap=True)
 sns.heatmap(cor, cmap=cmap, vmax=.99, vmin=-.99, annot=True)",result visualization,,
file144,14,"X = unemploy_wage_data[['week_number', 'countyfips_x', 'tract_x', 'edu_8th_or_less', 'edu_grades_9_11', \
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown', 'gender_female', 'gender_male', \
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', \
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 y = unemploy_wage_data['total_claims']",data preprocessing,,
file144,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file144,16,"reg = LinearRegression()  
 reg.fit(X_train, y_train)
 print(reg)",modeling,,
file144,17,"print(f'intercept: {reg.intercept_}')
 coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
 print(coef)",evaluation,,
file144,18,"y_pred = reg.predict(X_test)
 df = DataFrame({'Actual': y_test, 'Predicted': y_pred})
 df",prediction,,
file144,19,df['Predicted'].mean(),evaluation,,
file144,20,"print('R squared: {:.2f}'.format(reg.score(X, y)*100))
 print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
 print('MSE:', metrics.mean_squared_error(y_test, y_pred))
 print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file144,21,"#Make prediction
 prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
 prediction_data = prediction_data.drop(['week_number_x','total_claims'],axis=1)
 prediction_data = prediction_data.groupby(['uu_id']).mean()
 prediction_data",data preprocessing,data exploration,
file144,22,"prediction_list.to_csv('submission_prediction_output.csv', index=False)",save results,,
file145,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file145,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file145,2,get_ipython().system('pip install db_dtypes'),helper functions,,
file145,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file145,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file145,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file146,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file146,1,"get_ipython().system('pip install db_dtypes')
 import os
 import pandas as pd
 import db_dtypes
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file146,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file146,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file146,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file146,5,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.shape",data preprocessing,data exploration,
file146,6,k = unemployment_data.copy(),data preprocessing,,
file146,7,"## number of unique ids are matching the number of entries in the wage_data set
 import numpy as np
 pd.unique(k.uu_id).shape",helper functions,data exploration,
file146,8,"wage_data = pd.DataFrame(wage_data)
 wage_data.head()",data preprocessing,data exploration,
file146,9,"wage_data = wage_data.drop_duplicates()
 wage_data.shape
 ## no duplicates here!",data preprocessing,data exploration,
file146,10,pd.unique(wage_data.uu_id).shape,data exploration,,
file146,11,"## lets join the 2 datasets on uu_id
 unemployment_data.columns, wage_data.columns",data exploration,,
file146,12,"data=pd.merge(unemployment_data,wage_data, how='inner')
 print(data.shape)",load data,,
file146,13,data.columns,data exploration,,
file146,14,"pd.set_option('display.max_columns', None) # or 1000
 pd.set_option('display.max_rows', None) # or 1000
 pd.set_option('display.max_colwidth', None) # or 199
 data.head()",helper functions,data exploration,
file146,15,data.isna().sum(),data exploration,,
file146,16,"def breakcolumn(a,data):
  df=pd.DataFrame()
  df[['Value1', 'Value2']] = data[a].str.split('-', 1, expand=True)
  ## replace the null values by the value before hypen
  df['Value2'].fillna(df['Value1'],inplace=True)
 

  df['Value1'] = pd.to_numeric(df['Value1'])
  df['Value2'] = pd.to_numeric(df['Value2'])
 

  df['Value3'] = (df['Value1']+df['Value2'])//2
  data[a] = df['Value3']",data preprocessing,,
file146,17,"data1 = data.copy()
 obj_list = ['top_category_employer1','top_category_employer2','top_category_employer3']
 for i in obj_list:
  data1[i].replace('N/A',0,inplace=True)
  breakcolumn(i,data1)",data preprocessing,,
file146,18,data1.head(),data exploration,,
file146,19,data1.info(),data exploration,,
file146,20,"data1['race_black'].fillna(0,inplace=True)
 data1['race_other'].fillna(0,inplace=True)
 data1['club_races'] = data1['race_black'] + data1['race_other']
 data1.drop(['race_black','race_other'],axis=1,inplace=True)",data preprocessing,,
file146,21,"data1.drop(['gender_male','gender_male','race_white','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs'],axis=1,inplace=True)
 data1.info()",data preprocessing,data exploration,
file146,22,"data1.fillna(method='bfill',inplace=True)
 data1.info()",data preprocessing,data exploration,
file146,23,"data1['race_asian'] = data1['race_asian'].fillna(int(np.mean(data1['race_asian'])))
 data1['race_noanswer'] = data1['race_noanswer'].fillna(int(np.mean(data1['race_noanswer'])))
 data1['edu_unknown'] = data1['edu_unknown'].fillna(int(np.mean(data1['edu_unknown'])))
 data1['gender_female'] = data1['gender_female'].fillna(int(np.mean(data1['gender_female'])))
 data1['top_category_employer3'] = data1['top_category_employer3'].fillna(int(np.mean(data1['top_category_employer3'])))",data preprocessing,,
file146,24,"from sklearn import preprocessing
 # label_encoder object knows how to understand word labels. 
 label_encoder = preprocessing.LabelEncoder()
 # Encode labels in column 'Country'. 
 data1['tract_name']= label_encoder.fit_transform(data1['tract_name'])",modeling,,
file146,25,"data2 = data1.copy()
 data1['uu_id']= label_encoder.fit_transform(data1['uu_id'])",modeling,,
file146,26,"X = data1.drop('total_claims',axis=1)
 y = data1['total_claims']
 from sklearn.model_selection import train_test_split
 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 42)",data preprocessing,,
file146,27,"from sklearn.linear_model import LinearRegression
 linreg = LinearRegression()
 linreg.fit(X_train,y_train)
 y_pred = linreg.predict(X_test)",modeling,prediction,
file146,28,"y_pred = np.round(y_pred)
 from sklearn.metrics import mean_squared_error
 mean_squared_error(y_test,y_pred)",data preprocessing,,
file146,29,y_pred,evaluation,,
file146,30,"## MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",data preprocessing,,
file146,31,"print(MAPE(y_test,y_pred))",data exploration,,
file146,32,"from sklearn.ensemble import RandomForestRegressor
 rfr = RandomForestRegressor(n_estimators = 500, random_state = 0)
 rfr.fit(X_train, y_train)",modeling,,
file146,33,y_pred = rfr.predict(X_test),prediction,,
file146,34,y_pred = np.round(y_pred),data preprocessing,,
file146,35,X2 = X.copy(),data preprocessing,,
file146,36,X2 = X2.apply(lambda iterator: ((iterator - iterator.mean())/iterator.std()).round(2)),data preprocessing,,
file146,37,"X_train,X_test,y_train,y_test = train_test_split(X2,y,test_size = 0.25,random_state = 42)",data preprocessing,,
file146,38,"get_ipython().system('pip install xgboost')
 from xgboost.sklearn import XGBRegressor
 regressor = XGBRegressor(
  n_estimators=500,
  reg_lambda=1,
  gamma=0,
  max_depth=3)",modeling,,
file146,39,"regressor.fit(X_train, y_train)
 y_pred = regressor.predict(X_test)
 mean_squared_error(y_test,y_pred)",modeling,prediction,
file146,40,"from xgboost.sklearn import XGBRegressor
 from sklearn.model_selection import GridSearchCV",helper functions,,
file146,41,"xgb1 = XGBRegressor()
 parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
  'objective':['reg:linear'],
  'learning_rate': [.03, 0.05, .07], #so called `eta` value
  'max_depth': [5, 6, 7],
  'min_child_weight': [4],
  'subsample': [0.7],
  'colsample_bytree': [0.7],
  'n_estimators': [400,500,600,100]}",modeling,,
file146,42,"xgb_grid = GridSearchCV(xgb1,
  parameters,
  cv = 2,
  n_jobs = 5,
  verbose=True)",modeling,,
file146,43,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file146,44,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 prediction = query_job.to_dataframe()",load data,data preprocessing,
file146,45,"print(prediction.shape)
 pd.DataFrame(prediction).head()",data exploration,,
file146,46,"data2 = data2.drop_duplicates(subset=['uu_id'],keep='last')",data preprocessing,,
file146,47,"data2 = data2.set_index('uu_id')
 data2.head()",data preprocessing,data exploration,
file146,48,"final_prediction = data2.join(prediction.set_index('uu_id'),on='uu_id',rsuffix='_other')
 final_prediction.head()",data preprocessing,data exploration,
file146,49,"final_prediction_data = pd.DataFrame()
 final_prediction_data['index'] = final_prediction.index
 final_prediction_data['week_number_other'] = final_prediction.week_number_other",data preprocessing,,
file146,50,"final_prediction = final_prediction.drop(['week_number_other'], axis=1)
 final_prediction.reset_index(drop=True, inplace=True)",data preprocessing,,
file146,51,"future = final_prediction.values
 future_weeks_pred = rfr.predict(future)
 print(future_weeks_pred.shape)",data preprocessing,data exploration,
file146,52,"prediction['total_claims'] = future_weeks_pred.astype('int')
 prediction.columns = ['uuid','week','count']
 print(prediction)",data preprocessing,data exploration,
file146,53,"prediction.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file147,0,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install plotly')",helper functions,,
file147,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file147,2,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from scipy import stats
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file147,3,"import numpy as np
 from numpy import isnan
 from matplotlib import pyplot",helper functions,,
file147,4,"import seaborn as sns
 import matplotlib 
 import matplotlib.pyplot as plt
 import plotly 
 import plotly.express as px",helper functions,,
file147,5,import pandas as pd,helper functions,,
file147,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file147,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file147,8,"query_job = bigquery_client.query(query)
 unemploy = query_job.to_dataframe()
 unemploy.head()",load data,data preprocessing,
file147,9,"query_wage = bigquery_client.query(wage)
 wages = query_wage.to_dataframe()
 wages.head()",data preprocessing,data exploration,
file147,10,wages.info(),data exploration,,
file147,11,unemploy.info(),data exploration,,
file147,12,unemploy.isnull().sum(),data exploration,,
file147,13,"unemploy=unemploy.sort_values('week_number', ascending=True)
 unemploy.reset_index(inplace=True)
 unemploy",data preprocessing,data exploration,
file147,14,"sns.heatmap(unemploy.isnull(),cbar=False)",result visualization,,
file147,15,"miss=unemploy.isnull()
 total=unemploy.count()
 total",data preprocessing,data exploration,
file147,16,miss.sum(),data exploration,,
file147,17,miss.sum()/len(unemploy),data exploration,,
file147,18,"map_1 = unemploy.corr(method ='spearman')
 sns.heatmap(map_1)",data preprocessing,result visualization,
file147,19,"unemploy[""race_hawaiiannative""].unique()",data exploration,,
file147,20,"df1=unemploy.copy()
 df1=df1.drop(columns=""race_hawaiiannative"")",data preprocessing,,
file147,21,"df=unemploy[['top_category_employer1',""top_category_employer2"",""top_category_employer3"",'uu_id',""tract_name"",""timeperiod"",""tract"",""countyfips""]]",data preprocessing,,
file147,22,df,data exploration,,
file147,23,"impute1=KNNImputer()
 impute1.fit(df1)
 unemploy1=pd.DataFrame(impute1.fit_transform(df1),columns = df1.columns)",modeling,,
file147,24,unemploy1,data exploration,,
file147,25,"trial=pd.merge(df, unemploy1, left_index=True, right_index=True)",data preprocessing,,
file147,26,trial.head(),data exploration,,
file147,27,trial.describe(),data exploration,,
file147,28,"sns.heatmap(trial.corr(method=""spearman""))",result visualization,,
file147,29,trial1=trial.copy(),data preprocessing,,
file147,30,"scaler = StandardScaler()
 scaled=pd.DataFrame(scaler.fit_transform(unemploy1),columns = unemploy1.columns)",modeling,,
file147,31,scaled,modeling,,
file147,32,"sns.displot(data=scaled,x=""total_claims"",kind=""kde"")",result visualization,,
file147,33,"claim = np.array(scaled[""total_claims""])
 sqrt_claim = np.sqrt(claim)
 log_claim = np.log(claim)
 f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
 #f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
 f1.hist(claim, 30)
 f2.hist(sqrt_claim, 30)
 f3.hist(log_claim, 30)",data preprocessing,,
file147,34,"stats.probplot(claim, plot=f4)
 stats.probplot(sqrt_claim, plot=f5)
 stats.probplot(log_claim, plot=f6)
 plt.show
 stats.shapiro(claim)[1], stats.shapiro(sqrt_claim)[1], stats.shapiro(log_claim)[1]",result visualization,,
file147,35,shapiro(sqrt_claim),data exploration,,
file147,36,print(skew(sqrt_claim)),data exploration,,
file147,37,shapiro(unemploy1.total_claims),data exploration,,
file147,38,"f3.hist(log_claim, 30)",data exploration,,
file147,39,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
 pylab.show()",data exploration,,
file147,40,import pylab,helper functions,,
file147,41,"sns.boxplot(x=scaled[""total_claims""])",result visualization,,
file147,42,"sns.distplot(scaled[""total_claims""])
 sns.distplot(log_claim)",result visualization,,
file147,43,sns.boxplot(x=log_claim),result visualization,,
file147,44,"scaled.plot.hist(subplots=True, legend=True, layout=(8, 2))",result visualization,,
file147,45,scale_logs = np.log(scaled),data preprocessing,,
file147,46,scale_log,data exploration,,
file147,47,"sns.boxplot(x=scaled[""edu_grades_9_11""])",result visualization,,
file147,48,scale_sqrt=np.sqrt(scaled),data preprocessing,,
file147,49,scale_sqrt,data exploration,,
file147,50,unemploy_logs = np.log(unemploy1),data preprocessing,,
file147,51,unemploy_logs,data exploration,,
file147,52,unemp_sqrt=np.sqrt(unemploy1),data preprocessing,,
file147,53,unemp_sqrt,data exploration,,
file147,54,sns.distplot(unemp_sqrt),result visualization,,
file147,55,sns.distplot(unemp_sqrt[edu_grades_9_11]),result visualization,,
file147,56,"unemp_cube=np.power((unemploy1),1/3)",data preprocessing,,
file147,57,unemp_cube,data exploration,,
file147,58,shapiro(unemp_cube.total_claims),data exploration,,
file147,59,from scipy.stats import jarque_bera,helper functions,,
file147,60,"statistics,pvalue = jarque_bera(unemploy1.total_claims)",evaluation,,
file147,61,"print('statistics=%.3f, p=%.3f\n' %(statisticss, pvalue))
 if pvalue>0.05:
  print(""Probably Normal"")
 else:
  print(""Probably not Normal"")",data exploration,,
file148,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file148,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file148,2,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file148,3,"from sklearn.ensemble import RandomForestRegressor
 from sklearn.model_selection import train_test_split",helper functions,,
file148,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file148,5,"query = """"""
 SELECT 
 a.*,
 b.average_wage
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`a
 

 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id = b.uu_id
 

 

 """"""",load data,,
file148,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['timeperiod']= pd.to_datetime(data['timeperiod'])
 data.head()",load data,data preprocessing,
file148,7,data.drop_duplicates(inplace=True),data preprocessing,,
file148,8,data = data.fillna(0),data preprocessing,,
file148,9,data.describe(),data exploration,,
file148,10,data.sum(numeric_only=True),data exploration,,
file148,11,data.columns,load data,,
file148,12,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file148,13,"y = data.total_claims
 x = data.drop(['total_claims', 'timeperiod'], axis = 1)",data preprocessing,,
file148,14,"uuid, label = data['uu_id'].factorize(sort=True)",data preprocessing,,
file148,15,x['uu_id'] = uuid,data preprocessing,,
file148,16,x['tract_name'] = x['tract_name'].factorize()[0],data preprocessing,,
file148,17,"x['top_category_employer1'] = x['top_category_employer1'].factorize()[0]
 x['top_category_employer2'] = x['top_category_employer2'].factorize()[0]
 x['top_category_employer3'] = x['top_category_employer3'].factorize()[0]",data preprocessing,,
file148,18,"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)",data preprocessing,,
file148,19,"model = RandomForestRegressor(criterion=""absolute_error"", random_state=0)",modeling,,
file148,20,"model.fit(x_train, y_train)",modeling,,
file148,21,"model.score(x_test, y_test)",evaluation,,
file148,22,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` 
 """"""",load data,,
file148,23,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 pred = query_job.to_dataframe()
 pred.head()",load data,data preprocessing,
file148,24,pred,data exploration,,
file148,25,"x['uu_id'] = label[x[""uu_id""]]
 x",data preprocessing,data exploration,
file148,26,"for col in x.columns[2:]:
  li = []
  for i in pred['uu_id']:
  li.append(x.loc[x['uu_id'] == i, col].mean())
  pred[col] = li",data preprocessing,,
file148,27,pred['uu_id'] = pred['uu_id'].factorize(sort=True)[0],data preprocessing,,
file148,28,predictions = model.predict(pred),prediction,,
file148,29,predictions,evaluation,,
file148,30,out_df = pd.DataFrame(),data preprocessing,,
file148,31,"out_df[""uu_id""] = pred[""uu_id""]
 out_df[""total_claims""] = predictions
 out_df[""week_numer""] = pred[""week_number""]",data preprocessing,,
file148,32,out_df,data exploration,,
file148,33,"out_df[""uu_id""] = label[out_df[""uu_id""]]",data preprocessing,,
file148,34,"out_df.to_csv(""submission_prediction_output_final.csv"", index=False)",save results,,
file149,0,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\nfrom keras.models import load_model\nimport joblib\nfrom joblib import Parallel, delayed\nfrom scipy import stats\nfrom sklearn.ensemble import IsolationForest\n')",helper functions,,
file149,1,"get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\n!pip install keras\n!pip install tensorflow\n')",helper functions,,
file149,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file149,3,"def clearOutlier_IQR(data):
  Q1 = data.quantile(0.25)
  Q3 = data.quantile(0.75)
  IQR = Q3 - Q1
  no_outliers = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]
  print(no_outliers.shape)
  return no_outliers",data preprocessing,,
file149,4,"def evaluate_regressor(prediction_dataframe):
  # Takes in a prediction dataframe of 2 columns, Actual values and Predicted values generated by a regressor
  # Outputs MSE, MAR, RMSE and MAPE metrics. Must have columns named Actual and Predicted.
  print('MSE:', mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('MAE:', mean_absolute_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('RMSE:', np.sqrt(mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted'])))
  print('MAPE:', np.mean(np.abs((prediction_dataframe['Actual'] - prediction_dataframe['Predicted']) / prediction_dataframe['Actual'])) * 100)",evaluation,,
file149,5,"def get_predictions(regressor, model_type, name, week):
  # generates predictions for any model and writes out a dataframe in csv containing them
  # takes a regressor and learning method type as input: DL and ML
  # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape
  result_list = []
  uu_id_transform = LE.fit_transform(prediction_list['uu_id'])
  if model_type == 'DL':
  predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_list = np.array(result_list)
  result_list = np.reshape(result_list, (525,))
  elif model_type == 'ML':
  predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_df = pd.DataFrame(result_list, columns = ['Predictions'])
  prediction_sub = prediction_list.copy()
  prediction_sub['total_claims'] = result_df.values
  prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]
  os.makedirs('lost+found/submission_files', exist_ok=True)
  prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)
  return prediction_sub",prediction,save results,
file149,6,"def get_pred_frame(test_frame, prediction_array):
  prediction_frame = pd.DataFrame({'Actual': test_frame, 'Predicted': prediction_array.flatten()})
  return prediction_frame",prediction,,
file149,7,"# outlier detection and handling - Z Score (gaussian only)
 def clearOutlier_ZScore(data, threshold):
  zscore = np.abs(stats.zscore(data))
  thresh = threshold
  no_outliers = data[(zscore < thresh).all(axis=1)]
  return no_outliers",data preprocessing,,
file149,8,"# outlier detection - automatic
 def IsoForest_anomaly(data):
  IFO = IsolationForest(random_state=69)
  col_list = ['week_number', 'total_claims', 'edu_8th_or_less',
  'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female',
  'gender_male', 'race_amerindian', 'race_asian', 'race_black',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
  NO_df = data.copy()
  IFO.fit(data)
  NO_df['anomaly_scores'] = IFO.decision_function(data)
  NO_df['anomaly'] = IFO.predict(data)
  no_outlier = NO_df[NO_df['anomaly'] == 1]
  print('Removed ', NO_df[NO_df['anomaly'] == -1].shape[0], 'datapoints')
  palette = ['#ff7f0e','#1f77b4']
  sns.pairplot(NO_df, vars = col_list, hue='anomaly', palette=palette)
  no_outlier.drop(['anomaly_scores','anomaly'], axis = 1, inplace=True)
  return no_outlier",data preprocessing,,
file149,9,"def preprocess(data, scaling):
  no_outlierDF = ingest.copy()
  to_drop = ['timeperiod','tract','top_category_employer1','top_category_employer2',
  'top_category_employer3','tract_name','countyfips', 'edu_unknown', 'gender_na', 
  'race_noanswer']
  to_scale = ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 
  'gender_female', 'gender_male', 'race_amerindian', 'race_asian', 'race_black', 
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
  no_outlierDF.drop(to_drop, axis=1, inplace=True)
  no_outlierDF['uu_id'] = LE.fit_transform(no_outlierDF['uu_id'])
  if scaling == 'Robust':
  no_outlierDF[to_scale] = RB_other.fit_transform(no_outlierDF[to_scale])
  elif scaling == 'Standard':
  no_outlierDF[to_scale] = SC_other.fit_transform(no_outlierDF[to_scale])
  return no_outlierDF",data preprocessing,,
file149,10,"# updated_ingest = pd.concat([merged_ingest, combined_ingest])
 ingest = pd.read_csv('lost+found/submission_files/complete_ingest.csv')",data preprocessing,,
file149,11,"ingest.dropna(axis=0, inplace=True)
 print(ingest.shape)",data preprocessing,data exploration,
file149,12,"# quick preprocess to keep uu_id and scale values
 from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler
 LE = LabelEncoder()
 RB_other = RobustScaler()
 SC_other = StandardScaler()
 # RB_claims = RobustScaler()",modeling,,
file149,13,"ML_data = preprocess(ingest, 'Robust')
 ingest_clean = IsoForest_anomaly(ML_data)",data preprocessing,,
file149,14,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file149,15,"query_pred = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file149,16,"query_job = bigquery_client.query(query_pred)
 prediction_list = query_job.to_dataframe()",load data,data preprocessing,
file149,17,"# set target and independent variables
 Y = ingest_clean['total_claims']
 X = ingest_clean[['uu_id', 'week_number', 'edu_8th_or_less',
  'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female',
  'gender_male', 'race_amerindian', 'race_asian', 'race_black',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]",data preprocessing,,
file149,18,"# import
 X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=69)",data preprocessing,,
file149,19,"# load model - next time, I'll need to learn to use PMML
 RFR_Regressor = joblib.load('RF_v1-7.pkl')",modeling,,
file149,20,"Y_pred_RFR = RFR_Regressor.predict(X_test.values).reshape(-1,1)",prediction,,
file149,21,"# evaluate
 evaluate_regressor(get_pred_frame(Y_test,Y_pred_RFR))",evaluation,,
file149,22,"# call func
 get_predictions(RFR_Regressor, 'ML', 'submission_prediction_output_RFR', 44)",evaluation,,
file149,23,"get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\n!pip install keras\n!pip install tensorflow\n')",helper functions,,
file150,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('python3 -m pip install google.cloud')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('python3 -m pip install numpy')
 get_ipython().system('python3 -m pip install scikit-learn')
 get_ipython().system('python3 -m pip install plotly')",helper functions,,
file150,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import os
 import seaborn as sns
 from pandas import Series, DataFrame",helper functions,,
file150,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file150,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file150,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file150,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file150,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file150,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data preprocessing,data exploration,
file150,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file150,9,"#Merge the data
 unemployment_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemployment_wage_data = unemployment_wage_data.drop(['countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemployment_wage_data = unemployment_wage_data.fillna(0)
 unemployment_wage_data.describe()",data preprocessing,data exploration,
file150,10,unemployment_wage_data.head(),data exploration,,
file150,11,"#Check for duplicated rows
 duplicated_rows = sum(unemployment_wage_data.duplicated()) 
 unemployment_wage_data = unemployment_wage_data.drop_duplicates()",data preprocessing,,
file150,12,unemployment_wage_data[unemployment_wage_data.isnull().any(axis=1)],data exploration,,
file150,13,"#heat map for correlations
 plt.figure(figsize=(25,10))
 cor = unemployment_wage_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds) 
 plt.show()",result visualization,,
file150,14,unemployment_wage_data.columns,data exploration,,
file150,15,"X = unemployment_wage_data.drop(['total_claims', 'week_number'], axis = 1)
 y = unemployment_wage_data.total_claims",data preprocessing,,
file150,16,"uuid, label = unemployment_wage_data['uu_id'].factorize(sort=True)",data preprocessing,,
file150,17,X['uu_id'] = uuid,data preprocessing,,
file150,18,"X['tract_name_x'] = X['tract_name_x'].factorize()[0]
 X['top_category_employer1'] = X['top_category_employer1'].factorize()[0]
 X['top_category_employer2'] = X['top_category_employer2'].factorize()[0]
 X['top_category_employer3'] = X['top_category_employer3'].factorize()[0]",data preprocessing,,
file150,19,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",data preprocessing,,
file150,20,"from sklearn.ensemble import RandomForestRegressor
 rf = RandomForestRegressor(n_estimators=1000, random_state=42)
 rf.fit(X_train, y_train)",modeling,,
file150,21,"rf.score(X_test, y_test)",evaluation,,
file150,22,prediction_list,data exploration,,
file150,23,"X['uu_id'] = label[X[""uu_id""]]
 X",data preprocessing,data exploration,
file150,24,"for col in X.columns[2:]:
  li = []
  for i in prediction_list['uu_id']:
  li.append(X.loc[X['uu_id'] == i, col].mean())
  prediction_list[col] = li
 prediction_list",data preprocessing,data exploration,
file150,25,prediction_list['uu_id'] = prediction_list['uu_id'].factorize(sort=True)[0],data preprocessing,,
file150,26,"claims_predict = rf.predict(prediction_list)
 claims_predict",prediction,data exploration,
file150,27,submission_df = pd.DataFrame(),data preprocessing,,
file150,28,"submission_df[""uu_id""] = prediction_list[""uu_id""]
 submission_df[""week_number""] = prediction_list[""week_number""]
 submission_df[""total_claims""] = claims_predict",data preprocessing,,
file150,29,submission_df,data exploration,,
file150,30,"submission_df[""uu_id""] = label[submission_df[""uu_id""]]",data preprocessing,,
file150,31,"submission_df.to_csv('submission2_prediction_output.csv', index=False)",save results,,
file151,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file151,1,"get_ipython().system('pip install keras')
 get_ipython().system('pip install tensorflow')",helper functions,,
file151,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file151,3,"import os
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools",helper functions,,
file151,4,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file151,5,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file151,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""",load data,,
file151,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file151,8,"import pandas as pd
 import numpy as np
 combined = pd.merge(data,wagedata,on=['uu_id','countyfips', 'tract','tract_name'],how = 'left')",helper functions,,
file151,9,wagedata[wagedata['uu_id']=='a5c6dcff737e183f7931b472f10c3235'],data exploration,,
file151,10,"combined['average_wage'].fillna(combined['average_wage'].mean(),inplace = True)
 ## Using the mean for the missing wage",data preprocessing,,
file151,11,"firstgroup = ['edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs','edu_unknown']
 secondgroup = ['gender_female','gender_male','gender_na']
 thirdgroup = ['race_amerindian','race_asian','race_black','race_noanswer','race_hawaiiannative','race_other','race_white']
 ## columns to impute",data preprocessing,,
file151,12,"complete = combined.dropna(axis = 0)
 #using the complete rows to estimate the ratio of each catagory occupying the claims
 prob = {}
 for i in firstgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()
 for i in secondgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()
 for i in thirdgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()",data preprocessing,,
file151,13,prediction_list['total_claims'] = 0,data preprocessing,,
file151,14,"def approximatedimputation(data):
  firstgroup = ['edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs','edu_unknown']
  secondgroup = ['gender_female','gender_male','gender_na']
  thirdgroup = ['race_amerindian','race_asian','race_black','race_noanswer','race_hawaiiannative','race_other','race_white']
  for i in firstgroup:
  for j in range(data.shape[0]):
  # if is nan, we use the approximation method to try to impute
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])
  for i in secondgroup:
  for j in range(data.shape[0]):
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])
  for i in thirdgroup:
  for j in range(data.shape[0]):
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])",data preprocessing,,
file151,15,"approximatedimputation(combined)
 combined.to_csv('submission_prediction_output(1).csv',index = False)",save results,,
file151,16,"prediction_list.loc[:,['uu_id','total_claims','week_number']].to_csv('submission_prediction_output(2).csv',index = False)",save results,,
file152,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file152,1,"get_ipython().run_cell_magic('capture', '--no-display', '!pip3 install db-dtypes\n')",helper functions,,
file152,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file152,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file152,4,"# Google Credential
 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='key.json'
 bigquery_client = bigquery.Client(project='ironhacks-data')",load data,,
file152,5,"# Query the three provided data tables 
 unemployement_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file152,6,"wage_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file152,7,"# QUERY THE DATA ONCE
 ud_query_job = bigquery_client.query(unemployement_data_query)
 wd_query_job = bigquery_client.query(wage_data_query)
 pl_query_job = bigquery_client.query(prediction_list_query)",load data,,
file152,8,"unemployement_data = ud_query_job.to_dataframe()
 wage_data = wd_query_job.to_dataframe()
 prediction_list = pl_query_job.to_dataframe()",data preprocessing,,
file152,9,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",save results,data exploration,
file152,10,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",save results,data exploration,
file152,11,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)",save results,data exploration,
file152,12,"# check if how many weeks of data are provided for each uu_id
 query = """"""
 SELECT uu_id, COUNT(week_number) as num_of_weeks
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY uu_id
 ORDER BY num_of_weeks DESC
 """"""",load data,,
file152,13,bigquery_client.query(query).to_dataframe(),data preprocessing,,
file152,14,"# query week_number data for uu_id 920b9820c654673d472494c346da5651
 query = """"""
 SELECT uu_id, week_number
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 WHERE uu_id=""920b9820c654673d472494c346da5651""
 ORDER BY week_number
 """"""
 print(bigquery_client.query(query).to_dataframe().to_string(index=False))",load data,,
file152,15,"# use sub-query to retrieve the num_of_uuids vs num_of_weeks, plot the results
 query = """"""
 SELECT num_of_weeks, COUNT(num_of_weeks) as num_of_uuids
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as num_of_weeks
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
  ORDER BY num_of_weeks DESC
 )
 GROUP BY num_of_weeks
 ORDER BY num_of_weeks
 """"""",load data,,
file152,16,"week_count_label = list(map(str, list(uuid_weeks[""num_of_weeks""])))
 num_of_uuids = list(uuid_weeks[""num_of_uuids""])
 plt.figure(figsize=(6,6))
 plt.barh(week_count_label, num_of_uuids)
 plt.yticks(week_count_label)
 plt.ylabel(""How many weeks of data are included"")
 plt.xlabel(""Number of UUIDs"")
 for i, v in enumerate(num_of_uuids):
  plt.text(v + 1, i - 0.4, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file152,17,"print(str(np.array(num_of_uuids)[-6:].sum()), ""out of 573 uuids have no less than 30 weeks of datapoints."")
 print(str(np.array(num_of_uuids)[:9].sum()), ""out of 573 uuids have no more than 10 weeks of datapoints."")",data exploration,,
file152,18,"# query the number of datapoints for each week, use DISTINCT on uu_id to remove duplication
 QUERY = """"""
 SELECT week_number, Count(DISTINCT(uu_id)) as count
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY week_number
 ORDER BY week_number
 """"""
 query_job = bigquery_client.query(QUERY)
 week_number_count = query_job.to_dataframe()
 display(week_number_count.T)
 print(""week_number: \n"" + str(week_number_count.T.to_numpy()[0]))
 print(""count: \n"" + str(week_number_count.T.to_numpy()[1]))",load data,data exploration,
file152,19,"# fill the missing weeks' count with 0 for week 4 and week 23
 i = 0
 week_number_count_filled = []
 for w_c in week_number_count.to_numpy():
  i = i + 1
  if w_c[0] == i:
  week_number_count_filled.append(list(w_c))
  else:
  week_number_count_filled.append(list([i, 0]))
  week_number_count_filled.append(list(w_c))
  i = i + 1
 print(""Total number of weeks:"", i)",data preprocessing,,
file152,20,"# plot the filled result
 week_number = list(np.array(week_number_count_filled)[:,0])
 week_count = list(np.array(week_number_count_filled)[:,1])
 plt.figure(figsize=(8,6))
 plt.barh(week_number, week_count)
 plt.yticks(week_number)
 plt.ylabel(""Week number"") 
 plt.xlabel(""Number of Datapoints"")
 for i, v in enumerate(week_count):
  plt.text(v + 2, i + 0.7, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file152,21,"# select 10 of 96 uuids with 3 weeks' data
 QUERY = """"""
 SELECT uu_id
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as week_count
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
 )
 WHERE week_count = 35
 LIMIT 10
 """"""
 query_job = bigquery_client.query(QUERY)
 ten_uuid_with_35_weeks = query_job.to_dataframe()
 ten_uuid_with_35_weeks",load data,data preprocessing,
file152,22,"# query the 10 selected UUID data
 ten_35w_data = []
 for uu_id in ten_uuid_with_35_weeks[""uu_id""]:
  QUERY=""""""
  SELECT uu_id, week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uu_id)
  query_job = bigquery_client.query(QUERY)
  ten_35w_data.append(query_job.to_dataframe().drop_duplicates(ignore_index=True))",load data,,
file152,23,"# manually add week 4 and week 23 missing data
 def fill_week4_and_week23(df, method, replace=False):
  """"""
  Fill or replace the week 4 and week 23 missing total_claims data.
  Args:
  df: Dataframe
  The input dataframe with missing total claims or to be replaced.
  method: string
  prev - use previous week's value
  next - use next week's value
  mid - use mean value of previous and next weeks
  replace: bool
  True - replace the existing week 4 and week 23 total_claim values
  False - add the missing values
  Returns:
  Dataframe:
  Dataframe with added total_claim values or replaced values.
  """"""
  uuid = df[""uu_id""][0]
  week_list = list(df[""week_number""])
  if 4 in week_list and 23 in week_list and not replace:
  print(""Week 4 and week 23 data already exist, please use replace=True"")
  return df
  if 4 not in week_list and 23 not in week_list and replace:
  print(""Week 4 and week 23 data do not exist, replace failed"")
  return df
  if 4 not in week_list and 23 not in week_list and not replace:
  print(""Fill uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  if 4 in week_list and 23 in week_list and replace:
  print(""Replace uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  # remove exisiting value and insert again
  df = df.drop(df.index[[3, 22]])
  df = df.sort_index().reset_index(drop=True)
  
  if method == ""prev"":
  # use previous avaliable week's value
  val_4 = df[""total_claims""][2]
  val_23 = df[""total_claims""][20]
  if method == ""next"":
  # use next avaliable week's value
  val_4 = df[""total_claims""][3]
  val_23 = df[""total_claims""][21]
  if method == ""mid"":
  # use mean value of previous and next avaliable weeks
  val_4 = int((df[""total_claims""][2] + df[""total_claims""][3]) * 0.5)
  val_23 = int((df[""total_claims""][20] + df[""total_claims""][21]) * 0.5)
  
  # week 4
  df.loc[2.5] = [uuid, 4, val_4]
  # week 23
  df.loc[20.5] = [uuid, 23, val_23]
  return df.sort_index().reset_index(drop=True)",data preprocessing,,
file152,24,"# fill the missing data using mean value of prev week and next week for all 10 uuids
 for i in range(len(ten_35w_data)):
  ten_35w_data[i] = fill_week4_and_week23(ten_35w_data[i], ""mid"")",data preprocessing,,
file152,25,"# plot the 10 uuid total_claim data from week 1 to week 37
 plt.figure(figsize=(12,4))
 plt.title(""10 selected uuid with 37 weeks of data total_claim plot"")
 for i in range(len(ten_35w_data)):
  plt.plot(list(map(str, ten_35w_data[i][""week_number""])), 
  ten_35w_data[i][""total_claims""], 
  ""o-"", linewidth=1, markersize=5, alpha=0.7,
  label=ten_35w_data[i][""uu_id""][0])
  plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)
 plt.xlabel(""week_number"")
 plt.ylabel(""total_claims"")
 plt.grid(axis=""y"")
 plt.tight_layout()
 plt.show()",result visualization,,
file152,26,"# find out which weeks are available for these uuids
 week_available = []
 for i in range(len(data_less_10w)):
  week_available.append(list(data_less_10w[i][""week_number""]))",data preprocessing,,
file152,27,"# plot the avaliable weeks vs uu_id
 plt.figure(figsize=(12,4))
 plt.title(""week_number distribution for uuid with no more than 10 weeks of data"")
 colors = ['C{}'.format(i) for i in range(37)]
 plt.eventplot(week_available, orientation='vertical', linelengths=0.2, linewidths=4, colors=colors)
 plt.yticks([i for i in range(1,38,2)])
 plt.ylabel(""week_number"")
 plt.xticks([i for i in range(37)])
 plt.xlabel(""uu_id"")
 plt.grid(axis=""x"")
 plt.tight_layout()
 plt.show()",result visualization,,
file152,28,week_available.shape,data exploration,,
file152,29,len(week_available),data exploration,,
file152,30,prediction_list.shape[0],data exploration,,
file152,31,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",save results,data exploration,
file152,32,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",save results,data exploration,
file152,33,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)
 total_uuid = prediction_list.shape[0]",save results,data preprocessing,
file152,34,"# save query result into csv file
 raw_total_claims_df = pd.DataFrame.from_dict(raw_total_claims_data)
 raw_total_claims_df.to_csv(""raw_total_claims.csv"", index=False)",save results,,
file152,35,"# query all total_claims data and store in raw_total_claims_data dict
 def query_all_total_claims():
  temp_query_list = prediction_list[""uu_id""] # the list of uuids to be queried
  raw_total_claims_data = {uuid:[] for uuid in temp_query_list} # dict to store all query results
  total_week = 37
 

  progress = 0
  total = len(temp_query_list)
  for uuid in temp_query_list:
  progress = progress + 1
  print('\r', ""Querying"", str(progress) + ""/"" + str(total), ""uuid's data..."", end='\r')
  QUERY=""""""
  SELECT week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uuid)
  query_job = bigquery_client.query(QUERY)
  res = query_job.to_dataframe().drop_duplicates(ignore_index=True).to_numpy()
 

  # Fill the missing total_claims between week 1 and week 37 with value 0
  index = 0
  for w in range(1,total_week + 1):
  if index == len(res):
  raw_total_claims_data[uuid].append({w:0})
  continue
  if w == res[index][0]:
  raw_total_claims_data[uuid].append({w:res[index][1]})
  index = index + 1
  else:
  raw_total_claims_data[uuid].append({w:0})
  return raw_total_claims_data",data preprocessing,,
file152,36,"# commented for only excuting once
 raw_total_claims_data = query_all_total_claims()",data preprocessing,,
file152,37,"raw_total_claims_df = pd.read_csv(""raw_total_claims.csv"")",data preprocessing,,
file152,38,"for i in range(0, 37):
  for j in range(0, 573):
  d = raw_total_claims_df.iloc[i,j]
  # print(int(d.split("":"")[1].split(""}"")[0]))
  raw_total_claims_df.iloc[i,j] = int(d.split("":"")[1].split(""}"")[0])",data preprocessing,,
file152,39,"raw_total_claims_list = raw_total_claims_df.T.to_numpy()
 raw_total_claims_list = raw_total_claims_list[:]",data preprocessing,,
file152,40,"x_labels = [w for w in range(1, 38)]
 NUM_PLOTS = len(raw_total_claims_list)",data preprocessing,,
file152,41,"plt.figure(figsize=(15,5))
 plt.xticks(x_labels)
 for i in range(0, NUM_PLOTS):
  plt.plot(x_labels, raw_total_claims_list[i], "".-"")",result visualization,,
file152,42,"# fill week 4 and week 23 data with mean values for all uuids
 for row in raw_total_claims_list:
  temp_sum = 0
  w4_val = round(0.5 * (row[2] + row[4]) + 0.01)
  w23_val = round(0.5 * (row[21] + row[23]) + 0.01)
  row[3] = w4_val
  row[22] = w23_val",data preprocessing,,
file152,43,"def mse_loss(y_true, y_pred):
  """"""
  MSE loss function
  Inputs param
  -------------------------
  y_true: list
  The ground truth values
  y_pred: list
  The predicted values
  -------------------------
  return: number
  mean square error of the two input lists
  """"""
  if(len(y_true) != len(y_pred)):
  print(""True label len:"" + str(len(y_true)) + "", Predict label len: "" + str(len(y_pred)))
  raise Exception(""Input lists have different length"")
  mse = np.mean(np.array(y_true) - np.array(y_pred))**2
  return mse",data preprocessing,,
file152,44,"uuid_list = list(raw_total_claims_df.columns)
 pred_results = [round(np.array(row).mean() + 0.01) for row in raw_total_claims_list]",data preprocessing,,
file152,45,"# Mean value as the prediction result
 result_dict = {
  'uu_id':uuid_list, 
  'total_claims': pred_results,
  'week_number': [39 for i in range(573)]
 }
 result_to_csv = pd.DataFrame(result_dict)
 result_to_csv",data preprocessing,,
file152,46,"print(uuid_list[0])
 print(list(raw_total_claims_list[0]))",data exploration,,
file152,47,"print(list(raw_total_claims_df[""26c71b31d464bc7bedc8aed7e5c6e641""]))",data exploration,,
file152,48,"result_to_csv.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file152,49,temp_res = [row[-1] for row in raw_total_claims_list],data preprocessing,,
file152,50,"print(list(raw_total_claims_df[""8ba19786b86ae124a9d7eaa054f15d23""]))",data exploration,,
file152,51,"mae_loss(temp_res, pred_results)",evaluation,,
file152,52,"# plot the avaliable weeks vs uu_id
 plt.figure(figsize=(12,4))
 plt.title(""week_number distribution for uuid with no more than 10 weeks of data"")
 colors = ['C{}'.format(i) for i in range(total_uuid_less_10w)]
 plt.eventplot(week_available, orientation='vertical', linelengths=0.2, linewidths=4, colors=colors)
 plt.yticks([i for i in range(1,38,2)])
 plt.ylabel(""week_number"")
 plt.xticks([i for i in range(total_uuid_less_10w)])
 plt.xlabel(""uu_id"")
 plt.grid(axis=""x"")
 plt.axhline(4, alpha=0.5)
 plt.axhline(23, alpha=0.5)
 plt.axhline(37, alpha=0.5)
 plt.tight_layout()
 plt.show()",result visualization,,
file153,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file153,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!pip install db-dtypes pmdarima 'google-cloud-bigquery[pandas]' tqdm\n"")",helper functions,,
file153,2,"import numpy as np
 import pandas as pd
 from tqdm.auto import tqdm
 import seaborn
 from google.cloud import bigquery",helper functions,,
file153,3,from pmdarima.arima import AutoARIMA,helper functions,,
file153,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file153,5,"data_tables = bigquery_client.query(f""""""
  SELECT table_catalog, table_schema, table_name
  FROM `ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """""").to_dataframe()
 print(data_tables)",load data,,
file153,6,"# Read all data tables in ironhacks-data.ironhacks_competition
 data_dict = {
  table_name: bigquery_client.query(f""""""
  SELECT * FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """""").to_dataframe()
  for table_name in data_tables['table_name'].tolist()
 }",load data,,
file153,7,"common_cols = list(set(data_dict['unemployment_data'].columns) & set(data_dict['wage_data'].columns))
 print(f'Common columns: {common_cols}')
 data = data_dict['unemployment_data'].merge(
  data_dict['wage_data'],
  on=common_cols,
  how='left',
 ).sort_values(['countyfips', 'week_number']).drop_duplicates().reset_index(drop=True)
 data['timeperiod'] = pd.to_datetime(data['timeperiod'], format='%Y%m%d')
 data",data preprocessing,,
file153,8,"# For our own convenience, create a correspondance DataFrame for `week_number` and `timeperiod`.
 wt = pd.Series(
  range(1, 53),
  name='week_number',
  index=pd.date_range('2022-01-01', periods=52, freq='W-SAT').rename('timeperiod'),
 ).reset_index()
 wt['timeperiod'] = wt['timeperiod'].astype(str)
 wt",data preprocessing,,
file153,9,"# The following shows that there are a lot of missing data in exogenous features...
 for cname, cvalues in data.items():
  print('Column {} has {} ({}%) missing value(s)'.format(
  cname,
  cvalues.isna().sum(),
  round(100.0 * cvalues.isna().sum() / len(cvalues), 2),
  ))",data preprocessing,,
file153,10,"train_weeks = 37
 train_timeperiods = pd.to_datetime(wt.set_index('week_number')['timeperiod'].loc[:train_weeks])
 target_week = 39
 pred_results = []
 for i in tqdm(data['uu_id'].unique()):
  y = data[
  data['uu_id'] == i
  ].set_index('timeperiod')['total_claims'].asfreq('W-SAT').reindex(train_timeperiods).fillna(0)
  n_periods = target_week - train_weeks
  pred = AutoARIMA(
  seasonal=False,
  ).fit_predict(y, n_periods=n_periods).iloc[-1]
  pred_results.append([i, max(0.0, pred)])",modeling,prediction,
file153,11,"# Getting the output CSV for submission ready
 data_dict['prediction_list'].merge(
  pd.DataFrame(pred_results, columns=['uu_id', 'total_claims']),
  on=['uu_id'],
  how='left',
 ).to_csv('submission_prediction_output.csv', index=False)",save results,,
file154,0,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file154,1,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file154,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""",load data,,
file154,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(5)
 print(unemployment_data.columns)
 9/59:
 #SPLIT training set",load data,data preprocessing,
file154,4,"x = df['edu_post_hs'].values.reshape(-1,1)
 y = df['total_claims'].values.reshape(-1,1)
 9/60:
 #SPLIT training set",data preprocessing,,
file154,5,"X = df['edu_post_hs'].values.reshape(-1,1)
 Y = df['total_claims'].values.reshape(-1,1)
 9/61: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 9/62:
 regressor = LinearRegression()  
 regressor.fit(X_train, y_train)
 9/63:
 #To retrieve the intercept:
 print(regressor.intercept_)",modeling,,
file154,6,"#For retrieving the slope:
 print(regressor.coef_)
 9/64: y_pred = regressor.predict(X_test)
 9/65:
 df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df2",prediction,,
file154,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file154,8,df = unemployment_data.dropna(),data preprocessing,,
file154,9,"X = df['edu_post_hs'].values.reshape(-1,1)
 Y = df['total_claims'].values.reshape(-1,1)
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file154,10,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train)",modeling,,
file154,11,"#To retrieve the intercept:
 print(regressor.intercept_)",evaluation,,
file154,12,"#For retrieving the slope:
 print(regressor.coef_)
  y_pred = regressor.predict(X_test)",evaluation,prediction,
file154,13,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df2",data preprocessing,data exploration,
file155,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('python3 -m pip install google.cloud')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('python3 -m pip install numpy')
 get_ipython().system('python3 -m pip install scikit-learn')
 get_ipython().system('python3 -m pip install plotly')",helper functions,,
file155,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import os
 import seaborn as sns
 from pandas import Series, DataFrame",helper functions,,
file155,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file155,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file155,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file155,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file155,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file155,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data preprocessing,data exploration,
file155,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file155,9,"#Merge the data
 unemployment_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemployment_wage_data = unemployment_wage_data.drop(['countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemployment_wage_data = unemployment_wage_data.fillna(0)
 unemployment_wage_data.describe()",data preprocessing,data exploration,
file155,10,unemployment_wage_data.head(),data exploration,,
file155,11,"#Check for duplicated rows
 duplicated_rows = sum(unemployment_wage_data.duplicated()) 
 unemployment_wage_data = unemployment_wage_data.drop_duplicates()",data preprocessing,,
file155,12,unemployment_wage_data[unemployment_wage_data.isnull().any(axis=1)],data exploration,,
file155,13,"#heat map for correlations
 plt.figure(figsize=(25,10))
 cor = unemployment_wage_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds) 
 plt.show()",result visualization,,
file155,14,unemployment_wage_data.columns,data exploration,,
file155,15,"X = unemployment_wage_data.drop(['total_claims', 'week_number'], axis = 1)
 y = unemployment_wage_data.total_claims",data preprocessing,,
file155,16,"uuid, label = unemployment_wage_data['uu_id'].factorize(sort=True)",data preprocessing,,
file155,17,X['uu_id'] = uuid,data preprocessing,,
file155,18,"#convert data from string to float in order for random forest to work
 X['tract_name_x'] = X['tract_name_x'].factorize()[0]
 X['top_category_employer1'] = X['top_category_employer1'].factorize()[0]
 X['top_category_employer2'] = X['top_category_employer2'].factorize()[0]
 X['top_category_employer3'] = X['top_category_employer3'].factorize()[0]",data preprocessing,,
file155,19,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",data preprocessing,,
file155,20,"from sklearn.ensemble import RandomForestRegressor
 rf = RandomForestRegressor(n_estimators=1000, random_state=42)
 rf.fit(X_train, y_train)",modeling,,
file155,21,"rf.score(X_test, y_test)",evaluation,,
file155,22,prediction_list,data exploration,,
file155,23,"for col in X.columns[2:]:
  li = []
  for i in prediction_list['uu_id']:
  li.append(X.loc[X['uu_id'] == i, col].mean())
  prediction_list[col] = li",data preprocessing,,
file155,24,prediction_list['uu_id'] = prediction_list['uu_id'].factorize(sort=True)[0],data preprocessing,,
file155,25,"X['uu_id'] = label[X[""uu_id""]]
 X",data preprocessing,data exploration,
file155,26,"claims_predict = rf.predict(prediction_list)
 claims_predict",prediction,data exploration,
file155,27,submission_df = pd.DataFrame(),data preprocessing,,
file155,28,"submission_df[""uu_id""] = prediction_list[""uu_id""]
 submission_df[""week_number""] = prediction_list[""week_number""]
 submission_df[""total_claims""] = claims_predict",data preprocessing,,
file155,29,submission_df,data exploration,,
file156,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file156,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas db-dtypes\n"")",helper functions,,
file156,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file156,3,"def example_function():
  print('Hello World')",helper functions,,
file156,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file156,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file156,6,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data preprocessing,
file156,7,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 print(prediction_list)",load data,data preprocessing,
file156,8,"len(pd.unique(unemployment_data[""uu_id""]))",data exploration,,
file156,9,print(unemployment_data.loc[0]),data exploration,,
file156,10,"# remove unnecessary colomns and combine unemployment data together with wages
 unemployment_sample = unemployment_data.copy()
 unemployment_sample = unemployment_sample.drop(['timeperiod', 'tract', 'tract_name', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs',\
  'edu_unknown', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3', 'gender_female', 'gender_male',\
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative',\
  'race_other', 'race_white'], axis=1)
 wage_sample = wage_data.copy()
 wage_sample.drop(['tract', 'tract_name'], axis=1)
 prediction_sample = prediction_list.copy()
 wages_dict = {i:wage_sample[""average_wage""].values[k] for k,i in enumerate(wage_sample[""uu_id""])}
 wage_list = [wages_dict[i] for i in unemployment_sample[""uu_id""]]
 # print(wages_dict)
 unemployment_sample[""average_wage""] = wage_list",data preprocessing,,
file156,11,"tract_dic = {i:k for k,i in enumerate(wage_sample[""uu_id""])}",data preprocessing,,
file156,12,"unemployment_sample[""uu_id""] = [tract_dic[i] for i in unemployment_sample[""uu_id""]]",data preprocessing,,
file156,13,"X = unemployment_sample.drop([""total_claims""], axis=1).to_numpy() #to_numpy() values
 y = unemployment_sample['total_claims'].to_numpy()  #to_numpy() values
 # print(X.shape,y.shape)",data preprocessing,,
file156,14,"from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file156,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file156,16,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train) #training the algorithm",modeling,,
file156,17,print(X),data exploration,,
file156,18,"for k,i in enumerate(X):
  if np.nan in i:
  print(k,i)",data preprocessing,data exploration,
file156,19,"for k,i in enumerate(X):
  for j in i:
  if pd.isna(j)
  print(k,i)",data preprocessing,data exploration,
file156,20,"for k,i in enumerate(wage_list):
  if pd.isna(i):
  print(k,i)
  # for j in i:
  #  if pd.isna(j):
  #  print(k,i)",data preprocessing,data exploration,
file156,21,unemployment_sample = unemployment_sample.dropna(),data preprocessing,,
file156,22,"#To retrieve the intercept:
 print(regressor.intercept_)",evaluation,,
file156,23,"#For retrieving the slope:
 print(regressor.coef_)",evaluation,,
file156,24,"y_pred = regressor.predict(X_test)
 df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df",prediction,data exploration,
file156,25,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file156,26,"plt.scatter(X_test, y_test, color='gray')
 plt.plot(X_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file156,27,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file157,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file157,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file157,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file157,3,"def example_function():
  print('Hello World')",helper functions,,
file157,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file157,5,"def get_data(table_name):
  query = f""""""
  SELECT *
  FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """"""
 

  # QUERY THE DATA ONCE
  query_job = bigquery_client.query(query)
  return query_job.to_dataframe()",load data,,
file157,6,unemploy = get_data('unemployment_data'),load data,,
file157,7,wage = get_data('wage_data'),load data,,
file157,8,sub = get_data('prediction_list'),load data,,
file157,9,pred = unemploy.groupby('uu_id')['total_claims'].mean().apply(lambda x: int(x)),prediction,,
file157,10,pred = unemploy.groupby('uu_id').apply(lambda x: x.sort_values('week_number')['total_claims'].ewm(alpha=1/3).mean().to_numpy()[-1]),prediction,,
file157,11,pred = pred[sub['uu_id'].to_list()],prediction,,
file157,12,"pred = pd.DataFrame({
  'uu_id': pred.index,
  'week_number': 39,
  'total_claims': pred.to_list()
 })",prediction,,
file157,13,pred,data exploration,,
file157,14,"pred.to_csv('submission_prediction_output.csv', index=False)",save results,,
file157,15,df,data exploration,,
file158,0,"get_ipython().run_cell_magic('capture', 'cont', '!pip install db-dtypes\n')",helper functions,,
file158,1,cont.show(),data exploration,,
file158,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file158,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file158,4,"import statsmodels.api as sm
 import itertools",helper functions,,
file158,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file158,6,"#obtaining the unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file158,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file158,8,"data.info()  #Feature Matrix
 #X = data.drop(""date"",1)
 #y = data[""wind_speed""] #Target Variable",data exploration,,
file158,9,submit.info(),data exploration,,
file158,10,"data.drop_duplicates(inplace=True, ignore_index=True)
 data.info()",data preprocessing,data exploration,
file158,11,data.uu_id.unique().size,data exploration,,
file158,12,"data.fillna(0, inplace=True)",data preprocessing,,
file158,13,data.info(),data exploration,,
file158,14,"test=data[data['uu_id']=='bbcb018f0e5e49e13636f6e78ce9f60f']
 len(test)",data preprocessing,data exploration,
file158,15,"data['timeperiod']= pd.to_datetime(data['timeperiod'], format='%Y%m%d')",data preprocessing,,
file158,16,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file158,17,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.2]
 print(relevant_features)",data preprocessing,data exploration,
file158,18,columns_rel = relevant_features.index.to_list(),data preprocessing,,
file158,19,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file158,20,"min(data['timeperiod']),max(data['timeperiod'])",data exploration,,
file158,21,data.dtypes,data exploration,,
file158,22,data.index,data exploration,,
file158,23,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file158,24,data['total_claims'].plot(linewidth=.5);,result visualization,,
file158,25,"cols_plot = ['week_number','countyfips','tract','total_claims','edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs',
  'edu_unknown', 'gender_female', 'gender_male', 'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 
  'race_hawaiiannative', 'race_other', 'race_white']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('features')",result visualization,,
file158,26,"#fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 #for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
 # sns.boxplot(data=data, x='Month', y=name, ax=ax)
 # ax.set_ylabel('precipitation')
 # ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
 # if ax != axes[-1]:
 # ax.set_xlabel('')",result visualization,,
file158,27,"columns_rel.append('timeperiod')
 columns_rel.append('uu_id')
 columns_rel",data preprocessing,data exploration,
file158,28,data,data exploration,,
file158,29,"data_arima=data[columns_rel]
 data_arima",data preprocessing,data exploration,
file158,30,"get_ipython().run_cell_magic('capture', '', ""unique_id=list(data_arima['uu_id'].unique())\ndata_dict = {}\n\nfor i in unique_id:\n j = data_arima[data_arima['uu_id']==i].groupby('timeperiod')['total_claims'].sum().reset_index()\n j = j.set_index('timeperiod')\n data_dict[i] = j\n"")",helper functions,,
file158,31,"p = d = q = range(0, 2)
 pdq = list(itertools.product(p, d, q))
 seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
 print('Examples of parameter combinations for Seasonal ARIMA...')
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",data preprocessing,data exploration,
file158,32,"import warnings
 warnings.filterwarnings(""ignore"")",helper functions,,
file158,33,"result_dict = {}
 param_dict = {}
 seasonal_dict = {}
 for key in data_dict:
  y=data_dict[key]['total_claims']
  for param in pdq:
  for param_seasonal in seasonal_pdq:
  
  try:
  
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=param,
  seasonal_order=param_seasonal,
  enforce_stationarity=False,
  enforce_invertibility=False)
  result_dict[key] = mod.fit(disp=False)
  #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, result_dict[key].aic))
  param_dict[key]=param
  seasonal_dict[key]=param_seasonal
  except:
  continue",modeling,,
file158,34,"param_dict_copy=param_dict
 seasonal_dict_copy=seasonal_dict
 result_dict_copy=result_dict",data preprocessing,,
file158,35,"results = {}
 for key in data_dict:
  y=data_dict[key]['total_claims']
  print(y)
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=param_dict[key],
  seasonal_order=seasonal_dict[key],
  enforce_stationarity=False,
  enforce_invertibility=False)
  results[key] = mod.fit(disp=False)
  #print(results.summary().tables[1])",data preprocessing,,
file159,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file159,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file159,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from statsmodels.formula.api import ols
 from pandas import Series, DataFrame",helper functions,,
file159,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file159,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file159,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file159,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file159,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data preprocessing,data exploration,
file159,8,"query_job3 = bigquery_client.query(query3)
 prediction_list = query_job3.to_dataframe()
 prediction_list.head()",load data,data preprocessing,
file159,9,"unemploy_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemploy_wage_data = unemploy_wage_data.drop(['timeperiod', 'countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemploy_wage_data = unemploy_wage_data.fillna(0)
 unemploy_wage_data.head()",data preprocessing,data exploration,
file159,10,unemploy_wage_data.describe(),data exploration,,
file159,11,"sns.relplot(data=unemploy_wage_data, x='week_number', y='total_claims')",result visualization,,
file159,12,"sns.distplot(unemploy_wage_data.total_claims, bins=10)",result visualization,,
file159,13,"plt.figure(figsize=(20,18))
 cor = unemploy_wage_data.corr()
 cmap = sns.diverging_palette(210, 20, as_cmap=True)
 sns.heatmap(cor, cmap=cmap, vmax=.99, vmin=-.99, annot=True)",result visualization,,
file159,14,"X = unemploy_wage_data[['week_number', 'countyfips_x', 'tract_x', 'edu_8th_or_less', 'edu_grades_9_11', \
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown', 'gender_female', 'gender_male', \
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', \
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 y = unemploy_wage_data['total_claims']",data preprocessing,,
file159,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file159,16,"print(f'intercept: {reg.intercept_}')
 coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
 print(coef)",evaluation,,
file159,17,"reg = LinearRegression()  
 reg.fit(X_train, y_train)
 print(reg)",modeling,,
file159,18,"y_pred = reg.predict(X_test)
 df = DataFrame({'Actual': y_test, 'Predicted': y_pred})
 df",prediction,,
file159,19,"print('R squared: {:.2f}'.format(reg.score(X, y)*100))
 print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
 print('MSE:', metrics.mean_squared_error(y_test, y_pred))
 print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file159,20,df['Predicted'].mean(),data exploration,,
file159,21,"predicted_unemploy = pd.merge(unemploy_wage_data,df)",data preprocessing,,
file159,22,unemployment_data.week_number.unique(),data exploration,,
file159,23,"predicted_claims = prediction_list.values
 predicted_claims",data preprocessing,data exploration,
file159,24,"final_reg = LinearRegression()  
 final_reg.fit(X, y)
 print(final_reg)",modeling,data exploration,
file159,25,"prediction_data = pd.merge(unemploy_wage_data, prediction_data, on=['uu_id'], how='inner')
 prediction_data.head()",data preprocessing,data exploration,
file159,26,"prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
 prediction_data = prediction_data.drop(['week_number_x','total_claims'],axis=1)
 prediction_data.head()",data preprocessing,data exploration,
file159,27,unemploy_wage_data.groupby(['uu_id'])['total_claims'].mean(),data preprocessing,,
file159,28,"unemployment_prediction_data.to_csv('submission_prediction_output.csv', index=False)",save results,,
file160,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file160,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm",helper functions,,
file160,2,"# explore dataframe
 def dataExplore(data):
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data preprocessing,,
file160,3,"# check balance of data
 def dataBalanceCheck(data):
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data preprocessing,,
file160,4,"# fill NA with given value
 def dataFillNa(data, value):
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)
 def dataIdentifyDateMonth(data):
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  return(data)",data preprocessing,,
file160,5,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file160,6,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file160,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file160,8,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file160,9,"# Explore input data for NA and special values
 dataExplore(data)
 dataExplore(data_pred_query)",data preprocessing,,
file161,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file161,1,"get_ipython().run_cell_magic('capture', '', ""#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install google-cloud-bigquery\n!pip install google-cloud-bigquery[pandas]\n"")",helper functions,,
file161,2,"#- IMPORT THE LIBRARIES YOU WILL USE
 #------------------------------------------
 # You only need to import packages one time per notebook session. To keep your
 # notebook clean and organized you can handle all imports at the top of your file.
 # The following are included for example purposed, feel free to modify or delete 
 # anything in this section.
 import csv
 import pandas as pd
 import numpy as np
 import random as rand
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import RandomForestClassifier",helper functions,,
file161,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file161,4,"#query 1: overview of employment_data(week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC;
 """"""
 query_job = bigquery_client.query(query)
 overview = query_job.to_dataframe()
 overview.head()",load data,data exploration,
file161,5,"#query 3: overview of prediction list (week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query_job = bigquery_client.query(query)
 predn = query_job.to_dataframe()
 predn.head()
 print(predn.head())",load data,data preprocessing,
file161,6,"fig = plt.figure()
 plt.plot([employ.week_number], [employ.total_claims],'bs')
 plt.title('Distribution of claims through week')
 plt.xlabel('Weeks')
 plt.ylabel('Total claims')
 plt.show()",result visualization,,
file161,7,"#Calculate average number of claims/ rates of change/ estimated errors (week 30-37)
 employ['avg'] = (sum(employ['total_claims'])/len(employ['total_claims']))
 employ['error'] = ((employ['total_claims']-employ['avg'])/employ['avg'])
 employ['avgError'] = (employ['error']/len(employ['total_claims']))
 avgError = (employ['error']/len(employ['total_claims']))
 avgError = avgError.drop_duplicates()
 l = employ[['week_number','total_claims','avg','error','avgError']]
 print(l)",data preprocessing,,
file161,8,"labels = np.array(overview['total_claims'])
 features = employ.drop(['uu_id'], axis=1)
 feature_list = list(features.columns)
 features = np.array(features)",data preprocessing,,
file161,9,"#split data into train and test sets, split first 20% data
 x_train, x_test, y_train,y_test = train_test_split(features, labels, test_size = 0.15, random_state = 42)",data preprocessing,,
file161,10,"print(f'Training Features Shape: {x_train.shape}')
 print(f'Testing Features Shape: {x_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data exploration,,
file161,11,"from sklearn.ensemble import RandomForestRegressor
 regressor = RandomForestRegressor(n_estimators=1000, random_state=42)
 x = x_train
 y = y_train
 regressor.fit(x,y)",modeling,,
file161,12,"#visualizing the decision tree from the regressor
 from sklearn import tree
 tree.plot_tree(regressor.estimators_[0])",result visualization,,
file161,13,"predictions = regressor.predict(x_test).astype(int)
 predictions = np.round(predictions,decimals = 0, out = None)
 print(predictions)",prediction,data exploration,
file161,14,"errors = abs(y_test - predictions)
 print(f'List of Errors: {errors}')
 print(f'Mean Absolute Error: {np.mean(errors)*10:.4f}%')",data preprocessing,data exploration,
file161,15,"df = pd.DataFrame(predictions, columns=['total_claims'])
 week44 = predn.join(df).iloc[:,[0,2,1]]
 print(week44)
 print(f'Total predicting number of unemployment claims of week 44: {sum(predictions):.0f}')",data preprocessing,data exploration,
file161,16,"csv_data = week44.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file162,0,"get_ipython().run_cell_magic('capture', 'cont', '!pip install db-dtypes\n')",helper functions,,
file162,1,cont.show(),data exploration,,
file162,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file162,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file162,4,"import statsmodels.api as sm
 import itertools",helper functions,,
file162,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file162,6,"#obtaining the unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file162,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file162,8,"data.info()  #Feature Matrix
 #X = data.drop(""date"",1)
 #y = data[""wind_speed""] #Target Variable",data exploration,,
file162,9,submit.info(),data exploration,,
file162,10,"data.drop_duplicates(inplace=True, ignore_index=True)
 data.info()",data preprocessing,data exploration,
file162,11,data.uu_id.unique().size,data exploration,,
file162,12,"data.fillna(0, inplace=True)",data preprocessing,,
file162,13,data.info(),data exploration,,
file162,14,"test=data[data['uu_id']=='bbcb018f0e5e49e13636f6e78ce9f60f']
 len(test)",data preprocessing,,
file162,15,"data['timeperiod']= pd.to_datetime(data['timeperiod'], format='%Y%m%d')",data preprocessing,,
file162,16,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file162,17,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.2]
 print(relevant_features)",data exploration,data preprocessing,
file162,18,columns_rel = relevant_features.index.to_list(),data preprocessing,,
file162,19,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file162,20,"min(data['timeperiod']),max(data['timeperiod'])",data exploration,,
file162,21,data.dtypes,data exploration,,
file162,22,data.index,data exploration,,
file162,23,"sns.set(rc={'figure.figsize':(11, 4)})",helper functions,,
file162,24,data['total_claims'].plot(linewidth=.5);,result visualization,,
file162,25,"cols_plot = ['week_number','countyfips','tract','total_claims','edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs',
  'edu_unknown', 'gender_female', 'gender_male', 'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 
  'race_hawaiiannative', 'race_other', 'race_white']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('features')",result visualization,data preprocessing,
file162,26,"#fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 #for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
 # sns.boxplot(data=data, x='Month', y=name, ax=ax)
 # ax.set_ylabel('precipitation')
 # ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
 # if ax != axes[-1]:
 # ax.set_xlabel('')",comment only,,
file162,27,"columns_rel.append('timeperiod')
 columns_rel.append('uu_id')
 columns_rel",data preprocessing,data exploration,
file162,28,data,data exploration,,
file162,29,"data_arima=data[columns_rel]
 data_arima",data preprocessing,data exploration,
file162,30,"get_ipython().run_cell_magic('capture', '', ""unique_id=list(data_arima['uu_id'].unique())\ndata_dict = {}\n\nfor i in unique_id:\n j = data_arima[data_arima['uu_id']==i].groupby('timeperiod')['total_claims'].sum().reset_index()\n j = j.set_index('timeperiod')\n data_dict[i] = j\n"")",helper functions,,
file162,31,"p = d = q = range(0, 2)
 pdq = list(itertools.product(p, d, q))
 seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
 print('Examples of parameter combinations for Seasonal ARIMA...')
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",data preprocessing,data exploration,
file162,32,"import warnings
 warnings.filterwarnings(""ignore"")",helper functions,,
file162,33,with io.capture_output() as captured:,helper functions,,
file162,34,import csv,helper functions,,
file162,35,"filename =""param.csv""",load data,,
file162,36,"# opening the file using ""with""
 # statement
 with open(filename, 'r') as data:
  for line in csv.DictReader(data):
  print(line)",data exploration,,
file162,37,csv.DictReader(data),load data,,
file162,38,"filename =pd.read_csv(""param.csv"")",load data,,
file162,39,filename.head(),data exploration,,
file162,40,filename.to_dict(),data preprocessing,,
file162,41,"param_dict=file_param.to_dict('list')
 seasonal_dict=file_seasonal.to_dict('list')",data preprocessing,,
file162,42,"file_param =pd.read_csv(""param.csv"")
 file_seasonal =pd.read_csv(""seasonal.csv"")
 # opening the file using ""with""
 # statement
 #with open(filename, 'r') as data:
 # for line in csv.DictReader(data):
 # print(line)",load data,comment only,
file162,43,len(data_dict),data exploration,,
file162,44,"hundred=list(data_dict.keys())[:100]
 two_hundred=list(data_dict.keys())[100:200]
 three_hundred=list(data_dict.keys())[200:300]
 four_hundred=list(data_dict.keys())[300:400]
 five_hundred=list(data_dict.keys())[500:573]",data preprocessing,,
file162,45,"#results = {}
 for key in hundred:
  y=data_dict[key]['total_claims']
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=tuple(param_dict[key]),
  seasonal_order=tuple(seasonal_dict[key]),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results[key] = mod.fit(disp=False)
  #print(results.summary().tables[1])",modeling,,
file162,46,len(results),data exploration,,
file162,47,list(results.keys()),data exploration,,
file162,48,"results[x].plot_diagnostics(figsize=(16, 8))
 plt.show()",result visualization,,
file162,49,data_dict[x]['total_claims'],data exploration,,
file162,50,"pred = results[x].get_prediction(start=pd.to_datetime('2022-09-26'), dynamic=False)",prediction,,
file162,51,pred = results[x].get_prediction( dynamic=False),prediction,,
file162,52,"pred_ci = pred.conf_int()
 ax = data_dict[x]['total_claims'].plot(label='observed')
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Furniture Sales')
 plt.legend()
 plt.show()",result visualization,,
file162,53,pred = results[x].forecast(),prediction,,
file162,54,pred,data exploration,,
file162,55,"pred = []
 for key in hundred:
  i=results[key].forecast()
  pred.append(i)
  #print(results.summary().tables[1])",prediction,,
file162,56,pred.values,data exploration,,
file162,57,len(pred),data exploration,,
file162,58,hundred+two_hundred,data preprocessing,data exploration,
file162,59,uu_id=hundred+two_hundred+three_hundred+four_hundred+five_hundred+six_hundred,data preprocessing,data exploration,
file162,60,len(uu_id),data exploration,,
file162,61,submit.head(),data exploration,,
file162,62,"first_trial = pd.DataFrame(uu_id, columns=['uu_id'])
 first_trial.head()",data preprocessing,,
file162,63,first_trial['total_claims']=pred,data preprocessing,,
file162,64,first_trial.head(),data exploration,,
file162,65,"pd.merge(submit, first_trial, on='uu_id', how='outer')",data preprocessing,,
file162,66,"final_file=pd.merge(submit, first_trial, on='uu_id', how='outer')
 final_file.to_csv(./'submission.csv', index=False)",data preprocessing,,
file163,0,"#importing libraries
 import numpy as np
 import matplotlib.pyplot as plt
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file163,1,"#Code for BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file163,2,"#Testing bigquery
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file163,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data exploration,
file164,0,"import os
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file164,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemp_data = query_job.to_dataframe()
 unemp_data.head(5)",load data,data exploration,
file164,2,unemp_data.info(),data exploration,,
file164,3,wage_data.info(),data exploration,,
file164,4,unemp_data.isna().sum(),data exploration,,
file164,5,"set1 = set(list(unemp_data['top_category_employer1'].unique()))
 set1",data preprocessing,,
file164,6,"unemp_data['top_category_employer2'] = unemp_data['top_category_employer2'].replace('N/A',np.NaN)
 unemp_data['top_category_employer3'] = unemp_data['top_category_employer3'].replace('N/A',np.NaN)",data preprocessing,,
file164,7,unemp_data['countyfips'].unique(),data exploration,,
file164,8,(unemp_data.isna().sum()/len(unemp_data))*100,data exploration,,
file164,9,"unemp_data['edu_8th_or_less'] = unemp_data.groupby('countyfips')['edu_8th_or_less'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['edu_grades_9_11'] = unemp_data.groupby('countyfips')['edu_grades_9_11'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['edu_hs_grad_equiv'] = unemp_data.groupby('countyfips')['edu_hs_grad_equiv'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file164,10,"unemp_data['race_asian'] = unemp_data.groupby('countyfips')['race_asian'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_amerindian'] = unemp_data.groupby('countyfips')['race_amerindian'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_black'] = unemp_data.groupby('countyfips')['race_black'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_hawaiiannative'] = unemp_data.groupby('countyfips')['race_hawaiiannative'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_other'] = unemp_data.groupby('countyfips')['race_other'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_white'] = unemp_data.groupby('countyfips')['race_white'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file164,11,"unemp_data['race_noanswer'] = unemp_data['race_noanswer'].fillna(0)
 unemp_data['gender_na'] = unemp_data['gender_na'].fillna(0)
 unemp_data['edu_unknown'] = unemp_data['edu_unknown'].fillna(0)",data preprocessing,,
file164,12,"unemp_data['top_category_employer2'] = unemp_data.groupby('countyfips')['top_category_employer2'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'unknown'))
 unemp_data['top_category_employer3'] = unemp_data.groupby('countyfips')['top_category_employer3'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'unknown'))",data preprocessing,,
file164,13,"unemp_data['gender_male'] = unemp_data['gender_male'].astype(float)
 unemp_data['gender_female'] = unemp_data['gender_female'].astype(float)
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].astype(float)",data preprocessing,,
file164,14,"unemp_data['gender_male'] = unemp_data['gender_male'].fillna(unemp_data.groupby('countyfips')['gender_male'].transform('mean'))
 unemp_data['gender_female'] = unemp_data['gender_female'].fillna(unemp_data.groupby('countyfips')['gender_female'].transform('mean'))
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].fillna(unemp_data.groupby('countyfips')['edu_post_hs'].transform('mean'))",data preprocessing,,
file164,15,unemp_data[unemp_data.gender_male.isnull()],data exploration,,
file164,16,wage_data.describe(),data exploration,,
file164,17,wage_data['average_wage'] = wage_data['average_wage'].fillna(wage_data.groupby('countyfips')['average_wage'].transform('mean')),data preprocessing,,
file164,18,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
 final_data",data preprocessing,data exploration,
file164,19,"final_data1 = final_data.drop(['timeperiod','tract','tract_name'],axis=1)",data preprocessing,,
file164,20,final_data.info(),data exploration,,
file164,21,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y'],axis=1)",data preprocessing,,
file164,22,"fin = pd.get_dummies(final_data1,columns = ['week_number','countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file164,23,fin.shape(),data exploration,,
file164,24,"X = fin.drop(['total_claims'],axis=1)
 Y = fin['total_claims']",data preprocessing,,
file164,25,"from sklearn.decomposition import PCA
 pca = PCA(n_components=50)
 principalComponents = pca.fit_transform(X)",modeling,,
file164,26,principalComponents,data exploration,,
file164,27,df_X = pd.DataFrame(principalComponents),data preprocessing,,
file164,28,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 from sklearn.feature_selection import RFE
 X_train, X_test,Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=28)",helper functions,data preprocessing,
file164,29,"lm = LinearRegression().fit(X_train,Y_train)
 prediction = model.predict(X_test)",modeling,prediction,
file164,30,"lm.score(X_test, Y_test)",evaluation,,
file164,31,"mean_squared_error(Y_train, prediction)",evaluation,,
file164,32,"model = XGBRegressor(n_estimators=500, max_depth=4,eta=0.1).fit(X_train,Y_train)
 y_pred = model.predict(X_test)",modeling,prediction,
file164,33,import xgboost as xgb,helper functions,,
file164,34,get_ipython().system('pip install xgboost'),helper functions,,
file164,35,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file164,36,ttt = model.fit(pred_data),modeling,,
file164,37,pred = model.predict(pred_data),prediction,,
file164,38,pred_data.info(),data exploration,,
file164,39,unemp_data.describe(),data exploration,,
file164,40,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
 tt",data preprocessing,data exploration,
file164,41,fin = final_data1.set_index('uu_id'),data preprocessing,,
file164,42,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y'],axis=1)",data preprocessing,,
file164,43,"fin = pd.get_dummies(final_data1,columns = ['week_number','countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file164,44,"X_train_data = X_train.drop(['uu_id'],axis=1)
 X_test_data = X_test.drop(['uu_id'],axis=1)",data preprocessing,,
file164,45,"fin = fin.set_index('uu_id')
 X_train, X_test, y_train, y_test = test_train_split(fin.ix[:, ~fin.columns.isin(['total_claims'])], fin.total_claims)",data preprocessing,,
file164,46,"fin = fin.set_index('uu_id')
 X = fin.drop(['total_claims'],axis=1)
 Y=fin['total_claims']
 #X_train, X_test, y_train, y_test = train_test_split(fin.ix[:, ~fin.columns.isin(['total_claims'])], fin.total_claims)",data preprocessing,,
file164,47,"X = fin.drop(['uu_id','total_claims'],axis=1)
 Y=fin['total_claims']",data preprocessing,,
file164,48,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')",data preprocessing,,
file164,49,final_data.info(),data exploration,,
file164,50,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 X_train, X_test,Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=32)",helper functions,data preprocessing,
file164,51,"lm = LinearRegression().fit(X_train,Y_train)
 prediction = lm.predict(X_test)",modeling,prediction,
file164,52,"lm.score(X_test, Y_test)",evaluation,,
file164,53,"import xgboost as xgb
 model = xgb.XGBRegressor(n_estimators=500, max_depth=4,eta=0.1).fit(X_train,Y_train)
 y_pred = model.predict(X_test)",modeling,prediction,
file164,54,"df_pred_final = pred_data[['uu_id']]
 df_pred_final['week_number'] = pred_data[['week_number']]
 df_pred_final[""total_claims""] = y_pred",data preprocessing,,
file164,55,"Y_test_final = pred_data.merge(fin,how='left',on= 'uu_id')",data preprocessing,,
file164,56,Y_test_final,data exploration,,
file164,57,"df_test = fin[fin.uu_id == i for i in pred_data['uu_id']]
 df_test",data preprocessing,data exploration,
file164,58,Y_test_final = pred_data.join(fin),data preprocessing,,
file164,59,"pred_data = pred_data.set_index('uu_id')
 fin = fin.set_index('uu_id)
 Y_test_final = pred_data.join(fin)",data preprocessing,,
file164,60,"X_df_test = df_test.drop(['uu_id','total_claims'],axis=1)
 Y_df_test = df_test['total_claims']",data preprocessing,,
file164,61,prediction = model.predict(X_df_test),prediction,,
file164,62,df_test,data exploration,,
file164,63,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y','week_number'],axis=1)",data preprocessing,,
file164,64,"fin = pd.get_dummies(final_data1,columns = ['countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file164,65,"model.score(X_df_test,Y_df_test)",evaluation,,
file164,66,df_pred_final = pred_data[['uu_id']],data preprocessing,,
file164,67,"df_pred_final_yams=[]
 df_pred_final_yams == pred_data[['uu_id']]",data preprocessing,,
file164,68,df_pred_final_1['week_number'] = df_test[['week_number']],data preprocessing,,
file164,69,df_pred_final_1['total_claims'] = prediction,data preprocessing,,
file164,70,df_pred_final_1,data exploration,,
file164,71,df_final = df_pred_final_1.drop_duplicates(subset=['uu_id']),data preprocessing,,
file164,72,df_final,data exploration,,
file164,73,fin.shape,data exploration,,
file164,74,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file164,75,df_final.to_csv('submission_prediction_output.csv'),save results,,
file165,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file165,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file165,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file165,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file165,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file165,5,data.info(),data exploration,,
file165,6,"data[""top_category_employer2""].value_counts()",data exploration,,
file165,7,"data.loc[data[""uu_id""] == ""bd5b040eae3010ce09da8b176ed5aee0""]",data exploration,,
file165,8,"data[""edu_8th_or_less""].value_counts()",data exploration,,
file165,9,"data[""race_black""].value_counts()",data exploration,,
file165,10,"data[""gender_female""].value_counts()",data exploration,,
file166,0,"get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\n!pip install keras\n!pip install tensorflow\n')",helper functions,,
file166,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file166,2,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\nfrom keras.models import load_model\nimport joblib\nfrom joblib import Parallel, delayed\nfrom scipy import stats\nfrom sklearn.ensemble import IsolationForest\n')",helper functions,,
file166,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file166,4,"query_main = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file166,5,"query_job = bigquery_client.query(query_main)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file166,6,"def evaluate_regressor(prediction_dataframe):
  # Takes in a prediction dataframe of 2 columns, Actual values and Predicted values generated by a regressor
  # Outputs MSE, MAR, RMSE and MAPE metrics. Must have columns named Actual and Predicted.
  print('MSE:', mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('MAE:', mean_absolute_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('RMSE:', np.sqrt(mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted'])))
  print('MAPE:', np.mean(np.abs((prediction_dataframe['Actual'] - prediction_dataframe['Predicted']) / prediction_dataframe['Actual'])) * 100)",data exploration,,
file166,7,"def get_predictions(regressor, model_type, name, week):
  # generates predictions for any model and writes out a dataframe in csv containing them
  # takes a regressor and learning method type as input: DL and ML
  # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape
  result_list = []
  uu_id_transform = LE.fit_transform(prediction_list['uu_id'])
  if model_type == 'DL':
  predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_list = np.array(result_list)
  result_list = np.reshape(result_list, (525,))
  elif model_type == 'ML':
  predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_df = pd.DataFrame(result_list, columns = ['Predictions'])
  prediction_sub = prediction_list.copy()
  prediction_sub['total_claims'] = result_df.values
  prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]
  os.makedirs('lost+found/submission_files', exist_ok=True)
  prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)
  return prediction_sub",prediction,,
file166,8,"def get_pred_frame(test_frame, prediction_array):
  prediction_frame = pd.DataFrame({'Actual': test_frame, 'Predicted': prediction_array.flatten()})
  return prediction_frame",prediction,,
file166,9,"def clearOutlier_IQR(data):
  Q1 = data.quantile(0.25)
  Q3 = data.quantile(0.75)
  IQR = Q3 - Q1
  no_outliers = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]
  print(no_outliers.shape)
  return no_outliers",data preprocessing,,
file166,10,"# outlier detection and handling - Z Score (gaussian only)
 def clearOutlier_ZScore(data, threshold):
  zscore = np.abs(stats.zscore(data))
  thresh = threshold
  no_outliers = data[(zscore < thresh).all(axis=1)]
  return no_outliers",data preprocessing,,
file166,11,"# outlier detection - automatic
 def IsoForest_anomaly(data):
  IFO = IsolationForest(random_state=69)
  col_list = ['week_number', 'total_claims', 'edu_8th_or_less',
  'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female',
  'gender_male', 'race_amerindian', 'race_asian', 'race_black',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
  NO_df = data.copy()
  IFO.fit(data)
  NO_df['anomaly_scores'] = IFO.decision_function(data)
  NO_df['anomaly'] = IFO.predict(data)
  no_outlier = NO_df[NO_df['anomaly'] == 1]
  print('Removed ', NO_df[NO_df['anomaly'] == -1].shape[0], 'datapoints')
  palette = ['#ff7f0e','#1f77b4']
  sns.pairplot(NO_df, vars = col_list, hue='anomaly', palette=palette)
  no_outlier.drop(['anomaly_scores','anomaly'], axis = 1, inplace=True)
  return no_outlier",data preprocessing,,
file166,12,"def preprocess(data, scaling):
  no_outlierDF = ingest.copy()
  to_drop = ['timeperiod','tract','top_category_employer1','top_category_employer2',
  'top_category_employer3','tract_name','countyfips', 'edu_unknown', 'gender_na', 
  'race_noanswer']
  to_scale = ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 
  'gender_female', 'gender_male', 'race_amerindian', 'race_asian', 'race_black', 
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
  no_outlierDF.drop(to_drop, axis=1, inplace=True)
  no_outlierDF['uu_id'] = LE.fit_transform(no_outlierDF['uu_id'])
  if scaling == 'Robust':
  no_outlierDF[to_scale] = RB_other.fit_transform(no_outlierDF[to_scale])
  elif scaling == 'Standard':
  no_outlierDF[to_scale] = SC_other.fit_transform(no_outlierDF[to_scale])
  return no_outlierDF",data preprocessing,,
file166,13,"# updated_ingest = pd.concat([merged_ingest, combined_ingest])
 ingest = pd.read_csv('lost+found/submission_files/complete_ingest.csv')",data preprocessing,,
file166,14,"# quick preprocess to keep uu_id and scale values
 from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler
 LE = LabelEncoder()
 RB_other = RobustScaler()
 SC_other = StandardScaler()
 # RB_claims = RobustScaler()",modeling,,
file166,15,"ML_data = preprocess(ingest, 'Robust')
 ingest_clean = IsoForest_anomaly(ML_data)",data preprocessing,,
file166,16,"# set target and independent variables
 Y = ingest_clean['total_claims']
 X = ingest_clean[['uu_id', 'week_number', 'edu_8th_or_less',
  'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female',
  'gender_male', 'race_amerindian', 'race_asian', 'race_black',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]",data preprocessing,,
file166,17,"# import
 X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=69)",data preprocessing,,
file166,18,"# load model - next time, I'll need to learn to use PMML
 RFR_Regressor = joblib.load('RF_v1-7.pkl')",modeling,,
file166,19,"Y_pred_RFR = RFR_Regressor.predict(X_test.values).reshape(-1,1)",prediction,,
file166,20,"# evaluate
 evaluate_regressor(get_pred_frame(Y_test,Y_pred_RFR))",evaluation,,
file166,21,"# call func
 get_predictions(RFR_Regressor, 'ML', 'submission_prediction_output_RFR', 44)",prediction,,
file166,22,"plt.figure(figsize=(15,10))
 plt.plot(Y_test.values, color = 'red', label = 'Actual values')
 plt.plot(Y_pred_RFR, color='blue', label='Predicted values')
 plt.title('Model Prediction Visual')
 plt.legend()
 plt.show()",result visualization,,
file166,23,"# this needs different feature engineering, so I'm starting from scratch
 DL_data = preprocess(ingest, 'Standard')",data preprocessing,,
file166,24,DL_ingest = IsoForest_anomaly(DL_data),data preprocessing,,
file166,25,"# split set
 DL_Y = DL_ingest['total_claims']
 DL_X = DL_ingest[['uu_id', 'week_number', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male',
  'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 DL_XTrain, DL_XTest, DL_YTrain, DL_YTest = train_test_split(DL_X, DL_Y, test_size=0.20, random_state=69)",data preprocessing,,
file166,26,"# change to np vectors
 DL_XTrain = DL_XTrain.to_numpy()
 DL_XTest = DL_XTest.to_numpy()",data preprocessing,,
file166,27,"# reshape because F*** tensors
 DL_XTrain = np.reshape(DL_XTrain, (DL_XTrain.shape[0], DL_XTrain.shape[1], 1))",data preprocessing,,
file166,28,"# convert X and Y train to float because input dtype accepts floats only
 DL_YTrain = DL_YTrain.astype(float)
 DL_XTrain = DL_XTrain.astype(float)",data preprocessing,,
file166,29,"# load model
 StackLSTM_Regressor = load_model('BiDLSTM_v1-05.h5')",modeling,,
file166,30,"# summary for viewers
 StackLSTM_Regressor.summary()",evaluation,,
file166,31,"# float cast
 DL_XTest = DL_XTest.astype(float)
 # make predictions
 DL_XTest = np.reshape(DL_XTest, (DL_XTest.shape[0], DL_XTest.shape[1],1))
 predictions = StackLSTM_Regressor.predict(DL_XTest)",prediction,,
file166,32,"get_pred_frame(DL_YTest, predictions)",prediction,,
file166,33,"evaluate_regressor(get_pred_frame(DL_YTest, predictions))",evaluation,,
file166,34,"def get_predictions(regressor, model_type, name, week):
  # generates predictions for any model and writes out a dataframe in csv containing them
  # takes a regressor and learning method type as input: DL and ML
  # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape
  result_list = []
  uu_id_transform = LE.fit_transform(prediction_list['uu_id'])
  if model_type == 'DL':
  predict_arr = np.array(SC_other.transform([[0, 1, 6, 4, 4, 5, 0, 1, 2, 0, 4, 1, 4200.0]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_list = np.array(result_list)
  result_list = np.reshape(result_list, (525,))
  elif model_type == 'ML':
  predict_arr = np.array(RB_other.transform([[0, 1, 6, 4, 4, 5, 0, 1, 2, 0, 4, 1, 4200.0]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_df = pd.DataFrame(result_list, columns = ['Predictions'])
  prediction_sub = prediction_list.copy()
  prediction_sub['total_claims'] = result_df.values
  prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]
  os.makedirs('lost+found/submission_files', exist_ok=True)
  prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)
  return prediction_sub",prediction,,
file167,0,"import numpy as np
 import pandas as pd
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 pd.set_option('display.max_columns', None)",helper functions,,
file167,1,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file167,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file167,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file167,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 df = query_job.to_dataframe()",load data,data preprocessing,
file167,5,df.head(),data exploration,,
file167,6,df.info,data exploration,,
file167,7,df.describe(),data exploration,,
file168,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file168,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file168,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file168,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file168,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file168,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file168,6,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable
 X.head()",data preprocessing,,
file169,0,"import os
 import pandas as pd
 import db_dtypes
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file169,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file169,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file169,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file169,4,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.shape",data preprocessing,data exploration,
file169,5,k = unemployment_data.copy(),data preprocessing,,
file169,6,"## number of unique ids are matching the number of entries in the wage_data set
 import numpy as np
 pd.unique(k.uu_id).shape",helper functions,data exploration,
file169,7,"wage_data = pd.DataFrame(wage_data)
 wage_data.head()",data preprocessing,data exploration,
file169,8,"wage_data = wage_data.drop_duplicates()
 wage_data.shape
 ## no duplicates here!",data preprocessing,data exploration,
file169,9,pd.unique(wage_data.uu_id).shape,data exploration,,
file169,10,"## lets join the 2 datasets on uu_id
 unemployment_data.columns, wage_data.columns",data exploration,,
file169,11,"data=pd.merge(unemployment_data,wage_data, how='inner')
 print(data.shape)",data preprocessing,data exploration,
file169,12,data.columns,data exploration,,
file169,13,"pd.set_option('display.max_columns', None) # or 1000
 pd.set_option('display.max_rows', None) # or 1000
 pd.set_option('display.max_colwidth', None) # or 199
 data.head()",helper functions,data exploration,
file169,14,data.isna().sum(),data exploration,,
file169,15,"def breakcolumn(a,data):
  df=pd.DataFrame()
  df[['Value1', 'Value2']] = data[a].str.split('-', 1, expand=True)
  ## replace the null values by the value before hypen
  df['Value2'].fillna(df['Value1'],inplace=True)
 

  df['Value1'] = pd.to_numeric(df['Value1'])
  df['Value2'] = pd.to_numeric(df['Value2'])
 

  df['Value3'] = (df['Value1']+df['Value2'])//2
  data[a] = df['Value3']",data preprocessing,,
file169,16,"data1 = data.copy()
 obj_list = ['top_category_employer1','top_category_employer2','top_category_employer3']
 for i in obj_list:
  data1[i].replace('N/A',0,inplace=True)
  breakcolumn(i,data1)",data preprocessing,,
file169,17,data1.head(),data exploration,,
file169,18,data1.info(),data exploration,,
file169,19,"data1['race_black'].fillna(0,inplace=True)
 data1['race_other'].fillna(0,inplace=True)
 data1['club_races'] = data1['race_black'] + data1['race_other']
 data1.drop(['race_black','race_other'],axis=1,inplace=True)",data preprocessing,,
file169,20,"data1.drop(['gender_male','gender_male','race_white','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs'],axis=1,inplace=True)
 data1.info()",data preprocessing,data exploration,
file169,21,"data1.fillna(method='bfill',inplace=True)
 data1.info()",data preprocessing,data exploration,
file169,22,"data1['race_asian'] = data1['race_asian'].fillna(int(np.mean(data1['race_asian'])))
 data1['race_noanswer'] = data1['race_noanswer'].fillna(int(np.mean(data1['race_noanswer'])))
 data1['edu_unknown'] = data1['edu_unknown'].fillna(int(np.mean(data1['edu_unknown'])))
 data1['gender_female'] = data1['gender_female'].fillna(int(np.mean(data1['gender_female'])))
 data1['top_category_employer3'] = data1['top_category_employer3'].fillna(int(np.mean(data1['top_category_employer3'])))",data preprocessing,,
file169,23,"from sklearn import preprocessing
 # label_encoder object knows how to understand word labels. 
 label_encoder = preprocessing.LabelEncoder()
 # Encode labels in column 'Country'. 
 data1['tract_name']= label_encoder.fit_transform(data1['tract_name'])",modeling,,
file169,24,"data2 = data1.copy()
 data1['uu_id']= label_encoder.fit_transform(data1['uu_id'])",modeling,,
file169,25,"X = data1.drop('total_claims',axis=1)
 y = data1['total_claims']
 from sklearn.model_selection import train_test_split
 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 42)",data preprocessing,,
file169,26,"from sklearn.linear_model import LinearRegression
 linreg = LinearRegression()
 linreg.fit(X_train,y_train)
 y_pred = linreg.predict(X_test)",modeling,prediction,
file169,27,"y_pred = np.round(y_pred)
 from sklearn.metrics import mean_squared_error
 mean_squared_error(y_test,y_pred)",evaluation,,
file169,28,y_pred,data exploration,,
file169,29,"## MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",data preprocessing,,
file169,30,"print(MAPE(y_test,y_pred))",data exploration,,
file169,31,"from sklearn.ensemble import RandomForestRegressor
 rfr = RandomForestRegressor(n_estimators = 500, random_state = 0)
 rfr.fit(X_train, y_train)",modeling,,
file169,32,y_pred = rfr.predict(X_test),prediction,,
file169,33,y_pred = np.round(y_pred),data preprocessing,,
file169,34,X2 = X.copy(),data preprocessing,,
file169,35,X2 = X2.apply(lambda iterator: ((iterator - iterator.mean())/iterator.std()).round(2)),data preprocessing,,
file169,36,"X_train,X_test,y_train,y_test = train_test_split(X2,y,test_size = 0.25,random_state = 42)",data preprocessing,,
file169,37,"get_ipython().system('pip install xgboost')
 from xgboost.sklearn import XGBRegressor
 regressor = XGBRegressor(
  n_estimators=500,
  reg_lambda=1,
  gamma=0,
  max_depth=3)",modeling,,
file169,38,"regressor.fit(X_train, y_train)
 y_pred = regressor.predict(X_test)
 mean_squared_error(y_test,y_pred)",prediction,,
file169,39,"from xgboost.sklearn import XGBRegressor
 from sklearn.model_selection import GridSearchCV",helper functions,,
file169,40,"xgb1 = XGBRegressor()
 parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
  'objective':['reg:linear'],
  'learning_rate': [.03, 0.05, .07], #so called `eta` value
  'max_depth': [5, 6, 7],
  'min_child_weight': [4],
  'subsample': [0.7],
  'colsample_bytree': [0.7],
  'n_estimators': [400,500,600,100]}",modeling,,
file169,41,"xgb_grid = GridSearchCV(xgb1,
  parameters,
  cv = 2,
  n_jobs = 5,
  verbose=True)",modeling,,
file169,42,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file169,43,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 prediction = query_job.to_dataframe()",load data,,
file169,44,"print(prediction.shape)
 pd.DataFrame(prediction).head()",data exploration,,
file169,45,"data2 = data2.drop_duplicates(subset=['uu_id'],keep='last')",data preprocessing,,
file169,46,"data2 = data2.set_index('uu_id')
 data2.head()",data preprocessing,data exploration,
file169,47,"final_prediction = data2.join(prediction.set_index('uu_id'),on='uu_id',rsuffix='_other')
 final_prediction.head()",data preprocessing,data exploration,
file169,48,"final_prediction_data = pd.DataFrame()
 final_prediction_data['index'] = final_prediction.index
 final_prediction_data['week_number_other'] = final_prediction.week_number_other",data preprocessing,,
file169,49,"final_prediction = final_prediction.drop(['week_number_other'], axis=1)
 final_prediction.reset_index(drop=True, inplace=True)",data preprocessing,,
file169,50,"future = final_prediction.values
 future_weeks_pred = rfr.predict(future)
 print(future_weeks_pred.shape)",prediction,,
file169,51,"prediction['total_claims'] = future_weeks_pred.astype('int')
 prediction.columns = ['uuid','week','count']
 print(prediction)",prediction,,
file169,52,"prediction.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file169,53,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file169,54,"pip install db_dtypes
 import os
 import pandas as pd
 import db_dtypes
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file170,0,"import os
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file170,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemp_data = query_job.to_dataframe()
 unemp_data.head(5)",load data,data preprocessing,
file170,2,unemp_data.info(),data exploration,,
file170,3,"unemp_data['top_category_employer2'] = unemp_data['top_category_employer2'].replace('N/A',np.NaN)
 unemp_data['top_category_employer3'] = unemp_data['top_category_employer3'].replace('N/A',np.NaN)",data preprocessing,,
file170,4,unemp_data['race_asian'] = unemp_data.groupby('countyfips')['race_asian'].transform(lambda x: x.fillna(int(x.mean()))),data preprocessing,,
file170,5,"unemp_data['race_hawaiiannative'] = unemp_data.groupby('countyfips')['race_hawaiiannative'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_other'] = unemp_data.groupby('countyfips')['race_other'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_white'] = unemp_data.groupby('countyfips')['race_white'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file170,6,"unemp_data['race_noanswer'] = unemp_data['race_noanswer'].fillna(0)
 unemp_data['gender_na'] = unemp_data['gender_na'].fillna(0)
 unemp_data['edu_unknown'] = unemp_data['edu_unknown'].fillna(0)",data preprocessing,,
file170,7,unemp_data['edu_post_hs'] = unemp_data.groupby('countyfips')['edu_post_hs'].transform(lambda x: x.fillna(int(x.mean()) if not x.isnull() else 0)),data preprocessing,,
file170,8,"import numpy as np
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].replace(NA,np.NaN)
 unemp_data['edu_post_hs'] = unemp_data.groupby('countyfips')['edu_post_hs'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file170,9,unemp_data['edu_post_hs'],data preprocessing,,
file170,10,"#unemp_data['gender_female'] = unemp_data.groupby('countyfips')['gender_female'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['gender_male'] = unemp_data.groupby('countyfips')['gender_male'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file170,11,unemp_data.isna().sum(),data exploration,,
file170,12,unemp_data['top_category_employer2'] = unemp_data.groupby('countyfips')['top_category_employer2'].transform(lambda x: x.fillna(int(x.mode()[0] if not x.mode().empty else 'unknown'))),data preprocessing,,
file170,13,unemp_data['edu_post_hs'] = unemp_data.groupby('countyfips')['edu_post_hs'].fillna(0),data preprocessing,,
file170,14,unemp_data['gender_male'] = unemp_data['gender_male'].astype(float),data preprocessing,,
file170,15,"unemp_data['gender_female'] = unemp_data['gender_female'].fillna(unemp_data.groupby('countyfips')['gender_female'].transform('mean'))
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].fillna(unemp_data.groupby('countyfips')['edu_post_hs'].transform('mean'))",data preprocessing,,
file170,16,"unemp_data['gender_male'] = unemp_data['gender_male'].astype(float)
 unemp_data['gender_female'] = unemp_data['gender_female'].astype(float)
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].astype(float)",data preprocessing,,
file171,0,"import numpy as np
 import pandas as pd",helper functions,,
file171,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file171,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 df = query_job.to_dataframe()",load data,data preprocessing,
file171,3,"import numpy as np
 import pandas as pd
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file171,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file171,5,df.head(),data exploration,,
file171,6,df.info,data exploration,,
file171,7,df.describe(),data exploration,,
file171,8,df.value_counts(),data exploration,,
file171,9,df.value_counts().group('race_other'),data exploration,,
file171,10,df.describe(include='O'),data exploration,,
file172,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file172,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install db-dtypes\n!python3 -m pip install pandas\n!pip install pmdarima\n!pip install plotly==5.11.0\n!pip install scikit-hts[auto-arima]\n"")",helper functions,,
file172,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima
 import collections.abc
 collections.Iterable = collections.abc.Iterable
 collections.Mapping = collections.abc.Mapping
 collections.MutableSet = collections.abc.MutableSet
 collections.MutableMapping = collections.abc.MutableMapping
 import hts
 from hts.hierarchy import HierarchyTree
 from hts.model import AutoArimaModel
 from hts import HTSRegressor",helper functions,,
file172,3,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",load data,,
file172,4,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data preprocessing,,
file172,5,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",data preprocessing,,
file172,6,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: date for the first day, month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",data preprocessing,,
file172,7,"def MSPE(s1, s2):
  return(sum((s1 - s2)**2)/len(s1))",data preprocessing,,
file172,8,"def ARIMA_predict(df_input, cutoff_rate = 0.8, n_period = 15):
  cutoff = int(cutoff_rate * len(df_input))
  if cutoff_rate < 1:
  valid = df_input[cutoff:]
  train = df_input[:cutoff]
  model = auto_arima(train, trace=False, error_action='ignore', suppress_warnings=True)
  model.fit(train)
  forecast = model.predict(n_period)
  return(forecast)",modeling,,
file172,9,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file172,10,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file172,11,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file172,12,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file172,13,"# Explore input data for NA and special values
 dataExplore(data)
 # dataExplore(data_pred_query)
 # data_pred_query.head()
 # data.head()",data exploration,,
file172,14,"# Further check tracts with average_wage as Nan
 # I find three tracts with all average_wage as Nan. If I drop these tracts due to Nan value, they cannot be predicted
 for id in pd.unique(data[data['average_wage'].isna()][""uu_id""]):
  print(id)
  print(""All value are nan?"", data[data['uu_id'] == id][""average_wage""].isnull().all())
  print(""Included in prediction list?"", len(data_pred_query[data_pred_query['uu_id'] == id]) > 0)",data exploration,,
file172,15,"# Backup the data before pre-treatment
 data_backup = data.copy()
 data_pred_query_backup = data_pred_query.copy()",data preprocessing,,
file172,16,"# Pretreatment: convert week_number to month and week of month, to capture seasonality
 data = dataIdentifyDWM(data)",data preprocessing,,
file172,17,"# Check if the dataset is a balance panel (all tracts have value for all time periods)
 # 54% of tracts has less than 35 observations (total number of full time series), indicating it is unbalanced
 # Even if only checking data afer 2022/6/1, there are still 36% of tracts with incomplete series
 dataBalanceCheck(data)
 dataBalanceCheck(data[data[""date""] > ""2022-06-01""])",data preprocessing,,
file172,18,"# To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file172,19,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,,
file172,20,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file172,21,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file172,22,dataBalanceCheck(data_balance),data preprocessing,,
file172,23,"# Fill missing values of country_fips and tract
 data_balance.sort_values(by='uu_id',inplace=True)
 data_balance = data_balance.fillna(method='ffill')
 data_balance.sort_index()",data preprocessing,,
file172,24,"# Data clean up: convert NA to 0 for gender, race, education and top employer and recalculate unknown category
 # Based on the check of Nan in average_wage above, I also convert Nan to zero as well, but try models with and without ""average_wage"" variable
 data = dataFillNa(data, 0)",data preprocessing,,
file172,25,"# Split data to training and validaton sets
 # Max trainweek is 37, use a 80 / 20 rule
 train_week = int(max(pd.unique(data[""week_number""]))*0.8)",data preprocessing,,
file172,26,"data_train = data[data[""week_number""] < train_week]
 data_valid = data[data[""week_number""] >= train_week]",data preprocessing,,
file172,27,"data_train_x = data_train.drop(""total_claims"",1)
 data_train_y = data_train[""total_claims""]",data preprocessing,,
file172,28,"data_balance_valid_x = data_balance_valid.drop(""total_claims"",1)
 data_balance_valid_y = data_balance_valid[""total_claims""]",data preprocessing,,
file172,29,data_balance_valid_y.shape,data exploration,,
file172,30,"# Model 1 : Poisson regression with unbalanced data
 data_train_x_m1 = data_train_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_train_x_m1[""month""] = data_train_x_m1[""month""].astype(str)
 data_train_x_m1[""weekofmonth""] = data_train_x_m1[""weekofmonth""].astype(str)
 data_train_x_m1[""week_number2""] = data_train_x_m1[""week_number""]**2
 data_train_x_m1 = pd.get_dummies(data_train_x_m1)",data preprocessing,,
file172,31,"data_train_x_m1[""month_8""] = 0
 data_train_x_m1[""month_9""] = 0",data preprocessing,,
file172,32,"for i in range(8):
  data_valid_x_m1[""month_""+str(1+i)] = 0",data preprocessing,,
file172,33,"for i in range(5):
  if i == 1:
  pass
  data_valid_x_m1[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file172,34,"poission_model = sm.GLM(data_train_y.astype(int), data_train_x_m1.astype(float), family=sm.families.Poisson())
 result = poission_model.fit()
 result.summary()",modeling,,
file172,35,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
 print(""MAPE: "", MAPE(data_estimate_m1, data_valid_y))
 print(""MSPE: "", MSPE(data_estimate_m1, data_valid_y))",prediction,,
file172,36,"data_balance_valid_x_m1 = data_balance_valid_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_balance_valid_x_m1[""month""] = data_balance_valid_x_m1[""month""].astype(str)
 data_balance_valid_x_m1[""weekofmonth""] = data_balance_valid_x_m1[""weekofmonth""].astype(str)
 data_balance_valid_x_m1[""week_number2""] = data_balance_valid_x_m1[""week_number""]**2
 data_balance_valid_x_m1 = pd.get_dummies(data_balance_valid_x_m1)",data preprocessing,,
file172,37,"# Model 3 time series
 # First, visualize average total_claim
 data_balance_ave = data_balance[[""week_number"", ""total_claims"", ""uu_id""]]
 data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave = data_balance_ave.reset_index()
 data_balance_ave['MA4'] = data['total_claims'].rolling(4).mean()
 fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""])
 fig.show()",data preprocessing,,
file172,38,"# Model 4 Hierarchical ARIMA model
 # data_balance.groupby(""countyfips"")[""uu_id""].apply(set).to_frame()
 data_balance[""county_tract""] = data_balance.apply(lambda x: f""{x['countyfips']}_{x['uu_id']}"", axis=1)
 data_balance_hts = data_balance.copy()
 data_balance_hts = data_balance_hts[[""date"", ""uu_id"", ""countyfips"", ""total_claims"", ""county_tract""]]
 data_balance_hts[""countyfips""] = data_balance_hts[""countyfips""].astype(str)
 data_balance_hts.sort_values(by='date',inplace=True)",data preprocessing,,
file172,39,data_balance_hts,data exploration,,
file172,40,"df_bottom_level = data_balance_hts.pivot(index=""date"", columns=""county_tract"", values=""total_claims"")
 df_middle_level = data_balance_hts.groupby([""date"", ""countyfips""]) \
  .sum() \
  .reset_index(drop=False) \
  .pivot(index=""date"", columns=""countyfips"", values=""total_claims"")
 df_total = data_balance_hts.groupby(""date"")[""total_claims""] \
  .sum() \
  .to_frame() \
  .rename(columns={""total_claims"": ""total""})
 hierarchy_df = df_bottom_level.join(df_middle_level) \
  .join(df_total)
 hierarchy_df.index = pd.to_datetime(hierarchy_df.index)
 hierarchy_df = hierarchy_df.resample(""7D"") \
  .sum()",data preprocessing,,
file172,41,"print(f""Number of time series at the bottom level: {df_bottom_level.shape[1]}"")
 print(f""Number of time series at the middle level: {df_middle_level.shape[1]}"")
 hierarchy_df = hierarchy_df.fillna(0)",data exploration,,
file172,42,"county = data_balance_hts[""countyfips""].unique()
 tract = data_balance_hts[""county_tract""].unique()",data preprocessing,,
file172,43,"total = {'total': list(county)}
 county = {k: [v for v in tract if v.startswith(k)] for k in county}
 hierarchy = {**total, **county}",data preprocessing,,
file172,44,"ax = hierarchy_df[hierarchy['total']].plot(title=""Trips - state level"")
 ax.legend(bbox_to_anchor=(1.0, 1.0));",data preprocessing,,
file172,45,"data_balance[data_balance[""uu_id""] == ""001cd9ae23064d7f0fd3cd327c873d8d""]",data exploration,,
file172,46,"# Explore input data for NA and special values
 dataExplore(data)
 # dataExplore(data_pred_query)
 # data_pred_query.head()
 # data.head()",data exploration,,
file172,47,"data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)",data preprocessing,,
file172,48,"pd.unique(data[""week_number""])",data exploration,,
file172,49,"# To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file172,50,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,,
file172,51,"data_balance = data_balance.set_index('week_number',append=True)",data preprocessing,,
file172,52,data_balance.head,data exploration,,
file172,53,"data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file172,54,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])",data preprocessing,,
file172,55,"uu_id_list = pd.unique(data_balance[""uu_id""])
 # In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
 for i in range(len(uu_id_list)):
  print(i)
  print(data_balance[data_balance[""uu_id""] == uu_id_list[i] and data_balance[""week_numer""] == 4])
  break",data preprocessing,,
file172,56,"uu_id_list = pd.unique(data_balance[""uu_id""])
 # In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
 for i in range(len(uu_id_list)):
  print(i)
  
  print(data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']])
  print(data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']])
  print(data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']])
 

  break",data preprocessing,,
file172,57,"uu_id_list = pd.unique(data_balance[""uu_id""])
 # In checking, I find the total_claim for week number 4 and 23 are missing over all tracts. Use the average value before and after to replace
 for i in range(len(uu_id_list)):
  print(i)
  week_3 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 3), ['total_claims']]
  week_5 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 5), ['total_claims']]
  
  data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 4), ['total_claims']] = (week_3 + week_3)/2
  
  
  week_22 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 22), ['total_claims']]
  week_24 = data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 24), ['total_claims']]
  
  data_balance.loc[(data_balance.uu_id == uu_id_list[i]) & (data_balance.week_number == 23), ['total_claims']] = (week_22 + week_24)/2
  break",data preprocessing,,
file172,58,data = data_backup.copy(),data preprocessing,,
file172,59,"# Pretreatment: convert week_number to month and week of month, to capture seasonality
 data = dataIdentifyDWM(data)",data preprocessing,,
file172,60,"# Check if the dataset is a balance panel (all tracts have value for all time periods)
 # 54% of tracts has less than 35 observations (total number of full time series), indicating it is unbalanced
 # Even if only checking data afer 2022/6/1, there are still 36% of tracts with incomplete series
 dataBalanceCheck(data)
 dataBalanceCheck(data[data[""date""] > ""2022-06-01""])",data preprocessing,,
file172,61,data_balance = dataIdentifyDWM(data_balance),data preprocessing,,
file172,62,dataBalanceCheck(data_balance),data preprocessing,,
file172,63,"# Fill missing values of country_fips and tract
 data_balance.sort_values(by='uu_id',inplace=True)
 data_balance = data_balance.fillna(method='ffill')
 data_balance.sort_index()",data preprocessing,,
file172,64,"# Data clean up: convert NA to 0 for gender, race, education and top employer and recalculate unknown category
 # Based on the check of Nan in average_wage above, I also convert Nan to zero as well, but try models with and without ""average_wage"" variable
 data = dataFillNa(data, 0)",data preprocessing,,
file172,65,"# Split data to training and validaton sets
 # Max trainweek is 37, use a 80 / 20 rule
 train_week = int(max(pd.unique(data[""week_number""]))*0.8)",data preprocessing,,
file172,66,"data_train = data[data[""week_number""] < train_week]
 data_valid = data[data[""week_number""] >= train_week]",data preprocessing,,
file172,67,"data_train_x = data_train.drop(""total_claims"",1)
 data_train_y = data_train[""total_claims""]",data preprocessing,,
file172,68,"data_balance_valid_x = data_balance_valid.drop(""total_claims"",1)
 data_balance_valid_y = data_balance_valid[""total_claims""]",data preprocessing,,
file172,69,data_balance_valid_y.shape,data exploration,,
file172,70,"# Model 1 : Poisson regression with unbalanced data
 data_train_x_m1 = data_train_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_train_x_m1[""month""] = data_train_x_m1[""month""].astype(str)
 data_train_x_m1[""weekofmonth""] = data_train_x_m1[""weekofmonth""].astype(str)
 data_train_x_m1[""week_number2""] = data_train_x_m1[""week_number""]**2
 data_train_x_m1 = pd.get_dummies(data_train_x_m1)",data preprocessing,,
file172,71,"data_train_x_m1[""month_8""] = 0
 data_train_x_m1[""month_9""] = 0",data preprocessing,,
file172,72,"for i in range(8):
  data_valid_x_m1[""month_""+str(1+i)] = 0",data preprocessing,,
file172,73,"for i in range(5):
  if i == 1:
  pass
  data_valid_x_m1[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file172,74,"poission_model = sm.GLM(data_train_y.astype(int), data_train_x_m1.astype(float), family=sm.families.Poisson())
 result = poission_model.fit()
 result.summary()",modeling,,
file172,75,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
 print(""MAPE: "", MAPE(data_estimate_m1, data_valid_y))
 print(""MSPE: "", MSPE(data_estimate_m1, data_valid_y))",prediction,data exploration,
file172,76,"data_balance_valid_x_m1 = data_balance_valid_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_balance_valid_x_m1[""month""] = data_balance_valid_x_m1[""month""].astype(str)
 data_balance_valid_x_m1[""weekofmonth""] = data_balance_valid_x_m1[""weekofmonth""].astype(str)
 data_balance_valid_x_m1[""week_number2""] = data_balance_valid_x_m1[""week_number""]**2
 data_balance_valid_x_m1 = pd.get_dummies(data_balance_valid_x_m1)",data preprocessing,,
file172,77,"# Model 3 time series
 # First, visualize average total_claim
 data_balance_ave = data_balance[[""week_number"", ""total_claims"", ""uu_id""]]
 data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave = data_balance_ave.reset_index()
 data_balance_ave['MA4'] = data['total_claims'].rolling(4).mean()
 fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""])
 fig.show()",modeling,,
file172,78,"# Model 4 Hierarchical ARIMA model
 # data_balance.groupby(""countyfips"")[""uu_id""].apply(set).to_frame()
 data_balance[""county_tract""] = data_balance.apply(lambda x: f""{x['countyfips']}_{x['uu_id']}"", axis=1)
 data_balance_hts = data_balance.copy()
 data_balance_hts = data_balance_hts[[""date"", ""uu_id"", ""countyfips"", ""total_claims"", ""county_tract""]]
 data_balance_hts[""countyfips""] = data_balance_hts[""countyfips""].astype(str)
 data_balance_hts.sort_values(by='date',inplace=True)",modeling,,
file172,79,data_balance_hts,data exploration,,
file172,80,"df_bottom_level = data_balance_hts.pivot(index=""date"", columns=""county_tract"", values=""total_claims"")
 df_middle_level = data_balance_hts.groupby([""date"", ""countyfips""]) \
  .sum() \
  .reset_index(drop=False) \
  .pivot(index=""date"", columns=""countyfips"", values=""total_claims"")
 df_total = data_balance_hts.groupby(""date"")[""total_claims""] \
  .sum() \
  .to_frame() \
  .rename(columns={""total_claims"": ""total""})
 hierarchy_df = df_bottom_level.join(df_middle_level) \
  .join(df_total)
 hierarchy_df.index = pd.to_datetime(hierarchy_df.index)
 hierarchy_df = hierarchy_df.resample(""7D"") \
  .sum()",data preprocessing,,
file172,81,"print(f""Number of time series at the bottom level: {df_bottom_level.shape[1]}"")
 print(f""Number of time series at the middle level: {df_middle_level.shape[1]}"")
 hierarchy_df = hierarchy_df.fillna(0)",data exploration,data preprocessing,
file172,82,"county = data_balance_hts[""countyfips""].unique()
 tract = data_balance_hts[""county_tract""].unique()",data preprocessing,,
file172,83,"total = {'total': list(county)}
 county = {k: [v for v in tract if v.startswith(k)] for k in county}
 hierarchy = {**total, **county}",data preprocessing,,
file172,84,"ax = hierarchy_df[hierarchy['total']].plot(title=""total unemployment claims - county level"")
 ax.legend(bbox_to_anchor=(1.0, 1.0));",data preprocessing,,
file172,85,"model_ols_arima = hts.HTSRegressor(model='auto_arima', revision_method='OLS', n_jobs=0)
 model_ols_arima = model_ols_arima.fit(hierarchy_df, hierarchy)
 pred_ols_arima = model_ols_arima.predict(steps_ahead=10)",modeling,prediction,
file172,86,pred_ols_arima,modeling,,
file172,87,"# Based on MAPE and MSPE, now the ARIMA model has best prediction, so the following prediction is based on ARIMA model
 data_pred = data_pred_query.copy()",data preprocessing,,
file172,88,data_pred = dataIdentifyDWM(data_pred),data preprocessing,,
file172,89,"# Prediction with Hierarchical ARIMA model, for week 43
 pred_col = pred_ols_arima.columns
 pred = pred_ols_arima.copy()
 pred = pred.reset_index()
 pred_long = pd.melt(pred, id_vars='index', value_vars=pred_col)",modeling,,
file172,90,pred_long = pred_long[pred_long['variable'].str.contains('_')],data preprocessing,,
file172,91,"new = pred_long[""variable""].str.split(""_"", n = 1, expand = True)",data preprocessing,,
file172,92,"pred_long[""uu_id""] = new[1]
 pred_long = pred_long.rename(columns={""index"": ""date""})",data preprocessing,,
file172,93,"pred_long = pred_long[pred_long[""date""] == ""2022-10-22""]",data preprocessing,,
file172,94,pred_long,data exploration,,
file172,95,data_pred,data exploration,,
file172,96,"pred_long = pred_long.sort_values('value', ascending=False).drop_duplicates('uu_id').sort_index()",data preprocessing,,
file172,97,"data_pred_join = pd.merge(data_pred, pred_long, how='left', left_on=['date','uu_id'], right_on=['date','uu_id'])",data preprocessing,,
file172,98,"data_pred_join = data_pred_join.rename(columns={""value"": ""total_claims""})",data preprocessing,,
file172,99,"data_pred_join.loc[data_pred_join['total_claims']<0,'total_claims']=0",data exploration,,
file172,100,"data_pred = data_pred_join[[""uu_id"", ""total_claims"", ""week_number""]]",data preprocessing,,
file172,101,"## This can also be a good place for you to cleanup any input/output and export your results to a file.
 data_pred.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file172,102,"# model train and validation
 MAPE_list = []
 MSPE_list = []",data preprocessing,,
file172,103,"uu_id_list = pd.unique(data_balance[""uu_id""])",data preprocessing,,
file172,104,"for i in range(len(uu_id_list)):
  print(i)
  data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[i]]
  data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
  data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
  forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
  
  df_forecast = pd.DataFrame(forecast)
  df_forecast.index.name = ""week_number""
  df_forecast.columns = [""total_claim_pred""]
  
  data_balance_ave_valid_check = data_balance_tract_model.merge(df_forecast, on = ""week_number"")
  MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
  MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
  
  MAPE_list.append(MAPE_series)
  MSPE_list.append(MSPE_series)",modeling,,
file172,105,"# validation
 print(""MAPE: "", sum(MAPE_list)/len(MAPE_list))
 print(""MSPE: "", sum(MSPE_list)/len(MSPE_list))",data exploration,,
file172,106,"data_pred[""total_claims""] = 0
 for i in range(len(data_pred)):
  uu_id_pre = data_pred.loc[i][""uu_id""]
  data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
  data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
  data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
  forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 1, n_period = 15)
  
  df_forecast = pd.DataFrame(forecast)
  df_forecast.index.name = ""week_number""
  df_forecast.columns = [""total_claim_pred""]
  
  total_claim_pre = df_forecast.loc[data_pred.iloc[i][""week_number""]][""total_claim_pred""]
  
  data_pred.loc[i,""total_claims""] = total_claim_pre
  #print(i)
  #print(data_pred.iloc[i][""week_number""])
  #print(total_claim_pre)",modeling,data preprocessing,
file173,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file173,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file173,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file173,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file173,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file173,5,global df3,helper functions,,
file173,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file173,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file173,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file173,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file173,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file173,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file173,12,"import statsmodels.api as sm
 from statsmodels import tsa",helper functions,,
file173,13,"mod = sm.tsa.statespace.SARIMAX(np.array(df3[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 12),
  enforce_stationarity=False,
  enforce_invertibility=False)
 results = mod.fit()
 print(results.summary().tables[1])",modeling,data exploration,
file173,14,df3 =df3.dropna(),data preprocessing,,
file173,15,"for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 12),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  pred = results.get_prediction(start=2, dynamic=False)
  pred_ci = pred.conf_int()
  y = temp[""total_claims""]
  ax = y.plot(label='observed')
  print(pred.predicted_mean)
  val = pred.predicted_mean
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = pred.predicted_mean
 test_df",modeling,prediction,
file173,16,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",data preprocessing,,
file173,17,test_df.drop_duplicates(),data preprocessing,,
file173,18,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file173,19,"adfuller(df3.[""total_claims""))",data preprocessing,,
file173,20,"import statsmodels.tsa.stattools.adfuller
 adfuller(df3[""total_claims""])",helper functions,data preprocessing,
file173,21,from pandas.tools.plotting import autocorrelation_plot,helper functions,,
file173,22,"from pandas.tools.plotting import autocorrelation_plot
 autocorrelation_plot(df3[""total_claims""])",helper functions,result visualization,
file173,23,test_df,data exploration,,
file173,24,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data preprocessing,save results,
file173,25,"import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat(temp,test_df)
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  pred = results.get_prediction(start=28, end =40, dynamic=False)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = (pred.predicted_mean)
  df3_.loc[df3_.uu_id_enc==k,""predicted_total_claims""] = (pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",modeling,prediction,
file173,26,"df3_[[""total_claims"", ""predicted_total_claims""]].plot(figsize=(12, 8))",result visualization,,
file173,27,df3_,data exploration,,
file173,28,df3_.columns,data exploration,,
file173,29,"#Checking ARIMA
 df3_",data exploration,,
file173,30,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file173,31,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file173,32,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file173,33,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file173,34,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file173,35,global df3,helper functions,,
file173,36,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file173,37,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file173,38,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file173,39,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,,
file173,40,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file173,41,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,,
file173,42,"#df3_[""predicted_total_claims""]=li
 #df3_[[""total_claims"", ""predicted_total_claims""]].plot(figsize=(12, 8)) 
 df3_",data exploration,,
file173,43,"#df3_.loc[df3_[""week_number""]==28,""predicted_total_claims""]=li
 #df3_[[""total_claims"", ""predicted_total_claims""]].plot(figsize=(12, 8)) 
 df3_.loc[df3_[""week_number""]==28,""predicted_total_claims""]",data exploration,,
file173,44,test_df,data exploration,,
file173,45,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",save results,,
file173,46,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results = mod.fit()
  pred = results.get_prediction(start=28, end =28, dynamic=True)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = (pred.predicted_mean)
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,,
file173,47,"pd.merge(test_df, df3, on= [""uu_id"",""week_number""])",data preprocessing,,
file173,48,"res = pd.merge(test_df, df3, on= [""uu_id"",""week_number""])
 res = [[""uu_id"",""week_number"",""total_claims_x"",""total_claims_y""]]
 res",data preprocessing,data exploration,
file173,49,df3,data exploration,,
file173,50,test_df.astype(float),data exploration,,
file173,51,test_df[test_df.week_number==28],data exploration,,
file173,52,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 7
 test_df[""month""]=10",load data,data preprocessing,
file173,53,test_df.drop_duplicates(),data preprocessing,,
file173,54,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",data preprocessing,,
file173,55,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file173,56,"df3[df3.week_number==28].sort_values(""uu_id_enc"")",data exploration,,
file173,57,"test_df[test_df.week_number==28].sort_values(""uu_id_enc"")
 #test_df.dtypes",data exploration,,
file173,58,submission_prediction_output.uu_id.nunique(),data exploration,,
file173,59,df3.uu_id_enc.nunique(),data exploration,,
file173,60,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file173,61,"pred = results.get_prediction(start=39, dynamic=False)
 pred_ci = pred.conf_int()
 ax = y.plot(label='observed')
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Furniture Sales')
 plt.legend()
 plt.show()",prediction,,
file173,62,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 test_df.isna()",data exploration,,
file173,63,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output.isnull().values.any()",data exploration,,
file173,64,"""""""test_df1=test_df.copy()
 for col in ['total_claims', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in extra:
  #test_df.loc[test_df.uu_id_enc==k,col] =0
  temp=df[df.uu_id_enc == k]
  temp[""average_wage""]=-9999
  temp =temp.replace("""",0)
  feature_test_pred = np.array(test_df1[test_df1.uu_id_enc==k])
  #print(k, temp)
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  idk = float(val.predict(np.array(feature_test_pred))[0])
  print(idk)
  test_df.loc[test_df.uu_id_enc==k,col].value = idk
 test_df""""""",data preprocessing,,
file173,65,test_df,data exploration,,
file173,66,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",save results,data exploration,
file173,67,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)",data preprocessing,,
file173,68,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0]",data preprocessing,data exploration,
file173,69,"#test_df[test_df.""total_claims""==39].sort_values(""uu_id_enc"")
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k][""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()",data preprocessing,,
file173,70,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k][""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",save results,,
file173,71,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=True)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",modeling,prediction,
file173,72,submission_prediction_output[submission_prediction_output.total_claims<=0],data exploration,,
file174,0,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install plotly')",helper functions,,
file174,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file174,2,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from scipy import stats
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file174,3,"import numpy as np
 from numpy import isnan
 from matplotlib import pyplot",helper functions,,
file174,4,"import seaborn as sns
 import matplotlib 
 import matplotlib.pyplot as plt
 import plotly 
 import plotly.express as px",helper functions,,
file174,5,import pandas as pd,helper functions,,
file174,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file174,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file174,8,"query_job = bigquery_client.query(query)
 unemploy = query_job.to_dataframe()
 unemploy.head()",load data,data preprocessing,
file174,9,"query_wage = bigquery_client.query(wage)
 wages = query_wage.to_dataframe()
 wages.head()",load data,data preprocessing,
file174,10,wages.info(),data exploration,,
file174,11,unemploy.info(),data exploration,,
file174,12,unemploy.isnull().sum(),data exploration,,
file174,13,"unemploy=unemploy.sort_values('week_number', ascending=True)
 unemploy.reset_index(inplace=True)
 unemploy",data preprocessing,data exploration,
file174,14,"sns.heatmap(unemploy.isnull(),cbar=False)",result visualization,,
file174,15,"miss=unemploy.isnull()
 total=unemploy.count()
 total",data preprocessing,data exploration,
file174,16,miss.sum(),data exploration,,
file174,17,miss.sum()/len(unemploy),data exploration,,
file174,18,"map_1 = unemploy.corr(method ='spearman')
 sns.heatmap(map_1)",result visualization,,
file174,19,"unemploy[""race_hawaiiannative""].unique()",data exploration,,
file174,20,"df1=unemploy.copy()
 df1=df1.drop(columns=""race_hawaiiannative"")",data preprocessing,,
file174,21,"df=unemploy[['top_category_employer1',""top_category_employer2"",""top_category_employer3"",'uu_id',""tract_name"",""timeperiod"",""tract"",""countyfips""]]",data preprocessing,,
file174,22,df,data exploration,,
file174,23,"impute1=KNNImputer()
 impute1.fit(df1)
 unemploy1=pd.DataFrame(impute1.fit_transform(df1),columns = df1.columns)",data preprocessing,,
file174,24,unemploy1,data exploration,,
file174,25,"trial=pd.merge(df, unemploy1, left_index=True, right_index=True)",data preprocessing,,
file174,26,trial.head(),data exploration,,
file174,27,trial.describe(),data exploration,,
file174,28,"sns.heatmap(trial.corr(method=""spearman""))",result visualization,,
file174,29,trial1=trial.copy(),data preprocessing,,
file174,30,"scaler = StandardScaler()
 scaled=pd.DataFrame(scaler.fit_transform(unemploy1),columns = unemploy1.columns)",data preprocessing,,
file174,31,scaled,data exploration,,
file174,32,"sns.displot(data=scaled,x=""total_claims"",kind=""kde"")",result visualization,,
file174,33,"claim = np.array(scaled[""total_claims""])
 sqrt_claim = np.sqrt(claim)
 log_claim = np.log(claim)
 f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
 #f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
 f1.hist(claim, 30)
 f2.hist(sqrt_claim, 30)
 f3.hist(log_claim, 30)",data preprocessing,,
file174,34,"stats.probplot(claim, plot=f4)
 stats.probplot(sqrt_claim, plot=f5)
 stats.probplot(log_claim, plot=f6)
 plt.show
 stats.shapiro(claim)[1], stats.shapiro(sqrt_claim)[1], stats.shapiro(log_claim)[1]",result visualization,,
file174,35,shapiro(sqrt_claim),data exploration,,
file174,36,print(skew(sqrt_claim)),data exploration,,
file174,37,shapiro(unemploy1.total_claims),data exploration,,
file174,38,"f3.hist(log_claim, 30)",result visualization,,
file174,39,import pylab,helper functions,,
file174,40,"stats.probplot(log_claim,dist=""norm"",plot=pylab)
 pylab.show()",result visualization,,
file174,41,sns.distplot(une),result visualization,,
file174,42,"sns.boxplot(x=scale_logs[""edu_grades_9_11""])",result visualization,,
file174,43,unemploy_logs = np.log(unemploy1),data preprocessing,,
file174,44,"unemp_cube=np.power((unemploy1),1/3)",data preprocessing,,
file174,45,"statistics,pvalue = jarque_bera(unemp_cube)",,,
file174,46,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
 if pvalue>0.05:
  print(""Probably Normal"")
 else:
  print(""Probably not Normal"")",data exploration,,
file174,47,from scipy.stats import jarque_bera,helper functions,,
file174,48,"statistics,pvalue = jarque_bera(unemploy1[""edu_grades_9_11""])",data preprocessing,,
file174,49,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from scipy import stats
 from scipy.stats import boxcox
 from scipy.stats import jarque_bera
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file174,50,"import numpy as np
 from numpy import isnan
 from matplotlib import pyplot",helper functions,,
file174,51,"import seaborn as sns
 import matplotlib 
 import matplotlib.pyplot as plt
 import plotly 
 import plotly.express as px",helper functions,,
file174,52,import pandas as pd,helper functions,,
file174,53,"transformed_data, best_lambda = boxcox(unemploy1)
 sns.distpolt(transformed_data,)",result visualization,,
file174,54,"claim = np.array(transformed_data)
 sqrt_claim = np.sqrt(transformed_data)
 log_claim = np.logtransformed_data)
 f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
 #f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
 f1.hist(claim, 30)
 f2.hist(sqrt_claim, 30)
 f3.hist(log_claim, 30)",result visualization,,
file174,55,"stats.probplot(claim, plot=f4)
 stats.probplot(sqrt_claim, plot=f5)
 stats.probplot(log_claim, plot=f6)
 plt.show
 stats.shapiro(claim)[1], stats.shapiro(sqrt_claim)[1], stats.shapiro(log_claim)[1]",result visualization,,
file174,56,"statistics,pvalue = jarque_bera(transformed_data)",data preprocessing,,
file174,57,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
 sns.distplot(transformed_data)
 ed_9_11=boxcox(unemploy[""edu_grades_9_11""])",result visualization,,
file174,58,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
 sns.distplot(unemploy1[""race_white""])",result visualization,,
file174,59,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.preprocessing import MinMaxScaler",helper functions,,
file174,60,"from scipy import stats
 from scipy.stats import boxcox
 from scipy.stats import jarque_bera
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file174,61,"scaler= MinMaxScaler()
 scaler.fit(unemploy1)
 # transform the test test
 scaled1 = scaler.transform(unemploy1)",data preprocessing,,
file174,62,scaler,data exploration,,
file174,63,scaled1,data exploration,,
file174,64,"scaler= MinMaxScaler()
 # transform the test test
 scaled1 = pd.DataFrame(scaler.transform_fit(unemploy1),columns=unemploy1.columns)",data preprocessing,,
file174,65,"sns.distplot(scaled1[""total_claims""])",result visualization,,
file174,66,"unemploy1[""race_asian""].unique()",data exploration,,
file174,67,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1),columns=unemploy1)",data preprocessing,,
file174,68,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_quiv""]))",data preprocessing,,
file174,69,"statistics,pvalue = jarque_bera(unemploy_ish)",data preprocessing,,
file174,70,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
 if pvalue>0.05:
  print(""Probably Normal"")
 else:
  print(""Probably not Normal"")",data exploration,,
file174,71,stats.probplot(unemploy_ish),result visualization,,
file174,72,unemploy_ish,data exploration,,
file174,73,"stats.probplot(unemploy_ish,dist=""norm"",plot=pylab)
 pylab.show()",result visualization,,
file174,74,"unemploy_ish1=np.arcsinh(unemploy1[""edu_hs_grad_equiv""])
 sns.distplot(unemploy_ish)",result visualization,,
file174,75,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
 unemployed_ish=boxcox.fit_transform(unemploy_ish)
 sns.distplot(unemployed_ish)",result visualization,,
file174,76,"unemploy1[""gender_male""]",data exploration,,
file174,77,unemploy1,data exploration,,
file174,78,"Q1 = np.percentile(unemploy1[""total_claims""], 25,
  interpolation = 'midpoint')",data preprocessing,,
file174,79,"print(""Old Shape: "", unemploy1.shape)",data exploration,,
file174,80,"# Upper bound
 upper = np.where(unemploy1[""total_claims""]>= (Q3+1.5*IQR))
 # Lower bound
 lower = np.where(unemploy1[""total_claims""]<= (Q1-1.5*IQR))",data preprocessing,,
file174,81,"'' Removing the Outliers '''
 unemploy1.drop(upper[0], inplace = True)
 unemploy1.drop(lower[0], inplace = True)",data preprocessing,,
file174,82,unemploy1.shape,data exploration,,
file174,83,sns.distplot(unemploy1),result visualization,,
file174,84,"sns.distplot(unemploy1[""gender_male""])",result visualization,,
file174,85,"sns.distplot(unemploy1[""race_white""])",result visualization,,
file174,86,"sns.distplot(unemploy1[""edu_hs_grad_equiv""])",result visualization,,
file174,87,"sns.heatmap(unemploy1.corr(method=""spearman""))",result visualization,,
file174,88,"trial[""uuid""].unique()",data exploration,,
file175,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file175,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install xgboost')
 get_ipython().system('pip install impyute')
 get_ipython().system('pip install prophet')
 get_ipython().system('pip install plotly')
 get_ipython().system('pip install plotly-geo')",helper functions,,
file175,2,"import datetime
 import itertools
 import os
 import re
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import plotly.figure_factory",helper functions,,
file175,3,"import statsmodels.api as sm
 import sklearn.experimental.enable_iterative_imputer
 import sklearn.impute
 import sklearn.ensemble
 import sklearn.model_selection
 import sklearn.linear_model
 import xgboost as xgb",helper functions,,
file175,4,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file175,5,"pd.set_option('display.max_columns', 500)
 pd.set_option('display.max_rows', 500)
 pd.set_option('display.width', 1000)",helper functions,,
file175,6,"# load data
 def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data",load data,,
file175,7,"def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d",load data,,
file175,8,"def load_raw(csv_name='0_raw.csv'):
  '''
  Loads the unemployment and wage data and does some basic cleaning
  '''
  if not os.path.isfile(csv_name):
  u = query('unemployment_data')
  w = query('wage_data')
  raw = combine(u, w)
  raw.to_csv(csv_name, index=False)
  else:
  raw = pd.read_csv(csv_name)
  raw = raw.drop(['tract', 'timeperiod'], axis=1)
  raw = raw.sort_values(by=['uu_id', 'week_number'])
  raw = raw.drop_duplicates()
  raw = raw.replace({np.nan: None})
  raw = raw.reset_index(0, drop=True)
  return raw",load data,data preprocessing,
file175,9,load_raw().to_dict('records')[0],load data,data preprocessing,
file175,10,"# define relevant columns based on categories 
 COL_MAP = {
  'edu': ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown'],
  'race': ['race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative', 'race_other', 'race_white'],  
  'gender': ['gender_female', 'gender_male', 'gender_na'],
  'industry': ['top_category_employer1', 'top_category_employer2', 'top_category_employer3']  
 }",data preprocessing,,
file175,11,"def get_cols(names):
  l = []
  for name in names:
  if name in COL_MAP:
  l += COL_MAP[name]
  else:
  l += [name]
  return l",data preprocessing,,
file175,12,"def subset(df, uu_id):  
  return df.loc[df.uu_id == df.uu_id.unique()[uu_id], :]",data exploration,,
file175,13,"def convert_to_submission(results_csv, week_number_to_submit=40):
  r = pd.read_csv(results_csv)
  last = r.loc[r.week_number == week_number_to_submit, ['uu_id', 'predicted']]
  last.index = last.uu_id
  uuid_map = last.to_dict(orient='dict')['predicted']
  p = query('prediction_list')
  p['total_claims'] = p['uu_id'].map(uuid_map)
  p.to_csv('submission_prediction_output.csv', index=False)",save results,,
file175,14,"def get_week_number_map(g, colname):
  '''
  Creates a dictionary that maps from week number to an existing value in a given `colname`
  '''
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map",data preprocessing,,
file175,15,"def get_county(tract_name):
  m = re.search('Census Tract \S+, (.+) County, Indiana', tract_name)
  county = m.group(1)
  return county",data preprocessing,,
file175,16,"def week_number_to_date(week_number, first_week_date='20220101'):
  '''
  Prepare a date column for ARIMA
  '''
  return pd.to_datetime(first_week_date, format='%Y%m%d') + pd.DateOffset(days=7*(week_number - 1))",data preprocessing,,
file175,17,"def insert_na_week_number(g, max_week_number=37):  
  d = {}
  for colname in g.columns:
  if colname == 'week_number':
  continue
  week_number_map = get_week_number_map(g, colname)
  series = pd.Series(range(1, max_week_number+1))  
  d[colname] = series.map(week_number_map)  
  
  df = pd.DataFrame(d)
  df['week_number'] = range(1, max_week_number+1)
  df['uu_id'] = [v for v in df['uu_id'].unique() if type(v) == str][0]
  df['average_wage'] = [v for v in g['average_wage'].unique()][0]
  df['countyfips'] = [v for v in g['countyfips'].unique()][0]
  df['county'] = get_county([v for v in g['tract_name'].unique()][0])
  return df",data preprocessing,,
file175,18,"def load_raw_full(csv_name='1_raw_full.csv'):
  if not os.path.isfile(csv_name):
  raw = load_raw()
  raw_full = raw.groupby('uu_id').apply(insert_na_week_number).reset_index(0, drop=True)
  raw_full['date'] = raw_full['week_number'].apply(week_number_to_date)
  raw_full = raw_full.drop(['tract_name'], axis=1)
  raw_full.to_csv(csv_name, index=False)  
  else:
  raw_full = pd.read_csv(csv_name)
  return raw_full",load data,,
file175,19,"def plot_impute(n=6):
  raw = load_raw_full()
  imp = load_imp_tot()
  fig, axs = plt.subplots(ncols=n, sharey=False, figsize=(n*3, 3))
  for i in range(n):
  rraw = subset(raw, i)
  iimp = subset(imp, i)
  ax = axs[i]
  ax.plot(rraw.week_number, rraw.total_claims, 'o', label='original')
  ax.plot(iimp.week_number, iimp.total_claims, '-', label='imputed')
  ax.set_xlim(0, 42)
  axs[-1].legend(fancybox=False)",result visualization,,
file175,20,"def imp_tot(g):
  g['total_claims'] = g['total_claims'].interpolate(method='linear')
  g = g[g['total_claims'].notna()]
  return g",result visualization,,
file175,21,"def load_imp_tot(csv_name='2_imp_tot.csv'):
  if not os.path.isfile(csv_name):
  d = load_raw_full()
  d = d.groupby(['uu_id']).apply(imp_tot).reset_index(0, drop=True)
  d.to_csv(csv_name, index=False)
  else:
  d = pd.read_csv(csv_name)
  return d",load data,,
file175,22,plot_impute(),result visualization,,
file175,23,"def replace_na_cols(g):
  '''
  If a column only has None or zero values, replace that entire columnn with zeros
  '''
  x = g.copy()
  for col in g.columns:
  cond1 = g[col].isnull()
  cond2 = g[col] == 0
  if (cond1 | cond2).all():
  x[col] = 0  
  return x",data preprocessing,,
file175,24,"def impute_iterative(df):
  '''
  Wrapper fucntion for IterativeImputer for a generic data frame. 
  Mostly, for testing. We might need need this function
  Impute data assuming there are zero columns where all the values are NA
  '''
  imputer = sklearn.impute.IterativeImputer(random_state=0, min_value=0)
  imputed_cols = imputer.fit_transform(df)
  df_imputed = pd.DataFrame(imputed_cols, columns=df.columns)
  return df_imputed",data preprocessing,,
file175,25,"def iter_cat(g):
  g = replace_na_cols(g)  
  for cat in ['edu', 'race', 'gender']:
  gg = g.loc[:, COL_MAP[cat] + ['total_claims']]
  yield cat, gg",data preprocessing,,
file175,26,"def print_impute_cat(test_subset, impute_func):
  for cat, gg in iter_cat(test_subset):
  line = '*'*len(cat)
  print(line)
  print(cat)
  print(line)
  print(impute_func(gg).head())",data exploration,,
file175,27,"print_impute_cat(subset(load_imp_tot(), 6), impute_iterative)",data exploration,,
file175,28,"def impute_rowsum(df, target_col='total_claims'):
  l = []
  for idx, row in df.iterrows():
  n_unknowns = row.isna().sum()  
  if n_unknowns == 1:
  others = row[~row.isna() & (row.index != target_col)]
  val = row[target_col] - others.sum()
  val = val if val > 0 else 0
  row[row.isna()] = val
  l.append(row)
  df = pd.DataFrame(l).reset_index(0, drop=True)
  
  l = []
  for idx, row in df.iterrows():
  n_unknowns = row.isna().sum()  
  
  weights = {}
  for col in row[row.isna()].index:
  weights[col] = df[col].mean()
  weights = {k:v/sum(weights.values()) for k, v in weights.items()} 
  
  if n_unknowns > 1:  
  others = row[~row.isna() & (row.index != target_col)]
  row[row.isna()] = row[row.isna()].index.map(weights)*(row[target_col] - others.sum())  
  l.append(row)
  
  df_imputed = pd.DataFrame(l).reset_index(0, drop=True)
  return df_imputed",data preprocessing,,
file175,29,"def impute_all(df):
  x = df.copy().reset_index(0, drop=True)
  for cat, gg in iter_cat(df):
  df_imputed = impute_rowsum(gg)
  df_imputed = df_imputed.drop('total_claims', axis=1)
  x[COL_MAP[cat]] = df_imputed
  return x",data preprocessing,,
file175,30,"def load_clean(csv_name='3_clean.csv'):  
  if not os.path.isfile(csv_name):
  imp_tot = load_imp_tot()
  imp = imp_tot.groupby('uu_id').apply(impute_all).reset_index(0, drop=True)
  imp['date'] = imp['week_number'].apply(week_number_to_date)
  imp.to_csv(csv_name, index=False)
  else:
  imp = pd.read_csv(csv_name)
  
  return imp",data preprocessing,load data,
file175,31,load_clean().to_dict('records')[0],load data,,
file175,32,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]
  
  if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",data preprocessing,,
file175,33,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",data preprocessing,,
file175,34,load_imp_industry().isna().sum(),data exploration,,
file175,35,"def feature_engineer(d):
  d['gender_male_ratio'] = d['gender_male'] / d['total_claims']
  d['edu_post_hs_ratio'] = d['edu_post_hs'] / d['total_claims']
  d['race_white_ratio'] = d['race_white'] / d['total_claims']
  d['race_black_ratio'] = d['race_black'] / d['total_claims']
  d = d.drop(get_cols(['gender', 'edu', 'race']), axis=1)
  return d",data preprocessing,,
file175,36,"def load_featured(csv_name='5_featured.csv'):
  if not os.path.isfile(csv_name):
  d = load_imp_industry()
  d = feature_engineer(d)
  else:
  d = pd.read_csv(csv_name)
  return d",data preprocessing,load data,
file175,37,"def get_tot_by_county(d):
  l = []
  for (county, week_number), g in d.groupby(['county', 'week_number']):  
  l.append({
  'county': county,
  'fips': g.countyfips.values[0],
  'week_number': week_number,
  'total_claims': g.total_claims.mean()
  })
  c = pd.DataFrame(l)
  return c",data preprocessing,,
file175,38,"c = get_tot_by_county(load_featured())
 c = c.loc[c.week_number == 20, :]
 c.head()",data preprocessing,,
file175,39,"values = c.total_claims
 endpts = list(np.mgrid[min(values):max(values):4j])
 colorscale = [""#030512"",""#1d1d3b"",""#323268"",""#3d4b94"",""#3e6ab0"",
  ""#4989bc"",""#60a7c7"",""#85c5d3"",""#b7e0e4"",""#eafcfd""]
 # fig = plotly.figure_factory.create_choropleth(
 #  fips=c.fips, values=values, scope=['Indiana'], show_state_data=True,
 #  colorscale=colorscale, binning_endpoints=endpts, round_legend_values=True,
 #  plot_bgcolor='rgb(229,229,229)',
 #  paper_bgcolor='rgb(229,229,229)',
 #  legend_title='Average Total Claims by County',
 #  county_outline={'color': 'rgb(255,255,255)', 'width': 0.5},
 #  exponent_format=True,
 # )",data preprocessing,,
file175,40,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install xgboost')
 get_ipython().system('pip install impyute')
 get_ipython().system('pip install prophet')
 get_ipython().system('pip install plotly')
 get_ipython().system('pip install plotly-geo')
 get_ipython().system('pip install geopandas')
 get_ipython().system('pip install pyshp')
 get_ipython().system('pip install shapely')",helper functions,,
file176,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file176,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file176,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file176,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file176,4,"query = """"""
 select * from `ironhacks-data.ironhacks_competition`
 """"""",load data,,
file176,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']=pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file176,6,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file176,7,data['edu_8th_or_less'],data exploration,,
file176,8,data['edu_8th_or_less'].value_counts(),data exploration,,
file176,9,data['edu_grades_9_11'].value_counts(),data exploration,,
file176,10,data['edu_grades_9_11'].isna().sum(),data exploration,,
file176,11,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.dtypes()
 # data = data.drop([""tract_name"",""edu_8th_or_less"",",load data,data preprocessing,
file176,12,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 pd.set_option('display.max_columns', None)
 data = data.drop(['tract_name','edu_8th_or_less','edu_unknown'],axis=1)
 data",load data,data preprocessing,
file177,0,"import os
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file177,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemp_data = query_job.to_dataframe()
 unemp_data.head(5)",load data,data preprocessing,
file177,2,unemp_data.info(),data exploration,,
file177,3,wage_data.info(),data exploration,,
file177,4,unemp_data.isna().sum(),data exploration,,
file177,5,"set1 = set(list(unemp_data['top_category_employer1'].unique()))
 set1",data preprocessing,data exploration,
file177,6,"unemp_data['top_category_employer2'] = unemp_data['top_category_employer2'].replace('N/A',np.NaN)
 unemp_data['top_category_employer3'] = unemp_data['top_category_employer3'].replace('N/A',np.NaN)",data preprocessing,,
file177,7,unemp_data['countyfips'].unique(),data exploration,,
file177,8,(unemp_data.isna().sum()/len(unemp_data))*100,data exploration,,
file177,9,"unemp_data['edu_8th_or_less'] = unemp_data.groupby('countyfips')['edu_8th_or_less'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['edu_grades_9_11'] = unemp_data.groupby('countyfips')['edu_grades_9_11'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['edu_hs_grad_equiv'] = unemp_data.groupby('countyfips')['edu_hs_grad_equiv'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file177,10,"unemp_data['race_asian'] = unemp_data.groupby('countyfips')['race_asian'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_amerindian'] = unemp_data.groupby('countyfips')['race_amerindian'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_black'] = unemp_data.groupby('countyfips')['race_black'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_hawaiiannative'] = unemp_data.groupby('countyfips')['race_hawaiiannative'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_other'] = unemp_data.groupby('countyfips')['race_other'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_white'] = unemp_data.groupby('countyfips')['race_white'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file177,11,"unemp_data['race_noanswer'] = unemp_data['race_noanswer'].fillna(0)
 unemp_data['gender_na'] = unemp_data['gender_na'].fillna(0)
 unemp_data['edu_unknown'] = unemp_data['edu_unknown'].fillna(0)",data preprocessing,,
file177,12,"unemp_data['top_category_employer2'] = unemp_data.groupby('countyfips')['top_category_employer2'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'unknown'))
 unemp_data['top_category_employer3'] = unemp_data.groupby('countyfips')['top_category_employer3'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'unknown'))",data preprocessing,,
file177,13,"unemp_data['gender_male'] = unemp_data['gender_male'].astype(float)
 unemp_data['gender_female'] = unemp_data['gender_female'].astype(float)
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].astype(float)",data preprocessing,,
file177,14,"unemp_data['gender_male'] = unemp_data['gender_male'].fillna(unemp_data.groupby('countyfips')['gender_male'].transform('mean'))
 unemp_data['gender_female'] = unemp_data['gender_female'].fillna(unemp_data.groupby('countyfips')['gender_female'].transform('mean'))
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].fillna(unemp_data.groupby('countyfips')['edu_post_hs'].transform('mean'))",data preprocessing,,
file177,15,unemp_data[unemp_data.gender_male.isnull()],data exploration,,
file177,16,wage_data.describe(),data exploration,,
file177,17,wage_data['average_wage'] = wage_data['average_wage'].fillna(wage_data.groupby('countyfips')['average_wage'].transform('mean')),data preprocessing,,
file177,18,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
 final_data",data preprocessing,data exploration,
file177,19,"final_data1 = final_data.drop(['timeperiod','tract','tract_name'],axis=1)",data preprocessing,,
file177,20,final_data.info(),data exploration,,
file177,21,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y'],axis=1)",data preprocessing,,
file177,22,"fin = pd.get_dummies(final_data1,columns = ['week_number','countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file177,23,fin.shape(),data exploration,,
file177,24,"X = fin.drop(['total_claims'],axis=1)
 Y = fin['total_claims']",data preprocessing,,
file177,25,"from sklearn.decomposition import PCA
 pca = PCA(n_components=50)
 principalComponents = pca.fit_transform(X)",data preprocessing,,
file177,26,principalComponents,data exploration,,
file177,27,df_X = pd.DataFrame(principalComponents),data preprocessing,,
file177,28,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 from sklearn.feature_selection import RFE
 X_train, X_test,Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=28)",helper functions,data preprocessing,
file177,29,"lm = LinearRegression().fit(X_train,Y_train)
 prediction = model.predict(X_test)",modeling,prediction,
file177,30,"lm.score(X_test, Y_test)",evaluation,,
file177,31,"mean_squared_error(Y_train, prediction)",evaluation,,
file177,32,"model = XGBRegressor(n_estimators=500, max_depth=4,eta=0.1).fit(X_train,Y_train)
 y_pred = model.predict(X_test)",modeling,prediction,
file177,33,import xgboost as xgb,helper functions,,
file177,34,get_ipython().system('pip install xgboost'),helper functions,,
file177,35,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file177,36,ttt = model.fit(pred_data),modeling,,
file177,37,pred = model.predict(pred_data),prediction,,
file177,38,pred_data.info(),data exploration,,
file177,39,unemp_data.describe(),data exploration,,
file177,40,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
 tt",data preprocessing,data exploration,
file177,41,fin = final_data1.set_index('uu_id'),data preprocessing,,
file177,42,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y'],axis=1)",data preprocessing,,
file177,43,"fin = pd.get_dummies(final_data1,columns = ['week_number','countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file177,44,"X_train_data = X_train.drop(['uu_id'],axis=1)
 X_test_data = X_test.drop(['uu_id'],axis=1)",data preprocessing,,
file177,45,"fin = fin.set_index('uu_id')
 X_train, X_test, y_train, y_test = test_train_split(fin.ix[:, ~fin.columns.isin(['total_claims'])], fin.total_claims)",data preprocessing,,
file177,46,"fin = fin.set_index('uu_id')
 X = fin.drop(['total_claims'],axis=1)
 Y=fin['total_claims']
 #X_train, X_test, y_train, y_test = train_test_split(fin.ix[:, ~fin.columns.isin(['total_claims'])], fin.total_claims)",data preprocessing,,
file177,47,"X = fin.drop(['uu_id','total_claims'],axis=1)
 Y=fin['total_claims']",data preprocessing,,
file177,48,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')",data preprocessing,,
file177,49,final_data.info(),data exploration,,
file177,50,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 X_train, X_test,Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=32)",helper functions,data preprocessing,
file177,51,"lm = LinearRegression().fit(X_train,Y_train)
 prediction = lm.predict(X_test)",modeling,prediction,
file177,52,"lm.score(X_test, Y_test)",evaluation,,
file177,53,"import xgboost as xgb
 model = xgb.XGBRegressor(n_estimators=500, max_depth=4,eta=0.1).fit(X_train,Y_train)
 y_pred = model.predict(X_test)",modeling,prediction,
file177,54,"df_pred_final = pred_data[['uu_id']]
 df_pred_final['week_number'] = pred_data[['week_number']]
 df_pred_final[""total_claims""] = y_pred",data preprocessing,,
file177,55,"Y_test_final = pred_data.merge(fin,how='left',on= 'uu_id')",data preprocessing,,
file177,56,Y_test_final,data exploration,,
file177,57,"df_test = fin[fin.uu_id == i for i in pred_data['uu_id']]
 df_test",data preprocessing,data exploration,
file177,58,Y_test_final = pred_data.join(fin),data preprocessing,,
file177,59,"pred_data = pred_data.set_index('uu_id')
 fin = fin.set_index('uu_id)
 Y_test_final = pred_data.join(fin)",data preprocessing,,
file177,60,"X_df_test = df_test.drop(['uu_id','total_claims'],axis=1)
 Y_df_test = df_test['total_claims']",data preprocessing,,
file177,61,prediction = model.predict(X_df_test),prediction,,
file177,62,df_test,data exploration,,
file177,63,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y','week_number'],axis=1)",data preprocessing,,
file177,64,"fin = pd.get_dummies(final_data1,columns = ['countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file177,65,"model.score(X_df_test,Y_df_test)",evaluation,,
file177,66,df_pred_final = pred_data[['uu_id']],data preprocessing,,
file177,67,"df_pred_final_yams=[]
 df_pred_final_yams == pred_data[['uu_id']]",data preprocessing,,
file177,68,df_pred_final_1['week_number'] = df_test[['week_number']],data preprocessing,,
file177,69,df_pred_final_1['total_claims'] = prediction,data preprocessing,,
file177,70,df_pred_final_1,data exploration,,
file177,71,df_final = df_pred_final_1.drop_duplicates(subset=['uu_id']),data preprocessing,,
file177,72,df_final,data exploration,,
file177,73,fin.shape,data exploration,,
file177,74,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file177,75,df_final.to_csv('submission_prediction_output.csv'),save results,,
file178,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file178,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file178,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file178,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file178,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file178,5,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file178,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file178,7,"#Correlation with output variable
 cor_target = abs(cor[""wind_speed""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file178,8,"sns.set(rc={'figure.figsize':(11, 4)})",data preprocessing,,
file178,9,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file178,10,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file178,11,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file178,12,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file178,13,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file178,14,data['lag_1']=lag_1,data preprocessing,,
file179,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file179,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file179,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file179,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file179,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import OneHotEncoder",helper functions,,
file179,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file179,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file179,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file179,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file179,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,data preprocessing,
file179,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file179,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 39
 """"""",load data,,
file179,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file179,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file179,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file179,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file179,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file179,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file179,18,"data['tract_name'] = [i.split(',')[1].strip().split(' ')[0] for i in data['tract_name']]",data preprocessing,,
file179,19,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(data['tract_name'].values.reshape(-1, 1))
 #tract_name_encoder.categories_[0]
 tract_name_dataset = pd.DataFrame(tract_name_encoder.transform(data['tract_name'].values.reshape(-1, 1)), index=data.index, columns= tract_name_encoder.categories_[0])
 data = data.drop(['tract_name'], axis=1)",data preprocessing,,
file179,20,"data = data.drop(['top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'timeperiod'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file179,21,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file179,22,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file179,23,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file179,24,"data = pd.concat([data, tract_name_dataset], axis=1)
 display(data)",data preprocessing,data exploration,
file179,25,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file179,26,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file179,27,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,,
file179,28,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",data preprocessing,,
file179,29,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file179,30,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file179,31,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",evaluation,,
file179,32,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=10)",modeling,,
file179,33,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file179,34,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,,
file179,35,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",prediction,,
file179,36,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,,
file179,37,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",evaluation,,
file179,38,plt.show(),result visualization,,
file179,39,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file179,40,unemployment_prediction_data = query_from_statement(prediction_query),data preprocessing,,
file179,41,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file179,42,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data preprocessing,data exploration,
file179,43,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",prediction,data exploration,
file179,44,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,data exploration,
file179,45,"unemployment_prediction_data.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file179,46,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file179,47,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file179,48,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file180,0,"#importing libraries
 import numpy as np
 import matplotlib.pyplot as plt
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file180,1,"#Code for BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file180,2,"#Testing bigquery
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file180,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data preprocessing,
file181,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file181,1,"get_ipython().run_cell_magic('capture', '', ""#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install google-cloud-bigquery\n!pip install google-cloud-bigquery[pandas]\n"")",helper functions,,
file181,2,"#- IMPORT THE LIBRARIES YOU WILL USE
 #------------------------------------------
 # You only need to import packages one time per notebook session. To keep your
 # notebook clean and organized you can handle all imports at the top of your file.
 # The following are included for example purposed, feel free to modify or delete 
 # anything in this section.
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import RandomForestClassifier",helper functions,,
file181,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file181,4,"#query 3: overview of employment_data(week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC;
 """"""
 query_job = bigquery_client.query(query)
 overview = query_job.to_dataframe()
 overview.head()",load data,data preprocessing,
file181,5,"#query 3: overview of prediction list (week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query_job = bigquery_client.query(query)
 predn = query_job.to_dataframe()
 predn.head()
 print(predn.head())",load data,data preprocessing,
file182,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file182,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file182,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file182,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file182,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",load data,data preprocessing,
file182,5,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,data preprocessing,
file182,6,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]
 #df_three_col.shape",data preprocessing,,
file182,7,"df_three_col = df_three_col.drop_duplicates()
 #print(df_three_col.shape)",data preprocessing,,
file182,8,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
 #df_three_col.head(21)
 #df_three_col.tail(3)",data preprocessing,,
file182,9,print (df_three_col['week_number'].drop_duplicates()),data exploration,,
file182,10,"res = pd.DataFrame(columns = ['uu_id', 'total_claims', 'week_number'])",data preprocessing,,
file182,11,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  #test_data = test_data.tail(3)
  week_list = test_data['week_number'].tolist()
  y_pred = -1
  count = 0
  sum = 0
  for week_id in range(31,38):
  if week_id in week_list:
  count++
  sum += test_data[test_data['week_number].isin(week_id) ]].values(2)
  if count > 0:
  y_pred = int(sum/count)
  else:
  max_week = max(week_list)
  y_pred = test_data[test_data['week_number].isin(max_week) ]].values(2)
  cur_row = pd.DataFrame([[cur_uu_id, y_pred, 41]], columns=['uu_id', 'total_claims', 'week_number'] )
  res = pd.concat([res,cur_row] ,ignore_index = True)",data preprocessing,,
file182,12,"res.to_csv(""Nov21_submission_prediction_output.csv"", index=False)",save results,,
file183,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file183,1,"get_ipython().system('pip install keras')
 get_ipython().system('pip install tensorflow')",helper functions,,
file183,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file183,3,"import os
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools",helper functions,,
file183,4,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file183,5,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file183,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""",load data,,
file183,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file183,8,"import pandas as pd
 import numpy as np
 combined = pd.merge(data,wagedata,on=['uu_id','countyfips', 'tract','tract_name'],how = 'left')",helper functions,data preprocessing,
file183,9,wagedata[wagedata['uu_id']=='a5c6dcff737e183f7931b472f10c3235'],data exploration,,
file183,10,"combined['average_wage'].fillna(combined['average_wage'].mean(),inplace = True)
 ## Using the mean for the missing wage",data preprocessing,,
file183,11,"firstgroup = ['edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs','edu_unknown']
 secondgroup = ['gender_female','gender_male','gender_na']
 thirdgroup = ['race_amerindian','race_asian','race_black','race_noanswer','race_hawaiiannative','race_other','race_white']
 ## columns to impute",data preprocessing,,
file183,12,"complete = combined.dropna(axis = 0)
 #using the complete rows to estimate the ratio of each catagory occupying the claims
 prob = {}
 for i in firstgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()
 for i in secondgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()
 for i in thirdgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()",data preprocessing,,
file183,13,prediction_list['total_claims'] = 0,data preprocessing,,
file183,14,prediction_list,data exploration,,
file183,15,"prediction_list.loc[:,['uu_id','total_claims','week_number']].to_csv('submission_prediction_output(2).csv',index = False)",data exploration,,
file183,16,"approximatedimputation(combined)
 combined.to_csv('submission_prediction_output(1).csv',index = False)",save results,data preprocessing,
file183,17,"def approximatedimputation(data):
  firstgroup = ['edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs','edu_unknown']
  secondgroup = ['gender_female','gender_male','gender_na']
  thirdgroup = ['race_amerindian','race_asian','race_black','race_noanswer','race_hawaiiannative','race_other','race_white']
  for i in firstgroup:
  for j in range(data.shape[0]):
  # if is nan, we use the approximation method to try to impute
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])
  for i in secondgroup:
  for j in range(data.shape[0]):
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])
  for i in thirdgroup:
  for j in range(data.shape[0]):
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])",data preprocessing,,
file184,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file184,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file184,2,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file184,3,"from sklearn.ensemble import RandomForestRegressor
 from sklearn.model_selection import train_test_split
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 import tensorflow as tf",helper functions,,
file184,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file184,5,"query = """"""
 SELECT 
 a.*,
 b.average_wage
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`a
 

 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id = b.uu_id
 

 

 """"""",load data,,
file184,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['timeperiod']= pd.to_datetime(data['timeperiod'])
 data.head()",load data,data preprocessing,
file184,7,data.drop_duplicates(inplace=True),data preprocessing,,
file184,8,data = data.fillna(0),data preprocessing,,
file184,9,data.describe(),data exploration,,
file184,10,data.sum(numeric_only=True),data exploration,,
file184,11,data.columns,data exploration,,
file184,12,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file184,13,"y = data.total_claims
 x = data.drop(['total_claims', 'timeperiod'], axis = 1)",data preprocessing,,
file184,14,"uuid, label = data['uu_id'].factorize(sort=True)",data preprocessing,,
file184,15,x['uu_id'] = uuid,data preprocessing,,
file184,16,x['tract_name'] = x['tract_name'].factorize()[0],data preprocessing,,
file184,17,"x['top_category_employer1'] = x['top_category_employer1'].factorize()[0]
 x['top_category_employer2'] = x['top_category_employer2'].factorize()[0]
 x['top_category_employer3'] = x['top_category_employer3'].factorize()[0]",data preprocessing,,
file184,18,"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)",data preprocessing,,
file184,19,x_train,data exploration,,
file184,20,x_train.columns,data exploration,,
file184,21,"model = tf.keras.Sequential([
  tf.keras.layers.Dense(24),
  tf.keras.layers.Dense(128, activation=""relu""),
  tf.keras.layers.Dense(128, activation=""relu""),
  tf.keras.layers.Dense(128, activation=""relu""),
  tf.keras.layers.Dense(4)
 ])",modeling,,
file184,22,"model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])",modeling,,
file184,23,"model.fit(x_train, y_train, epochs=10)",modeling,,
file184,24,data = data.astype(float),data preprocessing,,
file184,25,"x = x.astype(float)
 y = y.astype(float)",data preprocessing,,
file184,26,x['top_category_employer1'].max(),data exploration,,
file184,27,x.columns,data exploration,,
file184,28,"for col in x.columns:
  x[col] = x[col]/x[col].max()",data preprocessing,,
file184,29,x_train.fillna(0),data preprocessing,,
file185,0,"from google.cloud import bigquery
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file185,1,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/.config/gcloud/application_default_credentials.json'
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file186,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file186,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC
 """"""",load data,,
file186,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data preprocessing,
file187,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file187,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file187,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file187,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file187,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file187,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file187,6,print(covid19_cases_data),data exploration,,
file187,7,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file187,8,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file188,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file188,1,"mport os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file188,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file188,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",data preprocessing,,
file188,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file188,5,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file188,6,print(covid19_cases_data),data exploration,,
file188,7,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file188,8,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file189,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file189,1,"get_ipython().run_cell_magic('capture', '', ""#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install google-cloud-bigquery\n!pip install google-cloud-bigquery[pandas]\n"")",helper functions,,
file189,2,"#- IMPORT THE LIBRARIES YOU WILL USE
 #------------------------------------------
 # You only need to import packages one time per notebook session. To keep your
 # notebook clean and organized you can handle all imports at the top of your file.
 # The following are included for example purposed, feel free to modify or delete 
 # anything in this section.
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import RandomForestClassifier",helper functions,,
file189,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file189,4,"#query 3: overview of employment_data(week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC;
 """"""
 query_job = bigquery_client.query(query)
 overview = query_job.to_dataframe()
 overview.head()",load data,data preprocessing,
file189,5,"#query 3: overview of prediction list (week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query_job = bigquery_client.query(query)
 predn = query_job.to_dataframe()
 predn.head()
 print(predn.head())",load data,data preprocessing,
file189,6,"fig = plt.figure()
 plt.plot([employ.week_number], [employ.total_claims],'bs')
 plt.title('Distribution of claims through week')
 plt.xlabel('Weeks')
 plt.ylabel('Total claims')
 plt.show()",result visualization,,
file189,7,"labels = np.array(overview['total_claims'])
 features = employ.drop(['uu_id'], axis=1)
 feature_list = list(features.columns)
 features = np.array(features)",data preprocessing,,
file189,8,"#split data into train and test sets, split first 20% data
 x_train, x_test, y_train,y_test = train_test_split(features, labels, test_size = 0.20, random_state = 42)",data preprocessing,,
file189,9,"print(f'Training Features Shape: {x_train.shape}')
 print(f'Testing Features Shape: {x_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data exploration,,
file189,10,"from sklearn.ensemble import RandomForestRegressor
 regressor = RandomForestRegressor(n_estimators=1000, random_state=42)
 x = x_train
 y = y_train
 regressor.fit(x,y)",modeling,,
file189,11,"#visualizing the decision tree from the regressor
 from sklearn import tree
 tree.plot_tree(regressor.estimators_[0])",result visualization,,
file189,12,"predictions = regressor.predict(x_test).astype(int)
 predictions = np.round(predictions,decimals = 0, out = None)
 print(predictions)",prediction,,
file189,13,"errors = abs(y_test - predictions)
 print(f'List of Errors: {errors}')
 print(f'Mean Absolute Error: {np.mean(errors)*10:.4f}%')",evaluation,,
file189,14,"df = pd.DataFrame(predictions, columns=['total_claims'])
 week41 = predn.join(df).iloc[:,[0,2,1]]
 print(week41)
 print(f'Total predicting number of unemployment claims of week 41: {sum(predictions):.0f}')",data preprocessing,data exploration,
file189,15,"csv_data = week41.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file190,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file190,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC
 """"""",load data,,
file190,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data preprocessing,
file190,3,get_ipython().system('pip install db-dtypes'),helper functions,,
file191,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file191,1,"get_ipython().run_cell_magic('capture', '--no-display', '!pip3 install db-dtypes\n')",helper functions,,
file191,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file191,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file191,4,"# Google Credential
 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='key.json'
 bigquery_client = bigquery.Client(project='ironhacks-data')",load data,,
file191,5,"# Query the three provided data tables 
 unemployement_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file191,6,"wage_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file191,7,"# QUERY THE DATA ONCE
 ud_query_job = bigquery_client.query(unemployement_data_query)
 wd_query_job = bigquery_client.query(wage_data_query)
 pl_query_job = bigquery_client.query(prediction_list_query)",load data,,
file191,8,"unemployement_data = ud_query_job.to_dataframe()
 wage_data = wd_query_job.to_dataframe()
 prediction_list = pl_query_job.to_dataframe()",data preprocessing,,
file191,9,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",data preprocessing,,
file191,10,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",data preprocessing,data exploration,
file191,11,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)
 total_uuid = prediction_list.shape[0]",data preprocessing,data exploration,
file191,12,"# check if how many weeks of data are provided for each uu_id
 query = """"""
 SELECT uu_id, COUNT(week_number) as num_of_weeks
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY uu_id
 ORDER BY num_of_weeks DESC
 """"""",load data,,
file191,13,bigquery_client.query(query).to_dataframe(),data preprocessing,,
file191,14,"# query week_number data for uu_id f43fb9e90c5ecf879016b159aaa17fcb
 query = """"""
 SELECT uu_id, week_number
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 WHERE uu_id=""f43fb9e90c5ecf879016b159aaa17fcb""
 ORDER BY week_number
 """"""
 print(bigquery_client.query(query).to_dataframe().to_string(index=False))",load data,data exploration,
file191,15,"# use sub-query to retrieve the num_of_uuids vs num_of_weeks, plot the results
 query = """"""
 SELECT num_of_weeks, COUNT(num_of_weeks) as num_of_uuids
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as num_of_weeks
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
  ORDER BY num_of_weeks DESC
 )
 GROUP BY num_of_weeks
 ORDER BY num_of_weeks
 """"""",load data,,
file191,16,"week_count_label = list(map(str, list(uuid_weeks[""num_of_weeks""])))
 num_of_uuids = list(uuid_weeks[""num_of_uuids""])
 plt.figure(figsize=(6,6))
 plt.barh(week_count_label, num_of_uuids)
 plt.yticks(week_count_label)
 plt.ylabel(""How many weeks of data are included"")
 plt.xlabel(""Number of UUIDs"")
 for i, v in enumerate(num_of_uuids):
  plt.text(v + 1, i - 0.4, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file191,17,"print(str(np.array(num_of_uuids)[-6:].sum()), ""out of"", total_uuid, ""uuids have no less than 30 weeks of datapoints."")
 print(str(np.array(num_of_uuids)[:9].sum()), ""out of"", total_uuid, ""uuids have no more than 10 weeks of datapoints."")",data exploration,,
file191,18,"# query the number of datapoints for each week, use DISTINCT on uu_id to remove duplication
 QUERY = """"""
 SELECT week_number, Count(DISTINCT(uu_id)) as count
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY week_number
 ORDER BY week_number
 """"""
 query_job = bigquery_client.query(QUERY)
 week_number_count = query_job.to_dataframe()
 display(week_number_count.T)
 print(""week_number: \n"" + str(week_number_count.T.to_numpy()[0]))
 print(""count: \n"" + str(week_number_count.T.to_numpy()[1]))",load data,data exploration,
file191,19,"# fill the missing weeks' count with 0 for week 4 and week 23
 i = 0
 week_number_count_filled = []
 for w_c in week_number_count.to_numpy():
  i = i + 1
  if w_c[0] == i:
  week_number_count_filled.append(list(w_c))
  else:
  week_number_count_filled.append(list([i, 0]))
  week_number_count_filled.append(list(w_c))
  i = i + 1
 print(""Total number of weeks:"", i)",data preprocessing,,
file191,20,"# plot the filled result
 week_number = list(np.array(week_number_count_filled)[:,0])
 week_count = list(np.array(week_number_count_filled)[:,1])
 plt.figure(figsize=(8,6))
 plt.barh(week_number, week_count)
 plt.yticks(week_number)
 plt.ylabel(""Week number"") 
 plt.xlabel(""Number of Datapoints"")
 for i, v in enumerate(week_count):
  plt.text(v + 2, i + 0.7, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file191,21,"# select 10 of 96 uuids with 3 weeks' data
 QUERY = """"""
 SELECT uu_id
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as week_count
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
 )
 WHERE week_count = 35
 LIMIT 10
 """"""
 query_job = bigquery_client.query(QUERY)
 ten_uuid_with_35_weeks = query_job.to_dataframe()
 ten_uuid_with_35_weeks",load data,data preprocessing,
file191,22,"# query the 10 selected UUID data
 ten_35w_data = []
 for uu_id in ten_uuid_with_35_weeks[""uu_id""]:
  QUERY=""""""
  SELECT uu_id, week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uu_id)
  query_job = bigquery_client.query(QUERY)
  ten_35w_data.append(query_job.to_dataframe().drop_duplicates(ignore_index=True))",load data,data preprocessing,
file191,23,"# manually add week 4 and week 23 missing data
 def fill_week4_and_week23(df, method, replace=False):
  """"""
  Fill or replace the week 4 and week 23 missing total_claims data.
  Args:
  df: Dataframe
  The input dataframe with missing total claims or to be replaced.
  method: string
  prev - use previous week's value
  next - use next week's value
  mid - use mean value of previous and next weeks
  replace: bool
  True - replace the existing week 4 and week 23 total_claim values
  False - add the missing values
  Returns:
  Dataframe:
  Dataframe with added total_claim values or replaced values.
  """"""
  uuid = df[""uu_id""][0]
  week_list = list(df[""week_number""])
  if 4 in week_list and 23 in week_list and not replace:
  print(""Week 4 and week 23 data already exist, please use replace=True"")
  return df
  if 4 not in week_list and 23 not in week_list and replace:
  print(""Week 4 and week 23 data do not exist, replace failed"")
  return df
  if 4 not in week_list and 23 not in week_list and not replace:
  print(""Fill uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  if 4 in week_list and 23 in week_list and replace:
  print(""Replace uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  # remove exisiting value and insert again
  df = df.drop(df.index[[3, 22]])
  df = df.sort_index().reset_index(drop=True)
  
  if method == ""prev"":
  # use previous avaliable week's value
  val_4 = df[""total_claims""][2]
  val_23 = df[""total_claims""][20]
  if method == ""next"":
  # use next avaliable week's value
  val_4 = df[""total_claims""][3]
  val_23 = df[""total_claims""][21]
  if method == ""mid"":
  # use mean value of previous and next avaliable weeks
  val_4 = int((df[""total_claims""][2] + df[""total_claims""][3]) * 0.5)
  val_23 = int((df[""total_claims""][20] + df[""total_claims""][21]) * 0.5)
  
  # week 4
  df.loc[2.5] = [uuid, 4, val_4]
  # week 23
  df.loc[20.5] = [uuid, 23, val_23]
  return df.sort_index().reset_index(drop=True)",data preprocessing,,
file191,24,"# fill the missing data using mean value of prev week and next week for all 10 uuids
 for i in range(len(ten_35w_data)):
  ten_35w_data[i] = fill_week4_and_week23(ten_35w_data[i], ""mid"")",data preprocessing,,
file191,25,"# plot the 10 uuid total_claim data from week 1 to week 37
 plt.figure(figsize=(12,4))
 plt.title(""10 selected uuid with 37 weeks of data total_claim plot"")
 for i in range(len(ten_35w_data)):
  plt.plot(list(map(str, ten_35w_data[i][""week_number""])), 
  ten_35w_data[i][""total_claims""], 
  ""o-"", linewidth=1, markersize=5, alpha=0.7,
  label=ten_35w_data[i][""uu_id""][0])
  plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)
 plt.xlabel(""week_number"")
 plt.ylabel(""total_claims"")
 plt.grid(axis=""y"")
 plt.tight_layout()
 plt.show()",result visualization,,
file191,26,"# find out which weeks are available for these uuids
 week_available = []
 for i in range(len(data_less_10w)):
  week_available.append(list(data_less_10w[i][""week_number""]))",data preprocessing,,
file191,27,"# plot the avaliable weeks vs uu_id
 plt.figure(figsize=(12,4))
 plt.title(""week_number distribution for uuid with no more than 10 weeks of data"")
 colors = ['C{}'.format(i) for i in range(total_uuid_less_10w)]
 plt.eventplot(week_available, orientation='vertical', linelengths=0.2, linewidths=4, colors=colors)
 plt.yticks([i for i in range(1,38,2)])
 plt.ylabel(""week_number"")
 plt.xticks([i for i in range(total_uuid_less_10w)])
 plt.xlabel(""uu_id"")
 plt.grid(axis=""x"")
 plt.axhline(4, alpha=0.5)
 plt.axhline(23, alpha=0.5)
 plt.axhline(37, alpha=0.5)
 plt.tight_layout()
 plt.show()",result visualization,,
file191,28,"# query all total_claims data and store in raw_total_claims_data dict
 def query_all_total_claims():
  temp_query_list = prediction_list[""uu_id""] # the list of uuids to be queried
  raw_total_claims_data = {uuid:[] for uuid in temp_query_list} # dict to store all query results
  total_week = 37
 

  progress = 0
  total = len(temp_query_list)
  for uuid in temp_query_list:
  progress = progress + 1
  print('\r', ""Querying"", str(progress) + ""/"" + str(total), ""uuid's data..."", end='\r')
  QUERY=""""""
  SELECT week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uuid)
  query_job = bigquery_client.query(QUERY)
  res = query_job.to_dataframe().drop_duplicates(ignore_index=True).to_numpy()
 

  # Fill the missing total_claims between week 1 and week 37 with value 0
  index = 0
  for w in range(1,total_week + 1):
  if index == len(res):
  raw_total_claims_data[uuid].append({w:0})
  continue
  if w == res[index][0]:
  raw_total_claims_data[uuid].append({w:res[index][1]})
  index = index + 1
  else:
  raw_total_claims_data[uuid].append({w:0})
  return raw_total_claims_data",load data,data preprocessing,
file191,29,"# commented for only excuting once
 raw_total_claims_data = query_all_total_claims()",data preprocessing,,
file191,30,"# save query result into csv file
 raw_total_claims_df = pd.DataFrame.from_dict(raw_total_claims_data)
 raw_total_claims_df.to_csv(""raw_total_claims.csv"", index=False)",save results,,
file191,31,"raw_total_claims_df = pd.read_csv(""raw_total_claims.csv"")",load data,,
file191,32,"for i in range(0, 37):
  for j in range(0, total_uuid):
  d = raw_total_claims_df.iloc[i,j]
  # print(int(d.split("":"")[1].split(""}"")[0]))
  raw_total_claims_df.iloc[i,j] = int(d.split("":"")[1].split(""}"")[0])",data preprocessing,,
file191,33,"raw_total_claims_list = raw_total_claims_df.T.to_numpy()
 raw_total_claims_list = raw_total_claims_list[:]",data preprocessing,,
file191,34,"x_labels = [w for w in range(1, 38)]
 NUM_PLOTS = len(raw_total_claims_list)",data preprocessing,,
file191,35,"plt.figure(figsize=(15,5))
 plt.xticks(x_labels)
 for i in range(0, NUM_PLOTS):
  plt.plot(x_labels, raw_total_claims_list[i], "".-"")",result visualization,,
file191,36,"# fill week 4 and week 23 data with mean values for all uuids
 for row in raw_total_claims_list:
  temp_sum = 0
  w4_val = round(0.5 * (row[2] + row[4]) + 0.01)
  w23_val = round(0.5 * (row[21] + row[23]) + 0.01)
  row[3] = w4_val
  row[22] = w23_val",data preprocessing,,
file191,37,"def mse_loss(y_true, y_pred):
  """"""
  MSE loss function
  Inputs param
  -------------------------
  y_true: list
  The ground truth values
  y_pred: list
  The predicted values
  -------------------------
  return: number
  mean square error of the two input lists
  """"""
  if(len(y_true) != len(y_pred)):
  print(""True label len:"" + str(len(y_true)) + "", Predict label len: "" + str(len(y_pred)))
  raise Exception(""Input lists have different length"")
  mse = np.mean(np.array(y_true) - np.array(y_pred))**2
  return mse",evaluation,,
file191,38,"uuid_list = list(raw_total_claims_df.columns)
 pred_results = [round(np.array(row).mean() + 0.01) for row in raw_total_claims_list]",data preprocessing,,
file191,39,"# Mean value as the prediction result
 result_dict = {
  'uu_id':uuid_list, 
  'total_claims': pred_results,
  'week_number': [39 for i in range(total_uuid)]
 }
 result_to_csv = pd.DataFrame(result_dict)
 result_to_csv",data preprocessing,data exploration,
file191,40,"print(uuid_list[0])
 print(list(raw_total_claims_list[0]))",data exploration,,
file191,41,"print(list(raw_total_claims_df[""8ba19786b86ae124a9d7eaa054f15d23""]))",data exploration,,
file191,42,"result_to_csv.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file191,43,temp_res = [row[-1] for row in raw_total_claims_list],data preprocessing,,
file191,44,"mae_loss(temp_res, pred_results)",evaluation,,
file191,45,"query = """"""
 SELECT Count(DISTINCT(uu_id)) as N_UUID
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 WHERE week_number = 37
 """"""
 query_job = bigquery_client.query(query)
 week_39_count = query_job.to_dataframe()
 week_39_count",load data,data preprocessing,
file191,46,"i = 0
 data = []
 for uu_id in prediction_list[""uu_id""]:
  i = i + 1
  query = """"""
  SELECT *
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uu_id)
  print(""Querying #"" + str(i) + "":"", uu_id, end='\r')
  query_job = bigquery_client.query(query)
  data.append(query_job.to_dataframe())",data preprocessing,,
file191,47,"query = """"""
 SELECT week_number, sum(total_claims) as all_claims
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY week_number
 ORDER BY week_number
 """"""",load data,,
file191,48,bigquery_client.query(query).to_dataframe(),data preprocessing,,
file191,49,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file192,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file192,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file192,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file192,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file192,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline",helper functions,,
file192,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file192,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file192,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file192,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file192,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,data preprocessing,
file192,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file192,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 37
 """"""",load data,,
file192,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file192,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file192,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file192,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file192,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file192,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file192,18,data['tract_name'],data exploration,,
file192,19,"data['tract_name'] = [i.split(',')[1].split(' ')[0] for i in data['tract_name']]",data preprocessing,,
file192,20,"tract_name_encoder = OneHotEncoder(sparse=False)
 data['tract_name'] = tract_name_encoder.fit_transform(data['tract_name'])",data preprocessing,,
file192,21,data['tract_name'].unique(),data exploration,,
file192,22,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.categories_",data preprocessing,data exploration,
file192,23,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(list(data['tract_name'].values.reshape(-1, 1)))
 tract_name_encoder.categories",data preprocessing,data exploration,
file192,24,"data['tract_name'] = tract_name_encoder.transform(list(data['tract_name'].values.reshape(-1, 1))).toarray()",data preprocessing,,
file192,25,tract_name_encoder.transform(data['tract_name'].values),data exploration,,
file192,26,data,data exploration,,
file192,27,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(data['tract_name'].values.reshape(-1, 1))
 #tract_name_encoder.categories_[0]
 data[tract_name_encoder.categories_[0]] = tract_name_encoder.transform(data['tract_name'].values.reshape(-1, 1))
 data = data.drop(['tract_name'], axis=1)",data preprocessing,,
file192,28,"data = data.drop(['top_category_employer1', 'top_category_employer2',
  'top_category_employer3'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file192,29,"plt.figure(figsize=(5,5))
 cor = data.corr().round(2)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={""size"": 6})
 plt.show()",result visualization,,
file192,30,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file192,31,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file192,32,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file192,33,"data = pd.concat([data, tract_name_dataset])
 display(data)",data preprocessing,data exploration,
file192,34,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file192,35,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file192,36,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,data exploration,
file192,37,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",data preprocessing,,
file192,38,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file192,39,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file192,40,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",evaluation,,
file192,41,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=10)",modeling,,
file192,42,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file192,43,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,,
file192,44,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",prediction,,
file192,45,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,,
file192,46,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",result visualization,,
file192,47,plt.show(),result visualization,,
file192,48,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file192,49,unemployment_prediction_data = query_from_statement(prediction_query),data preprocessing,,
file192,50,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file192,51,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data preprocessing,data exploration,
file192,52,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",prediction,,
file192,53,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,,
file193,0,"import os
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file193,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemp_data = query_job.to_dataframe()
 unemp_data.head(5)",load data,data exploration,
file193,2,unemp_data.info(),data exploration,,
file193,3,"unemp_data['top_category_employer2'] = unemp_data['top_category_employer2'].replace('N/A',np.NaN)
 unemp_data['top_category_employer3'] = unemp_data['top_category_employer3'].replace('N/A',np.NaN)",data preprocessing,,
file193,4,unemp_data['race_asian'] = unemp_data.groupby('countyfips')['race_asian'].transform(lambda x: x.fillna(int(x.mean()))),data preprocessing,,
file193,5,"unemp_data['race_hawaiiannative'] = unemp_data.groupby('countyfips')['race_hawaiiannative'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_other'] = unemp_data.groupby('countyfips')['race_other'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_white'] = unemp_data.groupby('countyfips')['race_white'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file193,6,"unemp_data['race_noanswer'] = unemp_data['race_noanswer'].fillna(0)
 unemp_data['gender_na'] = unemp_data['gender_na'].fillna(0)
 unemp_data['edu_unknown'] = unemp_data['edu_unknown'].fillna(0)",data exploration,,
file193,7,unemp_data['edu_post_hs'] = unemp_data.groupby('countyfips')['edu_post_hs'].transform(lambda x: x.fillna(int(x.mean()) if not x.isnull() else 0)),data preprocessing,,
file193,8,"import numpy as np
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].replace(NA,np.NaN)
 unemp_data['edu_post_hs'] = unemp_data.groupby('countyfips')['edu_post_hs'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file193,9,unemp_data['edu_post_hs'],data exploration,,
file193,10,"#unemp_data['gender_female'] = unemp_data.groupby('countyfips')['gender_female'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['gender_male'] = unemp_data.groupby('countyfips')['gender_male'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file193,11,unemp_data.isna().sum(),data exploration,,
file193,12,unemp_data['top_category_employer2'] = unemp_data.groupby('countyfips')['top_category_employer2'].transform(lambda x: x.fillna(int(x.mode()[0] if not x.mode().empty else 'unknown'))),data preprocessing,,
file193,13,unemp_data['edu_post_hs'] = unemp_data.groupby('countyfips')['edu_post_hs'].fillna(0),data preprocessing,,
file193,14,unemp_data['gender_male'] = unemp_data['gender_male'].astype(float),data preprocessing,,
file193,15,"unemp_data['gender_female'] = unemp_data['gender_female'].fillna(unemp_data.groupby('countyfips')['gender_female'].transform('mean'))
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].fillna(unemp_data.groupby('countyfips')['edu_post_hs'].transform('mean'))",data preprocessing,,
file193,16,"unemp_data['gender_male'] = unemp_data['gender_male'].astype(float)
 unemp_data['gender_female'] = unemp_data['gender_female'].astype(float)
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].astype(float)",data preprocessing,,
file194,0,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file194,1,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",helper functions,data preprocessing,
file194,2,"from google.cloud import bigquery
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file194,3,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/.config/gcloud/application_default_credentials.json'
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file195,0,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file195,1,"from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
 from sklearn.model_selection import train_test_split
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV",helper functions,,
file195,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file195,3,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file195,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file195,5,"query = """"""
 SELECT 
 a.*,
 b.average_wage
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`a
 

 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id = b.uu_id
 

 

 """"""",load data,,
file195,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['timeperiod']= pd.to_datetime(data['timeperiod'])
 data.head()",load data,data preprocessing,
file195,7,data.drop_duplicates(inplace=True),data preprocessing,,
file195,8,get_ipython().system('pip install db-dtypes'),helper functions,,
file195,9,data = data.fillna(0),data preprocessing,,
file195,10,data.describe(),data exploration,,
file195,11,data.sum(numeric_only=True),data exploration,,
file195,12,data.columns,data exploration,,
file195,13,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data exploration,
file195,14,"y = data.total_claims
 x = data.drop(['total_claims', 'timeperiod'], axis = 1)",data preprocessing,,
file195,15,"uuid, label = data['uu_id'].factorize(sort=True)",data preprocessing,,
file195,16,x['uu_id'] = uuid,data preprocessing,,
file195,17,x['tract_name'] = x['tract_name'].factorize()[0],data preprocessing,,
file195,18,"x['top_category_employer1'] = x['top_category_employer1'].factorize()[0]
 x['top_category_employer2'] = x['top_category_employer2'].factorize()[0]
 x['top_category_employer3'] = x['top_category_employer3'].factorize()[0]",data preprocessing,,
file195,19,"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)",data preprocessing,,
file195,20,"model = HistGradientBoostingRegressor(loss=""squared_error"")
 model.fit(x_train, y_train,
  verbose=False) # Change verbose to True if you want to see it train",modeling,evaluation,
file195,21,"model = HistGradientBoostingRegressor(loss=""squared_error"")
 model.fit(x_train, y_train)",modeling,evaluation,
file195,22,"model.score(x_test, y_test)",evaluation,,
file195,23,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` 
 """"""",load data,,
file195,24,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 pred = query_job.to_dataframe()
 pred.head()",load data,data preprocessing,
file195,25,pred,data exploration,,
file195,26,"x['uu_id'] = label[x[""uu_id""]]
 x",data preprocessing,data exploration,
file195,27,"for col in x.columns[2:]:
  li = []
  for i in pred['uu_id']:
  li.append(x.loc[x['uu_id'] == i, col].mean())
  pred[col] = li",data preprocessing,,
file195,28,pred['uu_id'] = pred['uu_id'].factorize(sort=True)[0],data preprocessing,,
file195,29,x_test,data exploration,,
file195,30,pred = pred.apply(pd.to_numeric),data preprocessing,,
file195,31,predictions = gridcv_xgb.predict(pred),prediction,,
file195,32,predictions,evaluation,,
file195,33,out_df = pd.DataFrame(),data preprocessing,,
file195,34,"out_df[""uu_id""] = pred[""uu_id""]
 out_df[""total_claims""] = predictions
 out_df[""week_numer""] = pred[""week_number""]",data preprocessing,,
file195,35,out_df,data exploration,,
file195,36,"uuid, label = uu_id['uu_id'].factorize(sort=True)",data preprocessing,,
file195,37,"out_df[""uu_id""] = label[out_df[""uu_id""]]",data preprocessing,,
file195,38,"out_df.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file196,0,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid]
 

  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = data.drop(['uu_id','total_claims'], axis = 1)
  y = data[['total_claims']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.8, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
  plt.title('Tract: '+uuid)
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",modeling,prediction,
file196,1,,,,
file196,2,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",evaluation,,
file196,3,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file196,4,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file196,5,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file196,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file196,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file196,8,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file196,9,"unemployment_data = unemployment_data.drop_duplicates().fillna(0, inplace=True)",data preprocessing,,
file196,10,unemployment_data,data exploration,,
file196,11,unemployment_data = unemployment_data,data preprocessing,,
file196,12,"sns.heatmap(unemployment_data.fillna(0).corr(), annot = True)
 plt.show()",result visualization,,
file196,13,unemployment_data.isnull().sum(),data exploration,,
file196,14,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file196,15,"unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file196,16,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file196,17,len(uuids),data exploration,,
file197,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file197,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file197,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file197,3,"def get_data(table_name):
  query = f""""""
  SELECT *
  FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """"""
 

  # QUERY THE DATA ONCE
  query_job = bigquery_client.query(query)
  return query_job.to_dataframe()",load data,data preprocessing,
file197,4,unemploy = get_data('unemployment_data),data preprocessing,,
file197,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file197,6,wage = get_data('wage_data'),load data,data preprocessing,
file197,7,sub = get_data('prediction_list'),load data,data preprocessing,
file197,8,sub['uu_id'],data exploration,,
file197,9,sub['uu_id'].to_list(),data exploration,,
file197,10,prediction_list.groupby('uu_id'),data exploration,,
file197,11,unemploy.groupby('uu_id'),data exploration,,
file197,12,unemploy.groupby('uu_id').get_group(0),data exploration,,
file197,13,unemploy.groupby('uu_id').mean('total_claims'),data exploration,,
file197,14,pred = unemploy.groupby('uu_id')['total_claims'].mean(),data exploration,,
file197,15,pred.index,data exploration,,
file197,16,pred,data exploration,,
file197,17,pred = pred[sub['uu_id'].to_list()],data preprocessing,,
file197,18,pred.columns,data exploration,,
file197,19,pred.to_numpy(,data exploration,,
file197,20,"pred = pd.Dataframe({
  'uu_id': pred.index,
  'total_claims': pred.to_list()
 })",data preprocessing,,
file197,21,"pred._to_csv('submission_prediction_output.csv', index=False)",save results,,
file197,22,unemploy.groupby('uu_id')['total_claims'].mean().apply(lambda x: int(x)),data exploration,,
file197,23,unemploy.groupby('uu_id').apply(lambda x: print(x)),data exploration,,
file197,24,unemploy.groupby('uu_id').apply(lambda x: x.sort_values('week_number').tail()),data exploration,,
file197,25,unemploy.groupby('uu_id').apply(lambda x: x.sort_values('week_number')['total_claims'].ewm(alpha=1/3).mean().to_numpy()[-1]),data exploration,,
file198,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file198,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file198,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file198,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file198,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file198,5,data.info(),data exploration,,
file198,6,"data[""top_category_employer2""].value_counts()",data exploration,,
file198,7,"data.loc[data[""uu_id""] == ""bd5b040eae3010ce09da8b176ed5aee0""]",data preprocessing,,
file198,8,"data[""edu_8th_or_less""].value_counts()",data exploration,,
file198,9,"data[""top_category_employer2""].map(lambda x: '32' if x =='31-33')",data exploration,,
file198,10,"data[""top_category_employer2""] = data[""top_category_employer2""].map(lambda x: '32' if x =='31-33' else x)",data preprocessing,,
file198,11,"df[""top_category_employer1""].astype(int)",data exploration,,
file198,12,"data[""race_black""].value_counts()",data exploration,,
file198,13,"data['edu_8th_or_less'].replace(np.NaN, data['edu_8th_or_less'].mean())",data exploration,,
file198,14,data.fillna('0'),data exploration,,
file198,15,data = data.fillna(0),data preprocessing,,
file198,16,data.isnull(),data exploration,,
file198,17,data['top_category_employer2'].astype(float),data exploration,,
file198,18,"data[""test""] = data.groupby(by=[""uu_id""]).sum()",data preprocessing,,
file198,19,"data.groupby(by=[""uu_id""]).mean()",data exploration,,
file198,20,"predict = data.groupby(by=[""uu_id""]).mean()
 predict",data preprocessing,data exploration,
file198,21,data['top_category_employer1'] = data['top_category_employer1'].astype(float),data preprocessing,,
file198,22,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import numpy as np",helper functions,,
file198,23,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file198,24,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file198,25,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file198,26,data.info(),data exploration,,
file198,27,get_ipython().system('pip install db-dtypes'),helper functions,,
file198,28,"X_train = data[""""]",data preprocessing,,
file198,29,"X_train = data[""edu_8th_or_less"", ""edu_grades_9_11"", ""edu_hs_grad_equiv"", ""gender_female"", gender_male, ]X_train = data[""edu_8th_or_less"", ""edu_grades_9_11"", ""edu_hs_grad_equiv"", ""gender_female"", ""gender_male"", ""race_amerindian"", ""race_asian"", ""race_black"", ""race_noanswer"", ""race_hawaiiannative"", ""race_other"", ""race_white""]",data preprocessing,,
file198,30,"X_train = data.iloc[:, 7:24]",data preprocessing,,
file198,31,"y_train = data[""total_claims""]",data preprocessing,,
file198,32,from sklearn.linear_model import LinearRegression,helper functions,,
file198,33,"model = LinearRegression().fit(X_train, y_train)",modeling,evaluation,
file198,34,"model.predict(X_test = predict.iloc[:, 7:24])",prediction,,
file198,35,model.predict(X_test),prediction,,
file198,36,"X_test = predict.iloc[:, 7:24]",prediction,,
file198,37,X_test,evaluation,,
file199,0,"query = """"""
 SELECT * FROM 'ironhacks-data.ironhacks_competition.unemployment_data'
 """"""",load data,,
file199,1,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file200,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file201,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file201,1,"import pandas as pd
 import numpy as np
 from google.cloud import bigquery",helper functions,,
file201,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file201,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file201,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file201,5,unemployment_data.shape,data exploration,,
file201,6,unemployment_data.columns,data exploration,,
file201,7,"unemployment_data.drop(['uu_id', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data preprocessing,,
file201,8,"unemployment_data.drop_duplicates(inplace=True)
 unemployment_data.sort_values(['week_number'])",data preprocessing,,
file201,9,"def add_missing_weeks(df):
  # Fill in missing weeks by taking the ceil of the average of prev and next
  for week in range(1, 37):",data preprocessing,,
file201,10,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file201,11,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file201,12,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day', 'week_number'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file201,13,"unemployment_data.sort_values(['ds'], inplace=True)",data preprocessing,,
file201,14,from prophet import Prophet,helper functions,,
file201,15,ud = unemployment_data,data preprocessing,,
file201,16,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file201,17,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",data preprocessing,,
file201,18,ud_train,data exploration,,
file201,19,"m = Prophet(weekly_seasonality=False,
  daily_seasonality=False,
  interval_width=0.95, 
  mcmc_samples = 500)",modeling,,
file201,20,m.fit(ud_train),evaluation,,
file201,21,"future = m.make_future_dataframe(periods=20, freq='W')",data preprocessing,,
file201,22,forecast = m.predict(df=future),prediction,,
file201,23,"m.fit(ud_train, show_console=True)",evaluation,,
file202,0,"vals = {}
 vals[""0.4""] = {}
 vals[""0.4""][6] = [0,39]
 vals[""0.4""][7] = [12,29]
 vals[""0.4""][8] = [15,30]
 vals[""0.4""][9] = [29,18]
 vals[""0.4""][10] = [34,16]",data preprocessing,,
file202,1,"plt.figure()
 for key1,values1 in vals.items():
  #print(values1)
  if key1!=""0.4"": continue
  C_arr = []
  R_arr = []
  for key2,values2 in values1.items():
  R = values2[0]
  C = values2[1]
  C_arr.append(C)
  R_arr.append(R)
  plt.plot(C_arr,label=""Clean samples"",linewidth=3)
  plt.plot(R_arr,label=""Refurbished samples"",linewidth=3)
 plt.legend()
 plt.xticks(ticks=range(0,5),labels=[""6"",""7"",""8"",""9"",""10""])
 plt.grid()
 plt.xlabel(""epochs"")
 plt.ylabel(""Sample count"")
 plt.savefig(""clean-samples.png"")",result visualization,,
file202,2,"import numpy as np
 import matplotlib.pyplot as plt",helper functions,,
file202,3,"labels = [""plane"",""automobile"",""bird"",""cat"",""deer"",""dog"",""frog"",""horse"",""ship"",""truck""]
 mat = np.matrix('3446 1554 0 0 0 0 0 0 0 0;0 3408 1592 0 0 0 0 0 0 0; 0 0 3524 1476 0 0 0 0 0 0;0 0 0 3482 1518 0 0 0 0 0; 0 0 0 0 3520 1480 0 0 0 0;0 0 0 0 0 3493 1507 0 0 0; 0 0 0 0 0 0 3511 1489 0 0; 0 0 0 0 0 0 0 3530 1470 0; 0 0 0 0 0 0 0 0 3514 1486; 1496 0 0 0 0 0 0 0 0 3504'
 )",data preprocessing,,
file202,4,"plt.figure(figsize=(10,8))
 plt.imshow(mat)
 plt.colorbar()
 plt.xticks(ticks=range(0,10),labels=labels,size=10)
 plt.yticks(ticks=range(0,10),labels=labels,size=10)
 #plt.xtickslabels(x_ticks_labels, rotation='vertical', fontsize=18)",result visualization,,
file202,5,plt.show(),result visualization,,
file202,6,"vals = {}
 vals[""0.4""] = {}
 vals[""0.4""][1] = [0.85,68.699]
 vals[""0.4""][2] = [0.6805,77.77]
 vals[""0.4""][3] = [0.7722,78.47]
 vals[""0.4""][4] = [0.7347,78.59]
 vals[""0.4""][5] = [0.7136,81.94]
 vals[""0.4""][6] = [0.5521,84.86]
 vals[""0.4""][7] = [0.446,88.59]
 vals[""0.4""][8] = [0.4624,88.06]
 vals[""0.4""][9] = [0.3605,90.50]
 vals[""0.4""][10] = [0.3691,90.43]",data preprocessing,,
file202,7,"vals[""0.3""] = {}
 vals[""0.3""][1] = [0.642,85]
 vals[""0.3""][2] = [0.629,88]
 vals[""0.3""][3] = [0.5632,89.52]
 vals[""0.3""][4] = [0.5826,89.36]
 vals[""0.3""][5] = [0.5632,88.68]
 vals[""0.3""][6] = [0.3787,89.61]
 vals[""0.3""][7] = [0.3125,91.45]
 vals[""0.3""][8] = [0.344,91.13]
 vals[""0.3""][9] = [0.339,91]
 vals[""0.3""][10] = [0.3538,90.75]",data preprocessing,,
file202,8,"vals[""0.1""] = {}
 vals[""0.1""][1] = [0.416,87.57]
 vals[""0.1""][2] = [0.3799,90.97]
 vals[""0.1""][3] = [0.3526,91.97]
 vals[""0.1""][4] = [0.3328,92.25]
 vals[""0.1""][5] = [0.3144,92.77]
 vals[""0.1""][6] = [0.2582,92.93]
 vals[""0.1""][7] = [0.2532,93.32]
 vals[""0.1""][8] = [0.2728,92.90]
 vals[""0.1""][9] = [0.2694,93.00]
 vals[""0.1""][10] = [0.2748,93.26]",data preprocessing,,
file202,9,"plt.figure()
 for key1,values1 in vals.items():
  #print(values1)
  loss_arr = []
  err_arr = []
  for key2,values2 in values1.items():
  loss = values2[0]
  acc = values2[1]
  err = 100-acc
  loss_arr.append(loss)
  err_arr.append(err)
  plt.plot(loss_arr,label=""Noise rate=""+key1)
  #plt.plot(err_arr,label=""Noise rate=""+key1)
 plt.legend()
 plt.grid()
 plt.xlabel(""epochs"")
 plt.ylabel(""Test-loss"")
 plt.savefig(""test-loss.png"")",result visualization,,
file203,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file203,1,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file203,2,from google.cloud import bigquery,helper functions,,
file203,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file203,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file203,5,covid19_cases_data.head(,data exploration,,
file203,6,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file203,7,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.unemployment_data`
 """"""",load data,,
file203,8,unemployment_data.head(),data exploration,,
file204,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file204,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file204,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file204,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file204,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file204,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,data preprocessing,
file204,6,empdata.head().transpose(),data exploration,,
file204,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file204,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file204,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data preprocessing,,
file204,10,empdata[['total_claims']].describe(),data exploration,,
file204,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file204,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",result visualization,,
file204,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file204,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data preprocessing,,
file204,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number')
 allweeks",data preprocessing,data exploration,
file204,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=6)[5]",modeling,prediction,
file205,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file205,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file205,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file205,3,"# get data information
 query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file205,4,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",load data,,
file205,5,"unemployment = query_job1.to_dataframe()
 wage = query_job2.to_dataframe()
 pred = query_job3.to_dataframe()",data preprocessing,,
file205,6,"# combine all data
 query4 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.uu_id
 """"""
 query_job4 = bigquery_client.query(query4)
 data = query_job4.to_dataframe()",load data,data preprocessing,
file205,7,"# drop duplicate columns
 dropcol = ['uu_id_1','countyfips_1','tract_1','tract_name_1']
 for col in dropcol:
  data = data.drop(col, axis=1)",data preprocessing,,
file205,8,data.head(),data exploration,,
file205,9,data1 = data.dropna(),data preprocessing,,
file205,10,data1,data exploration,,
file206,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file206,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file207,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import db_dtypes",helper functions,,
file207,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file207,2,"#QUERY DATA
 query_job = bigquery_client.query(query)
 print(query_job)
 unemp = query_job.to_dataframe()",load data,data preprocessing,
file207,3,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file207,4,"unemp.loc[unemp['tract_name'] == ""Allen County""]",data exploration,,
file207,5,"unemp.loc[unemp['tract_name'] == ""Census Tract 9, Allen County, Indiana""]",data exploration,,
file207,6,unemp['tract_name'].value_counts()['tract_name'],data exploration,,
file207,7,print(unemp['tract_name'].value_counts()),data exploration,,
file207,8,"X = data['total_claims'].values.reshape(-1,1)
 y = data['gender_male'].values.reshape(-1,1)",data preprocessing,,
file207,9,"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)",data preprocessing,,
file207,10,X_train.head(),data preprocessing,,
file207,11,print(X_train),data exploration,,
file207,12,"#Trains Algorithim
 regressor = LinearRegression()  
 regressor.fit(X_train, y_train)",modeling,evaluation,
file207,13,"X_train = np.array(X_train)
 Y_train = np.array(Y_train)",data preprocessing,,
file207,14,X.head,data exploration,,
file207,15,"#To retrieve the intercept:
 print(regressor.intercept_)",evaluation,,
file207,16,"#For retrieving the slope:
 print(regressor.coef_)",evaluation,,
file207,17,print(X_test),data exploration,,
file207,18,print(y),data exploration,,
file207,19,unemp = unemp[unemp['tract'] == 4300],data preprocessing,,
file207,20,"X = np.array(X)
 y = np.array(y)",data preprocessing,,
file207,21,unemp.groupby(['tract']).Tract.value_counts().nlargest(5),data exploration,,
file207,22,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file207,23,"#QUERY DATA
 query_job = bigquery_client.query(query)
 print(query_job)
 unemp = query_job.to_dataframe()",load data,data preprocessing,
file208,0,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install db-dtypes\n!python3 -m pip install pandas\n"")",helper functions,,
file208,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file208,2,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data preprocessing,data exploration,
file208,3,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data preprocessing,data exploration,
file208,4,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",data preprocessing,data exploration,
file208,5,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: data for the first day, its month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",data preprocessing,data exploration,
file208,6,"def MSPE(s1, s2):
  print(""MSPE: "", sum((s1 - s2)**2)/len(s1))",data preprocessing,,
file208,7,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file208,8,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file208,9,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file208,10,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file208,11,"# Further check tracts with average_wage as Nan
 # I find three tracts with all average_wage as Nan. If I drop these tracts due to Nan value, they cannot be predicted
 for id in pd.unique(data[data['average_wage'].isna()][""uu_id""]):
  print(id)
  print(""All value are nan?"", data[data['uu_id'] == id][""average_wage""].isnull().all())
  print(""Included in prediction list?"", len(data_pred_query[data_pred_query['uu_id'] == id]) > 0)",data exploration,,
file208,12,"# Backup the data before pre-treatment
 data_backup = data.copy()
 data_pred_query_backup = data_pred_query.copy()",data preprocessing,,
file208,13,"# Pretreatment: convert week_number to month and week of month, to capture seasonality
 data = dataIdentifyDWM(data)",data preprocessing,,
file208,14,"# To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file208,15,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,,
file208,16,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file208,17,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file208,18,dataBalanceCheck(data_balance),data preprocessing,,
file208,19,"# Data clean up: convert NA to 0 for gender, race, education and top employer and recalculate unknown category
 # Based on the check of Nan in average_wage above, I also convert Nan to zero as well, but try models with and without ""average_wage"" variable
 data = dataFillNa(data, 0)",data preprocessing,,
file208,20,"# Split data to training and validaton sets
 train_week = max(pd.unique(data[""week_number""]))",data preprocessing,,
file208,21,"data_train = data[data[""week_number""] < train_week]
 data_valid = data[data[""week_number""] >= train_week]",data preprocessing,,
file208,22,"data_train_x = data_train.drop(""total_claims"",1)
 data_train_y = data_train[""total_claims""]",data preprocessing,,
file208,23,"data_balance_valid_x = data_balance_valid.drop(""total_claims"",1)
 data_balance_valid_y = data_balance_valid[""total_claims""]",data preprocessing,,
file208,24,"# Model 1 : Poisson regression with unbalanced data
 data_train_x_m1 = data_train_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_train_x_m1[""month""] = data_train_x_m1[""month""].astype(str)
 data_train_x_m1[""weekofmonth""] = data_train_x_m1[""weekofmonth""].astype(str)
 data_train_x_m1[""week_number2""] = data_train_x_m1[""week_number""]**2
 data_train_x_m1 = pd.get_dummies(data_train_x_m1)",data preprocessing,,
file208,25,"for i in range(8):
  data_valid_x_m1[""month_""+str(1+i)] = 0",data preprocessing,,
file208,26,"for i in range(5):
  if i == 1:
  pass
  data_valid_x_m1[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file208,27,"poission_model = sm.GLM(data_train_y.astype(int), data_train_x_m1.astype(float), family=sm.families.Poisson())
 result = poission_model.fit()
 result.summary()",modeling,evaluation,
file208,28,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
 MSPE(data_estimate_m1, data_valid_y)
 MAPE(data_estimate_m1, data_valid_y)",evaluation,,
file208,29,"data_balance_valid_x_m1 = data_balance_valid_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_balance_valid_x_m1[""month""] = data_balance_valid_x_m1[""month""].astype(str)
 data_balance_valid_x_m1[""weekofmonth""] = data_balance_valid_x_m1[""weekofmonth""].astype(str)
 data_balance_valid_x_m1[""week_number2""] = data_balance_valid_x_m1[""week_number""]**2
 data_balance_valid_x_m1 = pd.get_dummies(data_balance_valid_x_m1)",data preprocessing,,
file208,30,"# Although using a balanced model has better fit on the training set, the MSPE and MAPE are still larger then the first model.
 # So for this submission, I sitll use the m1 for prediction.
 data_lastWeek = data[data[""week_number""] == train_week][[""uu_id"", ""average_wage""]]
 data_lastWeek = data_lastWeek.drop_duplicates()",data preprocessing,,
file208,31,"data_pred = data_pred_query.copy()
 data_pred = data_pred.set_index('uu_id').join(data_lastWeek.set_index('uu_id'))
 data_pred.head()
 data_pred = dataIdentifyDWM(data_pred)
 data_pred = dataFillNa(data_pred, 0)
 data_pred[""month""] = data_pred[""month""].astype(str)
 data_pred = data_pred.drop(""date"",1)
 data_pred = pd.get_dummies(data_pred)
 for i in range(8):
  data_pred[""month_""+str(1+i)] = 0
 for i in range(5):
  if i == 1:
  pass
  data_pred[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file208,32,"output = result.predict(data_pred.astype(float))
 output_df = pd.DataFrame(output, columns=[""total_claims""])[""total_claims""]
 output_df[""uu_id""] = output_df.index",prediction,evaluation,
file208,33,"data_pred_query = data_pred_query.merge(output_df, on = ""uu_id"")
 data_pred_query = data_pred_query[[""uu_id"", ""total_claims"", ""week_number""]]",data preprocessing,,
file208,34,"## This can also be a good place for you to cleanup any input/output and export your results to a file.
 data_pred_query.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file209,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file209,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file209,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file209,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file209,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file209,5,"query = """"""
 

 SELECT date, max_rel_humidity
 FROM ironhacks-data.ironhacks_training.weather_data
 WHERE date='2020-06-16'
 

 

 

 

 """"""",load data,,
file209,6,"query_job = bigquery_client.query(query)
 get_ipython().system('python3 -m pip install pandas')
 import pandas
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file210,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file210,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file210,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file210,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file210,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data` ) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file210,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,data preprocessing,
file210,6,empdata.head().transpose(),data exploration,,
file210,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file210,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file210,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",result visualization,,
file210,10,empdata[['total_claims']].describe(),data exploration,,
file210,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file210,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",result visualization,,
file210,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file210,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file210,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file210,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",modeling,prediction,
file210,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` order by uu_id
 """"""",load data,,
file210,18,uupred.head(),data exploration,,
file210,19,"last_week = int(empdata['week_number'].max())
 last_week",data preprocessing,data exploration,
file210,20,"pred_week = int(uupred['week_number'].max())
 pred_week",data preprocessing,data exploration,
file210,21,uupred['total_claims'] = 0,data preprocessing,,
file210,22,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values[:lastw], seasonal=False, error_action='ignore')
  pred = m.predict(n_periods=predw-lastw)
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = int(pred[predw-lastw-1])
  for i in range(predw-lastw):
  uupred.loc[uupred['uu_id'] == uu, ['w'+str(lastw+i+1)]] = int(pred[i])
  print(uu, pred)",modeling,prediction,
file210,23,"dopred(33,7)",evaluation,,
file210,24,uupred,evaluation,,
file210,25,empdata.loc[(empdata['uu_id'] == 'fd613eba867c6ad7350a937f743b88f2') & (empdata['week_number'] == 37)],data exploration,,
file210,26,"head(pd.merge(uupred, empdata, on='uu_id'))",data exploration,,
file210,27,"uupred.join(empdata, on='uu_id')",data preprocessing,,
file210,28,"uupred.join(empdata.loc[empdata['week_number'] == 37][['total_claims']], on='uu_id')",data preprocessing,,
file210,29,"uupred.loc[uupred['uu_id'] == uu, ['total_claims']]",data exploration,,
file210,30,"uupred.loc[uupred['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d', ['total_claims']]",data exploration,,
file210,31,"pd.merge(uupred, empdata, how='inner', on='uu_id')",data preprocessing,,
file210,32,"pd.merge(uupred, empdata.loc[empdata['week_number'] == 37], how='inner', on='uu_id')",data preprocessing,,
file210,33,"print('Mean Absolute Error:', metrics.mean_absolute_error(x['total_claims_y'].values, x['w37'].values))
 #print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 #print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file210,34,empdata.loc[empdata['uu_id'] == '001cd9ae23064d7f0fd3cd327c873d8d']],data exploration,,
file210,35,"# VARMAX example
 from statsmodels.tsa.statespace.varmax import VARMAX
 from random import random
 # contrived dataset with dependency
 data = list()
 for i in range(100):
  v1 = random()
  v2 = v1 + random()
  row = [v1, v2]
  data.append(row)
 data_exog = [x + random() for x in range(100)]
 # fit model
 model = VARMAX(data, exog=data_exog, order=(1, 1))
 model_fit = model.fit(disp=False)
 # make prediction
 data_exog2 = [[100]]
 yhat = model_fit.forecast(exog=data_exog2)
 print(yhat)",modeling,evaluation,
file210,36,yhat,evaluation,,
file210,37,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 uupred = query_job.to_dataframe()",load data,,
file210,38,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima
 from statsmodels.tsa.statespace.varmax import VARMAX",helper functions,,
file210,39,uupred.dtypes,data exploration,,
file210,40,allweeks,data exploration,,
file210,41,allweeks.dtypes,data exploration,,
file210,42,allweeks.to_numpy(),data exploration,,
file210,43,allweeks.values,data exploration,,
file210,44,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  allweeks.dtypes
  #m = VARMAX(list(allweeks['total_claims'].values[:lastw]), order=(1,1))
  m = VARMAX(allweeks.values)
  m_fit = m.fit(disp=False)
  pred = m_fit.simulate(predw-lastw)
 # uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = int(pred[predw-lastw-1])
 # for i in range(predw-lastw):
 # uupred.loc[uupred['uu_id'] == uu, ['w'+str(lastw+i+1)]] = int(pred[i])
  print(uu, pred)",modeling,evaluation,
file210,45,"dopred(33,37)",modeling,evaluation,
file210,46,allweeks.to_numpy(dtype='int64'),data preprocessing,,
file210,47,allweeks['total_claims'].to_numpy(dtype='float64'),data preprocessing,,
file210,48,allweeks.to_numpy(dtype='float64').shape[1],data preprocessing,,
file210,49,"last_week = int(empdata['week_number'].max())
 last_week",data preprocessing,data exploration,
file210,50,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file211,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file211,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file211,2,get_ipython().system('pip install db_dtypes'),helper functions,,
file211,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file211,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file211,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file212,0,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file212,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file212,2,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file212,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file213,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file213,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file213,2,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file213,3,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file213,4,get_ipython().system('pip3 install git+https://github.com/ourownstory/neural_prophet.git#egg=neuralprophet'),helper functions,,
file213,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file213,6,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file213,7,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data preprocessing,
file213,8,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file213,9,"#Number of rows in wage_data
 len(wage_data.index)",data exploration,,
file213,10,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 prediction_list.head(3)",load data,data preprocessing,
file214,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file214,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file214,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from statsmodels.formula.api import ols
 from pandas import Series, DataFrame",helper functions,,
file214,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file214,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file214,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file214,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file214,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data preprocessing,data exploration,
file214,8,"query_job3 = bigquery_client.query(query3)
 prediction_list = query_job3.to_dataframe()
 prediction_list.head()",load data,data preprocessing,
file214,9,"unemploy_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemploy_wage_data = unemploy_wage_data.drop(['timeperiod', 'countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemploy_wage_data = unemploy_wage_data.fillna(0)
 unemploy_wage_data.head()",data preprocessing,data exploration,
file214,10,unemploy_wage_data.describe(),data exploration,,
file214,11,"sns.relplot(data=unemploy_wage_data, x='week_number', y='total_claims')",result visualization,,
file214,12,"sns.distplot(unemploy_wage_data.total_claims, bins=10)",result visualization,,
file214,13,"plt.figure(figsize=(20,18))
 cor = unemploy_wage_data.corr()
 cmap = sns.diverging_palette(210, 20, as_cmap=True)
 sns.heatmap(cor, cmap=cmap, vmax=.99, vmin=-.99, annot=True)",result visualization,,
file214,14,"X = unemploy_wage_data[['week_number', 'countyfips_x', 'tract_x', 'edu_8th_or_less', 'edu_grades_9_11', \
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown', 'gender_female', 'gender_male', \
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', \
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 y = unemploy_wage_data['total_claims']",data preprocessing,,
file214,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file214,16,"print(f'intercept: {reg.intercept_}')
 coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
 print(coef)",evaluation,,
file214,17,"reg = LinearRegression()  
 reg.fit(X_train, y_train)
 print(reg)",modeling,evaluation,
file214,18,"y_pred = reg.predict(X_test)
 df = DataFrame({'Actual': y_test, 'Predicted': y_pred})
 df",prediction,evaluation,
file214,19,"print('R squared: {:.2f}'.format(reg.score(X, y)*100))
 print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
 print('MSE:', metrics.mean_squared_error(y_test, y_pred))
 print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",evaluation,,
file214,20,df['Predicted'].mean(),evaluation,,
file214,21,"predicted_unemploy = pd.merge(unemploy_wage_data,df)",data preprocessing,,
file214,22,unemployment_data.week_number.unique(),data exploration,,
file214,23,"predicted_claims = prediction_list.values
 predicted_claims",data preprocessing,data exploration,
file214,24,"final_reg = LinearRegression()  
 final_reg.fit(X, y)
 print(final_reg)",modeling,evaluation,
file214,25,"prediction_data = pd.merge(unemploy_wage_data, prediction_data, on=['uu_id'], how='inner')
 prediction_data.head()",data preprocessing,,
file214,26,"prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
 prediction_data = prediction_data.drop(['week_number_x','total_claims'],axis=1)
 prediction_data.head()",data preprocessing,data exploration,
file214,27,unemploy_wage_data.groupby(['uu_id'])['total_claims'].mean(),data exploration,,
file214,28,"unemployment_prediction_data.to_csv('submission_prediction_output.csv', index=False)",save results,,
file215,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file215,1,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file215,2,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file216,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file216,1,"#import cell
 import pandas as pd
 import numpy as np
 import statistics
 import csv
 import matplotlib.pyplot as plt
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file216,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file216,3,"#Gets the master unemployed table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file216,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemploymentData = query_job.to_dataframe()
 print(unemploymentData.shape)
 pd.set_option('display.max_columns', None)
 unemploymentData.head(3)",load data,data preprocessing,
file216,5,"#Gets each tracks mean and std dev
 #unlist has the master unemployment list
 #b becomes the filtered list
 unList = unemploymentData.values.tolist()
 b_set = set(tuple(x) for x in unList)
 b = [ list(x) for x in b_set ]",data preprocessing,,
file216,6,"uuid = []
 #makes a list of the unique uuid
 for x in b:
  if(uuid.count(x[0]) == 0):
  uuid.append(x[0])",data preprocessing,,
file216,7,"#setup for extract  
 values = []
 export = []",data preprocessing,,
file216,8,"#for each value make a list of each weeks claims
 for y in uuid:
  temp = [y]
  for x in b:
  if (x[0] == y):
  temp.append(x[6])
  values.append(temp)",data preprocessing,,
file216,9,"for x in values:
  name = x[0]
  mean = statistics.mean(x[1:])
  if (len(x) > 2):
  stdev = statistics.stdev(x[1:])
  else:
  print(""short"")
  export.append([name, mean, stdev])
 #Everything below this is testing",data preprocessing,,
file216,10,"#Make bar charts
 #unique list as guide to count
 x1 = []
 y1 = []
 for i in uuid[:1]:
  for k in b:
  if (k[0] == i):
  x1.append(k[2])
  y1.append(k[6])",data preprocessing,,
file216,11,"plt.bar(x1,y1)",result visualization,,
file216,12,"xValues = []
 xCount = []
 for i in uuid[:1]:
  for k in b:
  if (k[0] == i):
  xValues.append(k[6])
 c_set = set(tuple(x) for x in xValues)
 c = [ list(x) for x in c_set ]",data preprocessing,,
file216,13,"for x in c:
  xCount.append(xValues.count(x))",data preprocessing,,
file216,14,"plt.bar(xValues,xCount)",result visualization,,
file216,15,"for x in c:
  xCount.append(xValues.count(x))
 print(xValues)
 print(xCount)
 plt.bar(xValues,xCount)",data exploration,,
file216,16,"for x in c:
  xCount.append(xValues.count(x))
 print(statistics.mean(xValues))
 print(statistics.median(xValues))
 plt.bar(c,xCount)",data exploration,,
file216,17,"xValues = []
 xCount = []
 for i in uuid[:2]:
  for k in b:
  if (k[0] == i):
  xValues.append(k[6])
  c = list(dict.fromkeys(xValues))
 

  for x in c:
  xCount.append(xValues.count(x))
  print(statistics.mean(xValues))
  print(statistics.median(xValues))
  plt.bar(c,xCount)",data preprocessing,data exploration,
file217,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file217,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm",helper functions,,
file217,2,"# explore dataframe
 def dataExplore(data):
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data preprocessing,,
file217,3,"# check balance of data
 def dataBalanceCheck(data):
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data preprocessing,,
file217,4,"# fill NA with given value
 def dataFillNa(data, value):
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)
 def dataIdentifyDateMonth(data):
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  return(data)",data preprocessing,,
file217,5,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file217,6,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file217,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file217,8,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file217,9,"# Explore input data for NA and special values
 dataExplore(data)
 dataExplore(data_pred_query)",data exploration,,
file218,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file218,1,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file218,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file218,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file218,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file218,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file218,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file218,7,"query_job = bigquery_client.query(query)
 unemployment_data_table = query_job.to_dataframe()
 print unemployment_data_table",load data,data preprocessing,
file218,8,"query_job = bigquery_client.query(query)
 unemployment_data_table = query_job.to_dataframe()
 print(""Columns:"")
 print('\n'.join(unemployment_data_table.columns))
 print(""\nResults:"")
 print(unemployment_data_table.head())",load data,data preprocessing,
file218,9,"query2 = """"""
 SELECT week_number, SUM(total_claims)
 FROM 'ironhacks-data.ironhacks_competition.unemployment_data'
 GROUP BY week_number;
 """"""",load data,,
file218,10,"query_job = bigquery_client.query(query)
 unemployment_total_claims_by_week = query_job.to_dataframe()",load data,data preprocessing,
file218,11,print(unemployment_total_claims_by_week),data exploration,,
file219,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file219,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file219,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file219,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file219,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file219,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file219,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file219,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file219,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file219,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file219,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file219,11,"def example_function():
  print('Hello World')",helper functions,,
file219,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file219,13,"model.fit(x,y)
 LinearRegression",evaluation,,
file219,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file219,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file219,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file219,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file219,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file219,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",prediction,,
file219,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file219,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file219,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file219,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file219,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file219,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,evaluation,
file219,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",evaluation,,
file219,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,evaluation,
file219,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file219,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file219,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file219,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file219,32,"# Test Linear Regression
 x",data exploration,,
file219,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file219,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file219,35,"# Test Linear Regression
 print(results.summary())",evaluation,,
file219,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file219,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file219,38,prestige.head(),data exploration,,
file219,39,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file219,40,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file219,41,get_ipython().system('pip install db-dtypes'),helper functions,,
file219,42,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file219,43,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file219,44,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file219,45,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",load data,,
file219,46,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file219,47,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file219,48,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file219,49,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file219,50,"def example_function():
  print('Hello World')",helper functions,,
file219,51,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file219,52,"model.fit(x,y)
 LinearRegression",modeling,,
file219,53,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file219,54,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",evaluation,,
file219,55,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file219,56,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file219,57,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file219,58,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file219,59,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file219,60,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file219,61,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file219,62,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file219,63,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file219,64,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file219,65,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file219,66,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file219,67,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file219,68,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file219,69,"print(f""slope: {new_model.coef_}"")",prediction,,
file219,70,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file219,71,"# Test Linear Regression
 x",data exploration,,
file219,72,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file219,73,"# Test Linear Regression
 results = model.fit()",modeling,,
file219,74,"# Test Linear Regression
 print(results.summary())",evaluation,,
file219,75,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file219,76,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file219,77,prestige.head(),data exploration,,
file219,78,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,,
file219,79,dta = sm.datasets.statecrime.load_pandas().data,data preprocessing,,
file219,80,print(prestige_model.summary()),evaluation,,
file219,81,"crime_model = ols(""claim ~ uu_id + tract + age + single"", data=dta).fit()
 print(crime_model.summary())",modeling,data exploration,
file220,0,"# remove unnecessary colomns and combine unemployment data together with wages
 unemployment_sample = unemployment_data.copy()
 unemployment_sample = unemployment_sample.drop(['timeperiod', 'tract', 'tract_name', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs',\
  'edu_unknown', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3', 'gender_female', 'gender_male',\
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative',\
  'race_other', 'race_white'], axis=1)
 wage_sample = wage_data.copy()
 wage_sample.drop(['tract', 'tract_name'], axis=1)
 prediction_sample = prediction_list.copy()
 wages_dict = {i:wage_sample[""average_wage""].values[k] for k,i in enumerate(wage_sample[""uu_id""])}
 wage_list = [wages_dict[i] for i in unemployment_sample[""uu_id""]]
 print(wages_dict)
 unemployment_sample[""average_wage""] = wage_list",data preprocessing,,
file220,1,"tract_dic = {i:k for k,i in enumerate(wage_sample[""uu_id""])}",data preprocessing,,
file220,2,"unemployment_sample[""uu_id""] = [tract_dic[i] for i in unemployment_sample[""uu_id""]]",data preprocessing,,
file220,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file220,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file220,5,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file220,6,"def example_function():
  print('Hello World')",helper functions,,
file220,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file220,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file220,9,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data exploration,
file220,10,print(unemployment_sample.loc[1]),data exploration,,
file220,11,"X = unemployment_sample.drop([""total_claims""], axis=1).to_numpy() #to_numpy() values
 y = unemployment_sample['total_claims'].to_numpy()  #to_numpy() values
 # print(X.shape,y.shape)",data preprocessing,,
file221,0,"get_ipython().system('pip install transformers')
 get_ipython().system('pip install torch')
 get_ipython().system('pip install -r requirements.txt')",helper functions,,
file221,1,"import torch
 from transformers import AutoTokenizer, AutoModel
 tokenizer = AutoTokenizer.from_pretrained(""GanjinZero/UMLSBert_ENG"")
 model = AutoModel.from_pretrained(""GanjinZero/UMLSBert_ENG"")
 import pandas as pd",helper functions,modeling,
file221,2,"# read in scraped mayo symptoms
 with open(""mayo_data.json"", 'r') as j:
  data = json.loads(j.read())",load data,,
file221,3,"symptoms=[]
 specialties=[]
 diseases=[]",data preprocessing,,
file221,4,"for disease in list(data.keys()):
  list_of_symptoms=data[disease]['symptoms']
  if len(list_of_symptoms)==0:
  pass
  symptoms.append(list_of_symptoms)
  diseases.append([disease]*len(list_of_symptoms))
  list_of_specialties=data[disease]['specialties']
  specialtystr=','.join(list_of_specialties)
  specialties.append([specialtystr]*len(list_of_symptoms))",data preprocessing,,
file221,5,"def flatten(l):
  return [item for sublist in l for item in sublist]",data preprocessing,,
file221,6,"diseases=flatten(diseases)
 symptoms=flatten(symptoms)
 specialties=flatten(specialties)",data preprocessing,,
file221,7,"lst = [diseases,symptoms,specialties]
 df = pd.DataFrame(
  {'disease': diseases,
  'symptoms': symptoms,
  'specialties': specialties
  })",data preprocessing,,
file221,8,"def get_pooled_embedding(text, model, tokenizer):
  tokenized_input = tokenizer(text, return_tensors='pt')
  output = model(**tokenized_input)
  
  return output.pooler_output.detach().numpy()[0]",data preprocessing,,
file221,9,"def create_pooled_embedding_df(model, tokenizer, df):
  df['pooled_embedding'] = df.symptoms.apply(get_pooled_embedding, args=(model, tokenizer))
  
  return df",modeling,,
file221,10,"# add embedding column and save to pickle obj
 df= create_pooled_embedding_df(model, tokenizer, df)
 print(df)
 #df.to_pickle(""mayo_symptoms_embeddings.plk"")",modeling,data exploration,
file221,11,import torch,helper functions,,
file222,0,"param_dict_copy=param_dict
 seasonal_dict_copy=seasonal_dict
 result_dict_copy=result_dict",data preprocessing,,
file222,1,"get_ipython().run_cell_magic('capture', 'cont', '!pip install db-dtypes\n')",helper functions,,
file222,2,cont.show(),data exploration,,
file222,3,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file222,4,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file222,5,"import statsmodels.api as sm
 import itertools",helper functions,,
file222,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file222,7,"#obtaining the unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file222,8,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['date']= pd.to_datetime(data['date'])
 data.head()",data exploration,load data,
file222,9,"data.info()  #Feature Matrix
 #X = data.drop(""date"",1)
 #y = data[""wind_speed""] #Target Variable",data exploration,,
file222,10,submit.info(),data exploration,,
file222,11,"data.drop_duplicates(inplace=True, ignore_index=True)
 data.info()",data exploration,data preprocessing,
file222,12,data.uu_id.unique().size,data exploration,,
file222,13,"data.fillna(0, inplace=True)",data preprocessing,,
file222,14,data.info(),data exploration,,
file222,15,"test=data[data['uu_id']=='bbcb018f0e5e49e13636f6e78ce9f60f']
 len(test)",data exploration,data preprocessing,
file222,16,"data['timeperiod']= pd.to_datetime(data['timeperiod'], format='%Y%m%d')",data preprocessing,,
file222,17,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file222,18,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.2]
 print(relevant_features)",data exploration,data preprocessing,
file222,19,columns_rel = relevant_features.index.to_list(),data preprocessing,,
file222,20,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file222,21,"min(data['timeperiod']),max(data['timeperiod'])",data exploration,,
file222,22,data.dtypes,data exploration,,
file222,23,data.index,data exploration,,
file222,24,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file222,25,data['total_claims'].plot(linewidth=.5);,result visualization,,
file222,26,"cols_plot = ['week_number','countyfips','tract','total_claims','edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs',
  'edu_unknown', 'gender_female', 'gender_male', 'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 
  'race_hawaiiannative', 'race_other', 'race_white']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('features')",result visualization,,
file222,27,"#fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 #for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
 # sns.boxplot(data=data, x='Month', y=name, ax=ax)
 # ax.set_ylabel('precipitation')
 # ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
 # if ax != axes[-1]:
 # ax.set_xlabel('')",comment only,,
file222,28,"columns_rel.append('timeperiod')
 columns_rel.append('uu_id')
 columns_rel",data exploration,data preprocessing,
file222,29,data,data exploration,,
file222,30,"data_arima=data[columns_rel]
 data_arima",data exploration,data preprocessing,
file222,31,"get_ipython().run_cell_magic('capture', '', ""unique_id=list(data_arima['uu_id'].unique())\ndata_dict = {}\n\nfor i in unique_id:\n j = data_arima[data_arima['uu_id']==i].groupby('timeperiod')['total_claims'].sum().reset_index()\n j = j.set_index('timeperiod')\n data_dict[i] = j\n"")",helper functions,,
file222,32,"p = d = q = range(0, 2)
 pdq = list(itertools.product(p, d, q))
 seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
 print('Examples of parameter combinations for Seasonal ARIMA...')
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",data exploration,data preprocessing,
file222,33,"import warnings
 warnings.filterwarnings(""ignore"")",helper functions,,
file222,34,"results = {}
 for key in data_dict:
  y=data_dict[key]['total_claims']
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=(1,0,1),
  seasonal_order=(1,1,1,12),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results[key] = mod.fit(disp=False)
  #print(results.summary().tables[1])",data preprocessing,,
file222,35,"result_dict = {}
 param_dict = {}
 seasonal_dict = {}
 for key in data_dict:
  y=data_dict[key]['total_claims']
  for param in pdq:
  for param_seasonal in seasonal_pdq:
  
  try:
  
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=param,
  seasonal_order=param_seasonal,
  enforce_stationarity=False,
  enforce_invertibility=False)
  result_dict[key] = mod.fit(disp=False)
  #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, result_dict[key].aic))
  param_dict[key]=param
  seasonal_dict[key]=param_seasonal
  except:
  continue",data preprocessing,,
file222,36,"param_dict_copy=param_dict
 seasonal_dict_copy=seasonal_dict
 result_dict_copy=result_dict",data preprocessing,,
file222,37,len(result_dict),data exploration,,
file222,38,param_dict_copy.to_csv(index=False),save results,,
file222,39,"pd.DataFrame(param_dict_copy).to_csv('./param.csv',index=False)",save results,,
file222,40,"pd.DataFrame(param_dict_copy).to_csv('./param.csv',index=False)
 pd.DataFrame(seasonal_dict_copy).to_csv('./seasonal.csv',index=False)",save results,,
file222,41,data_dict.key(),data exploration,,
file222,42,list(data_dict.keys())[:100],data exploration,,
file222,43,results,data exploration,,
file222,44,"hundred=list(data_dict.keys())[:100]
 two_hundred=list(data_dict.keys())[100:200]",data preprocessing,,
file222,45,len(results),data exploration,,
file222,46,"hundred=list(data_dict.keys())[:100]
 two_hundred=list(data_dict.keys())[100:200]
 three_hundred=list(data_dict.keys())[200:300]",data preprocessing,,
file222,47,"hundred=list(data_dict.keys())[:100]
 two_hundred=list(data_dict.keys())[100:200]
 three_hundred=list(data_dict.keys())[200:300]
 four_hundred=list(data_dict.keys())[300:400]
 five_hundred=list(data_dict.keys())[500:]",data preprocessing,,
file223,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file223,1,"import numpy as np # linear algebra
 import pandas as pd
 from xgboost import plot_importance, plot_tree
 import xgboost as xgb
 import matplotlib.pyplot as plt
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error
 import seaborn as sns
 sns.set_style('whitegrid')
 plt.rcParams['figure.figsize']=(20,10) # for graphs styling
 plt.style.use('tableau-colorblind10')",helper functions,,
file223,2,get_ipython().system('pip install xgboost'),helper functions,,
file223,3,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file223,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file223,5,"query_job = bigquery_client.query(query)
 df = query_job.to_dataframe()
 df.head()",load data,data exploration,
file224,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file224,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file224,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file224,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file224,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,,
file224,5,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file224,6,"print(f""size of unemployment_data: {len(unemployment_data)}"")
 print(f""size of wage_data: {len(wage_data)}"")
 print(f""size of prediction_data: {len(prediction_data)}"")",data exploration,,
file224,7,unemployment_data.dtypes,data exploration,,
file224,8,"plt.figure(figsize=(12,10))
 cor = unemployment_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file224,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file224,10,unemployment_data = unemployment_data.set_index('timeperiod'),data preprocessing,,
file224,11,"X = unemployment_data.drop([""uu_id"", ""weak_number""], axis=1)
 X_cor = unemployment_data[[""edu_hs_grad_equiv"", ""edu_post_hs"",
  ""gender_female"", ""gender_male"",
  ""race_black"", ""race_white""]]
 y = unemployment_data[""total_claims""]",data preprocessing,,
file224,12,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file224,13,"from sklearn.impute import SimpleImputer
 imp = SimpleImputer(missing_values=np.nan, strategy='mean')
 imp.fit_transform(X_cor, y)",modeling,helper functions,
file224,14,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file224,15,y_pred = pd.Series(model.predict(X_cor)),prediction,,
file224,16,"# y_pred = pd.Series(model.predict(X_cor), index=X_cor.index)
 y_pred = pd.Series(model.predict(X_cor))",prediction,,
file224,17,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file224,18,"from sklearn.preprocessing import OneHotEncoder
 enc = OneHotEncoder()
 X = enc.fit_transform(data[""uu_id"", ""tract_name"",
  ""top_category_employer1"",
  ""top_category_employer2"",
  ""top_category_employer3""])",modeling,,
file224,19,"X = pd.get_dummies(df, prefix=['col1', 'col2'])",data preprocessing,,
file224,20,X = pd.get_dummies(unemployment_data),data preprocessing,,
file225,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file225,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file225,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file225,3,"def example_function():
  print('Hello World')",comment only,,
file225,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file225,5,"def get_data(table_name):
  query = f""""""
  SELECT *
  FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """"""
 

  # QUERY THE DATA ONCE
  query_job = bigquery_client.query(query)
  return query_job.to_dataframe()",load data,,
file225,6,unemploy = get_data('unemployment_data'),data preprocessing,,
file225,7,wage = get_data('wage_data'),data preprocessing,,
file225,8,sub = get_data('prediction_list'),data preprocessing,,
file225,9,pred = unemploy.groupby('uu_id')['total_claims'].mean().apply(lambda x: int(x)),data preprocessing,,
file225,10,pred = unemploy.groupby('uu_id').apply(lambda x: x.sort_values('week_number').drop_duplicates('week_number')['total_claims'].ewm(alpha=1/3).mean().to_numpy()[-1]),data preprocessing,,
file225,11,pred = pred[sub['uu_id'].to_list()],data preprocessing,,
file225,12,"df = pd.DataFrame({
  'uu_id': pred.index,
  'week_number': 39,
  'total_claims': pred.to_numpy().astype(np.int32)
 })",data preprocessing,,
file225,13,df,data exploration,,
file225,14,"df.to_csv('submission_prediction_output.csv', index=False)",save results,,
file226,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file226,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!pip install db-dtypes pmdarima 'google-cloud-bigquery[pandas]' tqdm\n"")",helper functions,,
file226,2,"import numpy as np
 import pandas as pd
 from tqdm.auto import tqdm
 import seaborn
 from google.cloud import bigquery",helper functions,,
file226,3,from pmdarima.arima import AutoARIMA,helper functions,,
file226,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file226,5,"data_tables = bigquery_client.query(f""""""
  SELECT table_catalog, table_schema, table_name
  FROM `ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """""").to_dataframe()
 print(data_tables)",data exploration,,
file226,6,"# Read all data tables in ironhacks-data.ironhacks_competition
 data_dict = {
  table_name: bigquery_client.query(f""""""
  SELECT * FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """""").to_dataframe()
  for table_name in data_tables['table_name'].tolist()
 }",data preprocessing,,
file226,7,"common_cols = list(set(data_dict['unemployment_data'].columns) & set(data_dict['wage_data'].columns))
 print(f'Common columns: {common_cols}')
 data = data_dict['unemployment_data'].merge(
  data_dict['wage_data'],
  on=common_cols,
  how='left',
 ).sort_values(['countyfips', 'week_number']).drop_duplicates().reset_index(drop=True)
 data['timeperiod'] = pd.to_datetime(data['timeperiod'], format='%Y%m%d')
 data",data exploration,data preprocessing,
file226,8,"# For our own convenience, create a correspondance DataFrame for `week_number` and `timeperiod`.
 wt = pd.Series(
  range(1, 53),
  name='week_number',
  index=pd.date_range('2022-01-01', periods=52, freq='W-SAT').rename('timeperiod'),
 ).reset_index()
 wt['timeperiod'] = wt['timeperiod'].astype(str)
 wt",data exploration,data preprocessing,
file226,9,"# The following shows that there are a lot of missing data in exogenous features...
 for cname, cvalues in data.items():
  print('Column {} has {} ({}%) missing value(s)'.format(
  cname,
  cvalues.isna().sum(),
  round(100.0 * cvalues.isna().sum() / len(cvalues), 2),
  ))",data exploration,data preprocessing,
file226,10,"train_weeks = 37
 train_timeperiods = pd.to_datetime(wt.set_index('week_number')['timeperiod'].loc[:train_weeks])
 target_week = 39
 pred_results = []
 for i in tqdm(data['uu_id'].unique()):
  y = data[
  data['uu_id'] == i
  ].set_index('timeperiod')['total_claims'].asfreq('W-SAT').reindex(train_timeperiods).fillna(0)
  n_periods = target_week - train_weeks
  pred = AutoARIMA(
  seasonal=False,
  ).fit_predict(y, n_periods=n_periods).iloc[-1]
  pred_results.append([i, max(0.0, pred)])",data preprocessing,,
file226,11,"# Getting the output CSV for submission ready
 data_dict['prediction_list'].merge(
  pd.DataFrame(pred_results, columns=['uu_id', 'total_claims']),
  on=['uu_id'],
  how='left',
 ).to_csv('submission_prediction_output.csv', index=False)",save results,,
file227,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file227,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file227,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file227,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file227,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import OneHotEncoder",helper functions,,
file227,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file227,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file227,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file227,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file227,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,,
file227,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file227,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 39
 """"""",data preprocessing,,
file227,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file227,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file227,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file227,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 print(duplicated_rows)
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data exploration,data preprocessing,
file227,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file228,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file228,1,"# query all total_claims data and store in raw_total_claims_data dict
 def query_all_total_claims():
  temp_query_list = prediction_list[""uu_id""] # the list of uuids to be queried
  raw_total_claims_data = {uuid:[] for uuid in temp_query_list} # dict to store all query results
  total_week = 37
 

  progress = 0
  total = len(temp_query_list)
  for uuid in temp_query_list:
  progress = progress + 1
  print('\r', ""Querying"", str(progress) + ""/"" + str(total), ""uuid's data..."", end='\r')
  QUERY=""""""
  SELECT week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uuid)
  query_job = bigquery_client.query(QUERY)
  res = query_job.to_dataframe().drop_duplicates(ignore_index=True).to_numpy()
 

  # Fill the missing total_claims between week 1 and week 37 with value 0
  index = 0
  for w in range(1,total_week + 1):
  if index == len(res):
  raw_total_claims_data[uuid].append({w:0})
  continue
  if w == res[index][0]:
  raw_total_claims_data[uuid].append({w:res[index][1]})
  index = index + 1
  else:
  raw_total_claims_data[uuid].append({w:0})
  return raw_total_claims_data",,,
file228,2,"# commented for only excuting once
 raw_total_claims_data = query_all_total_claims()",data preprocessing,,
file228,3,"# Query the three provided data tables 
 unemployement_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file228,4,"wage_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file228,5,"# QUERY THE DATA ONCE
 ud_query_job = bigquery_client.query(unemployement_data_query)
 wd_query_job = bigquery_client.query(wage_data_query)
 pl_query_job = bigquery_client.query(prediction_list_query)",load data,,
file228,6,"unemployement_data = ud_query_job.to_dataframe()
 wage_data = wd_query_job.to_dataframe()
 prediction_list = pl_query_job.to_dataframe()",data preprocessing,,
file228,7,"get_ipython().run_cell_magic('capture', '--no-display', '!pip3 install db-dtypes\n')",helper functions,,
file228,8,"import os
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file228,9,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file228,10,"# Google Credential
 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='key.json'
 bigquery_client = bigquery.Client(project='ironhacks-data')",load data,,
file228,11,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",save results,data exploration,
file228,12,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",save results,data exploration,
file228,13,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)",save results,data exploration,
file229,0,"get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\n!pip install keras\n!pip install tensorflow\n')",helper functions,,
file229,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file229,2,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\n')",helper functions,,
file229,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file229,4,"query_main = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file229,5,"query_job = bigquery_client.query(query_main)
 unemployment_data = query_job.to_dataframe()",load data,,
file229,6,"print(unemployment_data.info())
 print(wage_data.info())",data exploration,,
file229,7,"# check shape of both frames to see if they are joinable
 print('Unemployment df size:', unemployment_data.shape)
 print('Wage df size:', wage_data.shape)",data exploration,,
file229,8,unemployment_data.isnull().sum() / len(unemployment_data) * 100,data exploration,,
file229,9,wage_data.isnull().sum() / len(wage_data) * 100,data exploration,,
file229,10,"# replace values with 0
 clean_unemploymentDf = unemployment_data.copy()
 clean_unemploymentDf.fillna(0, inplace=True)
 clean_unemploymentDf",data exploration,data preprocessing,
file229,11,clean_unemploymentDf.isnull().sum() #check,data exploration,,
file229,12,"get_ipython().run_cell_magic('capture', '', ""unemp_dupl = clean_unemploymentDf[clean_unemploymentDf.duplicated()]\nprint('Duplicate rows: ', unemp_dupl)\n# ignore duplicates - different time periods\n"")",helper functions,,
file229,13,"# check correlation
 correlation = clean_unemploymentDf.corr()
 mask = np.triu(np.ones_like(correlation, dtype=bool))
 plt.figure(figsize=(15,10))
 sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')",result visualization,,
file229,14,"# check wage information
 wage_data[wage_data['average_wage'].isnull()] # there's 3 nulls here - might as well drop them and use this tract to attempt to join the datasets together; or impute with mean",data exploration,,
file229,15,"#wage_data.dropna(axis=0, inplace=True)
 wage_data['average_wage'].fillna(wage_data['average_wage'].mean(), inplace=True)
 wage_data.isnull().sum()",data exploration,data preprocessing,
file229,16,"wage_dupl = wage_data[wage_data.duplicated()]
 print('Duplicate rows: ', wage_dupl)",data exploration,data preprocessing,
file229,17,"# join df on tract
 main_df = pd.merge(clean_unemploymentDf, wage_data, on=['tract', 'uu_id'], how='outer')",data preprocessing,,
file229,18,main_df,data exploration,,
file229,19,"# drop columns created by merge and rename existing columns to original
 main_df.drop(['countyfips_y','tract_name_y'], axis=1, inplace=True)
 main_df.rename({'countyfips_x':'countyfips', 'tract_name_x':'tract_name'}, axis=1, inplace=True)",data preprocessing,,
file229,20,main_df.isnull().sum() # check again,data exploration,,
file229,21,"main_df.dropna(axis=0, inplace=True) #most of the rows with NA values have nothing to add and cannot be imputed",data preprocessing,,
file229,22,main_df.isnull().sum(),data exploration,,
file229,23,"def evaluate_regressor(prediction_dataframe):
  # Takes in a prediction dataframe of 2 columns, Actual values and Predicted values generated by a regressor
  # Outputs MSE, MAR, RMSE and MAPE metrics. Must have columns named Actual and Predicted.
  print('MSE:', mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('MAE:', mean_absolute_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('RMSE:', np.sqrt(mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted'])))
  print('MAPE:', np.mean(np.abs((prediction_dataframe['Actual'] - prediction_dataframe['Predicted']) / prediction_dataframe['Actual'])) * 100)",data exploration,,
file229,24,"def get_predictions(regressor, model_type, name):
  # generates predictions for any model and writes out a dataframe in csv containing them
  # takes a regressor and learning method type as input: DL and ML
  # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape
  result_list = []
  uu_id_transform = LE.fit_transform(prediction_list['uu_id'])
  if model_type == 'DL':
  predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, 43, axis=1)
  to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_list = np.array(result_list)
  result_list = np.reshape(result_list, (619,))
  elif model_type == 'ML':
  predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, 43, axis=1)
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_df = pd.DataFrame(result_list, columns = ['Predictions'])
  prediction_sub = prediction_list.copy()
  prediction_sub['total_claims'] = result_df.values
  prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]
  os.makedirs('lost+found/submission_files', exist_ok=True)
  prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)
  return prediction_sub",data preprocessing,save results,
file229,25,"def get_pred_frame(test_frame, prediction_array):
  prediction_frame = pd.DataFrame({'Actual': test_frame, 'Predicted': prediction_array.flatten()})
  return prediction_frame",data preprocessing,,
file229,26,"# updated_ingest = pd.concat([merged_ingest, combined_ingest])
 ingest = pd.read_csv('lost+found/submission_files/updated_ingest.csv')",load data,,
file229,27,ingest,data exploration,,
file229,28,ingest.shape,data exploration,,
file229,29,"new_data = main_df.copy()
 complete_ingest = pd.concat([ingest, new_data])
 complete_ingest.shape",data exploration,data preprocessing,
file229,30,"print(complete_ingest.shape)
 print(ingest.shape)",data exploration,,
file229,31,complete_ingest.isnull().sum(),data exploration,,
file229,32,"complete_ingest.dropna(axis=0, inplace=True)",data preprocessing,,
file229,33,complete_ingest.shape,data exploration,,
file229,34,"# check if new data is duplicate
 complete_ingest.equals(main_df)",data exploration,,
file229,35,"## write out combined data for use later
 os.makedirs('lost+found/submission_files', exist_ok=True)
 complete_ingest.to_csv('lost+found/submission_files/complete_ingest.csv', index=False)",save results,,
file229,36,prediction_list.to_csv('prediction_list.csv'),save results,,
file229,37,"get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\n!pip install keras\n!pip install tensorflow\n')",helper functions,,
file229,38,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file229,39,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\n')",helper functions,,
file229,40,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file229,41,"query_main = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file229,42,"query_job = bigquery_client.query(query_main)
 unemployment_data = query_job.to_dataframe()",data preprocessing,,
file229,43,"# replace values with 0
 clean_unemploymentDf = unemployment_data.copy()
 clean_unemploymentDf.fillna(0, inplace=True)
 clean_unemploymentDf",data exploration,data preprocessing,
file229,44,clean_unemploymentDf.isnull().sum() #check,data exploration,,
file229,45,"# check correlation
 correlation = clean_unemploymentDf.corr()
 mask = np.triu(np.ones_like(correlation, dtype=bool))
 plt.figure(figsize=(15,10))
 sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')",result visualization,,
file229,46,"# check wage information
 wage_data[wage_data['average_wage'].isnull()] # there's 3 nulls here - might as well drop them and use this tract to attempt to join the datasets together; or impute with mean",data exploration,,
file229,47,"#wage_data.dropna(axis=0, inplace=True)
 wage_data['average_wage'].fillna(wage_data['average_wage'].mean(), inplace=True)
 wage_data.isnull().sum()",data exploration,data preprocessing,
file229,48,"wage_dupl = wage_data[wage_data.duplicated()]
 print('Duplicate rows: ', wage_dupl)",data exploration,data preprocessing,
file229,49,"# join df on tract
 main_df = pd.merge(clean_unemploymentDf, wage_data, on=['tract', 'uu_id'], how='outer')",data preprocessing,,
file229,50,main_df,data exploration,,
file229,51,"# drop columns created by merge and rename existing columns to original
 main_df.drop(['countyfips_y','tract_name_y'], axis=1, inplace=True)
 main_df.rename({'countyfips_x':'countyfips', 'tract_name_x':'tract_name'}, axis=1, inplace=True)",data preprocessing,,
file229,52,main_df.isnull().sum() # check again,data exploration,,
file229,53,"main_df.dropna(axis=0, inplace=True) #most of the rows with NA values have nothing to add and cannot be imputed",data preprocessing,,
file229,54,main_df.isnull().sum(),data exploration,,
file229,55,"# load model
 StackLSTM_Regressor = load_model('BiDLSTM_v1-04.h5')",modeling,,
file229,56,"# summary for viewers
 StackLSTM_Regressor.summary()",data exploration,,
file229,57,"# quick preprocess to keep uu_id and scale values
 from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler
 LE = LabelEncoder()
 RB_other = RobustScaler()
 SC_other = StandardScaler()
 # RB_claims = RobustScaler()",helper functions,modeling,
file229,58,"# this needs different feature engineering, so I'm starting from scratch
 DL_data = updated_ingest.copy()",data preprocessing,,
file229,59,"drop_these = ['timeperiod','tract','top_category_employer1','top_category_employer2','top_category_employer3','tract_name','countyfips', 'edu_unknown', 'gender_na', 'race_noanswer']
 DL_data['uu_id'] = LE.fit_transform(DL_data['uu_id'])
 scale_these = ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male', 
  'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
 DL_data.drop(drop_these, axis = 1, inplace=True)
 DL_data[scale_these] = SC_other.fit_transform(DL_data[scale_these])",modeling,,
file229,60,DL_data.columns,data exploration,,
file229,61,"# split set
 DL_Y = DL_data['total_claims']
 DL_X = DL_data[['uu_id', 'week_number', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male',
  'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 DL_XTrain, DL_XTest, DL_YTrain, DL_YTest = train_test_split(DL_X, DL_Y, test_size=0.20, random_state=69)",data preprocessing,,
file229,62,"# change to np vectors
 DL_XTrain = DL_XTrain.to_numpy()
 DL_XTest = DL_XTest.to_numpy()",data preprocessing,,
file229,63,"# reshape because F*** tensors
 DL_XTrain = np.reshape(DL_XTrain, (DL_XTrain.shape[0], DL_XTrain.shape[1], 1))",data preprocessing,,
file229,64,"# convert X and Y train to float because input dtype accepts floats only
 DL_YTrain = DL_YTrain.astype(float)
 DL_XTrain = DL_XTrain.astype(float)",data preprocessing,,
file229,65,"# float cast
 DL_XTest = DL_XTest.astype(float)
 # make predictions
 DL_XTest = np.reshape(DL_XTest, (DL_XTest.shape[0], DL_XTest.shape[1],1))
 predictions = StackLSTM_Regressor.predict(DL_XTest)",prediction,data preprocessing,
file229,66,"get_pred_frame(DL_YTest, predictions)",data exploration,,
file229,67,"evaluate_regressor(get_pred_frame(DL_YTest, predictions))",data exploration,,
file229,68,"plt.figure(figsize=(15,10))
 plt.plot(DL_YTest.values, color = 'red', label = 'Actual values')
 plt.plot(predictions, color='blue', label='Predicted values')
 plt.title('Model Prediction Visual')
 plt.legend()
 plt.show()",result visualization,,
file229,69,"get_predictions(StackLSTM_Regressor, 'DL', 'submission_prediction_output')",data exploration,,
file229,70,"def get_predictions(regressor, model_type, name):
  # generates predictions for any model and writes out a dataframe in csv containing them
  # takes a regressor and learning method type as input: DL and ML
  # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape
  result_list = []
  uu_id_transform = LE.fit_transform(prediction_list['uu_id'])
  if model_type == 'DL':
  predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, 43, axis=1)
  to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_list = np.array(result_list)
  result_list = np.reshape(result_list, (525,))
  elif model_type == 'ML':
  predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, 43, axis=1)
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_df = pd.DataFrame(result_list, columns = ['Predictions'])
  prediction_sub = prediction_list.copy()
  prediction_sub['total_claims'] = result_df.values
  prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]
  os.makedirs('lost+found/submission_files', exist_ok=True)
  prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)
  return prediction_sub",data preprocessing,save results,
file229,71,prediction_list,data exploration,,
file229,72,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\nfrom keras.models import load_model\nimport joblib\nfrom joblib import Parallel, delayed\n')",helper functions,,
file229,73,"# updated_ingest = pd.concat([merged_ingest, combined_ingest])
 ingest = pd.read_csv('lost+found/submission_files/completed_ingest.csv')",data preprocessing,load data,
file229,74,"# set target and preprocess again
 to_drop = ['timeperiod','tract','total_claims','top_category_employer1','top_category_employer2','top_category_employer3','tract_name','countyfips', 'edu_unknown', 'gender_na', 'race_noanswer']
 Y = ingest['total_claims']
 X = ingest.drop(to_drop, axis=1)
 X['uu_id'] = LE.fit_transform(X['uu_id'])
 to_scale = ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male', 
  'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
 X[to_scale] = RB_other.fit_transform(X[to_scale])
 #X[['total_claims']] = RB_claims.fit_transform(main_df[['total_claims']])",modeling,data preprocessing,
file229,75,"# import
 X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=69)",data preprocessing,,
file229,76,"# load model
 RFR_Regressor = joblib.load('RF_v1-5.pkl')",load data,,
file229,77,"Y_pred_RFR = RFR_Regressor.predict(X_test.values).reshape(-1,1)",data preprocessing,,
file229,78,"# evaluate
 evaluate_regressor(get_pred_frame(Y_test,Y_pred_RFR))",data exploration,,
file229,79,"# call func
 get_predictions(RFR_Regressor, 'ML', 'submission_prediction_output_RFR', 44)",data exploration,,
file230,0,2+2 #this is a code cell,data exploration,,
file230,1,2^2,data exploration,,
file230,2,"import numpy as np
 import matplotlib.pyplot as plt
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file230,3,"x = np.linspace(0, 10, 500)
 y = np.cumsum(np.random.randn(500, 6), 0)",data preprocessing,,
file230,4,"plt.figure(figsize=(12, 7))
 plt.plot(x, y)
 plt.legend('ABCDEF', ncol=2, loc='upper left')",result visualization,,
file231,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",result visualization,,
file231,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",result visualization,,
file231,2,get_ipython().system('pip install db-dtypes'),result visualization,,
file231,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",result visualization,,
file231,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",result visualization,,
file231,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file231,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file231,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file231,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file231,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file231,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file231,11,"def example_function():
  print('Hello World')",comment only,,
file231,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file231,13,"model.fit(x,y)
 LinearRegression",modeling,,
file231,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file231,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",data exploration,modeling,
file231,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file231,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file231,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",data exploration,data preprocessing,
file231,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,,
file231,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,data exploration,
file231,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file231,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file231,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file231,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file231,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file231,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file231,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file231,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file231,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file231,30,"print(f""slope: {new_model.coef_}"")",data exploration,,
file231,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file231,32,"# Test Linear Regression
 x",data exploration,,
file231,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file231,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file231,35,"# Test Linear Regression
 print(results.summary())",data exploration,,
file231,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file231,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file231,38,print(prestige_model.summary()),data exploration,,
file231,39,dta = sm.datasets.statecrime.load_pandas().data,load data,,
file231,40,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",comment only,,
file231,41,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file231,42,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",load data,data preprocessing,
file231,43,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file231,44,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file231,45,get_ipython().system('pip install db-dtypes'),helper functions,,
file231,46,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file231,47,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file231,48,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file231,49,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file231,50,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file231,51,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file231,52,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file231,53,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file231,54,"def example_function():
  print('Hello World')",comment only,,
file231,55,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file231,56,"model.fit(x,y)
 LinearRegression",modeling,,
file231,57,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file231,58,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,load data,
file231,59,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file231,60,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file231,61,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file231,62,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,,
file231,63,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,,
file231,64,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file231,65,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file231,66,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file231,67,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file231,68,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file231,69,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file231,70,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file231,71,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file231,72,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file231,73,"print(f""slope: {new_model.coef_}"")",data exploration,,
file231,74,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data exploration,data preprocessing,
file231,75,"# Test Linear Regression
 x",data exploration,,
file231,76,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file231,77,"# Test Linear Regression
 results = model.fit()",modeling,,
file231,78,"# Test Linear Regression
 print(results.summary())",data exploration,,
file231,79,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file231,80,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file231,81,dta = sm.datasets.statecrime.load_pandas().data,load data,,
file231,82,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",comment only,,
file231,83,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file231,84,"data = sm.datasets.anes96.load_pandas()
 party_ID = np.arange(7)
 labels = [
  ""Strong Democrat"",
  ""Weak Democrat"",
  ""Independent-Democrat"",
  ""Independent-Independent"",
  ""Independent-Republican"",
  ""Weak Republican"",
  ""Strong Republican"",
 ]",data preprocessing,load data,
file231,85,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",comment only,,
file231,86,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file231,87,"# Defining MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",evaluation,,
file231,88,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file231,89,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file231,90,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,,
file231,91,"X = data['total_claims'].values.reshape(-1,1)
 y = data['week_number'].values.reshape(-1,1)",data preprocessing,,
file231,92,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file231,93,"df = pd.DataFrame(np.random.rand(10, 3), columns =['a', 'b', 'c'])",data preprocessing,,
file231,94,print(df),data exploration,,
file231,95,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file231,96,"# importing matplotlib
 import matplotlib.pyplot",helper functions,,
file231,97,"# importing pandas as pd
 import pandas as pd",helper functions,,
file231,98,"# importing numpy as np
 import numpy as np",helper functions,,
file231,99,"# creating a dataframe
 df = pd.DataFrame(np.random.rand(10, 10),
  columns =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])",data preprocessing,,
file231,100,df,data exploration,,
file231,101,"# using a function df.plot.bar()
 df.plot.bar()",result visualization,,
file231,102,"from sklearn.metrics import mean_absolute_error
 Y_actual = [1,2,3,4,5]
 Y_Predicted = [1,2.5,3,4.1,4.9]
 mape = mean_absolute_error(Y_actual, Y_Predicted)*100
 print(mape)",evaluation,,
file231,103,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(20)",load data,,
file231,104,"import numpy as np
 numbers = np.linspace(5, 50, 24, dtype=int).reshape(4, -1)
 numbers",data preprocessing,,
file231,105,"mask = numbers % 4 == 0
 mask",data exploration,data preprocessing,
file231,106,"# Input 4 creates the mask
 numbers[mask]",data exploration,,
file231,107,"by_four = numbers[numbers % 4 == 0]
 by_four",data exploration,data preprocessing,
file231,108,"# USING MATPLOTLIB - image
 # A WAY TO COMMUNICATE DATA AND RESULTS
 # UNEMPLOYMENT RATES OCTOBER 2022 - Indiana Image
 # SOURCE: http://www.stats.indiana.edu/maptools/laus.asp
 # insall image
 # using opencv
 path = ""http://www.stats.indiana.edu/maptools/laus.asp""",load data,,
file231,109,"import numpy as np
 import matplotlib.image as mpimg",helper functions,,
file231,110,"img = mpimg.imread (http://www.stats.indiana.edu/maptools/laus.asp)
 print(type(img) )
 print(img.shape )",data exploration,,
file232,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file232,1,"get_ipython().system('pip install db_dtypes')
 import os
 import pandas as pd
 import db_dtypes
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file232,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file232,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file232,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 unemployment_data = query_job.to_dataframe()",data exploration,,
file232,5,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.shape",data exploration,,
file232,6,k = unemployment_data.copy(),data preprocessing,,
file232,7,"## number of unique ids are matching the number of entries in the wage_data set
 import numpy as np
 pd.unique(k.uu_id).shape",data exploration,,
file232,8,"wage_data = pd.DataFrame(wage_data)
 wage_data.head()",data exploration,,
file232,9,"wage_data = wage_data.drop_duplicates()
 wage_data.shape
 ## no duplicates here!",data exploration,,
file232,10,pd.unique(wage_data.uu_id).shape,data exploration,,
file232,11,"## lets join the 2 datasets on uu_id
 unemployment_data.columns, wage_data.columns",data exploration,,
file232,12,"data=pd.merge(unemployment_data,wage_data, how='inner')
 print(data.shape)",data exploration,data preprocessing,
file232,13,data.columns,data exploration,,
file232,14,"pd.set_option('display.max_columns', None) # or 1000
 pd.set_option('display.max_rows', None) # or 1000
 pd.set_option('display.max_colwidth', None) # or 199
 data.head()",data exploration,,
file232,15,data.isna().sum(),data exploration,,
file232,16,"def breakcolumn(a,data):
  df=pd.DataFrame()
  df[['Value1', 'Value2']] = data[a].str.split('-', 1, expand=True)
  ## replace the null values by the value before hypen
  df['Value2'].fillna(df['Value1'],inplace=True)
 

  df['Value1'] = pd.to_numeric(df['Value1'])
  df['Value2'] = pd.to_numeric(df['Value2'])
 

  df['Value3'] = (df['Value1']+df['Value2'])//2
  data[a] = df['Value3']",data preprocessing,,
file232,17,"data1 = data.copy()
 obj_list = ['top_category_employer1','top_category_employer2','top_category_employer3']
 for i in obj_list:
  data1[i].replace('N/A',0,inplace=True)
  breakcolumn(i,data1)",data preprocessing,,
file232,18,data1.head(),data exploration,,
file232,19,data1.info(),data exploration,,
file232,20,"data1['race_black'].fillna(0,inplace=True)
 data1['race_other'].fillna(0,inplace=True)
 data1['club_races'] = data1['race_black'] + data1['race_other']
 data1.drop(['race_black','race_other'],axis=1,inplace=True)",data preprocessing,,
file232,21,"data1.drop(['gender_male','gender_male','race_white','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs'],axis=1,inplace=True)
 data1.info()",data exploration,data preprocessing,
file232,22,"data1.fillna(method='bfill',inplace=True)
 data1.info()",data exploration,data preprocessing,
file232,23,"data1['race_asian'] = data1['race_asian'].fillna(int(np.mean(data1['race_asian'])))
 data1['race_noanswer'] = data1['race_noanswer'].fillna(int(np.mean(data1['race_noanswer'])))
 data1['edu_unknown'] = data1['edu_unknown'].fillna(int(np.mean(data1['edu_unknown'])))
 data1['gender_female'] = data1['gender_female'].fillna(int(np.mean(data1['gender_female'])))
 data1['top_category_employer3'] = data1['top_category_employer3'].fillna(int(np.mean(data1['top_category_employer3'])))",data preprocessing,,
file232,24,"from sklearn import preprocessing
 # label_encoder object knows how to understand word labels. 
 label_encoder = preprocessing.LabelEncoder()
 # Encode labels in column 'Country'. 
 data1['tract_name']= label_encoder.fit_transform(data1['tract_name'])",modeling,,
file232,25,"data2 = data1.copy()
 data1['uu_id']= label_encoder.fit_transform(data1['uu_id'])",modeling,,
file232,26,"X = data1.drop('total_claims',axis=1)
 y = data1['total_claims']
 from sklearn.model_selection import train_test_split
 X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 42)",data preprocessing,,
file232,27,"from sklearn.linear_model import LinearRegression
 linreg = LinearRegression()
 linreg.fit(X_train,y_train)
 y_pred = linreg.predict(X_test)",modeling,prediction,
file232,28,"y_pred = np.round(y_pred)
 from sklearn.metrics import mean_squared_error
 mean_squared_error(y_test,y_pred)",evaluation,,
file232,29,y_pred,data exploration,,
file232,30,"## MAPE function
 def MAPE(Y_actual,Y_Predicted):
  mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100
  return mape",evaluation,,
file232,31,"print(MAPE(y_test,y_pred))",data exploration,,
file232,32,"from sklearn.ensemble import RandomForestRegressor
 rfr = RandomForestRegressor(n_estimators = 500, random_state = 0)
 rfr.fit(X_train, y_train)",helper functions,modeling,
file232,33,y_pred = rfr.predict(X_test),prediction,,
file232,34,y_pred = np.round(y_pred),data preprocessing,,
file232,35,X2 = X.copy(),data preprocessing,,
file232,36,X2 = X2.apply(lambda iterator: ((iterator - iterator.mean())/iterator.std()).round(2)),data preprocessing,,
file232,37,"X_train,X_test,y_train,y_test = train_test_split(X2,y,test_size = 0.25,random_state = 42)",data preprocessing,,
file232,38,"get_ipython().system('pip install xgboost')
 from xgboost.sklearn import XGBRegressor
 regressor = XGBRegressor(
  n_estimators=500,
  reg_lambda=1,
  gamma=0,
  max_depth=3)",helper functions,modeling,
file232,39,"regressor.fit(X_train, y_train)
 y_pred = regressor.predict(X_test)
 mean_squared_error(y_test,y_pred)",modeling,evaluation,
file232,40,"from xgboost.sklearn import XGBRegressor
 from sklearn.model_selection import GridSearchCV",helper functions,,
file232,41,"xgb1 = XGBRegressor()
 parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
  'objective':['reg:linear'],
  'learning_rate': [.03, 0.05, .07], #so called `eta` value
  'max_depth': [5, 6, 7],
  'min_child_weight': [4],
  'subsample': [0.7],
  'colsample_bytree': [0.7],
  'n_estimators': [400,500,600,100]}",modeling,,
file232,42,"xgb_grid = GridSearchCV(xgb1,
  parameters,
  cv = 2,
  n_jobs = 5,
  verbose=True)",modeling,,
file232,43,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file232,44,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 print(query_job)
 prediction = query_job.to_dataframe()",data exploration,load data,
file232,45,"print(prediction.shape)
 pd.DataFrame(prediction).head()",data exploration,,
file232,46,"data2 = data2.drop_duplicates(subset=['uu_id'],keep='last')",data preprocessing,,
file232,47,"data2 = data2.set_index('uu_id')
 data2.head()",data exploration,,
file232,48,"final_prediction = data2.join(prediction.set_index('uu_id'),on='uu_id',rsuffix='_other')
 final_prediction.head()",data exploration,data preprocessing,
file232,49,"final_prediction_data = pd.DataFrame()
 final_prediction_data['index'] = final_prediction.index
 final_prediction_data['week_number_other'] = final_prediction.week_number_other",data preprocessing,,
file232,50,"final_prediction = final_prediction.drop(['week_number_other'], axis=1)
 final_prediction.reset_index(drop=True, inplace=True)",data preprocessing,,
file232,51,"future = final_prediction.values
 future_weeks_pred = rfr.predict(future)
 print(future_weeks_pred.shape)",data exploration,prediction,
file232,52,"prediction['total_claims'] = future_weeks_pred.astype('int')
 prediction.columns = ['uuid','week','count']
 print(prediction)",data exploration,data preprocessing,
file232,53,"prediction.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file233,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file233,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file233,2,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file233,3,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file233,4,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file233,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file233,6,"query_job = bigquery_client.query(query)
 unemployment_data_table = query_job.to_dataframe()",load data,,
file233,7,"print(""Columns:"")
 print('\n'.join(unemployment_data_table.columns))
 print(""\nResults:"")
 print(unemployment_data_table.head())",data exploration,,
file233,8,"query = """"""
 SELECT week_number, SUM(total_claims)
 FROM 'ironhacks-data.ironhacks_competition.unemployment_data'
 GROUP BY week_number
 """"""",load data,,
file233,9,print(unemployment_total_claims_by_week.head()),data exploration,,
file234,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file234,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file234,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file234,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file234,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,load data,
file235,0,"data[data[""top_category_employer1""]]",data exploration,,
file235,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file235,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file235,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file235,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file235,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file235,6,data.info(),data exploration,,
file235,7,"data[""top_category_employer1""].value_counts()",data exploration,,
file236,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file236,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file236,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file236,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file236,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file236,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",data exploration,load data,
file236,6,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file236,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file236,8,"#Correlation with output variable
 cor_target = abs(cor[""potential_water_deficit""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file236,9,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file236,10,"min(data['date']),max(data['date'])",data exploration,,
file236,11,data.dtypes,data exploration,,
file236,12,data = data.set_index('date'),data preprocessing,,
file236,13,data.index,data exploration,,
file236,14,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,data exploration,
file236,15,data.loc['2019-08'],data exploration,,
file236,16,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file236,17,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file236,18,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file236,19,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",result visualization,,
file236,20,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file236,21,"sns.boxplot(data=data, x='Month', y='wind_speed');",result visualization,,
file236,22,"from statsmodels.graphics.tsaplots import plot_acf
 plot_acf(x=data['max_temperature'], lags=50)",result visualization,,
file236,23,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file236,24,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file236,25,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file236,26,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file236,27,data['lag_1']=lag_1,data preprocessing,,
file236,28,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable
 X.head()",data exploration,data preprocessing,
file237,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file237,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file237,2,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file237,3,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file237,4,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file237,5,global df3,data exploration,,
file237,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file237,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file237,8,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file237,9,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data exploration,helper functions,
file237,10,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file237,11,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data exploration,data preprocessing,
file237,12,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file237,13,df4[df4.total_claims>0.2],data exploration,,
file237,14,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(x = df3[feature], y = df3[""total_claims""])
  plt.show()",result visualization,data exploration,
file237,15,"features =df4[df4.total_claims>0.2].index
 for feature in features:
  print(feature)
  ax= sns.scatterplot(y = df3[feature], x = ""week_number"", data = df3)
  ax= sns.lineplot(y = df3[feature], x = ""week_number"", data = df3)
  plt.show()",result visualization,,
file237,16,"df3 = df.copy()
 df3.columns",data exploration,data preprocessing,
file237,17,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,data exploration,
file237,18,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file237,19,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file237,20,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file237,21,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data exploration,data preprocessing,
file237,22,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,data exploration,
file237,23,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file237,24,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file237,25,"features =df4[abs(df4.total_claims)>0.6].index
 features",data exploration,data preprocessing,
file237,26,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,,
file237,27,"temp = df3[[k for k in features]]
 temp",data preprocessing,data exploration,
file237,28,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",modeling,,
file237,29,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",modeling,data preprocessing,
file237,30,df3.columns,data exploration,,
file237,31,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file237,32,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 16
 test_df[""month""]=10",load data,data preprocessing,
file237,33,test_df.drop_duplicates(),data exploration,,
file237,34,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",comment only,,
file237,35,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file237,36,"from statsmodels.tsa.stattools import adfuller
 adfuller(df3[""total_claims""])",helper functions,modeling,
file237,37,"df3 =df3.dropna()
 for col in features:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,col] = val.predict(np.array(feature_test_pred))[0]
 test_df",data exploration,data preprocessing,
file237,38,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",modeling,prediction,
file237,39,test_df,data exploration,,
file237,40,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",,,
file237,41,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")",,,
file238,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file238,1,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file238,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file238,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file238,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file238,5,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file238,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file238,7,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file238,8,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file238,9,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data exploration,data preprocessing,
file238,10,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data exploration,data preprocessing,
file238,11,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]",modeling,data preprocessing,
file238,12,"model.fit(X_train,y)",modeling,,
file238,13,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 data.iloc[X_test.index(),'uu_id']",data preprocessing,data exploration,
file238,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data preprocessing,data exploration,
file238,15,"a=pd.Dataframe(data=temp)
 a",data exploration,data preprocessing,
file238,16,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a",data preprocessing,data exploration,
file238,17,"y_pred = model.predict(X_test)
 y_pred",prediction,data exploration,
file238,18,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a[count].astype('int')
 a",data preprocessing,data exploration,
file238,19,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 columns=['uu_id','week','count']
 a=a.loc[:,columns]
 a",data exploration,data preprocessing,
file238,20,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 a=a[['uu_id','week_number','count']]
 a['week_number'].replace('week')
 a",data exploration,data preprocessing,
file238,21,"a.to_csv(""submission.csv"",index=False)
 a",data exploration,save results,
file238,22,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 a=a[['uu_id','week_number','count']]
 a['week_number']=a['week_number+1']
 a=a.rename(columns={'week_number':'week'})
 a.to_string(index=False)",data preprocessing,,
file239,0,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file239,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file239,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file239,3,get_ipython().system('pip uninstall neuralprophet'),helper functions,,
file239,4,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file239,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data exploration,
file239,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file239,7,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file240,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file240,1,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file240,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file240,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file240,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file240,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",data exploration,load data,
file240,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file240,7,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head(4)",load data,data exploration,
file240,8,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file240,9,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file241,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file241,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file241,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file241,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file241,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data exploration,
file241,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",helper functions,,
file241,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file241,7,print(covid19_cases_data),data exploration,,
file241,8,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file241,9,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data exploration,,
file241,10,len(cdf),data exploration,,
file241,11,"cdf[""county""].unique()",data exploration,,
file241,12,"cdf[""week_number'].unique()",data exploration,,
file241,13,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file241,14,"udf.to_csv(""enemploy"")",save results,,
file241,15,udf.value_counts,data exploration,,
file241,16,udf.columns(),data exploration,,
file241,17,"udf[""tract_name""].nunique()",data exploration,,
file241,18,print(list(udf.head())),data exploration,,
file241,19,"# udf.to_csv(""unemploy.csv"")
 udf = pd.read_csv(""unemploy.csv"")",load data,,
file241,20,import pandas as pd,helper functions,,
file241,21,"udf[""tract_name""][10]",data exploration,,
file241,22,"# wdf.to_csv(""wage.csv"")
 # udf.to_csv(""unemploy.csv"")
 udf = pd.read_csv(""unemploy.csv"")
 wdf = pd.read_csv(""wage.csv"")",load data,,
file241,23,"len(udf[""week_number""])",data exploration,,
file241,24,"len(udf)
 udf.nunique()",data exploration,,
file241,25,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data` i, 
 `ironhacks-data.ironhacks_competition.unemployement_data` d
 where i.uu_id = d.uu_id
 """"""",load data,,
file241,26,"# QUERY THE DATA ONCE
 query1_job = bigquery_client.query(query1)
 Mdf = query1_job.to_dataframe()
 Mdf.head()",load data,data exploration,
file241,27,"len(wdf.columns)
 # udf.nunique()",data exploration,,
file241,28,len(mdf.columns),data exploration,,
file241,29,mdf.head(),data exploration,,
file241,30,"mdf.drop(['Unnamed:', 'uu_id_1'], axis=1)",data preprocessing,,
file241,31,"# wdf.to_csv(""wage.csv"")
 # udf.to_csv(""unemploy.csv"")
 Mdf.to_csv(""mixed.csv"")
 # udf = pd.read_csv(""unemploy.csv"")
 # wdf = pd.read_csv(""wage.csv"")
 # mdf = pd.read_csv(""mixed.csv"")",save results,,
file241,32,"mdf.drop(['Unnamed: 0', 'uu_id_1','countyfips_1', 'tract_1', 'tract_name_1'], axis=1)",data preprocessing,,
file241,33,"import pandas as pd
 import numpy as np",helper functions,,
file241,34,"mdf.replace(np.nan, 0)",data preprocessing,,
file241,35,"plt.figure(figsize=(12,10))
 cor = mdf.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file241,36,"import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file241,37,"udf.replace(np.nan, 0)
 udf.drop(""Unnamed: 0"",axis = 1)",data preprocessing,,
file241,38,mdf.value_counts(),data exploration,,
file241,39,mdf.columns.type,data exploration,,
file241,40,mdf.dtypes,data exploration,,
file241,41,"mdf[""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]",data exploration,,
file241,42,"mdf[""top_category_employer1""].nunique()",data exploration,,
file241,43,"mdf[[""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]].replace('31-33', ""32"")
 mdf[[""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]].replace('48-49', ""48"")
 mdf[[""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]].replace('44-45', ""44"")",data preprocessing,,
file241,44,"mdf[""tract_name""]",data exploration,,
file241,45,print(dict(mdf.head(1))),data exploration,,
file241,46,"emp_cols = [""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]
 mdf[emp_cols] = mdf[emp_cols].replace('31-33', ""32"")
 mdf[emp_cols] = mdf[emp_cols].replace('48-49', ""48"")
 mdf[emp_cols] = mdf[emp_cols].replace('44-45', ""44"")",data preprocessing,,
file241,47,mdf[emp_cols] = pd.to_numeric(mdf[emp_cols]),data preprocessing,,
file241,48,"mdf[emp_cols] = pd.to_numeric(mdf[""top_category_employer1""])",data preprocessing,,
file241,49,"mdf[""top_category_employer1""] = pd.to_numeric(mdf[""top_category_employer1""])
 mdf[""top_category_employer2""] = pd.to_numeric(mdf[""top_category_employer2""])
 mdf[""top_category_employer3""] = pd.to_numeric(mdf[""top_category_employer3""])",data preprocessing,,
file241,50,mdf.head(),data exploration,,
file241,51,len(pdf),data exploration,,
file241,52,"f_df = mdf[[""edu_hs_grad_equiv"",""edu_post_hs"",'gender_female', 'gender_male', 'race_black', 'race_white','timeperiod', 'week_number']]",data preprocessing,,
file241,53,f_df,data exploration,,
file241,54,"data = f_df.set_index('timeperiod')
 data.index",data preprocessing,data exploration,
file241,55,"data[""total_claims""].plot(figsize=(15, 6))
 plt.show()",result visualization,,
file241,56,"f_df[""timeperiod""].nunique()",data exploration,,
file241,57,f_df.groupby(['Animal']),data exploration,,
file241,58,f_df['uu_id'],data exploration,,
file241,59,f_df['uu_id'].unique,data exploration,,
file241,60,lst = list(f_df['uu_id'].unique()),data exploration,,
file241,61,data1 = f_df[f_df['uu_id'] = lst[0]],data preprocessing,,
file241,62,"f_df.set_index('week_number')
 data.index",data preprocessing,data exploration,
file241,63,"lst = list(f_df['week_number'].unique())
 len(lst)",data preprocessing,data exploration,
file241,64,f1_df = f_df.groupby(f_df['week_number']).mean(),data preprocessing,,
file241,65,"f2_df = f_df[f_df[""uu_id""] == lst[0]]
 f2_df[""week_number""]",data preprocessing,data exploration,
file242,0,"y_pred = model.predict(X_test)
 y_pred.shape()",data exploration,prediction,
file242,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file242,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file242,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file242,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file242,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file242,6,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file242,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file242,8,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file242,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file242,10,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data exploration,data preprocessing,
file242,11,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data exploration,data preprocessing,
file242,12,"from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]
 temp=test.loc[test[""week_number""]==34]",modeling,data preprocessing,
file242,13,"model.fit(X_train,y)",modeling,,
file242,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data exploration,data preprocessing,
file243,0,print(1+1),data exploration,,
file244,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file244,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file244,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file244,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file244,4,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file245,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file245,1,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 6
 test_df[""month""]=11",load data,data preprocessing,
file245,2,test_df.drop_duplicates(),data exploration,,
file245,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file245,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file245,5,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file245,6,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file245,7,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file245,8,global df3,data exploration,,
file245,9,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file245,10,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file245,11,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data exploration,helper functions,
file245,12,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file245,13,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data exploration,data preprocessing,
file245,14,"df3 = df.copy()
 df3.columns",data preprocessing,data exploration,
file245,15,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data exploration,data preprocessing,
file245,16,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file245,17,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file245,18,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file245,19,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,data exploration,
file245,20,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,data exploration,
file245,21,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file245,22,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file245,23,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data exploration,,
file245,24,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file245,25,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,helper functions,
file245,26,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,,
file245,27,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",data preprocessing,,
file245,28,"temp = df3[[k for k in features]]
 temp",data exploration,data preprocessing,
file245,29,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file245,30,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 6
 test_df[""month""]=11",data preprocessing,,
file245,31,test_df.drop_duplicates(),data exploration,,
file245,32,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",comment only,,
file245,33,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data exploration,data preprocessing,
file245,34,"""""""test_df1=test_df.copy()
 for col in ['total_claims', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in extra:
  #test_df.loc[test_df.uu_id_enc==k,col] =0
  temp=df[df.uu_id_enc == k]
  temp[""average_wage""]=-9999
  temp =temp.replace("""",0)
  feature_test_pred = np.array(test_df1[test_df1.uu_id_enc==k])
  #print(k, temp)
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  idk = float(val.predict(np.array(feature_test_pred))[0])
  print(idk)
  test_df.loc[test_df.uu_id_enc==k,col].value = idk
 test_df""""""",comment only,,
file245,35,df3,data exploration,,
file245,36,"from statsmodels.tsa.stattools import adfuller
 adfuller(df3[""total_claims""])",helper functions,modeling,
file245,37,"from pandas.plotting import autocorrelation_plot
 autocorrelation_plot(df3[""total_claims""])",helper functions,result visualization,
file245,38,"df3 =df3.dropna()
 for col in features:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,col] = val.predict(np.array(feature_test_pred))[0]
 test_df",data exploration,data preprocessing,
file245,39,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",modeling,prediction,
file245,40,test_df,data exploration,,
file245,41,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",data exploration,save results,
file245,42,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")",save results,data preprocessing,
file246,0,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
 pylab.show()",result visualization,,
file247,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file247,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install db-dtypes\n!python3 -m pip install pandas\n!pip install pycaret\n"")",helper functions,,
file247,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 from tqdm import tqdm
 from pycaret.regression import *",helper functions,,
file247,3,%%capture,helper functions,,
file247,4,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data exploration,,
file247,5,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data exploration,data preprocessing,
file247,6,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",data preprocessing,,
file247,7,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: data for the first day, its month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",data preprocessing,,
file247,8,"def MSPE(s1, s2):
  print(""MSPE: "", sum((s1 - s2)**2)/len(s1))",evaluation,,
file247,9,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file247,10,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file247,11,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file247,12,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file247,13,"# Further check tracts with average_wage as Nan
 # I find three tracts with all average_wage as Nan. If I drop these tracts due to Nan value, they cannot be predicted
 for id in pd.unique(data[data['average_wage'].isna()][""uu_id""]):
  print(id)
  print(""All value are nan?"", data[data['uu_id'] == id][""average_wage""].isnull().all())
  print(""Included in prediction list?"", len(data_pred_query[data_pred_query['uu_id'] == id]) > 0)",data exploration,,
file247,14,"# Explore input data for NA and special values
 # dataExplore(data)
 # dataExplore(data_pred_query)
 data_pred_query.head()",data exploration,,
file247,15,"# Backup the data before pre-treatment
 data_backup = data.copy()
 data_pred_query_backup = data_pred_query.copy()",data preprocessing,,
file247,16,"# Pretreatment: convert week_number to month and week of month, to capture seasonality
 data = dataIdentifyDWM(data)",data preprocessing,,
file247,17,"# Check if the dataset is a balance panel (all tracts have value for all time periods)
 # 54% of tracts has less than 35 observations (total number of full time series), indicating it is unbalanced
 # Even if only checking data afer 2022/6/1, there are still 36% of tracts with incomplete series
 dataBalanceCheck(data)
 dataBalanceCheck(data[data[""date""] > ""2022-06-01""])",data preprocessing,,
file247,18,"# To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file247,19,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,,
file247,20,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file247,21,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file247,22,dataBalanceCheck(data_balance),data exploration,,
file247,23,"train_week = max(pd.unique(data[""week_number""]))",data preprocessing,,
file247,24,"# Split data to training and validaton sets
 # Max trainweek is 37, use a 80 / 20 rule
 train_week = int(max(pd.unique(data[""week_number""]))*0.8)",data preprocessing,,
file247,25,"data_train = data[data[""week_number""] < train_week]
 data_valid = data[data[""week_number""] >= train_week]",data preprocessing,,
file247,26,"data_train_x = data_train.drop(""total_claims"",1)
 data_train_y = data_train[""total_claims""]",data preprocessing,,
file247,27,"data_balance_valid_x = data_balance_valid.drop(""total_claims"",1)
 data_balance_valid_y = data_balance_valid[""total_claims""]",data preprocessing,,
file247,28,data_balance_valid_x.head(),data exploration,,
file247,29,"# Model 1 : Poisson regression with unbalanced data
 data_train_x_m1 = data_train_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_train_x_m1[""month""] = data_train_x_m1[""month""].astype(str)
 data_train_x_m1[""weekofmonth""] = data_train_x_m1[""weekofmonth""].astype(str)
 data_train_x_m1[""week_number2""] = data_train_x_m1[""week_number""]**2
 data_train_x_m1 = pd.get_dummies(data_train_x_m1)",data preprocessing,,
file247,30,"for i in range(8):
  data_valid_x_m1[""month_""+str(1+i)] = 0",data preprocessing,,
file247,31,"for i in range(5):
  if i == 1:
  pass
  data_valid_x_m1[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file247,32,"poission_model = sm.GLM(data_train_y.astype(int), data_train_x_m1.astype(float), family=sm.families.Poisson())
 result = poission_model.fit()
 result.summary()",modeling,data exploration,
file247,33,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
 MSPE(data_estimate_m1, data_valid_y)
 MAPE(data_estimate_m1, data_valid_y)",prediction,evaluation,
file247,34,data_train_x_m1.describe(),data exploration,,
file247,35,"data_train_x_m1[""month_8""] = 0",data preprocessing,,
file247,36,"data_train_x_m1[""month_8""] = 0
 data_train_x_m1[""month_9""] = 0",data preprocessing,,
file247,37,"data_balance_valid_x_m1 = data_balance_valid_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_balance_valid_x_m1[""month""] = data_balance_valid_x_m1[""month""].astype(str)
 data_balance_valid_x_m1[""weekofmonth""] = data_balance_valid_x_m1[""weekofmonth""].astype(str)
 data_balance_valid_x_m1[""week_number2""] = data_balance_valid_x_m1[""week_number""]**2
 data_balance_valid_x_m1 = pd.get_dummies(data_balance_valid_x_m1)",data preprocessing,,
file247,38,"# Although using a balanced model has better fit on the training set, the MSPE and MAPE are still larger then the first model.
 # So for this submission, I sitll use the m1 for prediction.
 data_lastWeek = data[data[""week_number""] == train_week][[""uu_id"", ""average_wage""]]
 data_lastWeek = data_lastWeek.drop_duplicates()",data preprocessing,,
file247,39,"data_pred = data_pred_query.copy()
 data_pred = data_pred.set_index('uu_id').join(data_lastWeek.set_index('uu_id'))
 data_pred.head()
 data_pred = dataIdentifyDWM(data_pred)
 data_pred = dataFillNa(data_pred, 0)
 data_pred[""month""] = data_pred[""month""].astype(str)
 data_pred = data_pred.drop(""date"",1)
 data_pred = pd.get_dummies(data_pred)
 for i in range(8):
  data_pred[""month_""+str(1+i)] = 0
 for i in range(5):
  if i == 1:
  pass
  data_pred[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file247,40,"output = result.predict(data_pred.astype(float))
 output_df = pd.DataFrame(output, columns=[""total_claims""])[""total_claims""]
 output_df[""uu_id""] = output_df.index",data preprocessing,,
file247,41,"data_pred_query = data_pred_query.merge(output_df, on = ""uu_id"")
 data_pred_query = data_pred_query[[""uu_id"", ""total_claims"", ""week_number""]]",data preprocessing,,
file247,42,data_pred_query,data exploration,,
file247,43,import plotly.express as px,helper functions,,
file247,44,"fig = px.line(data_balance, x=""week_number"", y=[""total_claims""], template = 'plotly_dark')
 fig.show()",result visualization,,
file247,45,"# Model 3 time series
 # average over tracts
 data_balance_ave = data_balance[[""]]",data preprocessing,,
file247,46,data_balance_ave,data exploration,,
file247,47,data_balance_ave.groupby(['week_number']).mean(),data preprocessing,,
file247,48,"data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave.reset_index()",data preprocessing,,
file247,49,"data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave.reset_index()
 data_balance_ave['MA4'] = data['total_claims'].rolling(4).mean()",data preprocessing,,
file247,50,"# Model 3 time series
 # visualize average over tracts
 data_balance_ave = data_balance[[""week_number"", ""total_claims"", ""uu_id""]]
 data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave = data_balance_ave.reset_index()
 data_balance_ave['MA4'] = data['total_claims'].rolling(4).mean()
 fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""])
 fig.show()",result visualization,data preprocessing,
file247,51,%%capture,helper functions,,
file247,52,"#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED
 #------------------------------------------
 # This is normally not required. The hub environment comes preinstaled with 
 # many packages that you can already use without setup. In case there is some
 # other library you would like to use that isn't on the list you run this command
 # once to install them. If it is already installed this command has no effect.
 get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('pip install pmdarima')",helper functions,,
file247,53,"data_balance_ave_train = data_balance_ave[[""week_num"",""total_claims""]]",data preprocessing,,
file247,54,"data_balance_ave_train = data_balance_ave[[""week_number"",""total_claims""]]
 data_balance_ave_train.set_index(""week_number"")",data preprocessing,,
file247,55,"model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)
 model.fit(data_balance_ave_train)",modeling,,
file247,56,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima",helper functions,,
file247,57,"cutoff = int(0.8 * len(data_balance_ave_train))
 print(cutoff)",data exploration,data preprocessing,
file247,58,data_balance_ave_valid = data_balance_ave_train[cutoff:],data preprocessing,,
file247,59,forecast = model.predict(n_periods=len(data_balance_ave_valid)),prediction,,
file247,60,forecast,data exploration,,
file247,61,forecast = model.predict(n_periods=5),prediction,,
file247,62,model.forecast(),data exploration,,
file247,63,model.summary(),data exploration,,
file247,64,model.plot_predict(dynamics = False),result visualization,prediction,
file247,65,model.predict(),prediction,,
file247,66,"model.predict(,steps = 1)",prediction,,
file247,67,data_balance_ave_valid,data exploration,,
file247,68,"data_balance_ave_train = data_balance_ave[[""week_number"",""total_claims""]]
 data_balance_ave_train = data_balance_ave_train.set_index(""week_number"")
 cutoff = int(0.9 * len(data_balance_ave_train))
 print(cutoff)",data exploration,data preprocessing,
file247,69,"data_balance_ave_valid = data_balance_ave_model[cutoff:]
 data_balance_ave_train = data_balance_ave_model[:cutoff]",data preprocessing,,
file247,70,"data_balance_ave_model[""total_claims""] = data_balance_ave_model[""total_claims""] + data_balance_ave_model[""week_number""]* 0.5",data preprocessing,,
file247,71,"data_balance_ave_model = data_balance_ave_model.set_index(""week_number"")
 cutoff = int(0.8 * len(data_balance_ave_model))
 print(cutoff)",data exploration,data preprocessing,
file247,72,model.predict(n_periods = 13),prediction,,
file247,73,"# Model 3 time series
 # First, visualize average total_claim
 data_balance_ave = data_balance[[""week_number"", ""total_claims"", ""uu_id""]]
 data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave = data_balance_ave.reset_index()
 data_balance_ave['MA4'] = data['total_claims'].rolling(4).mean()
 fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""])
 fig.show()",result visualization,data preprocessing,
file247,74,"# model train and validation
 data_balance_ave_model = data_balance_ave[[""week_number"",""total_claims""]]
 data_balance_ave_model = data_balance_ave_model.set_index(""week_number"")
 cutoff = int(0.8 * len(data_balance_ave_model))
 data_balance_ave_valid = data_balance_ave_model[cutoff:]
 data_balance_ave_train = data_balance_ave_model[:cutoff]
 model = auto_arima(data_balance_ave_train, trace=False, error_action='ignore', suppress_warnings=True)
 model.fit(data_balance_ave_train)
 model.predict(n_periods = 15)",prediction,modeling,
file247,75,forecast.shape,data exploration,,
file247,76,"df_forecast = pd.DataFrame(forecast)
 df_forecast.index.name = ""week_number""
 df_forecast",data exploration,data preprocessing,
file247,77,"# validation
 df_forecast = pd.DataFrame(forecast)
 df_forecast.index.name = ""week_number""
 df_forecast.rename(""total_claim_pred"")
 df_forecast",data exploration,data preprocessing,
file247,78,"data_balance_ave_valid_check == data_balance_ave_valid(df_forecast, on = ""week_number"")",data preprocessing,,
file247,79,"data_balance_ave_valid_check[""AE""] = abs(data_balance_ave_valid_check[""total_claims""] - data_balance_ave_valid_check[""total_claim_pred""])
 data_balance_ave_valid_check[""SE""] = (data_balance_ave_valid_check[""total_claims""] - data_balance_ave_valid_check[""total_claim_pred""])**@",data preprocessing,,
file247,80,"AE = sum(data_balance_ave_valid_check[""AE""])/len(data_balance_ave_valid_check)",data preprocessing,,
file247,81,"AE = sum(data_balance_ave_valid_check[""AE""])/len(data_balance_ave_valid_check)
 SE = sum(data_balance_ave_valid_check[""SE""])/len(data_balance_ave_valid_check)",data preprocessing,,
file247,82,AE,data exploration,,
file247,83,SE,data exploration,,
file247,84,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data exploration,data preprocessing,
file247,85,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data exploration,data preprocessing,
file247,86,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",data preprocessing,,
file247,87,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: data for the first day, its month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",data preprocessing,,
file247,88,"def MSPE(s1, s2):
  return(sum((s1 - s2)**2)/len(s1))",evaluation,,
file247,89,"data_balance_estimate_m2 = result_m2.predict(data_balance_valid_x_m1.astype(float))
 print(""MSPE: "", MSPE(data_balance_estimate_m2, data_balance_valid_y))
 print(""MAPE: "", MAPE(data_balance_estimate_m2, data_balance_valid_y))",evaluation,,
file247,90,MAPE_series,data exploration,,
file247,91,"def ARIMA_predict(df_input, cutoff_rate = 0.8, n_period = 15):
  cutoff = int(cutoff_rate * len(df_input))
  valid = df_input[cutoff:]
  train = df_input[:cutoff]
  model = auto_arima(train, trace=False, error_action='ignore', suppress_warnings=True)
  model.fit(train)
  forecast = model.predict(n_period)
  return(forecast)",modeling,prediction,
file247,92,"# model train and validation
 data_balance_ave_model = data_balance_ave[[""week_number"",""total_claims""]]
 data_balance_ave_model = data_balance_ave_model.set_index(""week_number"")
 forecast = ARIMA_predict(data_balance_ave_model, cutoff = 0.8, n_period = 15)",modeling,prediction,
file247,93,"uu_id_list = pd.unique(data_balance[""uu_id""])",data preprocessing,,
file247,94,len(uu_id_list),data exploration,,
file247,95,"data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[0]]",data preprocessing,,
file247,96,data_balance_tract,data exploration,,
file247,97,"data_balance_ave_valid_check = data_balance_ave_valid.merge(df_forecast, on = ""week_number"")
 MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
 MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])",evaluation,,
file247,98,data_balance_ave_valid_check,data exploration,,
file247,99,"# model train and validation
 MAPE_list = []
 MSPE_list = []",evaluation,,
file247,100,"for i in range(len(uu_id_list)):
  data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[i]]
  data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
  data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
  forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
  
  df_forecast = pd.DataFrame(forecast)
  df_forecast.index.name = ""week_number""
  df_forecast.columns = [""total_claim_pred""]
  
  data_balance_ave_valid_check = data_balance_ave_valid.merge(df_forecast, on = ""week_number"")
  MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
  MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
  
  MAPE_list.append(MAPE_series)
  MSPE_list.append(MSPE_series)
  
  if i > 0:
  break",evaluation,,
file247,101,"# validation
 MAPE_list",data exploration,,
file247,102,"# validation
 print(""MAPE: "", sum(MAPE_list)/len(MAPE_list))
 print(""MSPE: "", sum(MSPE_list)/len(MSPE_list))",data exploration,,
file247,103,"# Based on MAPE and MSPE, now the ARIMA model has best prediction, so the following prediction is based on ARIMA model
 data_lastWeek = data[data[""week_number""] == train_week][[""uu_id"", ""average_wage""]]
 data_lastWeek = data_lastWeek.drop_duplicates()",data preprocessing,,
file247,104,"# Based on MAPE and MSPE, now the ARIMA model has best prediction, so the following prediction is based on ARIMA model
 data_pred = data_pred_query.copy()",data preprocessing,,
file247,105,"# Prediction with ARIMA model
 data_pred.head()",data exploration,,
file247,106,"# Prediction with ARIMA model, for week 41
 uu_id_list_pred = pd.unique(data_pred[""uu_id""])",data preprocessing,,
file247,107,data_pred.head(),data exploration,,
file247,108,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file247,109,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file247,110,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file247,111,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file247,112,"# Prediction with ARIMA model, for week 41
 data_pred[""total_claims""] = 0",data preprocessing,,
file247,113,len(data_pred),data exploration,,
file247,114,"data_pred.iloc[0][""uu_id""]",data exploration,,
file247,115,"for i in range(len(data_pred)):
  uu_id_pre = data_pred.iloc[i][""uu_id""]
  data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_pre]
  data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
  data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
  forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
  
  df_forecast = pd.DataFrame(forecast)
  df_forecast.index.name = ""week_number""
  df_forecast.columns = [""total_claim_pred""]
  
  print(df_forecast.loc[41][""total_claim_pred""])
 

  break",data preprocessing,,
file247,116,data_pred,data exploration,,
file247,117,"data_pred.iloc[1][""total_claims""]",data exploration,,
file247,118,data_pred.iloc[0],data exploration,,
file247,119,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data exploration,data preprocessing,
file247,120,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data exploration,data preprocessing,
file247,121,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",,,
file247,122,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: data for the first day, its month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",,,
file247,123,"def MSPE(s1, s2):
  return(sum((s1 - s2)**2)/len(s1))",evaluation,,
file247,124,"def ARIMA_predict(df_input, cutoff_rate = 0.8, n_period = 15):
  cutoff = int(cutoff_rate * len(df_input))
  if cutoff_rate < 1:
  valid = df_input[cutoff:]
  train = df_input[:cutoff]
  model = auto_arima(train, trace=False, error_action='ignore', suppress_warnings=True)
  model.fit(train)
  forecast = model.predict(n_period)
  return(forecast)",modeling,prediction,
file247,125,"# Based on MAPE and MSPE, now the ARIMA model has best prediction, so the following prediction is based on ARIMA model
 data_pred = data_pred_query.copy()",data preprocessing,,
file247,126,"data_pred = data_pred[[""uu_id"", ""total_claims"", ""week_number""]]",data preprocessing,,
file247,127,"## This can also be a good place for you to cleanup any input/output and export your results to a file.
 data_pred_query.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file248,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file248,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file248,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from statsmodels.formula.api import ols
 from pandas import Series, DataFrame",helper functions,,
file248,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file248,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file248,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,,
file248,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file248,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data exploration,data preprocessing,
file248,8,"query_job3 = bigquery_client.query(query3)
 prediction_list = query_job3.to_dataframe()
 prediction_list.head()",data exploration,load data,
file248,9,"unemploy_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemploy_wage_data = unemploy_wage_data.drop(['timeperiod', 'countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemploy_wage_data = unemploy_wage_data.fillna(0)
 unemploy_wage_data.head()",data exploration,data preprocessing,
file248,10,unemploy_wage_data.describe(),data exploration,,
file248,11,"sns.relplot(data=unemploy_wage_data, x='week_number', y='total_claims')",result visualization,,
file248,12,"sns.distplot(unemploy_wage_data.total_claims, bins=10)",result visualization,,
file248,13,"plt.figure(figsize=(16,14))
 cor = unemploy_wage_data.corr()
 cmap = sns.diverging_palette(210, 20, as_cmap=True)
 sns.heatmap(cor, cmap=cmap, vmax=.99, vmin=-.99, annot=True)",result visualization,data preprocessing,
file248,14,"X = unemploy_wage_data[['week_number', 'countyfips_x', 'tract_x', 'edu_8th_or_less', 'edu_grades_9_11', \
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown', 'gender_female', 'gender_male', \
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', \
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 y = unemploy_wage_data['total_claims']",data preprocessing,,
file248,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file248,16,"reg = LinearRegression()  
 reg.fit(X_train, y_train)
 print(reg)",modeling,data exploration,
file248,17,"print(f'intercept: {reg.intercept_}')
 coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
 print(coef)",data exploration,data preprocessing,
file248,18,"y_pred = reg.predict(X_test)
 df = DataFrame({'Actual': y_test, 'Predicted': y_pred})
 df",prediction,data exploration,
file248,19,df['Predicted'].mean(),data exploration,,
file248,20,"print('R squared: {:.2f}'.format(reg.score(X, y)*100))
 print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
 print('MSE:', metrics.mean_squared_error(y_test, y_pred))
 print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",data exploration,,
file248,21,"#Make prediction
 prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
 prediction_data = prediction_data.drop(['week_number_x','total_claims'],axis=1)
 prediction_data = prediction_data.groupby(['uu_id']).mean()
 prediction_data",data exploration,data preprocessing,
file248,22,"prediction_list.to_csv('submission_prediction_output.csv', index=False)",save results,,
file249,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file249,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file249,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file249,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file249,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file249,5,print(covid19_cases_data),data exploration,,
file249,6,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file249,7,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data exploration,data preprocessing,
file250,0,"print(""Hello World :)"")",data exploration,,
file250,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file250,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file250,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file250,4,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file250,5,"query_job = bigquery_client.query(query)
 print(query_job)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file250,6,"query_job = bigquery_client.query(query)
 print(query_job)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)
 unemployment_data",data exploration,load data,
file250,7,"query_get_tables = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 print(query_job)",load data,data exploration,
file250,8,"query_get_tables = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 query_job.to_dataframe().head()",load data,data exploration,
file250,9,"query_job = bigquery_client.query(query)
 prediction_list_data = query_job.to_dataframe()
 prediction_list_data.head(2)",data exploration,load data,
file250,10,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id IN (SELECT uu_id 
  FROM `ironhacks-data.ironhacks_competition.prediction_list`
  WHERE week_number == MAX(week_number))
 """"""",load data,,
file250,11,"query = """"""
 SELECT uu_id 
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 WHERE week_number = MAX(week_number)
 """"""",load data,,
file250,12,"query = """"""
 SELECT uu_id
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 HAVING week_number = (SELECT MAX(week_number) FROM `ironhacks-data.ironhacks_competition.prediction_list`)
 """"""",load data,,
file250,13,relevant_unemployment_df,data exploration,,
file250,14,relevant_unemployment_df.drop_duplicates(),data preprocessing,,
file250,15,"relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df",data exploration,data preprocessing,
file250,16,"relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.isna().sum()
 relevant_unemployment_df.info()",data exploration,data preprocessing,
file250,17,"# Drop duplicate columns
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
 # Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis=1,thresh=1)",data preprocessing,,
file250,18,relevant_unemployment_df['top_category_employer1'][0,data exploration,,
file250,19,relevant_unemployment_df['uu_id'][0],data exploration,,
file250,20,relevant_unemployment_df.convert_dtypes(),data exploration,,
file250,21,"# Drop duplicate columns
 # relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)
 # Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].apply(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].apply(str)
 relevant_unemployment_df.info()",data exploration,data preprocessing,
file250,22,print(relevant_unemployment_df['uu_id'].value_counts()),data exploration,,
file250,23,"print(relevant_unemployment_df['uu_id'].value_counts())
 print(relevant_unemployment_df['top_category_employer1'].value_counts())",data exploration,,
file251,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file251,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file251,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file251,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file251,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file251,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file251,6,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable
 X.head()",data exploration,data preprocessing,
file252,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file252,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file252,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file252,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file252,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",data exploration,load data,
file252,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file252,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file252,7,print(covid19_cases_data),data exploration,,
file252,8,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file253,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file253,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file253,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file253,3,"def example_function():
  print('Hello World')",comment only,,
file253,4,example_function(),data exploration,,
file253,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file253,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 #rn that error is expected, they r still tryna fix it",load data,,
file253,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file253,8,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file253,9,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 `
 """"""",load data,,
file253,10,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head(5)",data exploration,load data,
file253,11,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 #unemployment_data.head(5)
 unemployment_data.columns()",data exploration,load data,
file253,12,prediction_data.head(5),data exploration,,
file253,13,"query = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file253,14,table_data.head(5),data exploration,,
file253,15,"pip install google-cloud-bigquery
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file253,16,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file253,17,"edu_level = unemployment_data[['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']]
 edu_level.head(5)",data exploration,data preprocessing,
file253,18,"edu_lvl = edu_level.groupby(['uu_id'])['edu_8th_or_less'].count().reset_index(
  name='Count').sort_values(['Count'], ascending=True)",data preprocessing,,
file253,19,edu_lvl.head(5),data exploration,,
file253,20,edu_lvl.max(),data exploration,,
file253,21,"gender = unemployment_data[['uu_id', 'total_claims', 'gender_female', 'gender_male', 'gender_na']]
 edu_level.head(5)",data exploration,data preprocessing,
file253,22,gender.max(),data exploration,,
file253,23,gender.head(),data exploration,,
file253,24,"genders = gender.groupby(['total_claims', 'uu_id'])['gender_female', 'gender_male'].count().reset_index(
  name='Count').sort_values(['Count'], ascending=True)",data preprocessing,,
file253,25,"df = unemployment_data.dropna()
 df.head()",data exploration,data preprocessing,
file253,26,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file253,27,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file253,28,"x = df['edu_post_hs'].values.reshape(-1,1)
 y = df['total_claims'].values.reshape(-1,1)",data preprocessing,,
file253,29,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file253,30,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train)",modeling,,
file253,31,"#To retrieve the intercept:
 print(regressor.intercept_)",data exploration,,
file253,32,"#For retrieving the slope:
 print(regressor.coef_)",data exploration,,
file253,33,y_pred = regressor.predict(X_test),prediction,,
file253,34,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df2",data exploration,data preprocessing,
file253,35,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file253,36,"plt.scatter(X_test, y_test, color='gray')
 plt.plot(X_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file253,37,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",data exploration,,
file253,38,"#fit the model to the data
 model = LinearRegression()  
 model.fit(X_train, y_train)",modeling,,
file253,39,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""",load data,,
file253,40,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(1)
 #print(unemployment_data.columns)",data exploration,data preprocessing,
file253,41,"#
 y_pred = model.predict(X_test)",prediction,,
file253,42,len(X),data exploration,,
file253,43,len(X_test),data exploration,,
file253,44,"#create training & split data
 X_train, X_test, y_train, y_test = train_test_split(X, y, train.size=0.9, test_size=0.1, random_state=0)",data preprocessing,,
file253,45,"output = df2.to_csv('submission_prediction_output.csv')
 output",save results,data exploration,
file254,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file254,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file254,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file254,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file254,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",helper functions,load data,
file254,5,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,,
file254,6,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]
 #df_three_col.shape",data preprocessing,,
file254,7,"df_three_col = df_three_col.drop_duplicates()
 #print(df_three_col.shape)",data preprocessing,,
file254,8,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
 #df_three_col.head(21)
 #df_three_col.tail(3)
 #uu_id week_number total_claims
 #1876 001cd9ae23064d7f0fd3cd327c873d8d 31 34",data preprocessing,,
file254,9,"tmp_df = df_three_col['week_number']
 tmp_df = tmp_df.drop_duplicates()
 print(tmp_df)",data exploration,data preprocessing,
file254,10,"res = pd.DataFrame(columns = ['uu_id', 'total_claims', 'week_number'])",data preprocessing,,
file254,11,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  #test_data = test_data.tail(3)
  week_list = test_data['week_number'].tolist()
  y_pred = -1
  count = 0
  sum = 0
  for week_id in range(33,38):
  if week_id in week_list:
  count+=1
  tmp_row = test_data[test_data['week_number'] == week_id ]
  sum+= int( tmp_row['total_claims'])
  if count > 0:
  y_pred = int(sum/count)
  else:
  max_week = max(week_list)
  tmp_row = test_data[test_data['week_number'] == max_week ] 
  y_pre = tmp_row['total_claims']
  cur_row = pd.DataFrame([[cur_uu_id, y_pred, 42]], columns=['uu_id', 'total_claims', 'week_number'] )
  res = pd.concat([res,cur_row] ,ignore_index = True)",data preprocessing,,
file254,12,"res.to_csv(""Nov28_submission_prediction_output.csv"", index=False)",save results,,
file255,0,"#importing libraries
 import numpy as np
 import matplotlib.pyplot as plt
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file255,1,"#Code for BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file255,2,"#Testing bigquery
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file255,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,,
file255,4,"#Testing bigquery
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.unempoyment_data`
 """"""
 query2=""""""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.wage_data`
 """"""",load data,,
file255,5,"query_job = bigquery_client.query(query2)
 wage_data = query_job.to_dataframe()
 wage_data.head()",load data,data exploration,
file255,6,"#Query 
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file255,7,"#Query Unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 order by week_number
 """"""",load data,,
file256,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file256,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file256,2,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file256,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file256,4,"covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file256,5,print(covid19_cases_data),data exploration,,
file256,6,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file256,7,"import os
 import pandas",helper functions,,
file256,8,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,helper functions,
file256,9,"query_job = bigquery_client.query(query)
 import db_dtypes
 date_dtype_name = db.DateDtype.name
 data = query_job.to_dataframe()
 data.head()",load data,data exploration,
file257,0,"visual_data = data.groupby(['uu_id'])['total_claims'].sum().reset_index().merge(
  data_dict['wage_data'],
  on=['uu_id'],
  how='inner',
 )
 sns.scatterplot(data=visual_data, x='total_claims', y='average_wage', marker='+')",result visualization,data preprocessing,
file257,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file257,2,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!pip install db-dtypes pmdarima 'google-cloud-bigquery[pandas]' tqdm\n"")",helper functions,,
file257,3,"import numpy as np
 import pandas as pd
 from tqdm.auto import tqdm
 import seaborn as sns
 from google.cloud import bigquery",helper functions,,
file257,4,from pmdarima.arima import AutoARIMA,helper functions,,
file257,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file257,6,"def print_missing(data: pd.DataFrame):
  """"""Show how much missing data in each column of the input DataFrame.""""""
  for cname, cvalues in data.items():
  print('Column {} has {} ({}%) missing value(s)'.format(
  cname,
  cvalues.isna().sum(),
  round(100.0 * cvalues.isna().sum() / len(cvalues), 2),
  ))",data exploration,data preprocessing,
file257,7,"data_tables = bigquery_client.query(f""""""
  SELECT table_catalog, table_schema, table_name
  FROM `ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """""").to_dataframe()
 print(data_tables)",data exploration,,
file257,8,"# Read all data tables in ironhacks-data.ironhacks_competition
 data_dict = {
  table_name: bigquery_client.query(f""""""
  SELECT * FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """""").to_dataframe()
  for table_name in data_tables['table_name'].tolist()
 }",load data,,
file257,9,"common_cols = list(set(data_dict['unemployment_data'].columns) & set(data_dict['wage_data'].columns))
 print(f'Common columns: {common_cols}')
 data = data_dict['unemployment_data'].merge(
  data_dict['wage_data'],
  on=common_cols,
  how='left',
 ).sort_values(['countyfips', 'week_number']).drop_duplicates().reset_index(drop=True)
 data['timeperiod'] = pd.to_datetime(data['timeperiod'], format='%Y%m%d')
 data",data exploration,,
file257,10,"print(data.columns)
 print_missing(data)",data exploration,,
file257,11,"for cname in ['edu', 'gender', 'race']:
  cols = [c for c in data.columns if c.startswith(cname)]
  data[cols] = data[cols].fillna(0)
  data[f'{cname}_missing'] = data['total_claims'] - data[cols].sum(axis=1)",data preprocessing,,
file257,12,"cols = [c for c in data.columns if c.startswith('top_category_employer')]
 data[cols] = data[cols].replace('N/A', None)
 # data = pd.get_dummies(data, columns=cols, dummy_na=True)
 print_missing(data)",data exploration,data preprocessing,
file257,13,"cols_race = [c for c in data.columns if c.startswith('race')]
 data.groupby(['timeperiod'])[cols_race].sum().plot(legend=True)",data preprocessing,,
file257,14,"for cname in ['edu', 'gender', 'race']:
  plt.figure()
  cols = [c for c in data.columns if c.startswith(cname)]
  data.groupby(['timeperiod'])[cols_race].sum().plot(legend=True)",data preprocessing,,
file257,15,"# For our own convenience, create a correspondance DataFrame for `week_number` and `timeperiod`.
 wt = pd.Series(
  range(1, 53),
  name='week_number',
  index=pd.date_range('2022-01-01', periods=52, freq='W-SAT').rename('timeperiod'),
 ).reset_index()
 wt['timeperiod'] = wt['timeperiod'].astype(str)
 wt.head()",data exploration,data preprocessing,
file258,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file258,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file258,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file258,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file258,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file258,5,"query = """"""
 

 SELECT date, max_rel_humidity
 FROM ironhacks-data.ironhacks_training.weather_data
 WHERE date='2020-06-16'
 

 

 

 

 """"""",load data,,
file258,6,"query_job = bigquery_client.query(query)
 get_ipython().system('python3 -m pip install pandas')
 import pandas
 data = query_job.to_dataframe()
 data.head()",load data,,
file258,7,"query = """"""
 

 Select 
 a.*,
 b.cases 
 

 FROM 
 

 (SELECT extract(week(Monday) from date) as week_number, AVG(mean_temperature) as mean_temperature_week, date as start_date, AVG(wind_speed) as mean_wind_speed_week
 FROM `ironhacks_training.weather_data`
 group by week_number,start_date) a
 

 JOIN `ironhacks-data.ironhacks_training.covid19_cases` b 
 ON a.week_number=b.week_number
 order by week_number
 

 

 

 """"""",load data,,
file259,0,"print(""Hello World :)"")",comment only,,
file259,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file259,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file259,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file259,4,"query_job = bigquery_client.query(query)
 print(query_job)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",data exploration,load data,
file259,5,get_ipython().system('pip install db-dtypes'),helper functions,,
file260,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file260,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install db-dtypes\n!python3 -m pip install pandas\n!pip install pmdarima\n"")",helper functions,,
file260,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima",helper functions,,
file260,3,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data exploration,,
file260,4,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data exploration,data preprocessing,
file260,5,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",data preprocessing,,
file260,6,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: data for the first day, its month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",data preprocessing,,
file260,7,"def MSPE(s1, s2):
  return(sum((s1 - s2)**2)/len(s1))",evaluation,,
file260,8,"def ARIMA_predict(df_input, cutoff_rate = 0.8, n_period = 15):
  cutoff = int(cutoff_rate * len(df_input))
  if cutoff_rate < 1:
  valid = df_input[cutoff:]
  train = df_input[:cutoff]
  model = auto_arima(train, trace=False, error_action='ignore', suppress_warnings=True)
  model.fit(train)
  forecast = model.predict(n_period)
  return(forecast)",modeling,prediction,
file260,9,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file260,10,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file260,11,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file260,12,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file260,13,"# Explore input data for NA and special values
 dataExplore(data)
 # dataExplore(data_pred_query)
 # data_pred_query.head()",data exploration,,
file260,14,%%capture,helper functions,,
file260,15,"#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED
 #------------------------------------------
 # This is normally not required. The hub environment comes preinstaled with 
 # many packages that you can already use without setup. In case there is some
 # other library you would like to use that isn't on the list you run this command
 # once to install them. If it is already installed this command has no effect.
 get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('pip install pmdarima')
 get_ipython().system('pip install plotly==5.11.0')
 get_ipython().system('pip install hts_reconciliation')",load data,,
file260,16,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima
 from collections.abc import Iterable
 import hts
 from hts.hierarchy import HierarchyTree
 from hts.model import AutoArimaModel
 from hts import HTSRegressor",helper functions,,
file261,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file261,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file261,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file261,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file261,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file261,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data`
 """"""",load data,,
file261,6,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_competition.unemployment_data`
 """"""",load data,,
file262,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file262,1,"# # Run these terminal commands when your Notebook Session begins
 !gcloud auth login
 !gcloud auth application-default set-quota-project ironhacks-data",helper functions,,
file262,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file262,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file262,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file263,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file263,1,print(X_train),data exploration,,
file263,2,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file263,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file263,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data exploration,
file263,5,"X = data['min_temperature'].values.reshape(-1,1)
 y = data['max_temperature'].values.reshape(-1,1)",data preprocessing,,
file263,6,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file263,7,class(X_train),data exploration,,
file264,0,"# wdf.to_csv(""wage.csv"")
 # udf.to_csv(""unemploy.csv"")
 # Mdf.to_csv(""mixed.csv"")
 udf = pd.read_csv(""unemploy.csv"")
 wdf = pd.read_csv(""wage.csv"")
 mdf = pd.read_csv(""mixed.csv"")",load data,,
file264,1,"import pandas as pd
 import numpy as np",helper functions,,
file264,2,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file264,3,"# QUERY THE DATA ONCE
 query1_job = bigquery_client.query(query1)
 pdf = query1_job.to_dataframe()
 pdf.head()",data exploration,,
file264,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file264,5,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file264,6,get_ipython().system('pip install db-dtypes'),helper functions,,
file265,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = BIGQUERY_PROJECT.Client(project=BIGQUERY_PROJECT)",load data,,
file265,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file265,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file265,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file265,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file265,5,print(covid19_cases_data),data exploration,,
file265,6,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file265,7,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data exploration,data preprocessing,
file266,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file266,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file266,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file266,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file266,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file266,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,load data,
file266,6,data.describe(),data exploration,,
file266,7,data.shape,data exploration,,
file266,8,data.info(),data exploration,,
file266,9,y=data['total_claims'],data preprocessing,,
file266,10,"l=[]
 for i in data.columns:
  if sum(data[i].isnull())>0:
  l.append(i)
  print('The null values in',i,'are',sum(data[i].isnull()))",data exploration,data preprocessing,
file266,11,data.isnull().sum(axis=0),data preprocessing,,
file266,12,"data=data.drop('total_claims',axis=1)",data preprocessing,,
file266,13,"for i in data.columns:
  print('The unique values in',i,'are',len(data[i].value_counts()))",data exploration,,
file266,14,data.corr(),data exploration,,
file266,15,"plt.figure(figsize=(20,15))
 cor = data.corr()
 sns.heatmap(cor,annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file266,16,"for i in data.columns:
  if data[i].isnull().sum()>= 0.5*len(data)
  data=data.drop(i,axis=1)",data preprocessing,,
file266,17,data.columns,data exploration,,
file266,18,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print(data[i].value_counts())",data exploration,data preprocessing,
file266,19,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print('The value counts of feature',i)
  print(data[i].value_counts())",data exploration,data preprocessing,
file267,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file267,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file267,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file267,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file267,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file267,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,,
file267,6,empdata.head().transpose(),data exploration,,
file267,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file267,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data exploration,,
file267,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data exploration,,
file267,10,empdata[['total_claims']].describe(),data exploration,,
file267,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file267,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",result visualization,,
file267,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file267,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file267,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data exploration,data preprocessing,
file267,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=4)[3]",prediction,modeling,
file267,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file267,18,uupred.head(),data exploration,,
file267,19,"uupred['total_claims'] = 0
 uupred.head()",data exploration,data preprocessing,
file267,20,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values[:lastw], seasonal=False, error_action='ignore')
  pred = m.predict(n_periods=predw-lastw)
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",data exploration,prediction,
file267,21,"dopred(37,41)",data exploration,,
file267,22,"uupred = uupred[['uu_id', 'total_claims', 'week_number']]
 uupred.to_csv('submission_prediction_output.csv', index=False)",save results,,
file268,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file268,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file268,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file268,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition`
 """"""",load data,,
file268,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file268,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_competition.unemployment_data`
 """"""",load data,,
file269,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file269,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file269,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file269,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",,load data,
file269,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data exploration,
file269,5,"# wdf.to_csv(""wage.csv"")
 # udf.to_csv(""unemploy.csv"")
 # Mdf.to_csv(""mixed.csv"")
 pdf.to_csv(""pred.csv"")
 # udf = pd.read_csv(""unemploy.csv"")
 # wdf = pd.read_csv(""wage.csv"")
 # mdf = pd.read_csv(""mixed.csv"")",load data,,
file269,6,"import pandas as pd
 import numpy as np",helper functions,,
file269,7,print(dict(udf.head(1))),data exploration,,
file269,8,"# print(dict(mdf.head(1)))
 print(dict(udf.head(1)))",data exploration,,
file269,9,"# mdf.head()
 udf.head()",data exploration,,
file269,10,"# mdf = mdf.drop(['Unnamed: 0', 'uu_id_1','countyfips_1', 'tract_1', 'tract_name_1'], axis=1)
 udf = udf.drop(['Unnamed: 0'], axis=1)",data preprocessing,,
file269,11,"# mdf = mdf.replace(np.nan, 0)
 udf = udf.replace(np.nan, 0)",data preprocessing,,
file269,12,"emp_cols = [""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]
 mdf[emp_cols] = mdf[emp_cols].replace('31-33', ""32"")
 mdf[emp_cols] = mdf[emp_cols].replace('48-49', ""48"")
 mdf[emp_cols] = mdf[emp_cols].replace('44-45', ""44"")",data preprocessing,,
file269,13,"mdf[""top_category_employer1""] = pd.to_numeric(mdf[""top_category_employer1""])
 mdf[""top_category_employer2""] = pd.to_numeric(mdf[""top_category_employer2""])
 mdf[""top_category_employer3""] = pd.to_numeric(mdf[""top_category_employer3""])",data preprocessing,,
file269,14,"mdf[""top_category_employer1""].unique()",data preprocessing,,
file269,15,"# print(dict(mdf.head(1)))
 print(udf.dtypes)
 mdf = udf",data exploration,,
file269,16,mdf.head(),data exploration,,
file269,17,"import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file269,18,mdf.columns,data exploration,,
file269,19,"plt.figure(figsize=(20,14))
 cor = mdf.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file269,20,"f_df = mdf[[""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",'gender_female', 'gender_male', 'race_black', 'race_white','timeperiod', 'week_number',""uu_id""]]",data preprocessing,,
file269,21,"lst = list(f_df['uu_id'].unique())
 len(lst)",data exploration,,
file269,22,"f2_df = f_df[f_df[""uu_id""] == lst[2]]
 len(f2_df[""week_number""])",data exploration,,
file269,23,"f_df[""uu_id""].value_counts",data preprocessing,,
file269,24,"pdf = pd.read_csv(""pred.csv"")",load data,,
file269,25,pdf.value_counts(),data exploration,,
file269,26,"pdf.drop([""Unnamed: 0""], axis = 1)
 pdf.value_counts()",data exploration,,
file269,27,"pdf = pdf.drop([""Unnamed: 0""], axis = 1)
 sorted(pdf[""uu_id""])
 pdf.value_counts()",data exploration,,
file269,28,"sorted(pdf[""uu_id""])
 # pdf.value_counts()",data preprocessing,,
file269,29,"sorted(f_df[""uu_id""])",data preprocessing,,
file269,30,"f2_df = f_df[f_df[""uu_id""] == ""6a4357ec916e146c5329ac2498a84f66""]
 f2_df[""week_number""]",data exploration,data preprocessing,
file269,31,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
 f2_df[""week_number""]",data exploration,data preprocessing,
file269,32,"f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1"" and f_df[""week_number""] == 3]
 # f2_df[""week_number""].value_counts()",data exploration,data preprocessing,
file269,33,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
 # f3_df = f2_df[f2_df[""week_number""] == 33]
 # f3_df
 # f2_df.drop_duplicates()
 f2_df",data exploration,,
file269,34,"# f2_df = f_df[f_df[""uu_id""] == ""50247c509e6c47b550a516f66e35c1d1""]
 f3_df = f2_df.drop_duplicates()
 # f3_df = f2_df[f2_df[""week_number""] == 33]
 f3_df.value_count()",data exploration,,
file269,35,"sorted(f2_df[""uu_id""])
 f2_df[""uu_id""].value_counts()",data exploration,data preprocessing,
file269,36,"mdf = mdf.drop_duplicates()
 mdf.head()",data exploration,data preprocessing,
file269,37,"f1_df = f2_df.groupby(['week_number']).mean()
 f1_df",data exploration,data preprocessing,
file269,38,"data1 = f1_df.set_index('week_number')
 data1.index",data preprocessing,,
file269,39,"f1_df[""total_claims""].plot(figsize=(15, 6))
 plt.show()",result visualization,,
file270,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file270,1,"import csv
 import pandas as pd
 import numpy as np",helper functions,,
file270,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file270,3,"from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file270,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file270,5,"def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  #display(df.tail(n=5))
  #print(df.shape)
  return df",load data,,
file270,6,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 37
 order by week_number
 """"""",load data,,
file270,7,unemployment_claims_data = query_from_statement(u_claims_query),data preprocessing,,
file270,8,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file270,9,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file270,10,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file270,11,"print(unemployment_claims_data.isna().sum())
 print(unemployment_wage_data.isna().sum())",data exploration,,
file270,12,"unemployment_claims_data = unemployment_claims_data.fillna(0)
 unemployment_wage_data = unemployment_wage_data.fillna(0)",data preprocessing,,
file270,13,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file270,14,"data = data.drop(['tract_name', 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file270,15,"plt.figure(figsize=(8,6))
 cor = data.corr().round(2)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, annot_kws={""size"": 6})
 plt.show()",result visualization,,
file270,16,"data = data.drop(['uu_id','timeperiod'], axis=1)",data preprocessing,,
file270,17,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file270,18,"data = data.drop(['total_claims'], axis=1)
 X = data.values",data preprocessing,,
file270,19,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data exploration,,
file270,20,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train)
 print(regressor.intercept_)
 print(regressor.coef_)",modeling,,
file270,21,y_pred = regressor.predict(X_test),prediction,,
file270,22,"plt.scatter(y_test, y_pred, color='gray')
 plt.show()",result visualization,,
file270,23,"metrics.mean_absolute_percentage_error(y_test, y_pred)",evaluation,,
file270,24,"unemployment_prediction_data = query_from_statement(prediction_query)
 print(unemployment_prediction_data.shape)
 display(unemployment_prediction_data.head(n=5))",data exploration,,
file270,25,unemployclaims_supplemental_data = query_from_statement(unemployclaims_supplemental_query),load data,,
file270,26,"# APPLYING ALL TRANSFORMATIONS TO THE LATEST WEEK ONLY DATAFRAME
 unemployclaims_supplemental_data = unemployclaims_supplemental_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file270,27,"unemployclaims_supplemental_data = unemployclaims_supplemental_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 unemployclaims_supplemental_data = unemployclaims_supplemental_data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES
 unemployclaims_supplemental_data = unemployclaims_supplemental_data.drop(['tract_name', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3'], axis=1)
 unemployclaims_supplemental_data = unemployclaims_supplemental_data.drop(['timeperiod'], axis=1)
 unemployclaims_supplemental_data = unemployclaims_supplemental_data.drop(['total_claims'], axis=1)",data preprocessing,,
file270,28,unemployclaims_supplemental_data = unemployclaims_supplemental_data.fillna(0),data preprocessing,,
file270,29,"print(unemployclaims_supplemental_data.shape)
 display(unemployclaims_supplemental_data.head(n=5))",data exploration,,
file270,30,"final_prediction_data = unemployment_prediction_data.join(unemployclaims_supplemental_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 final_prediction_data = final_prediction_data.drop(['uu_id', 'week_number_other'], axis=1)
 print(final_prediction_data.shape)",data preprocessing,data exploration,
file270,31,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",prediction,data exploration,
file270,32,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,data exploration,
file270,33,"unemployment_prediction_data.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file271,0,"print(""Hello World"")",data exploration,,
file271,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file271,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file271,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file271,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file271,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file272,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file272,1,"import pandas as pd
 import numpy as np
 from google.cloud import bigquery",helper functions,,
file272,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file272,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file272,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file272,5,unemployment_data.shape,data exploration,,
file272,6,unemployment_data.sort_values(['week_number']),data exploration,,
file272,7,unemployment_data.columns,data exploration,,
file272,8,"unemployment_data.drop(['uu_id', 'week_number', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data preprocessing,,
file272,9,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file272,10,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file272,11,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file272,12,"unemployment_data.sort_values(['ds'], inplace=True)",data preprocessing,,
file272,13,unemployment_data.drop_duplicates(inplace=True),data preprocessing,,
file272,14,from prophet import Prophet,helper functions,,
file272,15,ud = unemployment_data,data preprocessing,,
file272,16,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file272,17,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",data preprocessing,,
file272,18,m = Prophet(),modeling,,
file272,19,m.fit(ud_train),modeling,,
file272,20,"future = m.make_future_dataframe(periods=ud_test.shape[0], freq='W')",data preprocessing,,
file272,21,forecast = m.predict(df=future),prediction,,
file272,22,forecast,data exploration,,
file272,23,plot1 = m.plot(forecast),result visualization,,
file272,24,ud_test,data exploration,,
file272,25,"ud_test
 ud_train",data exploration,,
file272,26,"print(ud_train, ud_test)",data exploration,,
file272,27,ud_train,data exploration,,
file272,28,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file272,29,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file272,30,unemployment_data.shape,data exploration,,
file272,31,unemployment_data.sort_values(['week_number']),data exploration,,
file272,32,unemployment_data.columns,data exploration,,
file272,33,"unemployment_data.drop(['uu_id', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data exploration,,
file272,34,"unemployment_data.drop_duplicates(inplace=True)
 unemployment_data.sort_values(['week_number'])",data preprocessing,,
file272,35,"m = Prophet(weekly_seasonality=False,
  daily_seasonality=False,
  interval_width=0.95, 
  mcmc_samples = 500)",modeling,,
file272,36,"m.add_seasonality(
  name='monthly', 
  period=30.5, 
  fourier_order=5
  )",modeling,,
file272,37,forecast['ds'],data exploration,,
file272,38,forecast['ds' == pd.to_datetime('2022-05-14')]).yhat,data exploration,,
file272,39,ud = unemployment_data,data preprocessing,,
file272,40,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file272,41,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",data preprocessing,,
file272,42,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file272,43,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file272,44,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day', 'week_number'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file272,45,"unemployment_data.sort_values(['ds'], inplace=True)",data exploration,,
file272,46,m.fit(ud_train),modeling,,
file272,47,"future = m.make_future_dataframe(periods=35, freq='W')",data preprocessing,,
file272,48,forecast = m.predict(df=future),prediction,,
file272,49,ud_train,data exploration,,
file273,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file273,1,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file273,2,get_ipython().system('pip3 install git+https://github.com/ourownstory/neural_prophet.git#egg=neuralprophet'),helper functions,,
file273,3,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file273,4,get_ipython().system('pip3 install neuralprophet'),helper functions,,
file273,5,"import pandas as pd 
 from neuralprophet import NeuralProphet
 from matplotlib import pyplot as plt
 import pickle",helper functions,,
file273,6,"!pip3 install neuralprophet
 get_ipython().system('pip uninstall neuralprophet')",helper functions,,
file273,7,get_ipython().system('pip install db-dtypes'),helper functions,,
file273,8,get_ipython().system('pip install torch==1.6.0'),helper functions,,
file273,9,"# Let's look at the wage_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file273,10,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head(3)",load data,data exploration,
file273,11,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file273,12,"#Number of rows in wage_data
 len(wage_data.index)",data exploration,,
file273,13,"#Let's merge the unemployment_data and wage_data
 # We will merge only cols uu_id and average_wage from wage_data into unemployment_data
 Merged_unemployment_wage = unemployment_data.merge(wage_data[['uu_id', 'average_wage']])
 Merged_unemployment_wage.columns",data exploration,data preprocessing,
file273,14,Merged_unemployment_wage.head(3),data exploration,,
file273,15,"#checking for duplicates rows
 print(Merged_unemployment_wage.duplicated().sum())",data exploration,,
file273,16,"#Drop duplicate rows
 drop_duplicates = Merged_unemployment_wage.drop_duplicates()",data preprocessing,,
file273,17,print(drop_duplicates.duplicated().sum()),data exploration,,
file273,18,"#Checking for nulls 
 drop_duplicates.isnull().sum()",data exploration,,
file273,19,"#Replaced the remaining null values with 0s
 cleaned_df = drop_duplicates.fillna(0)",data preprocessing,,
file273,20,cleaned_df.isnull().sum(),data exploration,,
file273,21,"# Is there a column with all 0's?
 (cleaned_df == 0).all()",data exploration,,
file273,22,"#Let's drop the column with no data
 Final_df=cleaned_df.drop(['race_hawaiiannative'], axis=1)",data preprocessing,,
file273,23,Final_df.shape,data exploration,,
file273,24,Final_df.dtypes,data exploration,,
file273,25,Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).sum().add_prefix('Sum_of_'),data preprocessing,,
file273,26,"plt.rcParams[""figure.figsize""] = (12,4)
 ax = Claims_by_week.plot(title='Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,,
file273,27,"Mean_Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).mean().add_prefix('Mean_of_')
 plt.rcParams[""figure.figsize""] = (12,4)
 ax = Mean_Claims_by_week.plot(title='Mean Of Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,,
file273,28,"matrix = Final_df.corr().round(2)
 sns.heatmap(matrix, annot=True)
 plt.show()",result visualization,,
file273,29,"#Top 3 UUID with the max number of claims
 Top_10_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False)
 Top_10_claimers.head(3)",data exploration,data preprocessing,
file273,30,"#UUIDs with the least number of claims
 Top_10_claimers.tail(3)",data exploration,,
file273,31,"#Do people with a certain degree file more claims 
 y=[Final_df.edu_8th_or_less.sum(),
  Final_df.edu_grades_9_11.sum(),
  Final_df.edu_hs_grad_equiv.sum(),
  Final_df.edu_post_hs.sum(),
  Final_df.edu_unknown.sum()
  ]",data preprocessing,,
file273,32,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(17, 7))
 plt.title(""Total Claims by Education"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['8th grade or less education','9 through 11','high school diploma or equivalent','completed a degree beyond high school','Education Unknown'],color='black')",result visualization,,
file273,33,"# To display sum values
 for index, value in enumerate(y):
  plt.text(value, index,
  str(value))
 plt.show()",result visualization,,
file273,34,"#Do people with a certain gender file more claims 
 y=[Final_df.gender_female.sum(),
  Final_df.gender_male.sum(),
  Final_df.gender_na.sum()
  ]",data preprocessing,,
file273,35,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(15,7))
 plt.title(""Total Claims by Gender"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['Female', 'Male' , 'Gender Unknown'],color='black')",result visualization,,
file273,36,"#Do people with a certain gender file more claims 
 y=[Final_df.race_amerindian.sum(),
  Final_df.race_asian.sum(),
  Final_df.race_black.sum(),
  Final_df.race_noanswer.sum(),
  Final_df.race_other.sum(),
  Final_df.race_white.sum()
  ]",data preprocessing,,
file273,37,"pd.set_option('float_format', '{:f}'.format)
 Final_df[['average_wage']].describe()",data preprocessing,,
file273,38,"Final_df[['top_category_employer1', 'top_category_employer2', 'top_category_employer3']].describe()",data preprocessing,,
file273,39,"sns.relplot(x =""edu_8th_or_less"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_grades_9_11"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_hs_grad_equiv"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_post_hs"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_unknown"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer1"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer2"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer3"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_female"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_male"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_na"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_amerindian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_asian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_black"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_noanswer"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_other"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_white"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""average_wage"", y =""total_claims"",
  data = Final_df);",,,
file273,40,"Final_df['timeperiod'] = pd.to_datetime(Final_df['timeperiod'],
  format='%Y%m%d')",data preprocessing,,
file273,41,"d = dict.fromkeys(Final_df.select_dtypes(np.int64).columns, np.int32)
 Final_df = Final_df.astype(d)
 Final_df.dtypes",data preprocessing,,
file273,42,Final_df.uu_id.unique(),data exploration,,
file273,43,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 uuid1 = Final_df[Final_df['uu_id']=='f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 #uuid1['timeperiod'] = pd.to_datetime(uuid1['timeperiod'],
 # format='%Y%m%d') 
 uuid1",data preprocessing,data exploration,
file273,44,"plt.plot(uuid1['timeperiod'], uuid1['total_claims'])
 plt.show()",result visualization,,
file273,45,"data = uuid1[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data exploration,data preprocessing,
file273,46,m = NeuralProphet(),modeling,,
file273,47,"from neuralprophet import NeuralProphet
 m = NeuralProphet()",helper functions,modeling,
file273,48,get_ipython().system('pip3 install neuralprophet[live]'),helper functions,,
file273,49,from neuralprophet import NeuralProphet,helper functions,,
file274,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file274,1,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file274,2,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file274,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file274,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file274,5,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file274,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file274,7,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file274,8,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file274,9,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data exploration,data preprocessing,
file274,10,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data exploration,data preprocessing,
file274,11,Using Linear regression model to predict from the data,helper functions,,
file274,12,"from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]
 temp=test.loc[test[""week_number""]==34]",modeling,data preprocessing,
file274,13,"model.fit(X_train,y)",modeling,,
file274,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data exploration,data preprocessing,
file274,15,"y_pred = model.predict(X_test)
 y_pred",data exploration,prediction,
file274,16,"a=pd.DataFrame(data=temp)
 a['count']=y_pred
 a['count']=a['count'].astype('int')
 a=a[['uu_id','week_number','count']]
 a['week_number']=a['week_number']+1
 a=a.rename(columns={'week_number':'week'})
 a.to_string(index=False)",data preprocessing,,
file274,17,"a.to_csv(""submission2.csv"",index=False)
 a",save results,data exploration,
file274,18,y_true=test.loc[['week_number'==35]],data preprocessing,,
file274,19,test['week_number],data exploration,,
file274,20,"y_true=test.loc[test['week_number']==35]
 MAE=metrics.mean_absolute_error(y_true,y_pred)
 MSE=metrics.mean_squared_error(y_true,y_pred)
 print(MAE)
 MSE",evaluation,data preprocessing,
file275,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file275,1,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file275,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file275,3,"#QUERY DATA
 query_job = bigquery_client.query(query)
 print(query_job)
 unemp = query_job.to_dataframe()",load data,,
file276,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file276,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file276,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file276,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file276,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,load data,
file276,5,data.info(),data exploration,,
file276,6,"data[""top_category_employer2""].value_counts()",data exploration,,
file276,7,"data.loc[data[""uu_id""] == ""bd5b040eae3010ce09da8b176ed5aee0""]",data exploration,,
file276,8,"data[""edu_8th_or_less""].value_counts()",data exploration,,
file276,9,"data[""race_black""].value_counts()",data exploration,,
file276,10,"data[""gender_female""].value_counts()",data exploration,,
file277,0,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file277,1,"get_ipython().run_cell_magic('capture', '', '%logstop\n%logstart -t -r -q ipython_command_log.py global\n')",helper functions,,
file277,2,"import os
 from datetime import datetime
 import IPython.core.history as history",helper functions,,
file277,3,"ha = history.HistoryAccessor()
 ha_tail = ha.get_tail(1)
 ha_cmd = next(ha_tail)
 session_id = str(ha_cmd[0])
 command_id = str(ha_cmd[1])
 timestamp = datetime.utcnow().isoformat()
 history_line = ','.join([session_id, command_id, timestamp]) + '\n'
 logfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')
 logfile.write(history_line)
 logfile.close()",data preprocessing,,
file277,4,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file277,5,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file277,6,"pip install google-cloud-bigquery
 pip install google-cloud-bigquery[pandas]",helper functions,,
file277,7,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file277,8,"query = """"""
 SELECT * FROM 'ironhacks-data.ironhacks_competition.unemployment_data'
 """"""",load data,,
file277,9,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,load data,
file277,10,"def querydb(request):
  query_job = bigquery_client.query(request)
  data = query_job.to_dataframe()
  return data",load data,,
file278,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file278,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file278,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file278,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file278,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data exploration,
file278,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file278,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file278,7,print(covid19_cases_data),data exploration,,
file278,8,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file278,9,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file279,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file279,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file279,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file279,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file279,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file279,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file279,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",load data,helper functions,
file279,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file279,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file279,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file279,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file279,11,"def example_function():
  print('Hello World')",comment only,,
file279,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file279,13,"model.fit(x,y)
 LinearRegression",modeling,,
file279,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file279,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",data exploration,modeling,
file279,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file279,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file279,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",data exploration,prediction,
file279,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file279,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,data exploration,
file279,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file279,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file279,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file279,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file279,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file279,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file279,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file279,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file279,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file279,30,"print(f""slope: {new_model.coef_}"")",data exploration,,
file279,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file279,32,"# Test Linear Regression
 x",data exploration,,
file279,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file279,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file279,35,from lmfit,helper functions,,
file279,36,"import Minimizer, Parameters, report_fit
 import numpy as np
 import matplotlib.pylab as plt",helper functions,,
file279,37,"# Test Linear Regression
 print(results.summary())",data exploration,,
file279,38,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file279,39,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file279,40,prestige.head(),data exploration,,
file279,41,print(prestige_model.summary()),data exploration,,
file279,42,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,,
file279,43,dta = sm.datasets.statecrime.load_pandas().data,load data,,
file279,44,"crime_model = ols(""murder ~ urban + poverty + hs_grad + single"", data=dta).fit()
 print(crime_model.summary())",modeling,data exploration,
file279,45,"fig = sm.graphics.plot_partregress_grid(crime_model)
 fig.tight_layout(pad=1.0)",result visualization,,
file279,46,"# https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 get_ipython().run_line_magic('matplotlib', 'inline')",comment only,,
file279,47,"import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm",helper functions,,
file279,48,"# STAT Models
 # https://www.statsmodels.org/dev/examples/notebooks/generated/plots_boxplots.html
 plt.rcParams[""figure.subplot.bottom""] = 0.23 # keep labels visible
 plt.rcParams[""figure.figsize""] = (10.0, 8.0) # make plot larger in notebook
 age = [data.exog[""age""][data.endog == id] for id in party_ID]
 fig = plt.figure()
 ax = fig.add_subplot(111)
 plot_opts = {
  ""cutoff_val"": 5,
  ""cutoff_type"": ""abs"",
  ""label_fontsize"": ""small"",
  ""label_rotation"": 30,
 }
 sm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)
 ax.set_xlabel(""Party identification of respondent."")
 ax.set_ylabel(""Age"")
 # plt.show()",comment only,result visualization,
file279,49,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file279,50,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file279,51,get_ipython().system('pip install db-dtypes'),helper functions,,
file279,52,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file279,53,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file279,54,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file279,55,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",helper functions,load data,
file279,56,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file279,57,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file279,58,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file279,59,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file279,60,"def example_function():
  print('Hello World')",comment only,,
file279,61,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file279,62,"model.fit(x,y)
 LinearRegression",modeling,,
file279,63,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file279,64,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,data exploration,
file279,65,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file279,66,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file279,67,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file279,68,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file279,69,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,data exploration,
file279,70,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file279,71,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file279,72,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file279,73,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file279,74,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file279,75,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file279,76,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file279,77,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,data exploration,
file279,78,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",data exploration,,
file279,79,"print(f""slope: {new_model.coef_}"")",data exploration,,
file279,80,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,,
file279,81,"# Test Linear Regression
 x",data exploration,,
file279,82,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file279,83,"# Test Linear Regression
 results = model.fit()",modeling,,
file279,84,"# Test Linear Regression
 print(results.summary())",data exploration,,
file279,85,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file279,86,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",result visualization,,
file279,87,prestige.head(),data exploration,,
file279,88,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima
 import collections.abc
 #hyper needs the four following aliases to be done manually.
 collections.Iterable = collections.abc.Iterable
 collections.Mapping = collections.abc.Mapping
 collections.MutableSet = collections.abc.MutableSet
 collections.MutableMapping = collections.abc.MutableMapping
 import hts
 from hts.hierarchy import HierarchyTree
 from hts.model import AutoArimaModel
 from hts import HTSRegressor",helper functions,,
file279,89,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file279,90,"To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file279,91,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,data exploration,
file279,92,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file279,93,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file279,94,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,,
file279,95,print(prestige_model.summary()),data exploration,,
file280,0,"get_ipython().run_cell_magic('capture', 'cont', '!pip install db-dtypes\n')",helper functions,,
file280,1,cont.show(),data exploration,,
file280,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file280,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file280,4,"import statsmodels.api as sm
 import itertools",helper functions,,
file280,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file280,6,"#obtaining the unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file280,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file280,8,"data.info()  #Feature Matrix
 #X = data.drop(""date"",1)
 #y = data[""wind_speed""] #Target Variable",data exploration,comment only,
file280,9,submit.info(),data exploration,,
file280,10,"data.drop_duplicates(inplace=True, ignore_index=True)
 data.info()",data preprocessing,data exploration,
file280,11,data.uu_id.unique().size,data exploration,,
file280,12,"data.fillna(0, inplace=True)",data exploration,data preprocessing,
file280,13,data.info(),data exploration,,
file280,14,"test=data[data['uu_id']=='bbcb018f0e5e49e13636f6e78ce9f60f']
 len(test)",data exploration,data preprocessing,
file280,15,"data['timeperiod']= pd.to_datetime(data['timeperiod'], format='%Y%m%d')",data preprocessing,,
file280,16,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file280,17,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.2]
 print(relevant_features)",data exploration,data preprocessing,
file280,18,columns_rel = relevant_features.index.to_list(),data preprocessing,,
file280,19,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file280,20,"min(data['timeperiod']),max(data['timeperiod'])",data exploration,,
file280,21,data.dtypes,data exploration,,
file280,22,data.index,data exploration,,
file280,23,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file280,24,data['total_claims'].plot(linewidth=.5);,result visualization,,
file280,25,"cols_plot = ['week_number','countyfips','tract','total_claims','edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs',
  'edu_unknown', 'gender_female', 'gender_male', 'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 
  'race_hawaiiannative', 'race_other', 'race_white']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('features')",data preprocessing,result visualization,
file280,26,"#fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 #for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
 # sns.boxplot(data=data, x='Month', y=name, ax=ax)
 # ax.set_ylabel('precipitation')
 # ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
 # if ax != axes[-1]:
 # ax.set_xlabel('')",comment only,,
file280,27,"columns_rel.append('timeperiod')
 columns_rel.append('uu_id')
 columns_rel",data preprocessing,,
file280,28,data,data exploration,,
file280,29,"data_arima=data[columns_rel]
 data_arima",data exploration,data preprocessing,
file280,30,"get_ipython().run_cell_magic('capture', '', ""unique_id=list(data_arima['uu_id'].unique())\ndata_dict = {}\n\nfor i in unique_id:\n j = data_arima[data_arima['uu_id']==i].groupby('timeperiod')['total_claims'].sum().reset_index()\n j = j.set_index('timeperiod')\n data_dict[i] = j\n"")",helper functions,,
file280,31,"p = d = q = range(0, 2)
 pdq = list(itertools.product(p, d, q))
 seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
 print('Examples of parameter combinations for Seasonal ARIMA...')
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",data exploration,data preprocessing,
file280,32,"import warnings
 warnings.filterwarnings(""ignore"")",helper functions,comment only,
file280,33,with io.capture_output() as captured:,data preprocessing,,
file280,34,import csv,helper functions,,
file280,35,"filename =""param.csv""",load data,,
file280,36,"# opening the file using ""with""
 # statement
 with open(filename, 'r') as data:
  for line in csv.DictReader(data):
  print(line)",load data,,
file280,37,csv.DictReader(data),load data,,
file280,38,"filename =pd.read_csv(""param.csv"")",load data,,
file280,39,filename.head(),data exploration,,
file280,40,filename.to_dict(),data preprocessing,,
file280,41,"param_dict=file_param.to_dict('list')
 seasonal_dict=file_seasonal.to_dict('list')",data preprocessing,,
file280,42,"file_param =pd.read_csv(""param.csv"")
 file_seasonal =pd.read_csv(""seasonal.csv"")
 # opening the file using ""with""
 # statement
 #with open(filename, 'r') as data:
 # for line in csv.DictReader(data):
 # print(line)",load data,comment only,
file280,43,len(data_dict),data exploration,,
file280,44,"hundred=list(data_dict.keys())[:100]
 two_hundred=list(data_dict.keys())[100:200]
 three_hundred=list(data_dict.keys())[200:300]
 four_hundred=list(data_dict.keys())[300:400]
 five_hundred=list(data_dict.keys())[500:573]",data preprocessing,,
file280,45,"#results = {}
 for key in hundred:
  y=data_dict[key]['total_claims']
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=tuple(param_dict[key]),
  seasonal_order=tuple(seasonal_dict[key]),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results[key] = mod.fit(disp=False)
  #print(results.summary().tables[1])",data preprocessing,modeling,
file280,46,len(results),data exploration,,
file280,47,list(results.keys()),data exploration,,
file280,48,"results[x].plot_diagnostics(figsize=(16, 8))
 plt.show()",result visualization,,
file280,49,data_dict[x]['total_claims'],data exploration,,
file280,50,"pred = results[x].get_prediction(start=pd.to_datetime('2022-09-26'), dynamic=False)",prediction,,
file280,51,pred = results[x].get_prediction( dynamic=False),prediction,,
file280,52,"pred_ci = pred.conf_int()
 ax = data_dict[x]['total_claims'].plot(label='observed')
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Furniture Sales')
 plt.legend()
 plt.show()",result visualization,,
file280,53,pred = results[x].forecast(),data preprocessing,,
file280,54,pred,data exploration,,
file280,55,"pred = []
 for key in hundred:
  i=results[key].forecast()
  pred.append(i)
  #print(results.summary().tables[1])",comment only,data preprocessing,
file280,56,pred.values,data exploration,,
file280,57,len(pred),data exploration,,
file280,58,hundred+two_hundred,data exploration,,
file280,59,uu_id=hundred+two_hundred+three_hundred+four_hundred+five_hundred+six_hundred,data preprocessing,,
file280,60,len(uu_id),data exploration,,
file280,61,submit.head(),data exploration,,
file280,62,"first_trial = pd.DataFrame(uu_id, columns=['uu_id'])
 first_trial.head()",data exploration,data preprocessing,
file280,63,first_trial['total_claims']=pred,data preprocessing,,
file280,64,first_trial.head(),data exploration,,
file280,65,"pd.merge(submit, first_trial, on='uu_id', how='outer')",data preprocessing,,
file280,66,"final_file=pd.merge(submit, first_trial, on='uu_id', how='outer')
 final_file.to_csv(./'submission.csv', index=False)",data preprocessing,save results,
file281,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file281,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas db-dtypes\n"")",helper functions,,
file281,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file281,3,"def example_function():
  print('Hello World')",comment only,,
file281,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file281,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file281,6,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data exploration,
file281,7,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 print(prediction_list)",load data,data exploration,
file281,8,"len(pd.unique(unemployment_data[""uu_id""]))",data exploration,,
file281,9,print(unemployment_data.loc[0]),data exploration,,
file281,10,"# remove unnecessary colomns and combine unemployment data together with wages
 unemployment_sample = unemployment_data.copy()
 unemployment_sample = unemployment_sample.drop(['timeperiod', 'tract', 'tract_name', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs',\
  'edu_unknown', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3', 'gender_female', 'gender_male',\
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative',\
  'race_other', 'race_white'], axis=1)
 wage_sample = wage_data.copy()
 wage_sample.drop(['tract', 'tract_name'], axis=1)
 prediction_sample = prediction_list.copy()
 wages_dict = {i:wage_sample[""average_wage""].values[k] for k,i in enumerate(wage_sample[""uu_id""])}
 wage_list = [wages_dict[i] for i in unemployment_sample[""uu_id""]]
 # print(wages_dict)
 unemployment_sample[""average_wage""] = wage_list",data preprocessing,,
file281,11,"tract_dic = {i:k for k,i in enumerate(wage_sample[""uu_id""])}",data preprocessing,,
file281,12,"unemployment_sample[""uu_id""] = [tract_dic[i] for i in unemployment_sample[""uu_id""]]",data preprocessing,,
file281,13,"X = unemployment_sample.drop([""total_claims""], axis=1).to_numpy() #to_numpy() values
 y = unemployment_sample['total_claims'].to_numpy()  #to_numpy() values
 # print(X.shape,y.shape)",data preprocessing,,
file281,14,"from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file281,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file281,16,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train) #training the algorithm",modeling,,
file281,17,print(X),data exploration,,
file281,18,"for k,i in enumerate(X):
  if np.nan in i:
  print(k,i)",data exploration,data preprocessing,
file281,19,"for k,i in enumerate(X):
  for j in i:
  if pd.isna(j)
  print(k,i)",data exploration,data preprocessing,
file281,20,"for k,i in enumerate(wage_list):
  if pd.isna(i):
  print(k,i)
  # for j in i:
  #  if pd.isna(j):
  #  print(k,i)",data exploration,data preprocessing,
file281,21,unemployment_sample = unemployment_sample.dropna(),data preprocessing,,
file281,22,"#To retrieve the intercept:
 print(regressor.intercept_)",data exploration,,
file281,23,"#For retrieving the slope:
 print(regressor.coef_)",data exploration,,
file281,24,"y_pred = regressor.predict(X_test)
 df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df",prediction,data preprocessing,
file281,25,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file281,26,"plt.scatter(X_test, y_test, color='gray')
 plt.plot(X_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file281,27,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",data exploration,,
file282,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file282,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file282,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file282,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file282,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file282,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",data exploration,load data,
file282,6,data.describe(),data exploration,,
file282,7,data.shape,data exploration,,
file282,8,data.info(),data exploration,,
file282,9,y=data['total_claims'],data preprocessing,,
file282,10,"l=[]
 for i in data.columns:
  if sum(data[i].isnull())>0:
  l.append(i)
  print('The null values in',i,'are',sum(data[i].isnull()))",data exploration,data preprocessing,
file282,11,data.isnull().sum(axis=0),data exploration,,
file282,12,"for i in data.columns:
  if data[i].isnull().sum()>= 0.4*len(data):
  data=data.drop(i,axis=1)",data preprocessing,,
file282,13,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print('The value counts of feature',i)
  print(data[i].value_counts(),'\n')",data exploration,data preprocessing,
file282,14,"for i in data.columns:
  print('The unique values in',i,'are',len(data[i].value_counts()))",data exploration,,
file282,15,data.corr(),data exploration,,
file282,16,"plt.figure(figsize=(20,15))
 cor = data.corr()
 sns.heatmap(cor,annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file283,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file283,1,"#import cell
 import pandas as pd
 import numpy as np
 import statistics
 import csv
 import matplotlib.pyplot as plt
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file283,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file283,3,"#Gets the master unemployed table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file283,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemploymentData = query_job.to_dataframe()
 print(unemploymentData.shape)
 pd.set_option('display.max_columns', None)
 unemploymentData.head(3)",data exploration,load data,
file283,5,"#Gets each tracks mean and std dev
 #unlist has the master unemployment list
 #b becomes the filtered list
 unList = unemploymentData.values.tolist()
 b_set = set(tuple(x) for x in unList)
 b = [ list(x) for x in b_set ]",data preprocessing,,
file283,6,"uuid = []
 #makes a list of the unique uuid
 for x in b:
  if(uuid.count(x[0]) == 0):
  uuid.append(x[0])",data preprocessing,,
file283,7,"#setup for extract  
 values = []
 export = []",data preprocessing,,
file283,8,"#for each value make a list of each weeks claims
 for y in uuid:
  temp = [y]
  for x in b:
  if (x[0] == y):
  temp.append(x[6])
  values.append(temp)",data preprocessing,,
file283,9,"for x in values:
  name = x[0]
  mean = statistics.mean(x[1:])
  if (len(x) > 2):
  stdev = statistics.stdev(x[1:])
  else:
  print(""short"")
  export.append([name, mean, stdev])",data exploration,data preprocessing,
file283,10,"#Everything above this is testing
 #Make the answer csv
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file283,11,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 predictionData = query_job.to_dataframe()",load data,,
file283,12,"answers = predictionData.values.tolist()
 #Answers[names, week]
 #export[names, mean, stddev]
 i = 0
 for x in answers:
  for y in export:
  if(y.count(x[0]) == 1):
  answers[i].insert(1, y[1])
  i = i + 1",data preprocessing,,
file283,13,"fields = ['uu_id', 'total_claims', 'week_number']
 with open('submission_prediction_output.csv', 'w') as f:
  write = csv.writer(f)
  write.writerow(fields)
  write.writerows(answers)",save results,,
file284,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file284,1,"# Run these terminal commands when your Notebook Session begins
 get_ipython().system('gcloud auth login')
 get_ipython().system('gcloud auth application-default set-quota-project ironhacks-data')",helper functions,,
file284,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file284,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file284,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file284,5,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file284,6,import db-dtypes,helper functions,,
file284,7,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file285,0,"import torch
 x = torch.rand(5, 3)
 print(x)",data exploration,comment only,
file285,1,get_ipython().system('pip install torch'),helper functions,,
file286,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file286,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file286,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file286,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file286,4,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file286,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file286,6,print(covid19_cases_data),data exploration,,
file286,7,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file286,8,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file287,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file287,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file287,2,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data exploration,
file287,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",load data,data exploration,
file288,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('python3 -m pip install google.cloud')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('python3 -m pip install numpy')
 get_ipython().system('python3 -m pip install scikit-learn')
 get_ipython().system('python3 -m pip install plotly')",helper functions,,
file288,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import os
 import seaborn as sns
 from pandas import Series, DataFrame",helper functions,,
file288,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file288,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file288,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file288,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data exploration,
file288,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file288,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data exploration,data preprocessing,
file288,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file288,9,"#Merge the data
 unemployment_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemployment_wage_data = unemployment_wage_data.drop(['countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemployment_wage_data = unemployment_wage_data.fillna(0)
 unemployment_wage_data.describe()",data exploration,data preprocessing,
file288,10,unemployment_wage_data.head(),data exploration,,
file288,11,"#Check for duplicated rows
 duplicated_rows = sum(unemployment_wage_data.duplicated()) 
 unemployment_wage_data = unemployment_wage_data.drop_duplicates()",data preprocessing,,
file288,12,unemployment_wage_data[unemployment_wage_data.isnull().any(axis=1)],data exploration,,
file288,13,"#heat map for correlations
 plt.figure(figsize=(25,10))
 cor = unemployment_wage_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds) 
 plt.show()",result visualization,,
file288,14,unemployment_wage_data.columns,data exploration,,
file288,15,"X = unemployment_wage_data.drop(['total_claims', 'week_number'], axis = 1)
 y = unemployment_wage_data.total_claims",data preprocessing,,
file288,16,"uuid, label = unemployment_wage_data['uu_id'].factorize(sort=True)",data preprocessing,,
file288,17,X['uu_id'] = uuid,data preprocessing,,
file288,18,"X['tract_name_x'] = X['tract_name_x'].factorize()[0]
 X['top_category_employer1'] = X['top_category_employer1'].factorize()[0]
 X['top_category_employer2'] = X['top_category_employer2'].factorize()[0]
 X['top_category_employer3'] = X['top_category_employer3'].factorize()[0]",data preprocessing,,
file288,19,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",data preprocessing,,
file288,20,"from sklearn.ensemble import RandomForestRegressor
 rf = RandomForestRegressor(n_estimators=1000, random_state=42)
 rf.fit(X_train, y_train)",helper functions,modeling,
file288,21,"rf.score(X_test, y_test)",evaluation,,
file288,22,prediction_list,data exploration,,
file288,23,"X['uu_id'] = label[X[""uu_id""]]
 X",data exploration,data preprocessing,
file288,24,"for col in X.columns[2:]:
  li = []
  for i in prediction_list['uu_id']:
  li.append(X.loc[X['uu_id'] == i, col].mean())
  prediction_list[col] = li
 prediction_list",data exploration,data preprocessing,
file288,25,prediction_list['uu_id'] = prediction_list['uu_id'].factorize(sort=True)[0],data preprocessing,,
file288,26,"claims_predict = rf.predict(prediction_list)
 claims_predict",data exploration,,
file288,27,submission_df = pd.DataFrame(),data preprocessing,,
file288,28,"submission_df[""uu_id""] = prediction_list[""uu_id""]
 submission_df[""week_number""] = prediction_list[""week_number""]
 submission_df[""total_claims""] = claims_predict",data preprocessing,,
file288,29,submission_df,data exploration,,
file288,30,"submission_df[""uu_id""] = label[submission_df[""uu_id""]]",data preprocessing,,
file288,31,"submission_df.to_csv('submission2_prediction_output.csv', index=False)",save results,,
file289,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file289,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC
 """"""",load data,,
file289,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,,
file289,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT week_number, COUNT(total_claims) AS total_claims, COUNT(edu_8th_or_less) AS 8th, COUNT(edu_grades_9_11) AS 9_11, COUNT(edu_hs_grad_equiv) AS hs_diploma, COUNT(edu_post_hs) AS post_hs, COUNT(edu_unknown) AS edu_unknown, COUNT(top_category_employer1) AS top_category_employer1, COUNT(top_category_employer2) AS top_category_employer2, COUNT(top_category_employer3) AS top_category_employer3, COUNT(gender_female) AS female, COUNT(gender_male) AS male, COUNT(gender_na) AS gender_na, COUNT(race_amerindian) AS american, COUNT(race_asian) AS asian, COUNT(race_black) AS black, COUNT(race_noanswer) AS r_noAns, COUNT(race_hawaiiannative) AS hawaii, COUNT(race_other) AS other, COUNT(race_white) AS white
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY week_number
 ORDER BY week_number ASC
 """"""",load data,,
file289,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT week_number, COUNT(total_claims) AS total_claims, 
 

 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY week_number
 ORDER BY week_number ASC
 """"""
 #COUNT(edu_8th_or_less) AS 8th, COUNT(edu_grades_9_11) AS 9_11, COUNT(edu_hs_grad_equiv) AS hs_diploma, COUNT(edu_post_hs) AS post_hs, COUNT(edu_unknown) AS edu_unknown, COUNT(top_category_employer1) AS top_category_employer1, COUNT(top_category_employer2) AS top_category_employer2, COUNT(top_category_employer3) AS top_category_employer3, COUNT(gender_female) AS female, COUNT(gender_male) AS male, COUNT(gender_na) AS gender_na, COUNT(race_amerindian) AS american, COUNT(race_asian) AS asian, COUNT(race_black) AS black, COUNT(race_noanswer) AS r_noAns, COUNT(race_hawaiiannative) AS hawaii, COUNT(race_other) AS other, COUNT(race_white) AS white
 #SELECT week_number, COUNT(total_claims) AS total_claims, COUNT(edu_8th_or_less) AS 8th, COUNT(edu_grades_9_11) AS 9_11, COUNT(edu_hs_grad_equiv) AS hs_diploma, COUNT(edu_post_hs) AS post_hs, COUNT(edu_unknown) AS edu_unknown, COUNT(top_category_employer1) AS top_category_employer1, COUNT(top_category_employer2) AS top_category_employer2, COUNT(top_category_employer3) AS top_category_employer3, COUNT(gender_female) AS female, COUNT(gender_male) AS male, COUNT(gender_na) AS gender_na, COUNT(race_amerindian) AS american, COUNT(race_asian) AS asian, COUNT(race_black) AS black, COUNT(race_noanswer) AS r_noAns, COUNT(race_hawaiiannative) AS hawaii, COUNT(race_other) AS other, COUNT(race_white) AS white
 #GROUP BY week_number",load data,,
file289,5,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file289,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 cor = unemployment_data.corr()
 cor_target = abs(cor[""total_claims""])
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,load data,
file289,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 GROUP BY week_number
 ORDER BY week_number ASC
 """"""",load data,,
file289,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 

 """"""",load data,,
file289,9,"query = """"""
 SELECT column_name
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.COLUMNS`
 WHERE table_name = 'wage_data'
 """"""",load data,,
file289,10,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.drop_duplicates(inplace = True)
 unemployment_data.plot(subplots=True, figsize=(20,24))",load data,result visualization,
file289,11,"query = """"""
 SELECT tract_name
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number 
 """"""",load data,,
file289,12,"query = """"""
 SELECT week_number, tract_name, COUNT(total_claims) AS total_claims
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY tract_name
 ORDER BY week_number 
 """"""",load data,,
file290,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file290,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file290,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.preprocessing import OneHotEncoder
 from sklearn.model_selection import train_test_split
 from sklearn.ensemble import RandomForestRegressor
 import matplotlib.pyplot as plt
 import datetime,itertools
 from statsmodels.tsa.statespace.sarimax import SARIMAX
 from statsmodels.tsa.arima.model import ARIMA
 import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
 from statsmodels.tsa.stattools import adfuller",helper functions,,
file290,3,"get_ipython().system('pip install pmdarima --quiet')
 import pmdarima as pm",helper functions,,
file290,4,"def example_function():
  print('Hello World')",comment only,,
file290,5,"def print_column_info(df):
  print(f'No. of columns: {len(df.columns)}')
  for col in df.columns:
  print(len(df[col].unique()),col,df[col].dtypes)
  print()",data exploration,data preprocessing,
file290,6,"def print_na_info(df):
  for col in df.columns:
  print(df[col].isnull().sum(),col,df[col].dtypes)",data exploration,data preprocessing,
file290,7,"def get_datetime(week_no):
  date = datetime.datetime.strptime(""2022-""+str(week_no)+""-1"",""%Y-%W-%w"")
  #print(date)
  return pd.to_datetime(date,format=""%Y-%m-%d"")",data preprocessing,,
file290,8,"def sarimax_gridsearch(ts, pdq, pdqs, maxiter=100, freq='D',disp=False):
  # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value
  ans = []
  for comb in pdq:
  for combs in pdqs:
  #try:
  mod = SARIMAX(ts,order=comb,
  seasonal_order=combs,
  enforce_stationarity=False,
  enforce_invertibility=False)
 

  output = mod.fit(maxiter=maxiter,disp=False) 
  ans.append([comb, combs, output.bic])
  #print('SARIMAX {} x {}12 : BIC Calculated ={}'.format(comb, combs, output.bic))
  #except:
  # continue
  ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic'])
  ans_df = ans_df.sort_values(by=['bic'],ascending=True)[0:5]
  
  return ans_df",modeling,data preprocessing,
file290,9,"def check_stationary(data,Print=0):
  adft = adfuller(data,autolag=""AIC"")
  output_df = pd.DataFrame({""Values"":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']] , ""Metric"":[""Test Statistics"",""p-value"",""No. of lags used"",""Number of observations used"", 
  ""critical value (1%)"", ""critical value (5%)"", ""critical value (10%)""]})
 

  critical_value = adft[4]['5%']
  if Print==1:
  print(output_df)
  if adft[1] < 0.05 and adft[0] < critical_value:
  return 1
  else:
  return 0",,,
file290,10,"def diff_inv(series, last_observation):
 

  series_undifferenced = series.copy()
 

  series_undifferenced.iat[0] = series_undifferenced.iat[0] + last_observation
 

  series_undifferenced = series_undifferenced.cumsum()
 

  return series_undifferenced",,,
file290,11,"def loss(pred,actual):
  pred = np.round(pred)
  errors = abs(actual-pred)
  print(f'Mean Absolute Error: {round(np.mean(errors), 2)}')
  print(f'Mean squared Error: {round(np.mean(errors**2), 2)}')
  mape = 100 * (errors/actual)
  # Calcualte and display accuracy
  accuracy = 100 - np.mean(mape)
  print(f'Accuracy: {round(accuracy, 2)}%.')",data exploration,evaluation,
file290,12,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file290,13,"#query = """"""
 #SELECT 
 #week_number,
 #cases 
 #FROM `ironhacks-data.ironhacks_training.covid19_cases`
 #Where week_number between 1 and 3
 #order by week_number
 #""""""
 print(""Datasets available:"")
 for dataset in list(bigquery_client.list_datasets()):
  print(dataset.dataset_id)
  if dataset.dataset_id == ""ironhacks_competition"":
  mydataset = dataset",load data,data exploration,
file290,14,"print(""\nTables available:"")
 for table in bigquery_client.list_tables(""ironhacks_competition""):
  print(table.table_id)",data exploration,data preprocessing,
file290,15,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file290,16,"query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file290,17,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",load data,,
file290,18,"prediction_data = query_job1.to_dataframe()
 unemployment_data = query_job2.to_dataframe()
 wage_data = query_job3.to_dataframe()",data preprocessing,,
file290,19,"##Dumping the df to csv
 week = ""week2""
 prediction_data.to_csv(""prediction_data_""+week+"".csv"",index=False)
 unemployment_data.to_csv(""unemployment_data_""+week+"".csv"",index=False)
 wage_data.to_csv(""wage_data_""+week+"".csv"",index=False)",save results,,
file290,20,"merged_data = pd.merge(unemployment_data,wage_data[[""uu_id"",""average_wage""]],on=""uu_id"",how=""left"")
 merged_data = merged_data.drop_duplicates()",data preprocessing,,
file290,21,"query4 = """"""
 SELECT table_id,
 DATE(TIMESTAMP_MILLIS(creation_time)) AS creation_date,
 DATE(TIMESTAMP_MILLIS(last_modified_time)) AS last_modified_date,
 row_count,
 size_bytes,
 CASE
  WHEN type = 1 THEN 'table'
  WHEN type = 2 THEN 'view'
  WHEN type = 3 THEN 'external'
  ELSE '?'
 END AS type,
 TIMESTAMP_MILLIS(creation_time) AS creation_time,
 TIMESTAMP_MILLIS(last_modified_time) AS last_modified_time,
 dataset_id,
 project_id
 FROM `ironhacks-data.ironhacks_competition.__TABLES__`""""""
 query_job4 = bigquery_client.query(query4)
 timestamp_data = query_job4.to_dataframe()
 for cnt,row in timestamp_data.iterrows():
  print(""\n"")
  print(row[""table_id""])
  print(row[""creation_time""])
  print(row[""last_modified_time""])",data exploration,load data,
file290,22,"##Printing (unique_vals, col name, datatype)
 print(f'\nunemployment_data : No. of columns = {len(unemployment_data.columns)},Max week number: {max(unemployment_data[""week_number""])},Min week number: {min(unemployment_data[""week_number""])}')
 print_column_info(unemployment_data)
 print(f'\nwage_data : No. of columns = {len(wage_data.columns)}')
 print_column_info(wage_data)
 print(f'\nprediction_data : No. of columns = {len(prediction_data.columns)},Max week number: {max(prediction_data[""week_number""])},Min week number: {min(prediction_data[""week_number""])}')
 print_column_info(prediction_data)",data exploration,,
file290,23,"count = 0
 for uuid in unemployment_data[""uu_id""].unique():
  if len(wage_data.loc[wage_data[""uu_id""]==uuid])==0:
  count+=1
  #print(f'UUID {uuid} found in unemployment_data but not in wage_data')
 print(f'{count} UUIDs found in unemployment_data but not in wage_data')
 count = 0
 for uuid in wage_data[""uu_id""].unique():
  if len(unemployment_data.loc[unemployment_data[""uu_id""]==uuid])==0:
  count+=1
  #print(f'UUID {uuid} found in unemployment_data but not in wage_data')
 print(f'{count} UUIDs found in wage_data but not in unemployment_data')",data exploration,data preprocessing,
file290,24,"count = 0
 for uuid in unemployment_data[""uu_id""].unique():
  if len(merged_data.loc[merged_data[""uu_id""]==uuid])==0:
  count+=1
  #print(f'UUID {uuid} found in unemployment_data but not in wage_data')
 print(f'{count} UUIDs found in unemployment_data but not in merged_data')",data exploration,data preprocessing,
file290,25,"##Merging average wage with unemployment data
 print(f'unemployment_data: Unique uu_id = {len(unemployment_data[""uu_id""].unique())}, Row count = {len(unemployment_data.index)}, Col count = {len(unemployment_data.columns)}')
 print(f'wage_data: Unique uu_id = {len(wage_data[""uu_id""].unique())}, Row count = {len(wage_data.index)}, Col count = {len(wage_data.columns)}')
 #print(f'Unique uu_id in wage_data = {len(wage_data[""uu_id""].unique())}')
 print(f'\nunemployment_data: NaN info, Total rows={len(unemployment_data.index)}')
 print_na_info(unemployment_data)
 print(f'merged_data: Unique uu_id = {len(merged_data[""uu_id""].unique())}, Row count = {len(merged_data.index)}, Col count = {len(merged_data.columns)}')",data exploration,,
file290,26,"print(f'\nmerged_data : No. of columns = {len(merged_data.columns)},Max week number: {max(merged_data[""week_number""])},Min week number: {min(merged_data[""week_number""])}')
 print_column_info(merged_data)
 print(f'\nmerged_data: NaN info, Total rows={len(merged_data.index)}')
 print_na_info(merged_data)",data exploration,,
file290,27,"#print(merged_data[""week_number""])
 merged_data[""datetime""] = [get_datetime(val) for val in merged_data[""week_number""]]
 merged_data = merged_data.set_index(['datetime'])
 merged_data = merged_data.sort_index()
 merged_data.head(5)",data exploration,data preprocessing,
file290,28,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.preprocessing import OneHotEncoder
 from sklearn.model_selection import train_test_split
 from sklearn.ensemble import RandomForestRegressor
 import matplotlib.pyplot as plt
 import datetime,itertools
 from statsmodels.tsa.statespace.sarimax import SARIMAX
 from statsmodels.tsa.arima.model import ARIMA
 import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
 from statsmodels.tsa.stattools import adfuller",helper functions,,
file290,29,"def example_function():
  print('Hello World')",comment only,,
file290,30,"def print_column_info(df):
  print(f'No. of columns: {len(df.columns)}')
  for col in df.columns:
  print(len(df[col].unique()),col,df[col].dtypes)
  print()",data exploration,data preprocessing,
file290,31,"def print_na_info(df):
  for col in df.columns:
  print(df[col].isnull().sum(),col,df[col].dtypes)",data exploration,data preprocessing,
file290,32,"def get_datetime(week_no):
  date = datetime.datetime.strptime(""2022-""+str(week_no)+""-1"",""%Y-%W-%w"")
  #print(date)
  return pd.to_datetime(date,format=""%Y-%m-%d"")",data preprocessing,,
file290,33,"def sarimax_gridsearch(ts, pdq, pdqs, maxiter=100, freq='D',disp=False):
  # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value
  ans = []
  for comb in pdq:
  for combs in pdqs:
  #try:
  mod = SARIMAX(ts,order=comb,
  seasonal_order=combs,
  enforce_stationarity=False,
  enforce_invertibility=False)
 

  output = mod.fit(maxiter=maxiter,disp=False) 
  ans.append([comb, combs, output.bic])
  #print('SARIMAX {} x {}12 : BIC Calculated ={}'.format(comb, combs, output.bic))
  #except:
  # continue
  ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic'])
  ans_df = ans_df.sort_values(by=['bic'],ascending=True)[0:5]
  
  return ans_df",modeling,data preprocessing,
file290,34,"def check_stationary(data,Print=0):
  adft = adfuller(data,autolag=""AIC"")
  output_df = pd.DataFrame({""Values"":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']] , ""Metric"":[""Test Statistics"",""p-value"",""No. of lags used"",""Number of observations used"", 
  ""critical value (1%)"", ""critical value (5%)"", ""critical value (10%)""]})
 

  critical_value = adft[4]['5%']
  if Print==1:
  print(output_df)
  if adft[1] < 0.05 and adft[0] < critical_value:
  return 1
  else:
  return 0",,,
file290,35,"def diff_inv(series, last_observation):
 

  series_undifferenced = series.copy()
 

  series_undifferenced.iat[0] = series_undifferenced.iat[0] + last_observation
 

  series_undifferenced = series_undifferenced.cumsum()
 

  return series_undifferenced",data preprocessing,,
file290,36,"def loss(pred,actual):
  pred = np.round(pred)
  errors = abs(actual-pred)
  print(f'Mean Absolute Error: {round(np.mean(errors), 2)}')
  print(f'Mean squared Error: {round(np.mean(errors**2), 2)}')
  mape = 100 * (errors/actual)
  # Calcualte and display accuracy
  accuracy = 100 - np.mean(mape)
  print(f'Accuracy: {round(accuracy, 2)}%.')",data exploration,evaluation,
file290,37,"print(f'merged_data: Unique uu_id = {len(merged_data[""uu_id""].unique())}, Row count = {len(merged_data.index)}, Col count = {len(merged_data.columns)}')",data exploration,,
file290,38,"entry_count = []
 for uuid in merged_data[""uu_id""].unique():
  entry_count.append([uuid,len(merged_data.loc[merged_data[""uu_id""]==uuid])])
 entry_count.sort(key=lambda x: x[1])
 for entry in entry_count:
  print(f'uuid: {entry[0]}, weeks of data available: {entry[1]}')",data exploration,data preprocessing,
file290,39,"exog = ['edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']
 exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']
 ids = merged_data[""uu_id""].unique()[:]
 output = []
 for cnt,idd in enumerate(ids):
  if cnt > 100: break
  data = merged_data.loc[merged_data[""uu_id""]==idd].copy()
  data = data.asfreq('W-MON')
  is_na = []
  for idx,row in data.iterrows():
  if pd.isna(row[""total_claims""]):
  #print(f'NA found in week {idx}')
  is_na.append(1)
  else:
  is_na.append(0)
  data[""is_na""] = is_na
  #data[""total_claims""] = data[""total_claims""].fillna(0)
  start_week = 0
  #print(data[""week_number""].max())
  boundary_week = data[""week_number""].max()-2
  data_train = data.loc[(data[""week_number""]<boundary_week) & (data[""week_number""]>start_week)].drop(""is_na"",axis=1)
  is_na = []
  for idx,row in data_train.iterrows():
  if pd.isna(row[""week_number""]):
  #print(f'NA found in week {idx}')
  is_na.append(1)
  else:
  is_na.append(0)
  data_train[""is_na""] = is_na
  data_train = data_train.asfreq('W-MON')
  #print(data_train)
  #SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(method=""ffill""),
  #  start_p=0, start_q=0,test=""adf"",
  #  max_p=5, max_q=5,m=4,d=None,
  #  trace=False,
  #  suppress_warnings=True, 
  #  maxiter=200,
  #  stepwise=True,seasonal=True,D=None)
  SARIMAX_model = pm.auto_arima(data_train[""total_claims""].fillna(0),alpha=0.05,d=None,max_order=0)
  max_week_no = data[""week_number""].max()
  pred_periods = max_week_no - data_train[""week_number""].max()
  #SARIMAX_model_fit = SARIMAX_model.fit(data_train[""total_claims""].fillna(0))
  #check_st = check_stationary(data_train[""total_claims""].fillna(0),1)
  prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)
  out = prediction.loc[prediction.index[-1]]
  #print(prediction)
  #print(prediction.loc[prediction.index[-1]])
  output.append([idd,max_week_no,out])
  #print(check_stationary(data_train[""total_claims""].fillna(method=""bfill""),1))
  if len(data.loc[data[""week_number""]==max_week_no,""total_claims""])>0:
  print(idd,max_week_no,out,data.loc[data[""week_number""]==max_week_no,""total_claims""].item(),""--"",len(data_train.index))
  else:
  print(idd,max_week_no,out,""--"",len(data_train.index))
  #break",,,
file290,40,"def sarimax_forecast(SARIMAX_model, periods=10,plot=0):
  # Forecast
  n_periods = periods
 

  forecast_df = data[data.index>=data_train.index[-1]]
  fitted, confint = SARIMAX_model.predict(n_periods=n_periods, 
  return_conf_int=True)
  #print(confint)
  index_of_fc = pd.date_range(data_train.index[-1], periods = n_periods, freq='W-MON')
 

  # make series for plotting purpose
  fitted_series = pd.Series(fitted, index=index_of_fc)
  lower_series = pd.Series(confint[:, 0], index=index_of_fc)
  upper_series = pd.Series(confint[:, 1], index=index_of_fc)
  
  # Plot
  if plot:
  plt.figure(figsize=(15,7))
  plt.plot(data[""total_claims""], color='#1f76b4')
  plt.plot(fitted_series, color='darkgreen')
  plt.fill_between(lower_series.index, 
  lower_series, 
  upper_series, 
  color='k', alpha=.15)
 

  plt.title(""SARIMAX"")
  plt.show()
  return fitted_series",result visualization,data preprocessing,
file290,41,"pred_df = pd.DataFrame()
 uuid_list = []
 week_number_list = []
 total_claims_list = []
 #stat=[]
 for l in output:
  uuid_list.append(l[0])
  week_number_list.append(l[1])
  if l[2]<0:
  l[2] = 0
  total_claims_list.append(int(np.floor(l[2])))
  #stat.append(l[1])
 pred_df[""uu_id""] = uuid_list
 pred_df[""week_number""] = week_number_list
 pred_df[""total_claims""] = total_claims_list
 #pred_df[""stationarity""] = stat",data preprocessing,,
file290,42,"pred = []
 actual = []
 pred_stat = []
 actual_stat = []
 pred_nonstat = []
 actual_nonstat = []
 ts_data = merged_data
 for id,row in pred_df.iterrows():
  uuid = row[""uu_id""]
  week_number = row[""week_number""]
  if len(ts_data.loc[(ts_data[""uu_id""]==uuid) & (ts_data[""week_number""] == week_number),""total_claims""])==0:
  continue
  actual_val = ts_data.loc[(ts_data[""uu_id""]==uuid) & (ts_data[""week_number""] == week_number),""total_claims""].item()
  pred.append(row[""total_claims""])
  actual.append(actual_val)",data preprocessing,,
file290,43,"loss(pred=pred,actual=actual)",data exploration,,
file290,44,"ids = merged_data[""uu_id""].unique()[:]
 data = merged_data.loc[merged_data[""uu_id""]==ids[500]].copy()
 print(data[""week_number""].max())
 data_train = data[data[""week_number""]<32]
 data_train = data_train.asfreq('W-MON')",data preprocessing,,
file290,45,"from statsmodels.tsa.seasonal import seasonal_decompose
 # Taking the decomposition
 decomposition = seasonal_decompose(data_train[""total_claims""])
 # Gathering and plotting the trend, seasonality, and residuals 
 trend = decomposition.trend
 seasonal = decomposition.seasonal
 residual = decomposition.resid",helper functions,data preprocessing,
file290,46,"ax = seasonal.plot(label='Seasonality', color='blue')
 min_ = seasonal.idxmin()
 max_ = seasonal.idxmax()
 min_2 = seasonal[max_:].idxmin()
 max_2 = seasonal[min_2:].idxmax()
 ax.axvline(min_,label='min 1',c='red')
 ax.axvline(min_2,label='min 2',c='red', ls=':')
 ax.axvline(max_,label='max 1',c='green')
 ax.axvline(max_2,label='max 2',c='green', ls=':')
 plt.legend(loc='upper right', fontsize='x-small')
 print(f'The time difference between the two minimums is {min_2-min_}')",result visualization,data exploration,
file290,47,from sklearn.linear_model import LinearRegression,helper functions,,
file290,48,"model = LinearRegression()
 X = [i for i in range(len(data_train[""total_claims""].fillna(0)))]
 y = data_train[""total_claims""].fillna(0)
 model.fit(X,y)",modeling,,
file290,49,"trend = model.predict(X)
 plt.plot(y)
 plt.plot(trend)
 plt.plot(show)",result visualization,,
file291,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file291,1,from google.cloud import bigquery,helper functions,,
file291,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file291,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file291,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file291,5,unemployment_data,data exploration,,
file291,6,unemployment_data['total_claims'].head,data exploration,,
file291,7,unemployment_data['week_number'].head,data exploration,,
file291,8,unemployment_data.describe(),data exploration,,
file291,9,unmeployment_data[['total_claims']].groupby('week_number').mean(),data exploration,,
file291,10,unemployment_data.groupby('week_number')['total_claims'].mean(),data exploration,,
file291,11,wage_data.head,data exploration,,
file291,12,prediction_list.head(,data exploration,,
file291,13,"merged_data = prediction_list.merge(unemployment_data, on='uu_id',how='left')",data preprocessing,,
file291,14,merged_data,data exploration,,
file291,15,"merged_data.shape,prediction_list.shape",data exploration,,
file291,16,"merged_data.shape,prediction_list.shape,unemployment_data.shape",data exploration,,
file291,17,import pandas as pd,helper functions,,
file291,18,merged_data.shape,data exploration,,
file291,19,merged_data.head(),data exploration,,
file291,20,unemplyment_data.value_counts(),data exploration,,
file292,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file292,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file292,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file292,3,"query = """"""
 SELECT *
 FROM `ironhacks-data:ironhacks_competition.unemployment_data`
 """"""",load data,,
file292,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment = query_job.to_dataframe()
 unemployment.head()",load data,,
file292,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file292,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file292,7,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file292,8,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",load data,,
file292,9,"unemployment = query_job1.to_dataframe()
 wage = query_job2.to_dataframe()
 pred = query_job3.to_dataframe()",,,
file292,10,wage.head(),data exploration,,
file292,11,wage.columns,data exploration,,
file292,12,pred.head(),data exploration,,
file292,13,import xgboost as xgb,helper functions,,
file292,14,get_ipython().system('python3 -m pip install xgboost'),helper functions,,
file292,15,"get_ipython().system('python3 -m pip install xgboost')
 get_ipython().system('python3 -m pip install sklearn')",helper functions,,
file292,16,unemployment.columns,data exploration,,
file292,17,"query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.wage.uu_id
 """"""",load data,,
file292,18,data,data exploration,,
file292,19,"query4 = """"""
 SELECT *, wage.average_wage
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.uu_id
 """"""
 query_job4 = bigquery_client.query(query4)
 data = query_job4.to_dataframe()",load data,,
file292,20,data = data.drop(data['uu_id_1']),data preprocessing,,
file292,21,"data = data.drop('uu_id_1', axis=1)",data preprocessing,,
file292,22,data.columns,data exploration,,
file292,23,"data = data.drop(['uu_id_1','countyfips_1','tract_1','tract_name_1'], axis=1)",data preprocessing,,
file292,24,"dropcol = ['uu_id_1','countyfips_1','tract_1','tract_name_1']
 for col in dropcol:
  data = data.drop(col, axis=1)",data preprocessing,,
file293,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file293,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file293,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file293,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file293,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file293,5,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file293,6,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file293,7,"#Correlation with output variable
 cor_target = abs(cor[""potential_water_deficit""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file293,8,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file293,9,"min(data['date']),max(data['date'])",data exploration,,
file293,10,data.dtypes,data exploration,,
file293,11,data = data.set_index('date'),data preprocessing,,
file293,12,data.index,data exploration,,
file293,13,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,,
file293,14,data.loc['2019-08'],data exploration,,
file293,15,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file293,16,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file293,17,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file293,18,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",result visualization,,
file293,19,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file293,20,"sns.boxplot(data=data, x='Month', y='wind_speed');",result visualization,,
file293,21,"from statsmodels.graphics.tsaplots import plot_acf
 plot_acf(x=data['max_temperature'], lags=50)",helper functions,result visualization,
file293,22,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",helper functions,modeling,
file293,23,"y_pred = pd.Series(model.predict(X), index=X.index)",data preprocessing,,
file293,24,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file293,25,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file293,26,data['lag_1']=lag_1,data preprocessing,,
file294,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file294,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file294,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file294,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file294,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 # data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file294,5,get_ipython().system('pip install db-dtypes'),helper functions,,
file295,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file295,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file295,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file295,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file295,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,,
file295,5,unemployment_data.total_claims.describe(),data exploration,,
file295,6,"avg = 22.64
 prediction_list[""total_claims""] = avg",data preprocessing,,
file295,7,prediction_list.head(),data exploration,,
file295,8,"prediction_list = prediction_list[['uu_id', 'total_claims', 'week_number']]",data preprocessing,,
file295,9,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file296,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file296,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file296,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file296,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file297,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file297,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file297,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file297,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file297,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file297,5,print(covid19_cases_data),data exploration,,
file297,6,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file297,7,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",,data exploration,
file298,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file298,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file298,2,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file298,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file298,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file298,5,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file298,6,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file299,0,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install plotly')",helper functions,,
file299,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file299,2,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from scipy import stats
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file299,3,"import numpy as np
 from numpy import isnan
 from matplotlib import pyplot",helper functions,,
file299,4,"import seaborn as sns
 import matplotlib 
 import matplotlib.pyplot as plt
 import plotly 
 import plotly.express as px",helper functions,,
file299,5,import pandas as pd,helper functions,,
file299,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file299,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file299,8,"query_job = bigquery_client.query(query)
 unemploy = query_job.to_dataframe()
 unemploy.head()",load data,,
file299,9,"query_wage = bigquery_client.query(wage)
 wages = query_wage.to_dataframe()
 wages.head()",load data,data exploration,
file299,10,wages.info(),data exploration,,
file299,11,unemploy.info(),data exploration,,
file299,12,unemploy.isnull().sum(),data exploration,,
file299,13,"unemploy=unemploy.sort_values('week_number', ascending=True)
 unemploy.reset_index(inplace=True)
 unemploy",data preprocessing,data exploration,
file299,14,"sns.heatmap(unemploy.isnull(),cbar=False)",result visualization,,
file299,15,"miss=unemploy.isnull()
 total=unemploy.count()
 total",data exploration,data preprocessing,
file299,16,miss.sum(),data exploration,,
file299,17,miss.sum()/len(unemploy),data exploration,,
file299,18,"map_1 = unemploy.corr(method ='spearman')
 sns.heatmap(map_1)",result visualization,data preprocessing,
file299,19,"unemploy[""race_hawaiiannative""].unique()",data exploration,,
file299,20,"df1=unemploy.copy()
 df1=df1.drop(columns=""race_hawaiiannative"")",data preprocessing,,
file299,21,"df=unemploy[['top_category_employer1',""top_category_employer2"",""top_category_employer3"",'uu_id',""tract_name"",""timeperiod"",""tract"",""countyfips""]]",data preprocessing,,
file299,22,df,data exploration,,
file299,23,"impute1=KNNImputer()
 impute1.fit(df1)
 unemploy1=pd.DataFrame(impute1.fit_transform(df1),columns = df1.columns)",modeling,data preprocessing,
file299,24,unemploy1,data exploration,,
file299,25,"trial=pd.merge(df, unemploy1, left_index=True, right_index=True)",data preprocessing,,
file299,26,trial.head(),data exploration,,
file299,27,trial.describe(),data exploration,,
file299,28,"sns.heatmap(trial.corr(method=""spearman""))",result visualization,,
file299,29,trial1=trial.copy(),data preprocessing,,
file299,30,"scaler = StandardScaler()
 scaled=pd.DataFrame(scaler.fit_transform(unemploy1),columns = unemploy1.columns)",modeling,data preprocessing,
file299,31,scaled,data exploration,,
file299,32,"sns.displot(data=scaled,x=""total_claims"",kind=""kde"")",result visualization,,
file299,33,"claim = np.array(scaled[""total_claims""])
 sqrt_claim = np.sqrt(claim)
 log_claim = np.log(claim)
 f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
 #f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
 f1.hist(claim, 30)
 f2.hist(sqrt_claim, 30)
 f3.hist(log_claim, 30)",result visualization,data preprocessing,
file299,34,"stats.probplot(claim, plot=f4)
 stats.probplot(sqrt_claim, plot=f5)
 stats.probplot(log_claim, plot=f6)
 plt.show
 stats.shapiro(claim)[1], stats.shapiro(sqrt_claim)[1], stats.shapiro(log_claim)[1]",result visualization,,
file299,35,shapiro(sqrt_claim),data exploration,,
file299,36,print(skew(sqrt_claim)),data exploration,,
file299,37,shapiro(unemploy1.total_claims),data exploration,,
file299,38,"f3.hist(log_claim, 30)",result visualization,,
file299,39,"stats.probplot(scaled[""total_claims""],dist=""norm"",plot=pylab)
 pylab.show()",result visualization,,
file299,40,import pylab,helper functions,,
file299,41,"sns.boxplot(x=scaled[""total_claims""])",result visualization,,
file299,42,"sns.distplot(scaled[""total_claims""])
 sns.distplot(log_claim)",result visualization,,
file299,43,sns.boxplot(x=log_claim),result visualization,,
file299,44,"scaled.plot.hist(subplots=True, legend=True, layout=(8, 2))",result visualization,,
file299,45,scale_logs = np.log(scaled),data preprocessing,,
file299,46,scale_log,data exploration,,
file299,47,"sns.boxplot(x=scaled[""edu_grades_9_11""])",result visualization,,
file299,48,scale_sqrt=np.sqrt(scaled),data preprocessing,,
file299,49,scale_sqrt,data exploration,,
file299,50,unemploy_logs = np.log(unemploy1),data preprocessing,,
file299,51,unemploy_logs,data exploration,,
file299,52,unemp_sqrt=np.sqrt(unemploy1),data preprocessing,,
file299,53,unemp_sqrt,data exploration,,
file299,54,sns.distplot(unemp_sqrt),result visualization,,
file299,55,sns.distplot(unemp_sqrt[edu_grades_9_11]),result visualization,,
file299,56,"unemp_cube=np.power((unemploy1),1/3)",data preprocessing,,
file299,57,unemp_cube,data exploration,,
file299,58,shapiro(unemp_cube.total_claims),data exploration,,
file299,59,from scipy.stats import jarque_bera,helper functions,,
file299,60,"statistics,pvalue = jarque_bera(unemploy1.total_claims)",data preprocessing,,
file299,61,"print('statistics=%.3f, p=%.3f\n' %(statisticss, pvalue))
 if pvalue>0.05:
  print(""Probably Normal"")
 else:
  print(""Probably not Normal"")",data exploration,data preprocessing,
file300,0,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,helper functions,
file300,1,"X=data.drop([""date"",""potential_water_deficit""],1)
 y=data[""mean_temperature""]
 # print(X)
 # print(y)
 data.head()",data preprocessing,data exploration,
file300,2,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file300,3,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file300,4,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file300,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file300,6,"query = """"""
 select * from `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file300,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']=pd.to_datetime(data['date'])
 # data.dtypes
 data.head()",load data,data exploration,
file300,8,"plt.figure(figsize=(7,7))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data preprocessing,
file300,9,"cor_target = abs(cor[""mean_temperature""])
 relevant_features = cor_target[cor_target>0.75]
 print(relevant_features)",data preprocessing,data exploration,
file300,10,"y_pred = pd.Series(model.predict(X), index=X.index)",data preprocessing,,
file300,11,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file300,12,"ax = y.plot(alpha=0.5)
 plt.figure().set_figwidth(3)
 ax = y_pred.plot(ax=ax, linewidth=1)",result visualization,,
file300,13,"from matplotlib import figure
 ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=1)
 figure(figsize(1,1))",helper functions,result visualization,
file300,14,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file300,15,data['lag_1']=lag_1,data preprocessing,,
file300,16,lag_1 = data['mean_temperature'].shift(1),data preprocessing,,
file301,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file301,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file301,2,"BIGQUERY_PROJECT = 'ironhacks-covid19-data'
 BIGQUERY_KEYPATH = 'service-account.json'",load data,,
file301,3,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = BIGQUERY_KEYPATH
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file301,4,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file301,5,"min(data['date']),max(data['date'])",data exploration,,
file301,6,data.dtypes,data exploration,,
file301,7,data = data.set_index('date'),data preprocessing,,
file301,8,data.index,data exploration,,
file301,9,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,,
file301,10,data.loc['2019-08'],data exploration,,
file301,11,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file301,12,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file301,13,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file301,14,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",helper functions,result visualization,
file301,15,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file301,16,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",helper functions,modeling,
file301,17,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,data preprocessing,
file301,18,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file301,19,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file301,20,data['lag_1']=lag_1,data preprocessing,,
file302,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file302,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file302,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file302,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file302,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file302,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file302,6,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file302,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file302,8,"#Correlation with output variable
 cor_target = abs(cor[""potential_water_deficit""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file302,9,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file302,10,"min(data['date']),max(data['date'])",data exploration,,
file302,11,data.dtypes,data exploration,,
file302,12,data = data.set_index('date'),data preprocessing,,
file302,13,data.index,data exploration,,
file302,14,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data exploration,data preprocessing,
file302,15,data.loc['2019-08'],data exploration,,
file302,16,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file302,17,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file302,18,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,data preprocessing,
file302,19,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",result visualization,helper functions,
file302,20,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,data preprocessing,
file302,21,"sns.boxplot(data=data, x='Month', y='wind_speed');",result visualization,,
file302,22,"from statsmodels.graphics.tsaplots import plot_acf
 plot_acf(x=data['max_temperature'], lags=50)",result visualization,helper functions,
file302,23,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,helper functions,
file302,24,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,data preprocessing,
file302,25,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file302,26,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file302,27,data['lag_1']=lag_1,data preprocessing,,
file303,0,data.info(),data exploration,,
file303,1,data.describe(),data exploration,,
file303,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file303,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file303,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file303,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file303,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 # data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file303,7,"final_training_data = pd.get_dummies(data, columns=['uu_id', 'tract_name', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3'])
 final_training_data.head()",data exploration,data preprocessing,
file303,8,data_3 = final_training_data.dropna(axis=0),data preprocessing,,
file303,9,"X = data_3.drop(""total_claims"",1)  #Feature Matrix",data preprocessing,,
file303,10,"y = data_3[""total_claims""] #Target Variable",data preprocessing,,
file303,11,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file303,12,y_pred = model.predict(X),prediction,,
file303,13,y_pred,data exploration,,
file303,14,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file304,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file304,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file305,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file305,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file305,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file305,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file306,0,"# Importing the required libraries
 from wordcloud import WordCloud, STOPWORDS
 import matplotlib.pyplot as plt",helper functions,,
file306,1,"# Replace end of line character with space
 text_raw.replace('\n', ' ')",data preprocessing,,
file306,2,"# Save a lower-case version of each word to a list
 words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']",data preprocessing,,
file306,3,"# Eliminate non alpha elements
 text_list = [word.lower() for word in words_list if word.isalpha()]",data preprocessing,,
file306,4,"# Transforming the list into a string for displaying
 text_str = ' '.join(text_list)",data preprocessing,,
file306,5,"# Defining the wordcloud parameters
 wc = WordCloud(background_color=""black"", max_words=2000,stopwords=stpwords)",data preprocessing,,
file306,6,"# Generate word cloud
 wc.generate(text_str)",data exploration,,
file306,7,"# Show the cloud
 plt.imshow(wc)
 plt.axis('off')
 # plt.show()",result visualization,,
file306,8,"words_list.plot.bar(x=""word"",y=""freq"")
 plt.show()",result visualization,,
file306,9,"df = pd.DataFrame(words_list)
 print(df)
 # df.plot.bar(x=""word"",y=""freq"")
 # plt.show()",data exploration,comment only,
file306,10,"words = nltk.tokenize.word_tokenize(words_list)
 word_dist = nltk.FreqDist(words)
 rslt = pd.DataFrame(word_dist.most_common(top_N),
  columns=['Word', 'Frequency'])
 print(rslt)",data exploration,data preprocessing,
file306,11,"df = pd.DataFrame.from(words_list,columns=['words'])
 # print(df)
 df.plot.bar(x=""words"")
 plt.show()",result visualization,data preprocessing,
file306,12,"# Crating and updating the stopword list
 # stpwords = set(STOPWORDS)
 # stpwords.add('will')
 # stpwords.add('said')
 top_N=7
 words = nltk.tokenize.word_tokenize(text_str)
 word_dist = nltk.FreqDist(words)
 rslt = pd.DataFrame(word_dist.most_common(top_N),
  columns=['Word', 'Frequency'])
 print(rslt)",,,
file306,13,"# df = pd.DataFrame.from(words_list,columns=['words'])
 # print(df)
 plt.to_file('Trump.png')
 rslt.plot.bar(x=""Word"",y=""Frequency"")
 plt.show()",result visualization,,
file306,14,"rslt.plot.bar(x=""Word"",y=""Frequency"")
 plt.show()
 plt.savefig('Trump.png')",result visualization,,
file307,0,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file307,1,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file307,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""",load data,,
file307,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(5)
 print(unemployment_data.columns)
 9/59:
 #SPLIT training set",data exploration,load data,
file307,4,"x = df['edu_post_hs'].values.reshape(-1,1)
 y = df['total_claims'].values.reshape(-1,1)
 9/60:
 #SPLIT training set",data preprocessing,,
file307,5,"X = df['edu_post_hs'].values.reshape(-1,1)
 Y = df['total_claims'].values.reshape(-1,1)
 9/61: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 9/62:
 regressor = LinearRegression()  
 regressor.fit(X_train, y_train)
 9/63:
 #To retrieve the intercept:
 print(regressor.intercept_)",data preprocessing,modeling,
file307,6,"#For retrieving the slope:
 print(regressor.coef_)
 9/64: y_pred = regressor.predict(X_test)
 9/65:
 df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df2",prediction,data exploration,
file307,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file307,8,df = unemployment_data.dropna(),data preprocessing,,
file307,9,"X = df['edu_post_hs'].values.reshape(-1,1)
 Y = df['total_claims'].values.reshape(-1,1)
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file307,10,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train)",modeling,,
file307,11,"#To retrieve the intercept:
 print(regressor.intercept_)",data exploration,,
file307,12,"#For retrieving the slope:
 print(regressor.coef_)
  y_pred = regressor.predict(X_test)",data exploration,prediction,
file307,13,"df2 = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df2",data exploration,data preprocessing,
file308,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file308,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file308,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file309,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file309,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file309,2,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,load data,
file309,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",data exploration,load data,
file310,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file311,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file311,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file311,2,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file311,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file311,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file311,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file311,6,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file311,7,unemployment_data,data exploration,,
file311,8,unemployment_data.isnull().sum(),data exploration,,
file311,9,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file311,10,"unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file311,11,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file311,12,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid]
 

  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]
 

  y = df[['price']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
  plt.title('Tract: '+uuid)
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",,,
file311,13,,prediction,result visualization,
file311,14,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",data exploration,,
file312,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file312,1,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file312,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file312,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file312,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file312,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file312,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file312,7,"X = data.drop(""total_claims"",1)  #Feature Matrix
 X = data.drop(""week_number"",1)
 y = data[""total_claims""] #Target Variable",data preprocessing,,
file312,8,"plt.figure(figsize=(24,20))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data preprocessing,
file312,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file312,10,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file312,11,"min(data['week_number']),max(data['week_number'])",data exploration,,
file312,12,data.dtypes,data exploration,,
file312,13,data = data.set_index('week_number'),data preprocessing,,
file312,14,data.index,data exploration,,
file312,15,"data.sample(37, random_state=0)",data exploration,,
file312,16,data.loc[37],data exploration,,
file312,17,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,helper functions,
file312,18,"y_pred = pd.Series(model.predict(X), index=X.index)",data preprocessing,,
file313,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file313,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file313,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file313,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file313,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,,
file314,0,get_ipython().system('pip install torch'),helper functions,,
file314,1,"import torch
 x = torch.rand(5, 3)
 print(x)",helper functions,data exploration,
file315,0,gcloud auth login,helper functions,,
file316,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('python3 -m pip install google.cloud')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('python3 -m pip install numpy')
 get_ipython().system('python3 -m pip install scikit-learn')
 get_ipython().system('python3 -m pip install plotly')",helper functions,,
file316,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import os
 import seaborn as sns
 from pandas import Series, DataFrame",helper functions,,
file316,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file316,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file316,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file316,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data exploration,
file316,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file316,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data exploration,data preprocessing,
file316,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file316,9,"#Merge the data
 unemployment_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemployment_wage_data = unemployment_wage_data.drop(['countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemployment_wage_data = unemployment_wage_data.fillna(0)
 unemployment_wage_data.describe()",data preprocessing,data exploration,
file316,10,unemployment_wage_data.head(),data exploration,,
file316,11,"#Check for duplicated rows
 duplicated_rows = sum(unemployment_wage_data.duplicated()) 
 unemployment_wage_data = unemployment_wage_data.drop_duplicates()",data preprocessing,,
file316,12,unemployment_wage_data[unemployment_wage_data.isnull().any(axis=1)],data exploration,,
file316,13,"#heat map for correlations
 plt.figure(figsize=(25,10))
 cor = unemployment_wage_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds) 
 plt.show()",result visualization,,
file316,14,unemployment_wage_data.columns,data exploration,,
file316,15,"X = unemployment_wage_data.drop(['total_claims', 'week_number'], axis = 1)
 y = unemployment_wage_data.total_claims",data preprocessing,,
file316,16,"uuid, label = unemployment_wage_data['uu_id'].factorize(sort=True)",data preprocessing,,
file316,17,X['uu_id'] = uuid,data preprocessing,,
file316,18,"#convert data from string to float in order for random forest to work
 X['tract_name_x'] = X['tract_name_x'].factorize()[0]
 X['top_category_employer1'] = X['top_category_employer1'].factorize()[0]
 X['top_category_employer2'] = X['top_category_employer2'].factorize()[0]
 X['top_category_employer3'] = X['top_category_employer3'].factorize()[0]",data preprocessing,,
file316,19,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",data preprocessing,,
file316,20,"from sklearn.ensemble import RandomForestRegressor
 rf = RandomForestRegressor(n_estimators=1000, random_state=42)
 rf.fit(X_train, y_train)",helper functions,modeling,
file316,21,"rf.score(X_test, y_test)",evaluation,,
file316,22,prediction_list,data exploration,,
file316,23,"for col in X.columns[2:]:
  li = []
  for i in prediction_list['uu_id']:
  li.append(X.loc[X['uu_id'] == i, col].mean())
  prediction_list[col] = li",data preprocessing,,
file316,24,prediction_list['uu_id'] = prediction_list['uu_id'].factorize(sort=True)[0],data preprocessing,,
file316,25,"X['uu_id'] = label[X[""uu_id""]]
 X",data preprocessing,data exploration,
file316,26,"claims_predict = rf.predict(prediction_list)
 claims_predict",prediction,data exploration,
file316,27,submission_df = pd.DataFrame(),data preprocessing,,
file316,28,"submission_df[""uu_id""] = prediction_list[""uu_id""]
 submission_df[""week_number""] = prediction_list[""week_number""]
 submission_df[""total_claims""] = claims_predict",data preprocessing,,
file316,29,submission_df,data exploration,,
file317,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file317,1,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\nfrom keras.models import load_model\nimport joblib\nfrom joblib import Parallel, delayed\n')",helper functions,,
file317,2,"def evaluate_regressor(prediction_dataframe):
  # Takes in a prediction dataframe of 2 columns, Actual values and Predicted values generated by a regressor
  # Outputs MSE, MAR, RMSE and MAPE metrics. Must have columns named Actual and Predicted.
  print('MSE:', mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('MAE:', mean_absolute_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))
  print('RMSE:', np.sqrt(mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted'])))
  print('MAPE:', np.mean(np.abs((prediction_dataframe['Actual'] - prediction_dataframe['Predicted']) / prediction_dataframe['Actual'])) * 100)",evaluation,helper functions,
file317,3,"def get_predictions(regressor, model_type, name, week):
  # generates predictions for any model and writes out a dataframe in csv containing them
  # takes a regressor and learning method type as input: DL and ML
  # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape
  result_list = []
  uu_id_transform = LE.fit_transform(prediction_list['uu_id'])
  if model_type == 'DL':
  predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_list = np.array(result_list)
  result_list = np.reshape(result_list, (525,))
  elif model_type == 'ML':
  predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))
  for val in uu_id_transform:
  to_predict = np.insert(predict_arr, 0, val, axis=1)
  to_predict = np.insert(to_predict, 1, week, axis=1)
  r = regressor.predict(to_predict)
  result_list.append(r)
  result_df = pd.DataFrame(result_list, columns = ['Predictions'])
  prediction_sub = prediction_list.copy()
  prediction_sub['total_claims'] = result_df.values
  prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]
  os.makedirs('lost+found/submission_files', exist_ok=True)
  prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)
  return prediction_sub",prediction,save results,
file317,4,"def get_pred_frame(test_frame, prediction_array):
  prediction_frame = pd.DataFrame({'Actual': test_frame, 'Predicted': prediction_array.flatten()})
  return prediction_frame",data preprocessing,,
file317,5,"# updated_ingest = pd.concat([merged_ingest, combined_ingest])
 ingest = pd.read_csv('lost+found/submission_files/complete_ingest.csv')",load data,,
file317,6,ingest.shape,data exploration,,
file317,7,ingest.isnull().sum(),data exploration,,
file317,8,"ingest.dropna(axis=0, inplace=True)
 print(ingest.shape)",data exploration,data preprocessing,
file317,9,"# Based on RFR results, there appears to be an issue with outliers - TODO: Find and remove them
 ingest.columns",data exploration,,
file318,0,2+2 #this is a code cell,helper functions,,
file318,1,2^2,helper functions,,
file318,2,"import numpy as np
 import matplotlib.pyplot as plt
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file318,3,"x = np.linspace(0, 10, 500)
 y = np.cumsum(np.random.randn(500, 6), 0)",data preprocessing,,
file318,4,"plt.figure(figsize=(12, 7))
 plt.plot(x, y)
 plt.legend('ABCDEF', ncol=2, loc='upper left')",result visualization,,
file319,0,"#Code for BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file319,1,"#importing libraries
 import numpy as np
 import matplotlib.pyplot as plt
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file319,2,"#Query Unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 order by uu_id
 """"""",load data,,
file319,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,,
file319,4,"query_job = bigquery_client.query(query2)
 wage_data = query_job.to_dataframe()",load data,,
file319,5,"get_ipython().run_cell_magic('capture', '', '\n#Query Unemployment data\nquery = """"""\nSELECT *\nFROM `ironhacks-data.ironhacks_competition.unemployment_data`\norder by tract, week_number\n\n""""""\n\n\n# QUERY THE DATA ONCE\nquery_job = bigquery_client.query(query)\nunemployment_data = query_job.to_dataframe()\nunemployment_data.drop_duplicates()\n')",helper functions,,
file319,6,wage_data.head(),data exploration,,
file319,7,"#Retain relevant columns and merge Data
 unemployment_data=unemployment_data.drop('timeperiod','countyfips','tract','tract_name','top_category_employer1','top_category_employer2','top_category_employer3')
 wage_data=wage_data.drop('countyfips','tract','tract_name')",data preprocessing,,
file319,8,unemployment_data.head(),data exploration,,
file319,9,"#Fill missing data using columnwise linear interpolation in unemployment data
 unemployment_data.interpolate(method='linear', axis=0)",data preprocessing,,
file319,10,type(unemployment_data),data exploration,,
file319,11,type(unemployment_data['race_white']),data exploration,,
file319,12,"#Fill missing data using columnwise linear interpolation in unemployment data
 num_data=unemployment_data.iloc[:,3:-1].astype(int32)
 num_data.interpolate(method='linear', limit_direction='both', axis=0)",data preprocessing,,
file319,13,"#Fill missing data using columnwise linear interpolation in unemployment data
 from sklearn.preprocessing import LabelEncoder",helper functions,,
file319,14,"unemployment_data=LabelEncoder.fit_transform(unemployment_data)
 series=unemployment_data.iloc[:,3].interpolate(method='linear', axis=0)",data preprocessing,,
file319,15,"series=LabelEncoder.fit_transform(unemployment_data.iloc[:,3])
 series=series.interploate(method='linear')",data preprocessing,,
file319,16,"#Fill missing data in unemployment data with 0
 unemployment_data.fillna(0)",data preprocessing,,
file319,17,"#Fill missing data in unemployment data with 0
 unemployment_data.fillna(0)
 #Fill missing data in wage data with pad
 wage_data.fillna(method='pad')",data preprocessing,,
file319,18,"work_data=pd.merge(unemployment_data, wage_data, on='uu_id')
 work_data.head()",data exploration,,
file319,19,"#Separate by uu_id
 work_data_collection={}
 uu_id_list=work_data['uu_id].drop_dulpicates()",data preprocessing,,
file319,20,"for id in uu_id_list:
  work_data_collection[id]=work_data[work_data['uu_id']==id]
  work_data_collection[id].drop(columns=['uu_id'])",data preprocessing,,
file319,21,work_data_collection[uu_id_list[0]],data exploration,,
file319,22,"for id in uu_id_list:
  work_data_collection[id]=work_data[work_data['uu_id']==id]
  work_data_collection[id]=work_data_collection[id].drop(columns=['uu_id'])
  work_data_collection[id]=work_data_collection[id].drop_duplicates()",data preprocessing,,
file319,23,"import seaborn as sns
 sns.heatmap(work_data_collection[uu_id_list[0]])",helper functions,result visualization,
file319,24,"work_data=work_data.drop_duplicates()
 X_data=work_data[columns=['week_num','average_wage']]
 y_data=work_data[columns=['total_claims']]",data preprocessing,,
file319,25,"from sklearn import linear_model
 lm = linear_model.LinearRegression()
 model=lm.fit(X,y)",helper functions,modeling,
file319,26,"lm.score(X,y)",evaluation,,
file319,27,"lm.score(X_data,y_data)",evaluation,,
file320,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file320,1,"get_ipython().run_cell_magic('capture', '', '!pip install google-cloud-bigquery\n!pip install google-cloud-bigquery[pandas]\n')",helper functions,,
file320,2,"#- IMPORT THE LIBRARIES YOU WILL USE
 #------------------------------------------
 # You only need to import packages one time per notebook session. To keep your
 # notebook clean and organized you can handle all imports at the top of your file.
 # The following are included for example purposed, feel free to modify or delete 
 # anything in this section.
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import RandomForestClassifier",helper functions,,
file320,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file320,4,"#query 3: overview of employment_data(week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC;
 """"""
 query_job = bigquery_client.query(query)
 overview = query_job.to_dataframe()
 overview.head()",load data,data exploration,
file320,5,"#query 3: overview of prediction list (week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query_job = bigquery_client.query(query)
 predn = query_job.to_dataframe()
 predn.head()
 print(predn.head())",data exploration,load data,
file320,6,"fig = plt.figure()
 plt.plot([employ.week_number], [employ.total_claims],'bs')
 plt.title('Distribution of claims through week')
 plt.xlabel('Weeks')
 plt.ylabel('Total claims')
 plt.show()",result visualization,,
file320,7,"labels = np.array(overview['total_claims'])
 features = employ.drop(['uu_id'], axis=1)
 feature_list = list(features.columns)
 features = np.array(features)",data preprocessing,,
file320,8,"#split data into train and test sets, split first 20% data
 x_train, x_test, y_train,y_test = train_test_split(features, labels, test_size = 0.20, random_state = 42)",data preprocessing,,
file320,9,"print(f'Training Features Shape: {x_train.shape}')
 print(f'Testing Features Shape: {x_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data exploration,,
file320,10,"from sklearn.ensemble import RandomForestRegressor
 regressor = RandomForestRegressor(n_estimators=1000, random_state=42)
 x = x_train
 y = y_train
 regressor.fit(x,y)",modeling,helper functions,
file320,11,"#visualizing the decision tree from the regressor
 from sklearn import tree
 tree.plot_tree(regressor.estimators_[0])",helper functions,result visualization,
file320,12,"#Shown detailed status of decision tree
 from sklearn.datasets import make_regression
 x,y = make_regression(n_features=4, n_informative=2,
  random_state=0, shuffle=False)
 regr = RandomForestRegressor(max_depth=2, random_state=0)
 regr.fit(x, y)",modeling,helper functions,
file320,13,"print(regr.predict([[0, 0, 0, 0]]))
 tree.plot_tree(regr.estimators_[0])",data exploration,result visualization,
file320,14,"predictions = regressor.predict(test_features).astype(int)
 predictions = np.round(predictions,decimals = 0, out = None)
 print(predictions)",prediction,data exploration,
file320,15,"errors = abs(y_test - predictions)
 print(f'List of Errors: {errors}')
 print(f'Mean Absolute Error: {np.mean(errors)*10:.4f}%')",data exploration,evaluation,
file320,16,"df = pd.DataFrame(predictions, columns=['total_claims'])
 week41 = predn.join(df).iloc[:,[0,2,1]]
 print(week41)
 print(f'Total predicting number of unemployment claims of week 41: {sum(predictions):.0f}')",data exploration,data preprocessing,
file320,17,"csv_data = week41.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file320,18,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file321,0,get_ipython().system('pip install torch'),helper functions,,
file321,1,"import torch
 x = torch.rand(5, 3)
 print(x)",helper functions,comment only,
file322,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file322,1,"get_ipython().run_cell_magic('capture', '--no-display', '!pip3 install db-dtypes\n')",helper functions,,
file322,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file322,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file322,4,"# Google Credential
 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='key.json'
 bigquery_client = bigquery.Client(project='ironhacks-data')",load data,comment only,
file322,5,"# Query the three provided data tables 
 unemployement_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,comment only,
file322,6,"wage_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file322,7,"# QUERY THE DATA ONCE
 ud_query_job = bigquery_client.query(unemployement_data_query)
 wd_query_job = bigquery_client.query(wage_data_query)
 pl_query_job = bigquery_client.query(prediction_list_query)",load data,comment only,
file322,8,"unemployement_data = ud_query_job.to_dataframe()
 wage_data = wd_query_job.to_dataframe()
 prediction_list = pl_query_job.to_dataframe()",data preprocessing,,
file322,9,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",save results,data exploration,
file322,10,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",save results,data exploration,
file322,11,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)",save results,data exploration,
file322,12,"# check if how many weeks of data are provided for each uu_id
 query = """"""
 SELECT uu_id, COUNT(week_number) as num_of_weeks
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY uu_id
 ORDER BY num_of_weeks DESC
 """"""",load data,,
file322,13,bigquery_client.query(query).to_dataframe(),data preprocessing,,
file322,14,"# query week_number data for uu_id 920b9820c654673d472494c346da5651
 query = """"""
 SELECT uu_id, week_number
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 WHERE uu_id=""920b9820c654673d472494c346da5651""
 ORDER BY week_number
 """"""
 print(bigquery_client.query(query).to_dataframe().to_string(index=False))",load data,data exploration,
file322,15,"# use sub-query to retrieve the num_of_uuids vs num_of_weeks, plot the results
 query = """"""
 SELECT num_of_weeks, COUNT(num_of_weeks) as num_of_uuids
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as num_of_weeks
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
  ORDER BY num_of_weeks DESC
 )
 GROUP BY num_of_weeks
 ORDER BY num_of_weeks
 """"""",load data,,
file322,16,"week_count_label = list(map(str, list(uuid_weeks[""num_of_weeks""])))
 num_of_uuids = list(uuid_weeks[""num_of_uuids""])
 plt.figure(figsize=(6,6))
 plt.barh(week_count_label, num_of_uuids)
 plt.yticks(week_count_label)
 plt.ylabel(""How many weeks of data are included"")
 plt.xlabel(""Number of UUIDs"")
 for i, v in enumerate(num_of_uuids):
  plt.text(v + 1, i - 0.4, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file322,17,"print(str(np.array(num_of_uuids)[-6:].sum()), ""out of 573 uuids have no less than 30 weeks of datapoints."")
 print(str(np.array(num_of_uuids)[:9].sum()), ""out of 573 uuids have no more than 10 weeks of datapoints."")",data exploration,,
file322,18,"# query the number of datapoints for each week, use DISTINCT on uu_id to remove duplication
 QUERY = """"""
 SELECT week_number, Count(DISTINCT(uu_id)) as count
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY week_number
 ORDER BY week_number
 """"""
 query_job = bigquery_client.query(QUERY)
 week_number_count = query_job.to_dataframe()
 display(week_number_count.T)
 print(""week_number: \n"" + str(week_number_count.T.to_numpy()[0]))
 print(""count: \n"" + str(week_number_count.T.to_numpy()[1]))",load data,data exploration,
file322,19,"# fill the missing weeks' count with 0 for week 4 and week 23
 i = 0
 week_number_count_filled = []
 for w_c in week_number_count.to_numpy():
  i = i + 1
  if w_c[0] == i:
  week_number_count_filled.append(list(w_c))
  else:
  week_number_count_filled.append(list([i, 0]))
  week_number_count_filled.append(list(w_c))
  i = i + 1
 print(""Total number of weeks:"", i)",data preprocessing,data exploration,
file322,20,"# plot the filled result
 week_number = list(np.array(week_number_count_filled)[:,0])
 week_count = list(np.array(week_number_count_filled)[:,1])
 plt.figure(figsize=(8,6))
 plt.barh(week_number, week_count)
 plt.yticks(week_number)
 plt.ylabel(""Week number"") 
 plt.xlabel(""Number of Datapoints"")
 for i, v in enumerate(week_count):
  plt.text(v + 2, i + 0.7, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,data preprocessing,
file322,21,"# select 10 of 96 uuids with 3 weeks' data
 QUERY = """"""
 SELECT uu_id
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as week_count
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
 )
 WHERE week_count = 35
 LIMIT 10
 """"""
 query_job = bigquery_client.query(QUERY)
 ten_uuid_with_35_weeks = query_job.to_dataframe()
 ten_uuid_with_35_weeks",load data,data exploration,
file322,22,"# query the 10 selected UUID data
 ten_35w_data = []
 for uu_id in ten_uuid_with_35_weeks[""uu_id""]:
  QUERY=""""""
  SELECT uu_id, week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uu_id)
  query_job = bigquery_client.query(QUERY)
  ten_35w_data.append(query_job.to_dataframe().drop_duplicates(ignore_index=True))",load data,data preprocessing,
file322,23,"# manually add week 4 and week 23 missing data
 def fill_week4_and_week23(df, method, replace=False):
  """"""
  Fill or replace the week 4 and week 23 missing total_claims data.
  Args:
  df: Dataframe
  The input dataframe with missing total claims or to be replaced.
  method: string
  prev - use previous week's value
  next - use next week's value
  mid - use mean value of previous and next weeks
  replace: bool
  True - replace the existing week 4 and week 23 total_claim values
  False - add the missing values
  Returns:
  Dataframe:
  Dataframe with added total_claim values or replaced values.
  """"""
  uuid = df[""uu_id""][0]
  week_list = list(df[""week_number""])
  if 4 in week_list and 23 in week_list and not replace:
  print(""Week 4 and week 23 data already exist, please use replace=True"")
  return df
  if 4 not in week_list and 23 not in week_list and replace:
  print(""Week 4 and week 23 data do not exist, replace failed"")
  return df
  if 4 not in week_list and 23 not in week_list and not replace:
  print(""Fill uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  if 4 in week_list and 23 in week_list and replace:
  print(""Replace uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  # remove exisiting value and insert again
  df = df.drop(df.index[[3, 22]])
  df = df.sort_index().reset_index(drop=True)
  
  if method == ""prev"":
  # use previous avaliable week's value
  val_4 = df[""total_claims""][2]
  val_23 = df[""total_claims""][20]
  if method == ""next"":
  # use next avaliable week's value
  val_4 = df[""total_claims""][3]
  val_23 = df[""total_claims""][21]
  if method == ""mid"":
  # use mean value of previous and next avaliable weeks
  val_4 = int((df[""total_claims""][2] + df[""total_claims""][3]) * 0.5)
  val_23 = int((df[""total_claims""][20] + df[""total_claims""][21]) * 0.5)
  
  # week 4
  df.loc[2.5] = [uuid, 4, val_4]
  # week 23
  df.loc[20.5] = [uuid, 23, val_23]
  return df.sort_index().reset_index(drop=True)",,,
file322,24,"# fill the missing data using mean value of prev week and next week for all 10 uuids
 for i in range(len(ten_35w_data)):
  ten_35w_data[i] = fill_week4_and_week23(ten_35w_data[i], ""mid"")",data preprocessing,,
file322,25,"# plot the 10 uuid total_claim data from week 1 to week 37
 plt.figure(figsize=(12,4))
 plt.title(""10 selected uuid with 37 weeks of data total_claim plot"")
 for i in range(len(ten_35w_data)):
  plt.plot(list(map(str, ten_35w_data[i][""week_number""])), 
  ten_35w_data[i][""total_claims""], 
  ""o-"", linewidth=1, markersize=5, alpha=0.7,
  label=ten_35w_data[i][""uu_id""][0])
  plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)
 plt.xlabel(""week_number"")
 plt.ylabel(""total_claims"")
 plt.grid(axis=""y"")
 plt.tight_layout()
 plt.show()",result visualization,,
file322,26,"# find out which weeks are available for these uuids
 week_available = []
 for i in range(len(data_less_10w)):
  week_available.append(list(data_less_10w[i][""week_number""]))",data preprocessing,,
file322,27,"# plot the avaliable weeks vs uu_id
 plt.figure(figsize=(12,4))
 plt.title(""week_number distribution for uuid with no more than 10 weeks of data"")
 colors = ['C{}'.format(i) for i in range(37)]
 plt.eventplot(week_available, orientation='vertical', linelengths=0.2, linewidths=4, colors=colors)
 plt.yticks([i for i in range(1,38,2)])
 plt.ylabel(""week_number"")
 plt.xticks([i for i in range(37)])
 plt.xlabel(""uu_id"")
 plt.grid(axis=""x"")
 plt.tight_layout()
 plt.show()",result visualization,,
file322,28,week_available.shape,data exploration,,
file322,29,len(week_available),data exploration,,
file322,30,prediction_list.shape[0],data exploration,,
file322,31,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",data exploration,save results,
file322,32,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",data exploration,save results,
file322,33,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)
 total_uuid = prediction_list.shape[0]",save results,data exploration,
file322,34,"# save query result into csv file
 raw_total_claims_df = pd.DataFrame.from_dict(raw_total_claims_data)
 raw_total_claims_df.to_csv(""raw_total_claims.csv"", index=False)",save results,data preprocessing,
file322,35,"# query all total_claims data and store in raw_total_claims_data dict
 def query_all_total_claims():
  temp_query_list = prediction_list[""uu_id""] # the list of uuids to be queried
  raw_total_claims_data = {uuid:[] for uuid in temp_query_list} # dict to store all query results
  total_week = 37
 

  progress = 0
  total = len(temp_query_list)
  for uuid in temp_query_list:
  progress = progress + 1
  print('\r', ""Querying"", str(progress) + ""/"" + str(total), ""uuid's data..."", end='\r')
  QUERY=""""""
  SELECT week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uuid)
  query_job = bigquery_client.query(QUERY)
  res = query_job.to_dataframe().drop_duplicates(ignore_index=True).to_numpy()
 

  # Fill the missing total_claims between week 1 and week 37 with value 0
  index = 0
  for w in range(1,total_week + 1):
  if index == len(res):
  raw_total_claims_data[uuid].append({w:0})
  continue
  if w == res[index][0]:
  raw_total_claims_data[uuid].append({w:res[index][1]})
  index = index + 1
  else:
  raw_total_claims_data[uuid].append({w:0})
  return raw_total_claims_data",load data,data preprocessing,
file322,36,"# commented for only excuting once
 raw_total_claims_data = query_all_total_claims()",data preprocessing,,
file322,37,"raw_total_claims_df = pd.read_csv(""raw_total_claims.csv"")",load data,,
file322,38,"for i in range(0, 37):
  for j in range(0, 573):
  d = raw_total_claims_df.iloc[i,j]
  # print(int(d.split("":"")[1].split(""}"")[0]))
  raw_total_claims_df.iloc[i,j] = int(d.split("":"")[1].split(""}"")[0])",data preprocessing,,
file322,39,"raw_total_claims_list = raw_total_claims_df.T.to_numpy()
 raw_total_claims_list = raw_total_claims_list[:]",data preprocessing,,
file322,40,"x_labels = [w for w in range(1, 38)]
 NUM_PLOTS = len(raw_total_claims_list)",data preprocessing,,
file322,41,"plt.figure(figsize=(15,5))
 plt.xticks(x_labels)
 for i in range(0, NUM_PLOTS):
  plt.plot(x_labels, raw_total_claims_list[i], "".-"")",result visualization,,
file322,42,"# fill week 4 and week 23 data with mean values for all uuids
 for row in raw_total_claims_list:
  temp_sum = 0
  w4_val = round(0.5 * (row[2] + row[4]) + 0.01)
  w23_val = round(0.5 * (row[21] + row[23]) + 0.01)
  row[3] = w4_val
  row[22] = w23_val",data preprocessing,,
file322,43,"def mse_loss(y_true, y_pred):
  """"""
  MSE loss function
  Inputs param
  -------------------------
  y_true: list
  The ground truth values
  y_pred: list
  The predicted values
  -------------------------
  return: number
  mean square error of the two input lists
  """"""
  if(len(y_true) != len(y_pred)):
  print(""True label len:"" + str(len(y_true)) + "", Predict label len: "" + str(len(y_pred)))
  raise Exception(""Input lists have different length"")
  mse = np.mean(np.array(y_true) - np.array(y_pred))**2
  return mse",evaluation,data exploration,
file322,44,"uuid_list = list(raw_total_claims_df.columns)
 pred_results = [round(np.array(row).mean() + 0.01) for row in raw_total_claims_list]",data preprocessing,,
file322,45,"# Mean value as the prediction result
 result_dict = {
  'uu_id':uuid_list, 
  'total_claims': pred_results,
  'week_number': [39 for i in range(573)]
 }
 result_to_csv = pd.DataFrame(result_dict)
 result_to_csv",data preprocessing,,
file322,46,"print(uuid_list[0])
 print(list(raw_total_claims_list[0]))",data exploration,,
file322,47,"print(list(raw_total_claims_df[""26c71b31d464bc7bedc8aed7e5c6e641""]))",data exploration,,
file322,48,"result_to_csv.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file322,49,temp_res = [row[-1] for row in raw_total_claims_list],data preprocessing,,
file322,50,"print(list(raw_total_claims_df[""8ba19786b86ae124a9d7eaa054f15d23""]))",data exploration,,
file322,51,"mae_loss(temp_res, pred_results)",evaluation,,
file322,52,"# plot the avaliable weeks vs uu_id
 plt.figure(figsize=(12,4))
 plt.title(""week_number distribution for uuid with no more than 10 weeks of data"")
 colors = ['C{}'.format(i) for i in range(total_uuid_less_10w)]
 plt.eventplot(week_available, orientation='vertical', linelengths=0.2, linewidths=4, colors=colors)
 plt.yticks([i for i in range(1,38,2)])
 plt.ylabel(""week_number"")
 plt.xticks([i for i in range(total_uuid_less_10w)])
 plt.xlabel(""uu_id"")
 plt.grid(axis=""x"")
 plt.axhline(4, alpha=0.5)
 plt.axhline(23, alpha=0.5)
 plt.axhline(37, alpha=0.5)
 plt.tight_layout()
 plt.show()",result visualization,,
file323,0,2+2 #this is a code cell,comment only,,
file323,1,2^2,comment only,,
file323,2,"import numpy as np
 import matplotlib.pyplot as plt
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file323,3,"x = np.linspace(0, 10, 500)
 y = np.cumsum(np.random.randn(500, 6), 0)",data preprocessing,,
file323,4,"plt.figure(figsize=(12, 7))
 plt.plot(x, y)
 plt.legend('ABCDEF', ncol=2, loc='upper left')",result visualization,,
file324,0,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file324,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.preprocessing import OneHotEncoder
 from sklearn.model_selection import train_test_split
 from sklearn.ensemble import RandomForestRegressor
 import matplotlib.pyplot as plt
 import datetime,itertools
 from statsmodels.tsa.statespace.sarimax import SARIMAX
 from statsmodels.tsa.arima.model import ARIMA
 import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)
 from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
 from statsmodels.tsa.stattools import adfuller",helper functions,,
file324,2,"def example_function():
  print('Hello World')",comment only,,
file324,3,"def print_column_info(df):
  print(f'No. of columns: {len(df.columns)}')
  for col in df.columns:
  print(len(df[col].unique()),col,df[col].dtypes)
  print()",data exploration,data preprocessing,
file324,4,"def print_na_info(df):
  for col in df.columns:
  print(df[col].isnull().sum(),col,df[col].dtypes)",data exploration,data preprocessing,
file324,5,"def get_datetime(week_no):
  date = datetime.datetime.strptime(""2022-""+str(week_no)+""-1"",""%Y-%W-%w"")
  #print(date)
  return pd.to_datetime(date,format=""%Y-%m-%d"")",data preprocessing,,
file324,6,"def sarimax_gridsearch(ts, pdq, pdqs, maxiter=100, freq='D',disp=False):
  # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value
  ans = []
  for comb in pdq:
  for combs in pdqs:
  #try:
  mod = SARIMAX(ts,order=comb,
  seasonal_order=combs,
  enforce_stationarity=False,
  enforce_invertibility=False)
 

  output = mod.fit(maxiter=maxiter,disp=False) 
  ans.append([comb, combs, output.bic])
  #print('SARIMAX {} x {}12 : BIC Calculated ={}'.format(comb, combs, output.bic))
  #except:
  # continue
  ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic'])
  ans_df = ans_df.sort_values(by=['bic'],ascending=True)[0:5]
  
  return ans_df",modeling,,
file324,7,"def check_stationary(data,Print=0):
  adft = adfuller(data,autolag=""BIC"")
  output_df = pd.DataFrame({""Values"":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']] , ""Metric"":[""Test Statistics"",""p-value"",""No. of lags used"",""Number of observations used"", 
  ""critical value (1%)"", ""critical value (5%)"", ""critical value (10%)""]})
 

  critical_value = adft[4]['5%']
  if Print==1:
  print(output_df)
  if adft[1] < 0.05 and adft[0] < critical_value:
  return 1
  else:
  return 0",data preprocessing,,
file324,8,"def diff_inv(series, last_observation):
 

  series_undifferenced = series.copy()
 

  series_undifferenced.iat[0] = series_undifferenced.iat[0] + last_observation
 

  series_undifferenced = series_undifferenced.cumsum()
 

  return series_undifferenced",data preprocessing,,
file324,9,"def loss(pred,actual):
  pred = np.round(pred)
  errors = abs(actual-pred)
  print(f'Mean Absolute Error: {round(np.mean(errors), 2)}')
  print(f'Mean squared Error: {round(np.mean(errors**2), 2)}')
  mape = 100 * (errors/actual)
  # Calcualte and display accuracy
  accuracy = 100 - np.mean(mape)
  print(f'Accuracy: {round(accuracy, 2)}%.')",data exploration,evaluation,
file324,10,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file324,11,"#query = """"""
 #SELECT 
 #week_number,
 #cases 
 #FROM `ironhacks-data.ironhacks_training.covid19_cases`
 #Where week_number between 1 and 3
 #order by week_number
 #""""""
 print(""Datasets available:"")
 for dataset in list(bigquery_client.list_datasets()):
  print(dataset.dataset_id)
  if dataset.dataset_id == ""ironhacks_competition"":
  mydataset = dataset",load data,data preprocessing,
file324,12,"print(""\nTables available:"")
 for table in bigquery_client.list_tables(""ironhacks_competition""):
  print(table.table_id)",data exploration,data preprocessing,
file324,13,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file324,14,"query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file324,15,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",data preprocessing,,
file324,16,"prediction_data = query_job1.to_dataframe()
 unemployment_data = query_job2.to_dataframe()
 wage_data = query_job3.to_dataframe()",data preprocessing,,
file324,17,"##Dumping the df to csv
 week = ""week2""
 prediction_data.to_csv(""prediction_data_""+week+"".csv"",index=False)
 unemployment_data.to_csv(""unemployment_data_""+week+"".csv"",index=False)
 wage_data.to_csv(""wage_data_""+week+"".csv"",index=False)",save results,,
file324,18,"merged_data = pd.merge(unemployment_data,wage_data[[""uu_id"",""average_wage""]],on=""uu_id"")
 merged_data = merged_data.drop_duplicates()",data preprocessing,,
file324,19,"query4 = """"""
 SELECT table_id,
 DATE(TIMESTAMP_MILLIS(creation_time)) AS creation_date,
 DATE(TIMESTAMP_MILLIS(last_modified_time)) AS last_modified_date,
 row_count,
 size_bytes,
 CASE
  WHEN type = 1 THEN 'table'
  WHEN type = 2 THEN 'view'
  WHEN type = 3 THEN 'external'
  ELSE '?'
 END AS type,
 TIMESTAMP_MILLIS(creation_time) AS creation_time,
 TIMESTAMP_MILLIS(last_modified_time) AS last_modified_time,
 dataset_id,
 project_id
 FROM `ironhacks-data.ironhacks_competition.__TABLES__`""""""
 query_job4 = bigquery_client.query(query4)
 timestamp_data = query_job4.to_dataframe()
 for cnt,row in timestamp_data.iterrows():
  print(""\n"")
  print(row[""table_id""])
  print(row[""creation_time""])
  print(row[""last_modified_time""])",load data,data exploration,
file325,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file325,1,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file325,2,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file325,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file325,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file326,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file326,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file326,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file326,3,"def example_function():
  print('Hello World')",comment only,,
file326,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file326,5,"def get_data(table_name):
  query = f""""""
  SELECT *
  FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """"""
 

  # QUERY THE DATA ONCE
  query_job = bigquery_client.query(query)
  return query_job.to_dataframe()",load data,,
file326,6,unemploy = get_data('unemployment_data'),load data,,
file326,7,wage = get_data('wage_data'),load data,,
file326,8,sub = get_data('prediction_list'),load data,,
file326,9,pred = unemploy.groupby('uu_id')['total_claims'].mean().apply(lambda x: int(x)),data preprocessing,,
file326,10,pred = unemploy.groupby('uu_id').apply(lambda x: x.sort_values('week_number').drop_duplicates('week_number')['total_claims'].ewm(alpha=1/3).mean().to_numpy()[-1]),data preprocessing,,
file326,11,pred = pred[sub['uu_id'].to_list()],data preprocessing,,
file326,12,"df = pd.DataFrame({
  'uu_id': pred.index,
  'week_number': 39,
  'total_claims': pred.to_numpy().astype(np.int32)
 })",data preprocessing,,
file326,13,df,data exploration,,
file326,14,"df.to_csv('submission_prediction_output.csv', index=False)",save results,,
file327,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file327,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file327,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file327,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file327,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data exploration,
file327,5,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file327,6,"print(f""size of unemployment_data: {len(unemployment_data)}"")
 print(f""size of wage_data: {len(wage_data)}"")
 print(f""size of prediction_data: {len(prediction_data)}"")",data exploration,,
file327,7,unemployment_data.dtypes,data exploration,,
file327,8,"plt.figure(figsize=(12,10))
 cor = unemployment_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file327,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data exploration,data preprocessing,
file327,10,unemployment_data = unemployment_data.set_index('timeperiod'),data preprocessing,,
file327,11,"X = unemployment_data.drop([""uu_id"", ""weak_number""], axis=1)
 X_cor = unemployment_data[[""edu_hs_grad_equiv"", ""edu_post_hs"",
  ""gender_female"", ""gender_male"",
  ""race_black"", ""race_white""]]
 y = unemployment_data[""total_claims""]",data preprocessing,,
file327,12,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",helper functions,modeling,
file327,13,"from sklearn.impute import SimpleImputer
 imp = SimpleImputer(missing_values=np.nan, strategy='mean')
 imp.fit_transform(X_cor, y)",helper functions,modeling,
file327,14,"y_pred = pd.Series(model.predict(X), index=X.index)",data preprocessing,prediction,
file327,15,y_pred = pd.Series(model.predict(X_cor)),data preprocessing,prediction,
file327,16,"# y_pred = pd.Series(model.predict(X_cor), index=X_cor.index)
 y_pred = pd.Series(model.predict(X_cor))",prediction,data preprocessing,
file327,17,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",save results,,
file327,18,"from sklearn.preprocessing import OneHotEncoder
 enc = OneHotEncoder()
 X = enc.fit_transform(data[""uu_id"", ""tract_name"",
  ""top_category_employer1"",
  ""top_category_employer2"",
  ""top_category_employer3""])",helper functions,data preprocessing,
file327,19,"X = pd.get_dummies(df, prefix=['col1', 'col2'])",data preprocessing,,
file327,20,X = pd.get_dummies(unemployment_data),data preprocessing,,
file328,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file328,1,"#import cell
 import pandas as pd
 import numpy as np
 import statistics
 import csv
 import matplotlib.pyplot as plt
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file328,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file328,3,"#Gets the master unemployed table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file328,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemploymentData = query_job.to_dataframe()
 print(unemploymentData.shape)
 pd.set_option('display.max_columns', None)
 unemploymentData.head(3)",load data,data exploration,
file328,5,"#Gets each tracks mean and std dev
 #unlist has the master unemployment list
 #b becomes the filtered list
 unList = unemploymentData.values.tolist()
 b_set = set(tuple(x) for x in unList)
 b = [ list(x) for x in b_set ]",data preprocessing,,
file328,6,"uuid = []
 #makes a list of the unique uuid
 for x in b:
  if(uuid.count(x[0]) == 0):
  uuid.append(x[0])",data preprocessing,,
file328,7,"#setup for extract  
 values = []
 export = []",data preprocessing,,
file328,8,"#for each value make a list of each weeks claims
 for y in uuid:
  temp = [y]
  for x in b:
  if (x[0] == y):
  temp.append(x[6])
  values.append(temp)",data preprocessing,,
file328,9,"for x in values:
  name = x[0]
  mean = statistics.mean(x[1:])
  if (len(x) > 2):
  stdev = statistics.stdev(x[1:])
  else:
  print(""short"")
  export.append([name, mean, stdev])
 #Everything below this is testing",data exploration,data preprocessing,
file328,10,"#Make bar charts
 #unique list as guide to count
 x1 = []
 y1 = []
 for i in uuid[:1]:
  for k in b:
  if (k[0] == i):
  x1.append(k[2])
  y1.append(k[6])",data preprocessing,,
file328,11,"plt.bar(x1,y1)",result visualization,,
file328,12,"xValues = []
 xCount = []
 for i in uuid[:1]:
  for k in b:
  if (k[0] == i):
  xValues.append(k[6])
 c_set = set(tuple(x) for x in xValues)
 c = [ list(x) for x in c_set ]",data preprocessing,,
file328,13,"for x in c:
  xCount.append(xValues.count(x))",data preprocessing,,
file328,14,"plt.bar(xValues,xCount)",result visualization,,
file328,15,"for x in c:
  xCount.append(xValues.count(x))
 print(xValues)
 print(xCount)
 plt.bar(xValues,xCount)",result visualization,data exploration,
file328,16,"for x in c:
  xCount.append(xValues.count(x))
 print(statistics.mean(xValues))
 print(statistics.median(xValues))
 plt.bar(c,xCount)",data exploration,result visualization,
file328,17,"xValues = []
 xCount = []
 for i in uuid[:2]:
  for k in b:
  if (k[0] == i):
  xValues.append(k[6])
  c = list(dict.fromkeys(xValues))
 

  for x in c:
  xCount.append(xValues.count(x))
  print(statistics.mean(xValues))
  print(statistics.median(xValues))
  plt.bar(c,xCount)",data exploration,data preprocessing,
file329,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file329,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm",helper functions,,
file329,2,"# explore dataframe
 def dataExplore(data):
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data exploration,data preprocessing,
file329,3,"# check balance of data
 def dataBalanceCheck(data):
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data exploration,data preprocessing,
file329,4,"# fill NA with given value
 def dataFillNa(data, value):
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)
 def dataIdentifyDateMonth(data):
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  return(data)",data preprocessing,,
file329,5,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file329,6,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file329,7,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file329,8,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file329,9,"# Explore input data for NA and special values
 dataExplore(data)
 dataExplore(data_pred_query)",load data,,
file330,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file330,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file330,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file330,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file330,4,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file331,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file331,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file332,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file332,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file332,2,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file332,3,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file332,4,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file332,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file332,6,"query_job = bigquery_client.query(query)
 unemployment_data_table = query_job.to_dataframe()",load data,,
file332,7,"print(""Columns:"")
 print('\n'.join(unemployment_data_table.columns))
 print(""\nResults:"")
 print(unemployment_data_table.head())",data exploration,,
file332,8,"query = """"""
 SELECT week_number, SUM(total_claims)
 FROM 'ironhacks-data.ironhacks_competition.unemployment_data'
 GROUP BY week_number
 """"""",load data,,
file332,9,print(unemployment_total_claims_by_week.head()),data exploration,,
file333,0,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file333,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file333,2,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file333,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,,
file334,0,get_ipython().system('nvidia-smi'),helper functions,,
file335,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file335,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install db-dtypes\n"")",helper functions,,
file335,2,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 from statsmodels.formula.api import ols
 from pandas import Series, DataFrame",helper functions,,
file335,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file335,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file335,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data exploration,
file335,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file335,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data exploration,data preprocessing,
file335,8,"query_job3 = bigquery_client.query(query3)
 prediction_list = query_job3.to_dataframe()
 prediction_list.head()",load data,data exploration,
file335,9,"unemploy_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemploy_wage_data = unemploy_wage_data.drop(['timeperiod', 'countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemploy_wage_data = unemploy_wage_data.fillna(0)
 unemploy_wage_data.head()",data exploration,data preprocessing,
file335,10,unemploy_wage_data.describe(),data exploration,,
file335,11,"sns.relplot(data=unemploy_wage_data, x='week_number', y='total_claims')",result visualization,,
file335,12,"sns.distplot(unemploy_wage_data.total_claims, bins=10)",result visualization,,
file335,13,"plt.figure(figsize=(16,14))
 cor = unemploy_wage_data.corr()
 cmap = sns.diverging_palette(210, 20, as_cmap=True)
 sns.heatmap(cor, cmap=cmap, vmax=.99, vmin=-.99, annot=True)",result visualization,data preprocessing,
file335,14,"X = unemploy_wage_data[['week_number', 'countyfips_x', 'tract_x', 'edu_8th_or_less', 'edu_grades_9_11', \
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown', 'gender_female', 'gender_male', \
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', \
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]
 y = unemploy_wage_data['total_claims']",data preprocessing,,
file335,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file335,16,"reg = LinearRegression()  
 reg.fit(X_train, y_train)
 print(reg)",modeling,data exploration,
file335,17,"print(f'intercept: {reg.intercept_}')
 coef = DataFrame(reg.coef_, X.columns, columns=['coefficients'])
 print(coef)",data exploration,data preprocessing,
file335,18,"y_pred = reg.predict(X_test)
 df = DataFrame({'Actual': y_test, 'Predicted': y_pred})
 df",prediction,data exploration,
file335,19,df['Predicted'].mean(),data exploration,,
file335,20,"print('R squared: {:.2f}'.format(reg.score(X, y)*100))
 print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
 print('MSE:', metrics.mean_squared_error(y_test, y_pred))
 print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",data exploration,,
file335,21,"#Make prediction
 prediction_data = pd.merge(unemploy_wage_data, prediction_list, on=['uu_id'], how='inner')
 prediction_data = prediction_data.drop(['week_number_x','total_claims'],axis=1)
 prediction_data = prediction_data.groupby(['uu_id']).mean()
 prediction_data",data preprocessing,data exploration,
file335,22,"prediction_list.to_csv('submission_prediction_output.csv', index=False)",save results,,
file336,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file336,1,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 6
 test_df[""month""]=11",load data,data preprocessing,
file336,2,test_df.drop_duplicates(),data exploration,,
file336,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file336,4,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!pip install chardet\n!pip install db-dtypes\n!pip install catboost\n"")",helper functions,,
file336,5,"#Importing Libraries
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file336,6,"import pandas as pd
 import csv
 import numpy as np",helper functions,,
file336,7,"from google.cloud import bigquery
 import scipy as sp
 import statsmodels as sm
 import statistics as stat
 import sklearn
 import patsy as pt
 import chardet as ch
 import click as cl
 import cytoolz as cz
 import dask
 import decorator as dr
 #import pyjson5 as py
 import jsonschema as js
 import tables
 import db_dtypes
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error",helper functions,,
file336,8,global df3,data exploration,,
file336,9,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file336,10,"query_job1 = bigquery_client.query(query)
 query_job1",load data,data exploration,
file336,11,"!pip install db-dtypes
 df1 = query_job2.to_dataframe()
 df1.uu_id.nunique()",data preprocessing,data exploration,
file336,12,"df3 = pd.merge(df1,df2, on= [""uu_id"",""countyfips"",""tract"",""tract_name""])",data preprocessing,,
file336,13,"#Expanding timeperiod to day, month and year
 df3[""time""] = pd.to_datetime(df3[""timeperiod""], format= '%Y%m%d', errors = ""coerce"")
 df3 =df3.drop(""timeperiod"", axis = 1)
 df3[""month""] = df3[""time""].dt.month
 df3[""day""] = df3[""time""].dt.day
 str_col = [""tract_name"", 'top_category_employer1', 'top_category_employer2',
  'top_category_employer3',""time""]
 df3[""uu_id_enc""] = df3[""uu_id""].astype('category').cat.codes
 #Encoding Strings
 for col in str_col :
  df3[col] = df3[col].astype('category').cat.codes
 int_col = list(set(df3.columns) -set(str_col)-set([""uu_id""]))
 #dropping and replacing Nan, NaT 
 df3 =df3.astype(str)
 df3 = df3[df3[""uu_id""] != np.nan]
 df3 = df3.replace('NaT', np.nan)
 df3 = df3.replace(str(np.nan),str(0))
 #df3 = df3.dropna(subset = [""uu_id""], inplace=True)
 for col in df3.columns:
  df3[col] = df3[col].str.replace(str(""<NA>""),""0"")
 for col in int_col:
  df3[col] =df3[col].astype(float)
 df = df3.copy()
 df3.uu_id.nunique()",data preprocessing,data exploration,
file336,14,"df3 = df.copy()
 df3.columns",,,
file336,15,"races = ['race_black', 'race_other', 'race_white']
 #for j in races:
 g = (sum([df3[k] for k in races]))
 df3[""normalized_""+ str(""races"") ] = g
 df3[""normalized_""+ str(""races"") ].unique()",data preprocessing,data exploration,
file336,16,"ed = ['edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs']
 for j in ed:
  df3[""normalized_""+ str(""education"") ] = sum([df3[k] for k in ed])",data preprocessing,,
file336,17,"gen = [ 'gender_female', 'gender_male']
 for j in gen:
  df3[""normalized_""+ str(""gender"") ] = sum([df3[k] for k in gen])",data preprocessing,,
file336,18,"df3[""sum""] = (df3[""normalized_""+ str(""education"") ]+df3[""normalized_""+ str(""races"") ]+df3[""normalized_""+ str(""gender"") ])",data preprocessing,,
file336,19,"def opt_features(x):
  df3[""opt_var""]= (x[3]*df3[""normalized_""+ str(""education"") ]**x[0]+x[4]*df3[""normalized_""+ str(""races"") ]**x[1]+x[5]*df3[""normalized_""+ str(""gender"") ]**x[2]+x[6])
  df4 =df3.corr()
  corr = -df4.loc[""opt_var"",""total_claims""]
  return corr
 import scipy 
 op = scipy.optimize.minimize(opt_features, [1]*7,method=""SLSQP"") 
 op",data preprocessing,data exploration,
file336,20,"def opt_features2(x):
  #corr = -df4.loc[""opt_var1"",""total_claims""]
  df3[""opt_var2""]=x[0]*df3[""opt_var""]+x[1]*df3[""week_number""]
  df3[""error""]= df3[""total_claims""]-df3[""opt_var2""]
  return abs(df3[""error""]).sum()
 cons = ({'type': 'ineq',
  'fun': lambda x: opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: -opt_features2(x)
  },
  {'type': 'ineq',
  'fun': lambda x: x[1]-1
  })
 op2 = scipy.optimize.minimize(opt_features2, [1]*7,method=""SLSQP"", constraints = cons) 
 op2",data preprocessing,data exploration,
file336,21,"df3[""opt_var2""]=df3[""opt_var""]+df3[""opt_var2""]",data preprocessing,,
file336,22,"df4 = df3.corr()
 df4",data preprocessing,data exploration,
file336,23,"df4[abs(df4.total_claims)>0.5]
 #df4.loc[""ult"",""total_claims""]",data preprocessing,,
file336,24,"features =df4[abs(df4.total_claims)>0.6].index
 features",data preprocessing,data exploration,
file336,25,"import itertools
 colors = itertools.cycle(sns.color_palette(""tab10""))
 for feature in features:
  fig, ax = plt.subplots(figsize=(12,8)) 
  c = next(colors)
  print(feature, c)
  #sns.scatterplot(x= feature, y = ""week_number"", data =df3)
  sns.lineplot(y= ""total_claims"", x = ""week_number"", data =df3, color = ""black"", label = ""total_claims"", linestyle= ""--"")
  sns.lineplot(y= feature, x = ""week_number"", data =df3, color = c, label = feature)
  plt.show()",result visualization,helper functions,
file336,26,"from sklearn.model_selection import train_test_split 
 from sklearn.preprocessing import StandardScaler
 from sklearn.ensemble import RandomForestRegressor as rg
 sc = StandardScaler()",helper functions,modeling,
file336,27,"def final_pred(t):
  Y = np.array(t[""total_claims""])
  X = np.array(t[[k for k in features]])
  X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.01, random_state =5)
  X_train = sc.fit_transform(X_train)
  X_test = sc.transform(X_test)
  rf = rg(n_estimators=1000, random_state=2)
  rf.fit(X_train, Y_train)
  return rf",modeling,data preprocessing,
file336,28,"temp = df3[[k for k in features]]
 temp",data preprocessing,data exploration,
file336,29,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file336,30,"query_job3 = bigquery_client.query(query)
 test_df = query_job3.to_dataframe()
 #test_df[""time""] = pd.datetime(2022-09-24, format = %Y%m%d)
 test_df[""day""] = 6
 test_df[""month""]=11",load data,data preprocessing,
file336,31,test_df.drop_duplicates(),data exploration,,
file336,32,"""""""extras = set(test_df.uu_id.unique())-set(submission_prediction_output.uu_id.unique())
 extra = [df.loc[df.uu_id==k][""uu_id_enc""].values[0] for k in extras]
 extra""""""",comment only,,
file336,33,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id"").drop([""uu_id""], axis=1).drop_duplicates()
 feature_test_pred = test_df.copy()
 test_df",data preprocessing,data exploration,
file336,34,"""""""test_df1=test_df.copy()
 for col in ['total_claims', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in extra:
  #test_df.loc[test_df.uu_id_enc==k,col] =0
  temp=df[df.uu_id_enc == k]
  temp[""average_wage""]=-9999
  temp =temp.replace("""",0)
  feature_test_pred = np.array(test_df1[test_df1.uu_id_enc==k])
  #print(k, temp)
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  idk = float(val.predict(np.array(feature_test_pred))[0])
  print(idk)
  test_df.loc[test_df.uu_id_enc==k,col].value = idk
 test_df""""""",,,
file336,35,df3,data exploration,,
file336,36,"from statsmodels.tsa.stattools import adfuller
 adfuller(df3[""total_claims""])",helper functions,modeling,
file336,37,"from pandas.plotting import autocorrelation_plot
 autocorrelation_plot(df3[""total_claims""])",helper functions,modeling,
file336,38,"df3 =df3.dropna()
 for col in features:
  print(col)
  test_df[col]=0
  val = (""rf"")+""_""+str(col) 
  for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  val = time_pred(temp, col)
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))
  test_df.loc[test_df.uu_id_enc==k,col] = val.predict(np.array(feature_test_pred))[0]
 test_df",data exploration,data preprocessing,
file336,39,"#df3 =df3.dropna()
 li=[]
 import statsmodels.api as sm
 import statistics  
 for k in df3.uu_id_enc.unique():
  temp=df3[df3.uu_id_enc == k]
  temp = temp.sort_values(""week_number"")
  df3_ = pd.concat([temp,test_df])
  df3_=df3_[[""week_number"",""uu_id_enc"",""total_claims""]].drop_duplicates()
  #df3_[""predicted_total_claims""]=0
  mod = sm.tsa.statespace.SARIMAX(np.array(temp[""total_claims""]),
  order=(1, 1, 1),
  seasonal_order=(1, 1, 1, 2),
  enforce_stationarity=False,
  enforce_invertibility=False)
  try: 
  results = mod.fit()
  except IndexError:
  g = df3[df3.uu_id_enc==k]
  val= g[g.week_number==39]['total_claims'].mean()
  pred = results.get_prediction(start=40, end =40, dynamic=False)
  val = (pred.predicted_mean)
  test_df.loc[test_df.uu_id_enc==k,""total_claims""] = val
  li.append(pred.predicted_mean)
  pred_ci = pred.conf_int()
  #print(test_df.loc[test_df.uu_id_enc==k,col],val.predict(np.array(feature_test_pred)))",data preprocessing,,
file336,40,test_df,,,
file336,41,"test_df = pd.merge(test_df, df3[[""uu_id"",""uu_id_enc""]], on=""uu_id_enc"")
 submission_prediction_output= test_df[[""uu_id"",""week_number"",""total_claims""]].drop_duplicates()
 submission_prediction_output[""total_claims""] = submission_prediction_output[""total_claims""].fillna(0)
 for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""].value=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")
 submission_prediction_output",,,
file336,42,"for k in submission_prediction_output.loc[submission_prediction_output[""total_claims""]<=0][""uu_id""]:
  submission_prediction_output.loc[submission_prediction_output[""uu_id""]==k,""total_claims""]=df3[df3.uu_id==k][""total_claims""].mean()
 submission_prediction_output[submission_prediction_output.total_claims<=0]
 submission_prediction_output.to_csv(""submission_prediction_output.csv"")",data preprocessing,,
file337,0,"y_pred = model.predict(X_test)
 y_pred.shape()",prediction,data exploration,
file337,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file337,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns",helper functions,,
file337,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file337,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file337,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data exploration,
file337,6,"# Getting the no of columns to understand and choose the required ones
 data.shape",data exploration,,
file337,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data preprocessing,
file337,8,"data['total_claims']
 print(""min"",data['total_claims'].min(),""max"",data['total_claims'].max())
 print(data.columns)",data exploration,,
file337,9,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,data exploration,
file337,10,"print(data.columns)
 data=data[[""uu_id"",""week_number"",""total_claims"",""edu_hs_grad_equiv"",""edu_post_hs"",""gender_female"",""gender_male"",""race_black"",""race_white""]]
 data.columns
 test=data",data preprocessing,,
file337,11,"test=test.fillna(0,axis=0)
 test[""week_number""]= test[""week_number""].astype(""int"")
 test",data preprocessing,data exploration,
file337,12,"from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 model = LinearRegression()
 X_train=test.drop([""uu_id""],axis=1)
 X_train=X_train.loc[X_train[""week_number""]<=33]
 y=X_train[""total_claims""]
 X_test=test.drop([""uu_id""],axis=1)
 X_test = X_test.loc[X_test[""week_number""]==34]
 temp=test.loc[test[""week_number""]==34]",data preprocessing,,
file337,13,"model.fit(X_train,y)",modeling,,
file337,14,"X_test = X_test.loc[X_test[""week_number""]==34]
 X_test.head(10)
 temp",data exploration,data preprocessing,
file338,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file338,1,"get_ipython().system('pip install keras')
 get_ipython().system('pip install tensorflow')",helper functions,,
file338,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file338,3,"import os
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools",helper functions,,
file338,4,BIGQUERY_PROJECT = 'ironhacks-data',load data,,
file338,5,bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file338,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""",load data,,
file338,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,,
file338,8,"import pandas as pd
 import numpy as np
 combined = pd.merge(data,wagedata,on=['uu_id','countyfips', 'tract','tract_name'],how = 'left')",helper functions,,
file338,9,wagedata[wagedata['uu_id']=='a5c6dcff737e183f7931b472f10c3235'],data exploration,,
file338,10,"combined['average_wage'].fillna(combined['average_wage'].mean(),inplace = True)
 ## Using the mean for the missing wage",data preprocessing,,
file338,11,"firstgroup = ['edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs','edu_unknown']
 secondgroup = ['gender_female','gender_male','gender_na']
 thirdgroup = ['race_amerindian','race_asian','race_black','race_noanswer','race_hawaiiannative','race_other','race_white']
 ## columns to impute",data preprocessing,,
file338,12,"complete = combined.dropna(axis = 0)
 #using the complete rows to estimate the ratio of each catagory occupying the claims
 prob = {}
 for i in firstgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()
 for i in secondgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()
 for i in thirdgroup:
  prob[i] = (complete[i]/complete['total_claims']).mean()",data preprocessing,,
file338,13,prediction_list['total_claims'] = 0,data preprocessing,,
file338,14,prediction_list,data exploration,,
file338,15,"prediction_list.loc[:,['uu_id','total_claims','week_number']].to_csv('submission_prediction_output(2).csv',index = False)",data exploration,,
file338,16,"approximatedimputation(combined)
 combined.to_csv('submission_prediction_output(1).csv',index = False)",save results,,
file338,17,"def approximatedimputation(data):
  firstgroup = ['edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs','edu_unknown']
  secondgroup = ['gender_female','gender_male','gender_na']
  thirdgroup = ['race_amerindian','race_asian','race_black','race_noanswer','race_hawaiiannative','race_other','race_white']
  for i in firstgroup:
  for j in range(data.shape[0]):
  # if is nan, we use the approximation method to try to impute
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])
  for i in secondgroup:
  for j in range(data.shape[0]):
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])
  for i in thirdgroup:
  for j in range(data.shape[0]):
  if pd.isna(data[i].values[j]):
  data[i].values[j] = int(prob[i]*data['total_claims'].values[j])",data preprocessing,,
file339,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",,,
file339,1,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data exploration,
file339,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file339,3,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file339,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file339,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data exploration,
file339,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file339,7,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head(4)",load data,data exploration,
file339,8,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,data preprocessing,
file339,9,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file340,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file340,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 un_data = querydb(query)",load data,,
file340,2,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file340,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data exploration,
file340,4,"get_ipython().run_cell_magic('capture', '', '%logstop\n%logstart -t -r -q ipython_command_log.py global\n')",helper functions,,
file340,5,"import os
 from datetime import datetime
 import IPython.core.history as history",helper functions,,
file340,6,"ha = history.HistoryAccessor()
 ha_tail = ha.get_tail(1)
 ha_cmd = next(ha_tail)
 session_id = str(ha_cmd[0])
 command_id = str(ha_cmd[1])
 timestamp = datetime.utcnow().isoformat()
 history_line = ','.join([session_id, command_id, timestamp]) + '\n'
 logfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')
 logfile.write(history_line)
 logfile.close()",save results,,
file340,7,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file340,8,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file340,9,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file340,10,"query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 wage_data.head()",load data,data exploration,
file341,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file341,1,"import numpy as np # linear algebra
 import pandas as pd
 from xgboost import plot_importance, plot_tree
 import xgboost as xgb
 import matplotlib.pyplot as plt
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import mean_squared_error, mean_absolute_error
 import seaborn as sns
 sns.set_style('whitegrid')
 plt.rcParams['figure.figsize']=(20,10) # for graphs styling
 plt.style.use('tableau-colorblind10')",helper functions,,
file341,2,get_ipython().system('pip install xgboost'),helper functions,,
file341,3,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,load data,
file341,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file341,5,"query_job = bigquery_client.query(query)
 df = query_job.to_dataframe()
 df.head()",load data,,
file342,0,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 df = Final_df.sort_values('week_number')
 df",data exploration,data preprocessing,
file342,1,"Final_df.columns
 Final_df.head()",data exploration,,
file342,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file342,3,"#LIBRARIES
 import os
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import matplotlib.pyplot as plt
 from pandas import Series, DataFrame
 import warnings 
 warnings.filterwarnings('ignore')
 from scipy import stats  
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.impute import SimpleImputer
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file342,4,"from neuralprophet import NeuralProphet
 import pickle",helper functions,,
file342,5,get_ipython().system('pip install db-dtypes'),helper functions,,
file342,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file342,7,"# Let's look at the unemployment_data table
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file342,8,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data exploration,
file342,9,"#Number of rows in unemployment_data
 len(unemployment_data.index)",data exploration,,
file342,10,Merged_unemployment_wage.head(3),data exploration,,
file342,11,"#Number of rows in wage_data
 len(wage_data.index)",data exploration,,
file342,12,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 prediction_list.head(3)",load data,,
file342,13,"#Let's merge the unemployment_data and wage_data
 # We will merge only cols uu_id and average_wage from wage_data into unemployment_data
 Merged_unemployment_wage = unemployment_data.merge(wage_data[['uu_id', 'average_wage']])
 Merged_unemployment_wage.columns",data exploration,data preprocessing,
file342,14,"#checking for duplicates rows
 print(Merged_unemployment_wage.duplicated().sum())",data exploration,,
file342,15,"#Drop duplicate rows
 drop_duplicates = Merged_unemployment_wage.drop_duplicates()",data preprocessing,,
file342,16,print(drop_duplicates.duplicated().sum()),data exploration,,
file342,17,"#Checking for nulls 
 drop_duplicates.isnull().sum()",data exploration,data preprocessing,
file342,18,"#Replaced the remaining null values with 0s
 cleaned_df = drop_duplicates.fillna(0)",data preprocessing,,
file342,19,cleaned_df.isnull().sum(),data exploration,data preprocessing,
file342,20,"# Is there a column with all 0's?
 (cleaned_df == 0).all()",data exploration,data preprocessing,
file342,21,"#Let's drop the column with no data
 Final_df=cleaned_df.drop(['race_hawaiiannative'], axis=1)",data preprocessing,,
file342,22,Final_df.shape,data exploration,,
file342,23,Final_df.dtypes,data exploration,,
file342,24,Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).sum().add_prefix('Sum_of_'),data preprocessing,,
file342,25,"plt.rcParams[""figure.figsize""] = (12,4)
 ax = Claims_by_week.plot(title='Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,,
file342,26,"Mean_Claims_by_week = Final_df[['total_claims']].groupby(cleaned_df.week_number).mean().add_prefix('Mean_of_')
 plt.rcParams[""figure.figsize""] = (12,4)
 ax = Mean_Claims_by_week.plot(title='Mean Of Total Claims By Week').set(ylabel='Total Claims', xlabel='Weeks')
 plt.grid(True)",result visualization,,
file342,27,"matrix = Final_df.corr().round(2)
 sns.heatmap(matrix, annot=True)
 plt.show()",result visualization,,
file342,28,"#Top 3 UUID with the max number of claims
 Top_10_claimers = Final_df.groupby(['uu_id'])['total_claims'].sum().sort_values(ascending=False)
 Top_10_claimers.head(3)",data exploration,data preprocessing,
file342,29,"#UUIDs with the least number of claims
 Top_10_claimers.tail(3)",data exploration,,
file342,30,"#Do people with a certain degree file more claims 
 y=[Final_df.edu_8th_or_less.sum(),
  Final_df.edu_grades_9_11.sum(),
  Final_df.edu_hs_grad_equiv.sum(),
  Final_df.edu_post_hs.sum(),
  Final_df.edu_unknown.sum()
  ]",data preprocessing,,
file342,31,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(17, 7))
 plt.title(""Total Claims by Education"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['8th grade or less education','9 through 11','high school diploma or equivalent','completed a degree beyond high school','Education Unknown'],color='black')",result visualization,,
file342,32,"# To display sum values
 for index, value in enumerate(y):
  plt.text(value, index,
  str(value))
 plt.show()",result visualization,,
file342,33,"#Do people with a certain gender file more claims 
 y=[Final_df.race_amerindian.sum(),
  Final_df.race_asian.sum(),
  Final_df.race_black.sum(),
  Final_df.race_noanswer.sum(),
  Final_df.race_other.sum(),
  Final_df.race_white.sum()
  ]",data preprocessing,,
file342,34,"n=len(y)
 x = np.arange(n)
 plt.subplots(figsize =(12,3))
 plt.title(""Total Claims by Gender"", fontweight ='bold', fontsize = 15)
 plt.barh(x,y, height=0.55,color='lightblue', edgecolor='black',linewidth=2)
 plt.xlabel('Count')
 plt.yticks(x,['Amerindian', 'Asian' , 'Black', 'No answer', 'Other', 'White'],color='black')",result visualization,,
file342,35,"pd.set_option('float_format', '{:f}'.format)
 Final_df[['average_wage']].describe()",data exploration,helper functions,
file342,36,"sns.relplot(x =""edu_8th_or_less"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_grades_9_11"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_hs_grad_equiv"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_post_hs"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""edu_unknown"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer1"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer2"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""top_category_employer3"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_female"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_male"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""gender_na"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_amerindian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_asian"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_black"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_noanswer"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_other"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""race_white"", y =""total_claims"",
  data = Final_df);
 sns.relplot(x =""average_wage"", y =""total_claims"",
  data = Final_df);",result visualization,,
file342,37,"Final_df.columns
 Final_df.head()",data exploration,,
file342,38,from sklearn.preprocessing import OrdinalEncoder,helper functions,,
file342,39,"ord_enc = OrdinalEncoder()
 df[""uu_id_code""] = ord_enc.fit_transform(df[[""uu_id""]])
 df[[""uu_id"", ""uu_id_code""]].head()",data preprocessing,data exploration,
file342,40,"ord_enc = OrdinalEncoder()
 Final_df[""uu_id_code""] = ord_enc.fit_transform(Final_df[[""uu_id""]])
 df = Final_df[[""uu_id"", ""uu_id_code"", ""timeperiod"",""week_number"",""total_claims""]]",data preprocessing,,
file342,41,"#Final_df.loc[Merged_unemployment_wage['uu_id'] == 'f013068de98db1470bd986137a0c6d23'].sort_values('week_number')
 df.dtypes",data exploration,,
file342,42,"df['timeperiod'] = pd.to_datetime(df['timeperiod'],
  format='%Y%m%d')",data preprocessing,,
file342,43,df.dtypes,data exploration,,
file342,44,"plt.plot(df['timeperiod'], df['total_claims'])
 plt.show()",result visualization,,
file342,45,"data = df[['timeperiod', 'total_claims']] 
 data.dropna(inplace=True)
 data.columns = ['ds', 'y'] 
 data.head()",data exploration,data preprocessing,
file343,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file343,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file343,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file343,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file343,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,,
file343,5,data.info(),data exploration,,
file343,6,"data[""top_category_employer2""].value_counts()",data exploration,,
file343,7,"data.loc[data[""uu_id""] == ""bd5b040eae3010ce09da8b176ed5aee0""]",data exploration,,
file343,8,"data[""edu_8th_or_less""].value_counts()",data exploration,,
file343,9,"data[""top_category_employer2""].map(lambda x: '32' if x =='31-33')",data exploration,,
file343,10,"data[""top_category_employer2""] = data[""top_category_employer2""].map(lambda x: '32' if x =='31-33' else x)",data preprocessing,,
file343,11,"df[""top_category_employer1""].astype(int)",data preprocessing,,
file343,12,"data[""race_black""].value_counts()",data exploration,,
file343,13,"data['edu_8th_or_less'].replace(np.NaN, data['edu_8th_or_less'].mean())",data exploration,data preprocessing,
file343,14,data.fillna('0'),data exploration,data preprocessing,
file343,15,data = data.fillna(0),data preprocessing,,
file343,16,data.isnull(),data exploration,,
file343,17,data['top_category_employer2'].astype(float),data preprocessing,data exploration,
file343,18,"data[""test""] = data.groupby(by=[""uu_id""]).sum()",data preprocessing,,
file343,19,"data.groupby(by=[""uu_id""]).mean()",data preprocessing,data exploration,
file343,20,"predict = data.groupby(by=[""uu_id""]).mean()
 predict",data preprocessing,data exploration,
file343,21,data['top_category_employer1'] = data['top_category_employer1'].astype(float),data preprocessing,,
file343,22,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import numpy as np",helper functions,,
file343,23,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file343,24,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file343,25,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data exploration,
file343,26,data.info(),data exploration,,
file343,27,get_ipython().system('pip install db-dtypes'),helper functions,,
file343,28,"X_train = data[""""]",data preprocessing,,
file343,29,"X_train = data[""edu_8th_or_less"", ""edu_grades_9_11"", ""edu_hs_grad_equiv"", ""gender_female"", gender_male, ]X_train = data[""edu_8th_or_less"", ""edu_grades_9_11"", ""edu_hs_grad_equiv"", ""gender_female"", ""gender_male"", ""race_amerindian"", ""race_asian"", ""race_black"", ""race_noanswer"", ""race_hawaiiannative"", ""race_other"", ""race_white""]",data preprocessing,,
file343,30,"X_train = data.iloc[:, 7:24]",data preprocessing,,
file343,31,"y_train = data[""total_claims""]",data preprocessing,,
file343,32,from sklearn.linear_model import LinearRegression,helper functions,,
file343,33,"model = LinearRegression().fit(X_train, y_train)",modeling,,
file343,34,"model.predict(X_test = predict.iloc[:, 7:24])",prediction,,
file343,35,model.predict(X_test),prediction,,
file343,36,"X_test = predict.iloc[:, 7:24]",data preprocessing,,
file343,37,X_test,data exploration,,
file344,0,"get_ipython().run_line_magic('history', '-g')",helper functions,,
file345,0,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install plotly')",helper functions,,
file345,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file345,2,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from scipy import stats
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file345,3,"import numpy as np
 from numpy import isnan
 from matplotlib import pyplot",helper functions,,
file345,4,"import seaborn as sns
 import matplotlib 
 import matplotlib.pyplot as plt
 import plotly 
 import plotly.express as px",helper functions,,
file345,5,import pandas as pd,helper functions,,
file345,6,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file345,7,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file345,8,"query_job = bigquery_client.query(query)
 unemploy = query_job.to_dataframe()
 unemploy.head()",load data,data exploration,
file345,9,"query_wage = bigquery_client.query(wage)
 wages = query_wage.to_dataframe()
 wages.head()",load data,data exploration,
file345,10,wages.info(),data exploration,,
file345,11,unemploy.info(),data exploration,,
file345,12,unemploy.isnull().sum(),data exploration,data preprocessing,
file345,13,"unemploy=unemploy.sort_values('week_number', ascending=True)
 unemploy.reset_index(inplace=True)
 unemploy",data exploration,data preprocessing,
file345,14,"sns.heatmap(unemploy.isnull(),cbar=False)",result visualization,,
file345,15,"miss=unemploy.isnull()
 total=unemploy.count()
 total",data exploration,data preprocessing,
file345,16,miss.sum(),data exploration,data preprocessing,
file345,17,miss.sum()/len(unemploy),data exploration,data preprocessing,
file345,18,"map_1 = unemploy.corr(method ='spearman')
 sns.heatmap(map_1)",result visualization,data preprocessing,
file345,19,"unemploy[""race_hawaiiannative""].unique()",data exploration,,
file345,20,"df1=unemploy.copy()
 df1=df1.drop(columns=""race_hawaiiannative"")",data preprocessing,,
file345,21,"df=unemploy[['top_category_employer1',""top_category_employer2"",""top_category_employer3"",'uu_id',""tract_name"",""timeperiod"",""tract"",""countyfips""]]",data preprocessing,,
file345,22,df,data exploration,,
file345,23,"impute1=KNNImputer()
 impute1.fit(df1)
 unemploy1=pd.DataFrame(impute1.fit_transform(df1),columns = df1.columns)",modeling,data preprocessing,
file345,24,unemploy1,data exploration,,
file345,25,"trial=pd.merge(df, unemploy1, left_index=True, right_index=True)",data preprocessing,,
file345,26,trial.head(),data exploration,,
file345,27,trial.describe(),data exploration,,
file345,28,"sns.heatmap(trial.corr(method=""spearman""))",result visualization,,
file345,29,trial1=trial.copy(),data preprocessing,,
file345,30,"scaler = StandardScaler()
 scaled=pd.DataFrame(scaler.fit_transform(unemploy1),columns = unemploy1.columns)",data preprocessing,,
file345,31,scaled,data exploration,,
file345,32,"sns.displot(data=scaled,x=""total_claims"",kind=""kde"")",result visualization,,
file345,33,"claim = np.array(scaled[""total_claims""])
 sqrt_claim = np.sqrt(claim)
 log_claim = np.log(claim)
 f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
 #f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
 f1.hist(claim, 30)
 f2.hist(sqrt_claim, 30)
 f3.hist(log_claim, 30)",result visualization,data preprocessing,
file345,34,"stats.probplot(claim, plot=f4)
 stats.probplot(sqrt_claim, plot=f5)
 stats.probplot(log_claim, plot=f6)
 plt.show
 stats.shapiro(claim)[1], stats.shapiro(sqrt_claim)[1], stats.shapiro(log_claim)[1]",result visualization,,
file345,35,shapiro(sqrt_claim),data exploration,,
file345,36,print(skew(sqrt_claim)),data exploration,,
file345,37,shapiro(unemploy1.total_claims),data exploration,,
file345,38,"f3.hist(log_claim, 30)",result visualization,,
file345,39,import pylab,helper functions,,
file345,40,"stats.probplot(log_claim,dist=""norm"",plot=pylab)
 pylab.show()",result visualization,,
file345,41,sns.distplot(une),result visualization,,
file345,42,"sns.boxplot(x=scale_logs[""edu_grades_9_11""])",result visualization,,
file345,43,unemploy_logs = np.log(unemploy1),data preprocessing,,
file345,44,"unemp_cube=np.power((unemploy1),1/3)",data preprocessing,,
file345,45,"statistics,pvalue = jarque_bera(unemp_cube)",data preprocessing,,
file345,46,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
 if pvalue>0.05:
  print(""Probably Normal"")
 else:
  print(""Probably not Normal"")",data exploration,data preprocessing,
file345,47,from scipy.stats import jarque_bera,helper functions,,
file345,48,"statistics,pvalue = jarque_bera(unemploy1[""edu_grades_9_11""])",data preprocessing,,
file345,49,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from scipy import stats
 from scipy.stats import boxcox
 from scipy.stats import jarque_bera
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file345,50,"import numpy as np
 from numpy import isnan
 from matplotlib import pyplot",helper functions,,
file345,51,"import seaborn as sns
 import matplotlib 
 import matplotlib.pyplot as plt
 import plotly 
 import plotly.express as px",helper functions,,
file345,52,import pandas as pd,helper functions,,
file345,53,"transformed_data, best_lambda = boxcox(unemploy1)
 sns.distpolt(transformed_data,)",result visualization,data preprocessing,
file345,54,"claim = np.array(transformed_data)
 sqrt_claim = np.sqrt(transformed_data)
 log_claim = np.logtransformed_data)
 f, ((f1, f2, f3), (f4, f5, f6)) = plt.subplots(2, 3)
 #f, ((f1, f2), (f4, f5)) = plt.subplots(2, 2)
 f1.hist(claim, 30)
 f2.hist(sqrt_claim, 30)
 f3.hist(log_claim, 30)",result visualization,data preprocessing,
file345,55,"stats.probplot(claim, plot=f4)
 stats.probplot(sqrt_claim, plot=f5)
 stats.probplot(log_claim, plot=f6)
 plt.show
 stats.shapiro(claim)[1], stats.shapiro(sqrt_claim)[1], stats.shapiro(log_claim)[1]",result visualization,,
file345,56,"statistics,pvalue = jarque_bera(transformed_data)",data preprocessing,,
file345,57,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
 sns.distplot(transformed_data)
 ed_9_11=boxcox(unemploy[""edu_grades_9_11""])",result visualization,,
file345,58,"transformed_data, best_lambda = boxcox(unemploy1[""total_claims""])
 sns.distplot(unemploy1[""race_white""])",result visualization,data preprocessing,
file345,59,"from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.impute import KNNImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.preprocessing import MinMaxScaler",helper functions,,
file345,60,"from scipy import stats
 from scipy.stats import boxcox
 from scipy.stats import jarque_bera
 from scipy.stats import shapiro
 from scipy.stats import skew",helper functions,,
file345,61,"scaler= MinMaxScaler()
 scaler.fit(unemploy1)
 # transform the test test
 scaled1 = scaler.transform(unemploy1)",data preprocessing,modeling,
file345,62,scaler,data exploration,,
file345,63,scaled1,data exploration,,
file345,64,"scaler= MinMaxScaler()
 # transform the test test
 scaled1 = pd.DataFrame(scaler.transform_fit(unemploy1),columns=unemploy1.columns)",data preprocessing,,
file345,65,"sns.distplot(scaled1[""total_claims""])",result visualization,,
file345,66,"unemploy1[""race_asian""].unique()",data exploration,,
file345,67,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1),columns=unemploy1)",data preprocessing,,
file345,68,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_quiv""]))",data preprocessing,,
file345,69,"statistics,pvalue = jarque_bera(unemploy_ish)",data preprocessing,,
file345,70,"print('statistics=%.3f, p=%.3f\n' %(statistics, pvalue))
 if pvalue>0.05:
  print(""Probably Normal"")
 else:
  print(""Probably not Normal"")",data exploration,data preprocessing,
file345,71,stats.probplot(unemploy_ish),data exploration,,
file345,72,unemploy_ish,data exploration,,
file345,73,"stats.probplot(unemploy_ish,dist=""norm"",plot=pylab)
 pylab.show()",result visualization,,
file345,74,"unemploy_ish1=np.arcsinh(unemploy1[""edu_hs_grad_equiv""])
 sns.distplot(unemploy_ish)",result visualization,data preprocessing,
file345,75,"unemploy_ish=pd.DataFrame(np.arcsinh(unemploy1[""edu_hs_grad_equiv""]))
 unemployed_ish=boxcox.fit_transform(unemploy_ish)
 sns.distplot(unemployed_ish)",result visualization,data preprocessing,
file345,76,"unemploy1[""gender_male""]",data exploration,,
file345,77,unemploy1,data exploration,,
file345,78,"Q1 = np.percentile(unemploy1[""total_claims""], 25,
  interpolation = 'midpoint')",data preprocessing,,
file345,79,"print(""Old Shape: "", unemploy1.shape)",data exploration,,
file345,80,"# Upper bound
 upper = np.where(unemploy1[""total_claims""]>= (Q3+1.5*IQR))
 # Lower bound
 lower = np.where(unemploy1[""total_claims""]<= (Q1-1.5*IQR))",data preprocessing,,
file345,81,"'' Removing the Outliers '''
 unemploy1.drop(upper[0], inplace = True)
 unemploy1.drop(lower[0], inplace = True)",data preprocessing,,
file345,82,unemploy1.shape,data exploration,,
file345,83,sns.distplot(unemploy1),result visualization,,
file345,84,"sns.distplot(unemploy1[""gender_male""])",result visualization,,
file345,85,"sns.distplot(unemploy1[""race_white""])",result visualization,,
file345,86,"sns.distplot(unemploy1[""edu_hs_grad_equiv""])",result visualization,,
file345,87,"sns.heatmap(unemploy1.corr(method=""spearman""))",result visualization,,
file345,88,"trial[""uuid""].unique()",data exploration,,
file346,0,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file346,1,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data exploration,
file346,2,"from google.cloud import bigquery
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file346,3,"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jovyan/.config/gcloud/application_default_credentials.json'
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file347,0,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file347,1,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file347,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file347,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file347,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(3)",load data,data exploration,
file347,5,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file347,6,from google.cloud import bigquery,helper functions,,
file347,7,"# Construct a BigQuery client object.
 client = bigquery.Client()",data preprocessing,,
file347,8,dataset_id = 'ironhacks-data.ironhacks_competition',data preprocessing,,
file347,9,tables = client.list_tables() # Make an API request.,data preprocessing,,
file347,10,"print(""Tables contained in '{}':"".format(dataset_id))
 for table in tables:
  print(""{}.{}.{}"".format(table.project, table.dataset_id, table.table_id))",data exploration,,
file348,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file348,1,"import pandas as pd
 import numpy as np
 from google.cloud import bigquery",helper functions,,
file348,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file348,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id='e201385d37b5f6eea30f6d6d4106dc6f'
 """"""",load data,,
file348,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,,
file348,5,unemployment_data.shape,data exploration,,
file348,6,unemployment_data.columns,data exploration,,
file348,7,"unemployment_data.drop(['uu_id', 'countyfips', 'tract',
  'tract_name', 'edu_8th_or_less', 'edu_grades_9_11',
  'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',
  'top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',
  'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',
  'race_hawaiiannative', 'race_other', 'race_white'], axis=1, inplace=True)",data preprocessing,,
file348,8,"unemployment_data.drop_duplicates(inplace=True)
 unemployment_data.sort_values(['week_number'])",data preprocessing,,
file348,9,"def add_missing_weeks(df):
  # Fill in missing weeks by taking the ceil of the average of prev and next
  for week in range(1, 37):",data preprocessing,,
file348,10,"unemployment_data['year'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[:4])
 unemployment_data['month'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[4:6])
 unemployment_data['day'] = unemployment_data['timeperiod'].apply(lambda x: str(x)[6:])",data preprocessing,,
file348,11,unemployment_data['ds'] = pd.DatetimeIndex(unemployment_data['year'] + '-' + unemployment_data['month'] + '-' + unemployment_data['day']),data preprocessing,,
file348,12,"unemployment_data.drop(['timeperiod', 'year', 'month', 'day', 'week_number'], axis=1, inplace=True)
 unemployment_data.columns = ['y', 'ds']",data preprocessing,,
file348,13,"unemployment_data.sort_values(['ds'], inplace=True)",data preprocessing,,
file348,14,from prophet import Prophet,helper functions,,
file348,15,ud = unemployment_data,data preprocessing,,
file348,16,"threshold_date = pd.to_datetime('2022-05-14')
 mask = ud['ds'] < threshold_date",data preprocessing,,
file348,17,"# Split the data and select `ds` and `y` columns.
 ud_train = ud[mask][['ds', 'y']]
 ud_test = ud[~mask][['ds', 'y']]",data preprocessing,,
file348,18,ud_train,data exploration,,
file348,19,"m = Prophet(weekly_seasonality=False,
  daily_seasonality=False,
  interval_width=0.95, 
  mcmc_samples = 500)",modeling,,
file348,20,m.fit(ud_train),modeling,,
file348,21,"future = m.make_future_dataframe(periods=20, freq='W')",data preprocessing,,
file348,22,forecast = m.predict(df=future),prediction,,
file348,23,"m.fit(ud_train, show_console=True)",modeling,,
file349,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file349,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file349,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file349,3,"# get data information
 query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 query2 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query3 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file349,4,"query_job1 = bigquery_client.query(query1)
 query_job2 = bigquery_client.query(query2)
 query_job3 = bigquery_client.query(query3)",load data,,
file349,5,"unemployment = query_job1.to_dataframe()
 wage = query_job2.to_dataframe()
 pred = query_job3.to_dataframe()",data preprocessing,,
file349,6,"# combine all data
 query4 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` as unem
 inner join `ironhacks-data.ironhacks_competition.wage_data` as wage
 on wage.uu_id = unem.uu_id
 """"""
 query_job4 = bigquery_client.query(query4)
 data = query_job4.to_dataframe()",load data,data preprocessing,
file349,7,"# drop duplicate columns
 dropcol = ['uu_id_1','countyfips_1','tract_1','tract_name_1']
 for col in dropcol:
  data = data.drop(col, axis=1)",data preprocessing,,
file349,8,data.head(),data exploration,,
file349,9,data1 = data.dropna(),data preprocessing,,
file349,10,data1,data exploration,,
file350,0,"import os
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",helper functions,,
file350,1,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 # QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemp_data = query_job.to_dataframe()
 unemp_data.head(5)",load data,data exploration,
file350,2,unemp_data.info(),data exploration,,
file350,3,wage_data.info(),data exploration,,
file350,4,unemp_data.isna().sum(),data preprocessing,,
file350,5,"set1 = set(list(unemp_data['top_category_employer1'].unique()))
 set1",data preprocessing,data exploration,
file350,6,"unemp_data['top_category_employer2'] = unemp_data['top_category_employer2'].replace('N/A',np.NaN)
 unemp_data['top_category_employer3'] = unemp_data['top_category_employer3'].replace('N/A',np.NaN)",data preprocessing,,
file350,7,unemp_data['countyfips'].unique(),data exploration,,
file350,8,(unemp_data.isna().sum()/len(unemp_data))*100,data preprocessing,data exploration,
file350,9,"unemp_data['edu_8th_or_less'] = unemp_data.groupby('countyfips')['edu_8th_or_less'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['edu_grades_9_11'] = unemp_data.groupby('countyfips')['edu_grades_9_11'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['edu_hs_grad_equiv'] = unemp_data.groupby('countyfips')['edu_hs_grad_equiv'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file350,10,"unemp_data['race_asian'] = unemp_data.groupby('countyfips')['race_asian'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_amerindian'] = unemp_data.groupby('countyfips')['race_amerindian'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_black'] = unemp_data.groupby('countyfips')['race_black'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_hawaiiannative'] = unemp_data.groupby('countyfips')['race_hawaiiannative'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_other'] = unemp_data.groupby('countyfips')['race_other'].transform(lambda x: x.fillna(int(x.mean())))
 unemp_data['race_white'] = unemp_data.groupby('countyfips')['race_white'].transform(lambda x: x.fillna(int(x.mean())))",data preprocessing,,
file350,11,"unemp_data['race_noanswer'] = unemp_data['race_noanswer'].fillna(0)
 unemp_data['gender_na'] = unemp_data['gender_na'].fillna(0)
 unemp_data['edu_unknown'] = unemp_data['edu_unknown'].fillna(0)",data preprocessing,,
file350,12,"unemp_data['top_category_employer2'] = unemp_data.groupby('countyfips')['top_category_employer2'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'unknown'))
 unemp_data['top_category_employer3'] = unemp_data.groupby('countyfips')['top_category_employer3'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'unknown'))",data preprocessing,,
file350,13,"unemp_data['gender_male'] = unemp_data['gender_male'].astype(float)
 unemp_data['gender_female'] = unemp_data['gender_female'].astype(float)
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].astype(float)",data preprocessing,,
file350,14,"unemp_data['gender_male'] = unemp_data['gender_male'].fillna(unemp_data.groupby('countyfips')['gender_male'].transform('mean'))
 unemp_data['gender_female'] = unemp_data['gender_female'].fillna(unemp_data.groupby('countyfips')['gender_female'].transform('mean'))
 unemp_data['edu_post_hs'] = unemp_data['edu_post_hs'].fillna(unemp_data.groupby('countyfips')['edu_post_hs'].transform('mean'))",data preprocessing,,
file350,15,unemp_data[unemp_data.gender_male.isnull()],data exploration,,
file350,16,wage_data.describe(),data exploration,,
file350,17,wage_data['average_wage'] = wage_data['average_wage'].fillna(wage_data.groupby('countyfips')['average_wage'].transform('mean')),data preprocessing,,
file350,18,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id)
 final_data",data preprocessing,data exploration,
file350,19,"final_data1 = final_data.drop(['timeperiod','tract','tract_name'],axis=1)",data preprocessing,,
file350,20,final_data.info(),data exploration,,
file350,21,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y'],axis=1)",data preprocessing,,
file350,22,"fin = pd.get_dummies(final_data1,columns = ['week_number','countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file350,23,fin.shape(),data exploration,,
file350,24,"X = fin.drop(['total_claims'],axis=1)
 Y = fin['total_claims']",data preprocessing,,
file350,25,"from sklearn.decomposition import PCA
 pca = PCA(n_components=50)
 principalComponents = pca.fit_transform(X)",modeling,helper functions,
file350,26,principalComponents,data exploration,,
file350,27,df_X = pd.DataFrame(principalComponents),data preprocessing,,
file350,28,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 from sklearn.feature_selection import RFE
 X_train, X_test,Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=28)",helper functions,data preprocessing,
file350,29,"lm = LinearRegression().fit(X_train,Y_train)
 prediction = model.predict(X_test)",modeling,prediction,
file350,30,"lm.score(X_test, Y_test)",evaluation,,
file350,31,"mean_squared_error(Y_train, prediction)",evaluation,,
file350,32,"model = XGBRegressor(n_estimators=500, max_depth=4,eta=0.1).fit(X_train,Y_train)
 y_pred = model.predict(X_test)",prediction,modeling,
file350,33,import xgboost as xgb,helper functions,,
file350,34,get_ipython().system('pip install xgboost'),helper functions,,
file350,35,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file350,36,ttt = model.fit(pred_data),modeling,,
file350,37,pred = model.predict(pred_data),prediction,,
file350,38,pred_data.info(),data exploration,,
file350,39,unemp_data.describe(),data exploration,,
file350,40,"tt = pred_data.merge(fin,how = 'left',on= 'uu_id')
 tt",data preprocessing,data exploration,
file350,41,fin = final_data1.set_index('uu_id'),data preprocessing,,
file350,42,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y'],axis=1)",data preprocessing,,
file350,43,"fin = pd.get_dummies(final_data1,columns = ['week_number','countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file350,44,"X_train_data = X_train.drop(['uu_id'],axis=1)
 X_test_data = X_test.drop(['uu_id'],axis=1)",data preprocessing,,
file350,45,"fin = fin.set_index('uu_id')
 X_train, X_test, y_train, y_test = test_train_split(fin.ix[:, ~fin.columns.isin(['total_claims'])], fin.total_claims)",data preprocessing,,
file350,46,"fin = fin.set_index('uu_id')
 X = fin.drop(['total_claims'],axis=1)
 Y=fin['total_claims']
 #X_train, X_test, y_train, y_test = train_test_split(fin.ix[:, ~fin.columns.isin(['total_claims'])], fin.total_claims)",data preprocessing,,
file350,47,"X = fin.drop(['uu_id','total_claims'],axis=1)
 Y=fin['total_claims']",data preprocessing,,
file350,48,"final_data = unemp_data.merge(wage_data,how='left', on = 'uu_id')",data preprocessing,,
file350,49,final_data.info(),data exploration,,
file350,50,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression
 X_train, X_test,Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=32)",helper functions,data preprocessing,
file350,51,"lm = LinearRegression().fit(X_train,Y_train)
 prediction = lm.predict(X_test)",modeling,prediction,
file350,52,"lm.score(X_test, Y_test)",evaluation,,
file350,53,"import xgboost as xgb
 model = xgb.XGBRegressor(n_estimators=500, max_depth=4,eta=0.1).fit(X_train,Y_train)
 y_pred = model.predict(X_test)",modeling,prediction,
file350,54,"df_pred_final = pred_data[['uu_id']]
 df_pred_final['week_number'] = pred_data[['week_number']]
 df_pred_final[""total_claims""] = y_pred",data preprocessing,,
file350,55,"Y_test_final = pred_data.merge(fin,how='left',on= 'uu_id')",data preprocessing,,
file350,56,Y_test_final,data exploration,,
file350,57,"df_test = fin[fin.uu_id == i for i in pred_data['uu_id']]
 df_test",data preprocessing,data exploration,
file350,58,Y_test_final = pred_data.join(fin),data preprocessing,,
file350,59,"pred_data = pred_data.set_index('uu_id')
 fin = fin.set_index('uu_id)
 Y_test_final = pred_data.join(fin)",data preprocessing,,
file350,60,"X_df_test = df_test.drop(['uu_id','total_claims'],axis=1)
 Y_df_test = df_test['total_claims']",data preprocessing,,
file350,61,prediction = model.predict(X_df_test),prediction,,
file350,62,df_test,data exploration,,
file350,63,"final_data1 = final_data.drop(['timeperiod','tract_x','tract_name_x','tract_y','tract_name_y','countyfips_y','week_number'],axis=1)",data preprocessing,,
file350,64,"fin = pd.get_dummies(final_data1,columns = ['countyfips_x','top_category_employer1','top_category_employer2','top_category_employer3'])",data preprocessing,,
file350,65,"model.score(X_df_test,Y_df_test)",evaluation,,
file350,66,df_pred_final = pred_data[['uu_id']],data preprocessing,,
file350,67,"df_pred_final_yams=[]
 df_pred_final_yams == pred_data[['uu_id']]",data preprocessing,,
file350,68,df_pred_final_1['week_number'] = df_test[['week_number']],data preprocessing,,
file350,69,df_pred_final_1['total_claims'] = prediction,data preprocessing,,
file350,70,df_pred_final_1,data exploration,,
file350,71,df_final = df_pred_final_1.drop_duplicates(subset=['uu_id']),data preprocessing,,
file350,72,df_final,data exploration,,
file350,73,fin.shape,data exploration,,
file350,74,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file350,75,df_final.to_csv('submission_prediction_output.csv'),save results,,
file351,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file351,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('pip install xgboost')
 get_ipython().system('pip install impyute')
 get_ipython().system('pip install prophet')",helper functions,,
file351,2,"import datetime
 import itertools
 import os
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt",helper functions,,
file351,3,"import statsmodels.api as sm
 import sklearn.experimental.enable_iterative_imputer
 import sklearn.impute
 import sklearn.ensemble
 import sklearn.model_selection
 import sklearn.linear_model
 import xgboost as xgb",helper functions,,
file351,4,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file351,5,"pd.set_option('display.max_columns', 500)
 pd.set_option('display.max_rows', 500)
 pd.set_option('display.width', 1000)",helper functions,,
file351,6,"# load data
 def query(table):
  bigquery_client = bigquery.Client(project='ironhacks-data')
  query_str = f'''
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.{table}`
 '''
  query_job = bigquery_client.query(query_str)
  data = query_job.to_dataframe()
  return data",load data,data preprocessing,
file351,7,"def combine(u, w):
  '''
  Joins the unemployment data and the wage data on `uu_id`
  '''
  ww = w.loc[:, ['uu_id', 'average_wage']]
  d = u.join(ww.set_index('uu_id'), on='uu_id')
  return d",data preprocessing,,
file351,8,"def load_raw(csv_name='0_raw.csv'):
  '''
  Loads the unemployment and wage data and does some basic cleaning
  '''
  if not os.path.isfile(csv_name):
  u = query('unemployment_data')
  w = query('wage_data')
  raw = combine(u, w)
  raw.to_csv(csv_name, index=False)
  else:
  raw = pd.read_csv(csv_name)
  raw = raw.drop(['countyfips', 'tract', 'tract_name', 'timeperiod'], axis=1)
  raw = raw.sort_values(by=['uu_id', 'week_number'])
  raw = raw.drop_duplicates()
  raw = raw.replace({np.nan: None})
  raw = raw.reset_index(0, drop=True)
  return raw",data preprocessing,load data,
file351,9,load_raw(),load data,,
file351,10,"# define relevant columns based on categories 
 COL_MAP = {
  'edu': ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown'],
  'race': ['race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative', 'race_other', 'race_white'],  
  'gender': ['gender_female', 'gender_male', 'gender_na'],
  'industry': ['top_category_employer1', 'top_category_employer2', 'top_category_employer3']  
 }",data preprocessing,,
file351,11,"def get_cols(names):
  l = []
  for name in names:
  if name in COL_MAP:
  l += COL_MAP[name]
  else:
  l += [name]
  return l",data preprocessing,,
file351,12,"def week_number_to_date(week_number, first_week_date='20220101'):
  '''
  Prepare a date column for ARIMA
  '''
  return pd.to_datetime(first_week_date, format='%Y%m%d') + pd.DateOffset(days=7*(week_number - 1))",data preprocessing,,
file351,13,"def get_subset(df, uu_id):  
  return df.loc[df.uu_id == df.uu_id.unique()[uu_id], :]",data preprocessing,,
file351,14,"def convert_to_submission(results_csv, observed_label='total_claims', week_number_to_submit=40):
  r = pd.read_csv(results_csv)
  # last_few = r.loc[(30 <= r.week_number) & (r.week_number <= 37), :]
  # print('rms:', get_rms(last_few[observed_label], last_few.predicted))
  # print('mae:', get_mae(last_few[observed_label], last_few.predicted))
  last = r.loc[r.week_number == week_number_to_submit, ['uu_id', 'predicted']]
  last.index = last.uu_id
  uuid_map = last.to_dict(orient='dict')['predicted']
  p = query('prediction_list')
  p['total_claims'] = p['uu_id'].map(uuid_map)  
  p.to_csv('submission_prediction_output.csv', index=False)",save results,data preprocessing,
file351,15,"def get_week_number_map(g, colname):
  '''
  Creates a dictionary that maps from week number to an existing value in a given `colname`
  '''
  g = g[['week_number', colname]]
  week_number_map = dict(sorted(g.values.tolist()))
  return week_number_map",data preprocessing,,
file351,16,"def insert_na_week_number(g, max_week_number=43):  
  d = {}
  for colname in g.columns:
  if colname == 'week_number':
  continue
  week_number_map = get_week_number_map(g, colname)
  series = pd.Series(range(1, max_week_number+1))  
  d[colname] = series.map(week_number_map)  
  
  df = pd.DataFrame(d)
  df['week_number'] = range(1, max_week_number+1)
  df['uu_id'] = [v for v in df['uu_id'].unique() if type(v) == str][0]
  df['average_wage'] = [v for v in g['average_wage'].unique()][0]
  return df",data preprocessing,,
file351,17,"def load_raw_full(csv_name='1_raw_full.csv'):
  if not os.path.isfile(csv_name):
  raw = load_raw()
  raw_full = raw.groupby('uu_id').apply(insert_na_week_number).reset_index(0, drop=True)
  raw_full['date'] = raw_full['week_number'].apply(week_number_to_date)
  raw_full.to_csv(csv_name, index=False)
  
  else:
  raw_full = pd.read_csv(csv_name)
  return raw_full",load data,data preprocessing,
file351,18,load_raw_full(),data exploration,,
file351,19,"def impute_iterative(df):
  '''
  Wrapper fucntion for IterativeImputer for a generic data frame. 
  Mostly, for testing. We might need need this function
  Impute data assuming there are zero columns where all the values are NA
  '''
  imputer = sklearn.impute.IterativeImputer(random_state=0, min_value=0)
  imputed_cols = imputer.fit_transform(df)
  df_imputed = pd.DataFrame(imputed_cols, columns=df.columns)
  return df_imputed",data preprocessing,,
file351,20,"def plot_impute_iterative():
  raw_full = load_raw_full()
  nplots = 6
  
  fig, axs = plt.subplots(ncols=nplots, sharey=True, figsize=(15, 3))
  for i, (uu_id, g) in enumerate(raw_full.groupby('uu_id')):
  g_imp = impute_iterative(g.loc[:, ['week_number', 'total_claims']])  
  if i < nplots:
  ax = axs[i]
  ax.plot(g.week_number, g.total_claims, 'o-', zorder=10, label='raw')
  ax.plot(g_imp.week_number, g_imp.total_claims, 'o--', zorder=0, label='imputed')
  ax.set_title(f'uu_id {i}')
  ax.set_xlabel('week_number')
  # elif uu_id == 'ffbc87dc4bde6828daff6ad43e12db4a':
  #  ax = axs[-1]  
  #  ax.plot(g.week_number, g.total_claims, 'o-', zorder=10, label='raw')
  #  ax.plot(g_imp.week_number, g_imp.total_claims, 'o--', zorder=0, label='imputed')
  #  ax.set_title(f'uu_id {i}')
  #  ax.set_xlabel('week_number')
 

  
  axs[0].set_ylabel('total_claims')
  axs[-1].legend(frameon=False)
  plt.show()",result visualization,,
file351,21,plot_impute_iterative(),,,
file351,22,"def impute_total_claims(g):
  x = g.copy().reset_index(0, drop=True)
  g_imp = impute_iterative(g.loc[:, ['week_number', 'total_claims']]).reset_index(0, drop=True)
  x['total_claims'] = g_imp['total_claims']
  return x",,,
file351,23,"def load_imp_tot(csv_name='2_imp_tot.csv'):
  if not os.path.isfile(csv_name):
  raw_full = load_raw_full()
  imp_tot = raw_full.groupby(['uu_id']).apply(impute_total_claims).reset_index(0, drop=True)
  imp_tot.to_csv(csv_name, index=False)
  else:
  imp_tot = pd.read_csv(csv_name)
  return imp_tot",data preprocessing,save results,
file351,24,"imp_tot = load_imp_tot()
 imp_tot",data exploration,load data,
file351,25,"def replace_na_cols(g):
  '''
  If a column only has None or zero values, replace that entire columnn with zeros
  '''
  x = g.copy()
  for col in g.columns:
  cond1 = g[col].isnull()
  cond2 = g[col] == 0
  if (cond1 | cond2).all():
  x[col] = 0  
  return x",data preprocessing,,
file351,26,"def iter_cat(g):
  g = replace_na_cols(g)  
  for cat in ['edu', 'race', 'gender']:
  gg = g.loc[:, COL_MAP[cat] + ['total_claims']]
  yield cat, gg",data preprocessing,,
file351,27,"def print_impute_cat(test_subset, impute_func):
  for cat, gg in iter_cat(test_subset):
  print(impute_func(gg).head())",data preprocessing,data exploration,
file351,28,"print_impute_cat(get_subset(imp_tot, 6), impute_iterative)",data exploration,,
file351,29,"def impute_rowsum(df, target_col='total_claims'):
  l = []
  for idx, row in df.iterrows():
  n_unknowns = row.isna().sum()  
  if n_unknowns == 1:
  others = row[~row.isna() & (row.index != target_col)]
  val = row[target_col] - others.sum()
  val = val if val > 0 else 0
  row[row.isna()] = val
  l.append(row)
  df = pd.DataFrame(l).reset_index(0, drop=True)
  
  l = []
  for idx, row in df.iterrows():
  n_unknowns = row.isna().sum()  
  
  weights = {}
  for col in row[row.isna()].index:
  weights[col] = df[col].mean()
  weights = {k:v/sum(weights.values()) for k, v in weights.items()} 
  
  if n_unknowns > 1:  
  others = row[~row.isna() & (row.index != target_col)]
  row[row.isna()] = row[row.isna()].index.map(weights)*(row[target_col] - others.sum())  
  l.append(row)
  
  df_imputed = pd.DataFrame(l).reset_index(0, drop=True)
  return df_imputed",data preprocessing,,
file351,30,"def impute_all(df):
  x = df.copy().reset_index(0, drop=True)
  for cat, gg in iter_cat(df):
  df_imputed = impute_rowsum(gg)
  df_imputed = df_imputed.drop('total_claims', axis=1)
  x[COL_MAP[cat]] = df_imputed
  return x",data preprocessing,,
file351,31,"def load_clean(csv_name='3_clean.csv'):  
  if not os.path.isfile(csv_name):
  imp_tot = load_imp_tot()
  imp = imp_tot.groupby('uu_id').apply(impute_all).reset_index(0, drop=True)
  imp['date'] = imp['week_number'].apply(week_number_to_date)
  imp.to_csv(csv_name, index=False)
  else:
  imp = pd.read_csv(csv_name)
  
  return imp",load data,data preprocessing,
file351,32,load_clean(),data exploration,,
file351,33,"def impute_logistic(g, ycol):
  g = g[[ycol, 'week_number', 'total_claims']]
  xcols = ['week_number', 'total_claims']
  ycols = [ycol]
  
  mask_train = ~g[ycol].isnull()
  x_train, x_test = g.loc[mask_train, xcols], g.loc[~mask_train, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[~mask_train, ycols]
  
  if y_train.shape[0] == 0:
  g[ycol] = None
  return g[ycol]
  
  classes = y_train[ycol].unique()
  if len(classes) == 1:
  yhat = [classes[0]]
  else:
  model = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000).fit(x_train, y_train.values.ravel())
  yhat = model.predict(x_test)
  g.loc[~mask_train, ycols] = yhat
  return g[ycol]",modeling,data preprocessing,
file351,34,"def impute_industry(g, max_week_number=37):
  g = g.loc[g.week_number <= max_week_number, :]
  x = g.copy()
  for colname in COL_MAP['industry']:
  x[colname] = impute_logistic(g, colname)
  return x",data preprocessing,,
file351,35,"d = load_imp_industry()
 d",load data,data exploration,
file351,36,"import warnings
 from statsmodels.tools.sm_exceptions import ConvergenceWarning
 warnings.simplefilter('ignore', ConvergenceWarning)",helper functions,,
file351,37,"def arimax(y, order, seasonal_order):
  model = sm.tsa.statespace.SARIMAX(y, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
  results = model.fit(maxiter=300, disp=False)
  return results",modeling,,
file351,38,"def get_best_params(y, period=0):
  r1 = r2 = r3 = range(2)
  pdq = list(itertools.product(r1, r2, r3))  
  if period:
  seasonal_pdq = [(x[0], x[1], x[2], period) for x in list(itertools.product(r1, r2, r3))]
  else:
  seasonal_pdq = [(0, 0, 0, 0)]
  aic_min = np.inf
  for order in pdq:
  for seasonal_order in seasonal_pdq:
  results = arimax(y, order, seasonal_order)
  if results.aic < aic_min:
  aic_min = results.aic
  best_params = [order, seasonal_order, results.aic]  
  return best_params",data preprocessing,,
file351,39,"def predict(g, ylabel='total_claims', target_week_number=41):
  g = g.loc[:, ['uu_id', 'week_number', 'date', ylabel]]
  g = g.set_index('date')
  g.index = pd.DatetimeIndex(g.index).to_period('W')
  y = g[ylabel].astype(np.float64)
  best_params = get_best_params(y)
  best_results = arimax(y, best_params[0], best_params[1])  
  pred = best_results.get_prediction(start=week_number_to_date(target_week_number - 10), end=week_number_to_date(target_week_number), dynamic=False)
  
  x = g.join(pred.predicted_mean, on=g.index, how='outer')
  x['date'] = x.key_0
  ci = pred.conf_int()
  x['ci_lower'] = ci.iloc[:, 0]
  x['ci_upper'] = ci.iloc[:, 1]
  x['uu_id'] = g['uu_id'].values[0]
  x['week_number'] = np.arange(1, target_week_number + 1)
  x['predicted'] = x['predicted_mean']
  x = x.reset_index(0, drop=True)
  return x",data preprocessing,,
file351,40,"def predict_all(d, ylabel, csv_name):
  l = []
  for i, (uu_id, g) in enumerate(d.groupby('uu_id')):  
  if i % 100 == 0:
  print(f'processed {i} UUIDs')
  predicted = predict(g, ylabel=ylabel)
  l.append(predicted)
  df = pd.concat(l, ignore_index=True)
  df.to_csv(csv_name, index=False)",save results,data preprocessing,
file351,41,"if not os.path.isfile('results_arima.csv'):
  predict_all(d, 'total_claims', 'results_arima.csv')",save results,,
file351,42,"# now try to see if cleaning ""noises"" with exponentially weighted moving average (EWM) will help improve the model before doing ARIMA
 # and also figure out which value of the `alpha` parameters to use
 def moving_average(g, alpha=0.2): 
  x = g.copy()
  x['predicted_ewm'] = g['total_claims'].ewm(alpha=alpha).mean()  
  return x",data preprocessing,,
file351,43,"def test_ewm(d, n=6):
  fig, axs = plt.subplots(ncols=n, figsize=(18, 3), sharey=True)
  for i in range(n):
  g = get_subset(d, i)  
  ax = axs[i]
  ax.plot(g.week_number, g.total_claims, 'o-', label='observed')
  for alpha in [0.6]:
  ax.plot(g.week_number, moving_average(g, alpha=alpha).predicted_ewm, '-o', label=f'predicted_ewm: alpha={alpha}')
  axs[-1].legend(fancybox=False)
  plt.show()
  plt.close()",result visualization,,
file351,44,test_ewm(d),result visualization,,
file351,45,"def load_ewm(csv_name='5_ewm.csv', alpha=0.6):  
  if os.path.isfile(csv_name):
  ewm = pd.read_csv(csv_name)
  else:
  d = load_imp_industry() 
  ewm = d.groupby(['uu_id']).apply(moving_average, alpha=alpha)
  ewm.to_csv(csv_name, index=False)
  ewm['total_claims'] = ewm['predicted_ewm']
  return ewm",load data,data preprocessing,
file351,46,d = load_ewm(),load data,data preprocessing,
file351,47,"def plot(ax, g):  
  for i, method in enumerate(g.method.unique()):  
  gg = g.loc[g.method == method, :]
  if i == 0:
  ax.plot(gg.week_number, gg.total_claims, 'o-', label='observed')
  ax.plot(gg.week_number, gg.predicted, 'o-', label=method)",result visualization,,
file351,48,"def compare_ewm():
  arma = pd.read_csv('results_arima.csv')
  arma['method'] = 'arima'
  arma_ewm = pd.read_csv('results_arima_ewm.csv')
  arma_ewm['method'] = 'arima_ewm'
  a = pd.concat([arma, arma_ewm], ignore_index=True)
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i, (uu_id, g) in enumerate(a.groupby('uu_id')):
  if i < 6:
  ax = axs[i]
  plot(ax, g)
  ax.set_xlabel('week_number')
  ax.set_title(f'uu_id: {i}')
  axs[0].set_ylabel('total_claims')
  axs[-1].legend(frameon=False)",result visualization,,
file351,49,compare_ewm(),result visualization,,
file351,50,"# check how good our prediction is
 def get_rms(y_observed, y_predicted):
  n = len(y_observed)
  rms = sum((y_observed - y_predicted)**2)/n
  return rms",evaluation,,
file351,51,"def compare_arima():
  r = pd.read_csv('results_arima.csv')
  rr = r.loc[(30 <= r.week_number) & (r.week_number <= 37), :]
  y = rr.total_claims
  yhat = rr.predicted
  print('ARIMA without EWM smoothing')
  print('rms:', get_rms(y, yhat))
  print('mae:', get_mae(y, yhat))
 

  rewm = pd.read_csv('results_arima_ewm.csv')
  rrewm = rewm.loc[(30 <= rewm.week_number) & (rewm.week_number <= 37), :]
  yhatewm = rrewm.predicted
  print('ARIMA with EWM smoothing')
  print('rms:', get_rms(y, yhatewm))
  print('mae:', get_mae(y, yhatewm))",prediction,,
file351,52,compare_arima(),prediction,,
file351,53,"# Since ARIMA without smoothing seems better, we submit the corresponding results
 convert_to_submission('results_arima.csv', observed_label='total_claims')",save results,,
file351,54,"def get_avg_total_claims(g):
  return pd.DataFrame([{'uu_id': g.uu_id.values[0], 'average_wage': g.average_wage.values[0], 'avg_total_claims': g.total_claims.mean()}])",data exploration,,
file351,55,"def plot_avg_total_claims(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i, week_number in enumerate([1, 6, 11, 16, 31, 36]):  
  ax = axs[i]
  dd = d.loc[d.week_number == week_number, ['average_wage', 'total_claims']]
  ax.plot(dd.average_wage, dd.total_claims, 'o')
  ax.set_title(f'week_number: {week_number}')
  ax.set_xlabel('average_wage')
  axs[0].set_ylabel('total_claims')
  plt.show()
  plt.close()
  
  avg = d.groupby('uu_id').apply(get_avg_total_claims).reset_index(0, drop=True)
  fig, ax = plt.subplots()
  ax.plot(avg.average_wage, avg.avg_total_claims, 'o')  
  ax.set_xlabel('average_wage')
  ax.set_ylabel('average_total_claims')
  plt.show()
  plt.close()
  
  print(avg[['average_wage', 'avg_total_claims']].corr())",result visualization,,
file351,56,plot_avg_total_claims(d),result visualization,,
file351,57,"def plot_cat(d, cat):
  colnames = COL_MAP[cat]
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i in range(6):
  ax = axs[i]
  dd = get_subset(d, i)[['week_number'] + colnames]
  for colname in colnames:
  ax.plot(dd.week_number, dd[colname], 'o-', label=colname)
  ax.set_xlabel('week_number')
  
  axs[-1].legend(frameon=False)
  axs[0].set_ylabel('claims')
  plt.show()",result visualization,,
file351,58,"plot_cat(d, 'gender')
 plot_cat(d, 'edu')
 plot_cat(d, 'race')",result visualization,,
file351,59,"def get_train_test(g, max_week_number=37):
  xcols = COL_MAP['edu'] + COL_MAP['gender'] + COL_MAP['race'] + ['week_number']
  ycols = ['total_claims']  
  mask_train = g.week_number <= max_week_number
  mask_test = g.week_number > max_week_number
  
  x_train, x_test = g.loc[mask_train, xcols], g.loc[mask_test, xcols]
  y_train, y_test = g.loc[mask_train, ycols], g.loc[mask_test, ycols]
  return x_train, x_test, y_train, y_test",result visualization,,
file351,60,"def run_rf(g, max_week_number=37):
  x_train, x_test, y_train, y_test = get_train_test(g, max_week_number=max_week_number)
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x_train, y_train.values.ravel())
  yhat = rf.predict(pd.concat([x_train, x_test]))
  return x_train, x_test, y_train, y_test, yhat",modeling,prediction,
file351,61,"def plot_rf(d):
  fig, axs = plt.subplots(ncols=6, figsize=(18, 3), sharey=True)
  for i in range(6):
  ax = axs[i]
  dd = get_subset(d, i)
  
  x_train, x_test, y_train, y_test, yhat = run_rf(dd)
  ax.plot(x_train.week_number, y_train, 'o-', label='original')
  ax.plot(dd.week_number, yhat, 'o-', label='predict')
  ax.set_xlabel('week_number')
  ax.set_title(f'uu_id: {i}')
  
  axs[-1].legend(frameon=False)
  axs[0].set_ylabel('claims')
  plt.show()",result visualization,,
file351,62,plot_rf(d),result visualization,,
file351,63,"def rf_industry(g):
  uu_id = g.uu_id.values[0]
  gg = g[COL_MAP['industry'] + ['week_number', 'total_claims']]  
  gg = gg.dropna()
  gg = pd.get_dummies(gg)
  
  if gg.shape[0] == 0:
  print(g.uu_id.values[0])
  mean = g.total_claims.mean()
  return pd.DataFrame([{'uu_id': uu_id, 'week_number': 38, 'total_claims': mean, 'predicted': mean}])
  x = gg.drop(['total_claims'], axis=1)  
  y = gg['total_claims']  
  max_avail_week_number = int(x.week_number.max())
  rf = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=0).fit(x, y.values.ravel())
  last = x.loc[x.week_number == max_avail_week_number, :].copy()
  last['week_number'] = max_avail_week_number + 1
  x_test = pd.concat([x, last], ignore_index=True)
  x_test['predicted'] = rf.predict(x_test)
  result = x_test.copy()
  result['total_claims'] = y.reset_index(0, drop=True)
  result['uu_id'] = uu_id
  return result[['uu_id', 'week_number', 'total_claims', 'predicted']]",result visualization,,
file351,64,plot_industry(d),result visualization,,
file351,65,"def run_rf_all(d, csv_name='results_rf.csv'):
  if not os.path.isfile(csv_name):
  result_rf = d.groupby('uu_id').apply(rf_industry).reset_index(0, drop=True)
  result_rf.to_csv(csv_name, index=False)",data preprocessing,save results,
file351,66,"run_rf_all(d)
 convert_to_submission('results_rf.csv', observed_label='total_claims', week_number_to_submit=38)",save results,,
file351,67,from statsmodels.tsa.api import VAR,helper functions,,
file351,68,"def get_industry(g, cat=True):
  gg = g[['uu_id', 'date', 'total_claims'] + COL_MAP['industry']]
  if cat:
  g_industry_cat = pd.get_dummies(gg[COL_MAP['industry']])
  g_no_industry = gg.loc[:, gg.columns.difference(COL_MAP['industry'])]
  gg = pd.concat([g_no_industry, g_industry_cat], axis=1)  
  return gg",data preprocessing,,
file351,69,"def run_var(g):  
  g = g[['uu_id', 'date', 'total_claims'] + COL_MAP['edu'] + COL_MAP['race'] + COL_MAP['gender']]
  uu_id = g.uu_id.values[0]
  g.index = pd.DatetimeIndex(g.date)
  g.index = pd.DatetimeIndex(g.index.values, freq=g.index.inferred_freq)
  g = g.drop(['uu_id', 'date'], axis=1)
  g = g.loc[:, (g != 0).any(axis=0)]
  if g.shape[1] == 1:
  total_claims = g.iloc[:, 0].values.mean()
  else:
  var = VAR(g)
  results = var.fit(maxlags=3)
  ahead = 5
  total_claims = results.forecast(g.values[-10:], ahead)[:, 0]
  print(g['total_claims'][-6:])
  print(total_claims)
  fig, ax = plt.subplots()
  n = len(g.total_claims)
  ax.plot(range(n), g.total_claims, 'o-', color='blue')
  ax.plot(range(n, n + len(total_claims)), total_claims, 'o-', color='red')
  return pd.DataFrame([{'uu_id': uu_id, 'predicted': total_claims, 'week_number': 41}])",data preprocessing,,
file351,70,"def run_var_all(d, csv_name='results_var.csv'):  
  r = d.groupby('uu_id').apply(run_var).reset_index(0, drop=True)
  r.loc[r.predicted < 0, 'predicted'] = 0
  r.to_csv(csv_name, index=False)",data preprocessing,save results,
file351,71,"run_var(get_subset(d, 1))
 # convert_to_submission('results_var.csv', week_number_to_submit=41)",save results,,
file351,72,"def varmax(y, order, exog=None):
  mod = sm.tsa.VARMAX(y, order=order, trend='n', enforce_invertibility=False, exog=exog
  # enforce_stationarity=False,
  )
  results = mod.fit(maxiter=1000, disp=False)
  return results",modeling,,
file351,73,"def run_varmax(g, plot=False):
  g = g[['uu_id', 'week_number', 'date', 'total_claims'] + COL_MAP['edu'] + COL_MAP['race'] + COL_MAP['gender']]
  uu_id = g.uu_id.values[0]
  g.index = pd.DatetimeIndex(g.date)
  g.index = pd.DatetimeIndex(g.index.values, freq=g.index.inferred_freq)
  y = g.drop(['uu_id', 'week_number', 'date'], axis=1)
  y = y.loc[:, (y != 0).any(axis=0)]
  x = y.drop('total_claims', axis=1)
  if y.shape[1] <= 2:
  # total_claims = g.iloc[:, 0].values.mean()
  pass
  else:
  results = varmax(y, (2, 0), exog=x)
  predicted = results.get_prediction(start=week_number_to_date(30), end=week_number_to_date(41), dynamic=False).predicted_mean
  if plot:
  results.plot_diagnostics(figsize=(10, 7))
  fig, ax = plt.subplots()
  ax.plot(y.index, y.total_claims, 'o-')
  ax.plot(predicted.index, predicted.total_claims, 'o-')",modeling,prediction,
file351,74,"def plot_varmax(d):
  for i in range(6):
  g = get_subset(d, i)
  run_varmax(g, plot=True)",result visualization,,
file351,75,"def get_train_test(g):
  x = g[COL_MAP['edu'] + COL_MAP['race'] + COL_MAP['gender'] + ['week_number']]
  y = g['total_claims']
  last_week_number = x.week_number.max()
  x_test_last = x.loc[x.week_number == last_week_number, :].copy()
  x_test_last['week_number'] = last_week_number + 1
  x_test = pd.concat([x, x_test_last], ignore_index=True)
  return x, y, x_test",data preprocessing,,
file351,76,"def run_xgb_old(g, params={'n_estimators': 1000}):
  uu_id = g.uu_id.values[0]
  x, y, x_test = get_train_test(g)
  reg = xgb.XGBRegressor(objective='reg:squarederror', **params)
  reg.fit(x, y, verbose=True)
  yhat = reg.predict(x_test)
  x_test['predicted'] = yhat
  x_test['uu_id'] = uu_id
  return x_test",modeling,prediction,
file351,77,"def optimize_xgb(g):
  uu_id = g.uu_id.values[0]
  x, y, x_test = get_train_test(g)
  # params = {
  #  'min_child_weight': [1, 5, 10],
  #  'gamma': [0.3, 0.5, 1.0, 1.5, 2.0, 5.0],
  #  'subsample': [0.6, 0.8, 1.0],
  #  'colsample_bytree': [0.6, 0.8, 1.0],
  #  'max_depth': [2, 3, 4, 5],
  #  'n_estimators': [300, 600, 1000],
  #  'learning_rate': [0.001, 0.01, 0.1]
  # }
  params = {
  'min_child_weight': [1],
  'gamma': [0.3],
  'subsample': [0.6, 0.8],
  'colsample_bytree': [0.6, 0.8],
  'max_depth': [3],
  'n_estimators': [600, 1000],
  # 'learning_rate': [0.3]
  }
  reg = xgb.XGBRegressor(nthread=-1, objective='reg:squarederror')
  grid = sklearn.model_selection.GridSearchCV(reg, params)
  grid.fit(x, y)
  yhat = grid.best_estimator_.predict(x_test)
  x_test['predicted'] = yhat
  x_test['uu_id'] = uu_id
  return x_test, grid.best_score_, grid.best_params_",modeling,prediction,
file351,78,"def plot_xgb(d, n=6):  
  fig, axs = plt.subplots(ncols=n, figsize=(18, 3), sharey=True)
  for i in range(n):
  ax = axs[i]
  g = get_subset(d, i)  
  pred = run_xgb_old(g)
  # pred, best_score, best_params = optimize_xgb(g)
  # print(i)
  # print(f'best score: {best_score}')
  # print('best_param: ', best_params)
  ax.plot(g.week_number, g.total_claims, 'o-')
  ax.plot(pred.week_number, pred.predicted, 'o-')",result visualization,,
file351,79,"plot_xgb(d)
 # get_train_test(get_subset(d, 6))",result visualization,,
file351,80,"def run_xgb_all(d, csv_name='results_xgb.csv'):
  if os.path.isfile(csv_name):
  return
  l = []
  for i, (uu_id, g) in enumerate(d.groupby('uu_id')):  
  if i % 100 == 0:
  print(f'processed {i} UUIDs')
  predicted = run_xgb(g)
  l.append(predicted)
  df = pd.concat(l, ignore_index=True)
  df.to_csv(csv_name, index=False)",save results,,
file351,81,"def plot(d, n=6):
  fig, axs = plt.subplots(ncols=n, sharey=True, figsize=(n*3, 3))
  for i in range(n):
  g = get_subset(d, i)
  ax = axs[i]
  ax.plot(g.week_number, g.total_claims, 'o-')
  ax.set_xlim((0, 42))",result visualization,,
file351,82,"def get_dummies(d, cols):
  d_cat = pd.get_dummies(d.loc[:, cols])
  d_others = d.drop(cols, axis=1)
  d = pd.concat([d_others, d_cat], axis=1)
  return d",data preprocessing,,
file351,83,"def get_train_test_xgb(g):
  mask_train = g.week_number <= 30
  mask_test = ~mask_train
  # x_train = g.loc[mask_train, ]",load data,,
file351,84,"def preprocess_xgb(d):
  d = d[get_cols(['total_claims', 'uu_id', 'average_wage', 'week_number'])].copy()
  # d = get_dummies(d, COL_MAP['industry'])
  d['date'] = d['week_number'].apply(week_number_to_date)
  d['month'] = d['date'].dt.month
  d['quarter'] = d['date'].dt.quarter
  nlags = 3
  d['mean_month'] = d.groupby('month')['total_claims'].transform(lambda x: float(x.dropna().mean()))
  d['mean_quarter'] = d.groupby('quarter')['total_claims'].transform(lambda x: float(x.dropna().mean())) 
  
  # for lag in range(1, nlags + 1):  
  #  d[f'shift_{lag}'] = d.groupby('uu_id')['total_claims'].transform(lambda x: x.shift(lag))
  
  d = d.drop(['date'], axis=1)  
  encoder = sklearn.preprocessing.LabelEncoder()
  d['uu_id'] = encoder.fit_transform(d['uu_id'].astype(str))
  return d",data preprocessing,,
file351,85,"d = load_raw_full()
 # d = preprocess_xgb(d)
 d",data preprocessing,,
file351,86,"# from impyute.imputation.cs import mice
 import impyute",helper functions,,
file351,87,"def impute_mice(d):
  cols_numeric = ['total_claims', 'average_wage', 'week_number']
  d_numeric = d.loc[:, cols_numeric].astype('float64')
  d_numeric_imputed = impyute.imputation.cs.mice(d_numeric.values)
  d['total_claims_imputed'] = d_numeric_imputed[:, 0]
  return d",data preprocessing,,
file351,88,plot_impute(impute_mice(d)),result visualization,,
file351,89,"d = load_raw_full()
 d['total_claims_imputed'] = d['total_claims'].interpolate(method='linear')
 plot_impute(d)",data preprocessing,result visualization,
file351,90,"d['total_claims'] = d['total_claims'].interpolate(method='linear')
 d['date'] = d['week_number'].apply(week_number_to_date)
 if not os.path.isfile('results_arima_linear.csv'):
  predict_all(d, 'total_claims', 'results_arima_linear.csv')",data preprocessing,save results,
file351,91,"def plot_arima_linear(n=6):
  d = load_raw_full()
  r = pd.read_csv('results_arima_linear.csv')
  fig, axs = plt.subplots(ncols=n, sharey=True, figsize=(n*3, 3))
  for i in range(n):
  ax = axs[i]
  dd = get_subset(d, i)
  rr = get_subset(r, i)
  ax.plot(dd.week_number, dd.total_claims, 'o-', label='raw')
  ax.plot(rr.week_number, rr.predicted, 'o-', label='predicted')
  ax.set_xlim(0, 42)",result visualization,,
file351,92,plot_arima_linear(),result visualization,,
file351,93,import prophet,helper functions,,
file351,94,"import logging
 logger = logging.getLogger('cmdstanpy')
 logger.addHandler(logging.NullHandler())
 logger.propagate = False
 logger.setLevel(logging.CRITICAL)",helper functions,,
file351,95,"def run_prophet(g, period=0, growth='logistic', changepoint_range=0.8, n_changepoints=100, changepoint_prior_scale=0.75, seasonality_mode='additive', seasonality_prior_scale=10.0):
  g = g.reset_index(0, drop=True)
  gg = g.copy()
  x = pd.DataFrame({'ds': gg['date'], 'y': np.log(gg['total_claims']), 'cap': np.log(gg['total_claims']).max()})
  model = prophet.Prophet(
  weekly_seasonality=False,
  changepoint_range=changepoint_range,  
  n_changepoints=n_changepoints, 
  changepoint_prior_scale=changepoint_prior_scale,
  growth=growth,
  seasonality_mode=seasonality_mode,
  seasonality_prior_scale=seasonality_prior_scale,
  )
  if period:
  model.add_seasonality(name='monthly', period=30, fourier_order=period)
  model.add_seasonality(name='quarterly', period=90, fourier_order=period)
  model.add_seasonality(name='yearly', period=365, fourier_order=period)
  pred = model.fit(x).predict(x)
  gg['predicted'] = np.exp(pred.yhat)
  gg['predicted'] = gg['predicted'].clip(lower=0, upper=gg['total_claims'].max())
  return gg",modeling,prediction,
file351,96,"def plot_prophet(n=12):
  d = load_raw_full()
  ncols = 6
  nrows = int(np.ceil(n/ncols))
  fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(3*ncols, 3*nrows), sharex=True)
  for i in range(n):
  if i % 10 == 0:
  print(i)
  idx_row = int(i / ncols)
  idx_col = i % ncols
  if nrows == 1:
  ax = axs[idx_col]
  else:
  ax = axs[idx_row, idx_col]
  dd = get_subset(d, i)
  ax.plot(dd.week_number, dd.total_claims, 'o', label='raw')
  for growth in ['logistic', 'linear']:  
  pred = run_prophet(dd,
  growth=growth
  )
  ax.plot(pred.week_number, pred.predicted, '-', label=f'fb: {growth}')
  if idx_row == 0 and idx_col == ncols - 1:
  ax.legend()",result visualization,,
file351,97,"# d = load_raw_full()
 # run_prophet(get_subset(d, 6))
 plot_prophet(n=36)",result visualization,,
file351,98,"def run_prophet_all(csv_name='results_prophet.csv'):
  if not os.path.isfile(csv_name):
  d = load_raw_full()
  d['total_claims'] = d['total_claims'].interpolate(method='linear')
  l = []
  for i, (uu_id, g) in enumerate(d.groupby('uu_id')):
  if i % 20 == 0:
  print(i)
  r = run_prophet(g)
  l.append(r)
  pred = pd.concat(l, ignore_index=True)
  pred.to_csv(csv_name, index=False)",save results,,
file351,99,run_prophet_all(),save results,,
file351,100,"convert_to_submission('results_prophet.csv', week_number_to_submit=43)",save results,,
file352,0,"import csv
 import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file352,1,"from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
 from sklearn.model_selection import train_test_split
 from sklearn.metrics import r2_score
 from sklearn.model_selection import GridSearchCV",helper functions,,
file352,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file352,3,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file352,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file352,5,"query = """"""
 SELECT 
 a.*,
 b.average_wage
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`a
 

 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id = b.uu_id
 

 

 """"""",load data,,
file352,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['timeperiod']= pd.to_datetime(data['timeperiod'])
 data.head()",load data,data preprocessing,
file352,7,data.drop_duplicates(inplace=True),data preprocessing,,
file352,8,get_ipython().system('pip install db-dtypes'),helper functions,,
file352,9,data = data.fillna(0),data preprocessing,,
file352,10,data.describe(),data exploration,,
file352,11,data.sum(numeric_only=True),data exploration,,
file352,12,data.columns,data exploration,,
file352,13,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",data preprocessing,,
file352,14,"y = data.total_claims
 x = data.drop(['total_claims', 'timeperiod'], axis = 1)",data preprocessing,,
file352,15,"uuid, label = data['uu_id'].factorize(sort=True)",data preprocessing,,
file352,16,x['uu_id'] = uuid,data preprocessing,,
file352,17,x['tract_name'] = x['tract_name'].factorize()[0],data preprocessing,,
file352,18,"x['top_category_employer1'] = x['top_category_employer1'].factorize()[0]
 x['top_category_employer2'] = x['top_category_employer2'].factorize()[0]
 x['top_category_employer3'] = x['top_category_employer3'].factorize()[0]",data preprocessing,,
file352,19,"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)",data preprocessing,,
file352,20,"model = HistGradientBoostingRegressor(loss=""squared_error"")
 model.fit(x_train, y_train,
  verbose=False) # Change verbose to True if you want to see it train",modeling,,
file352,21,"model = HistGradientBoostingRegressor(loss=""squared_error"")
 model.fit(x_train, y_train)",modeling,,
file352,22,"model.score(x_test, y_test)",evaluation,,
file352,23,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list` 
 """"""",load data,,
file352,24,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 pred = query_job.to_dataframe()
 pred.head()",load data,data exploration,
file352,25,pred,data exploration,,
file352,26,"x['uu_id'] = label[x[""uu_id""]]
 x",data preprocessing,data exploration,
file352,27,"for col in x.columns[2:]:
  li = []
  for i in pred['uu_id']:
  li.append(x.loc[x['uu_id'] == i, col].mean())
  pred[col] = li",data preprocessing,,
file352,28,pred['uu_id'] = pred['uu_id'].factorize(sort=True)[0],data preprocessing,,
file352,29,x_test,data exploration,,
file352,30,pred = pred.apply(pd.to_numeric),data preprocessing,,
file352,31,predictions = gridcv_xgb.predict(pred),prediction,,
file352,32,predictions,data exploration,,
file352,33,out_df = pd.DataFrame(),data preprocessing,,
file352,34,"out_df[""uu_id""] = pred[""uu_id""]
 out_df[""total_claims""] = predictions
 out_df[""week_numer""] = pred[""week_number""]",data preprocessing,,
file352,35,out_df,data exploration,,
file352,36,"uuid, label = uu_id['uu_id'].factorize(sort=True)",data preprocessing,,
file352,37,"out_df[""uu_id""] = label[out_df[""uu_id""]]",data preprocessing,,
file352,38,"out_df.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file353,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file353,1,"get_ipython().run_cell_magic('capture', '--no-display', '!pip3 install db-dtypes\n')",helper functions,,
file353,2,"import os
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import warnings
 warnings.filterwarnings('ignore')",helper functions,,
file353,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file353,4,"# Google Credential
 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='key.json'
 bigquery_client = bigquery.Client(project='ironhacks-data')",load data,,
file353,5,"# Query the three provided data tables 
 unemployement_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file353,6,"wage_data_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file353,7,"# QUERY THE DATA ONCE
 ud_query_job = bigquery_client.query(unemployement_data_query)
 wd_query_job = bigquery_client.query(wage_data_query)
 pl_query_job = bigquery_client.query(prediction_list_query)",load data,,
file353,8,"unemployement_data = ud_query_job.to_dataframe()
 wage_data = wd_query_job.to_dataframe()
 prediction_list = pl_query_job.to_dataframe()",data preprocessing,,
file353,9,"# save the query results to csv files
 unemployement_data.to_csv(""data/unemployment_data.csv"")
 print(""unemployment_data shape:"", unemployement_data.shape)",save results,data exploration,
file353,10,"wage_data.to_csv(""data/wage_data.csv"")
 print(""wage_data shape:"", wage_data.shape)",save results,data exploration,
file353,11,"prediction_list.to_csv(""data/prediction_list.csv"")
 print(""prediction_list shape:"", prediction_list.shape)
 total_uuid = prediction_list.shape[0]",save results,data exploration,
file353,12,"# check if how many weeks of data are provided for each uu_id
 query = """"""
 SELECT uu_id, COUNT(week_number) as num_of_weeks
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY uu_id
 ORDER BY num_of_weeks DESC
 """"""",load data,,
file353,13,bigquery_client.query(query).to_dataframe(),data preprocessing,,
file353,14,"# query week_number data for uu_id f43fb9e90c5ecf879016b159aaa17fcb
 query = """"""
 SELECT uu_id, week_number
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 WHERE uu_id=""f43fb9e90c5ecf879016b159aaa17fcb""
 ORDER BY week_number
 """"""
 print(bigquery_client.query(query).to_dataframe().to_string(index=False))",load data,data exploration,
file353,15,"# use sub-query to retrieve the num_of_uuids vs num_of_weeks, plot the results
 query = """"""
 SELECT num_of_weeks, COUNT(num_of_weeks) as num_of_uuids
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as num_of_weeks
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
  ORDER BY num_of_weeks DESC
 )
 GROUP BY num_of_weeks
 ORDER BY num_of_weeks
 """"""",load data,,
file353,16,"week_count_label = list(map(str, list(uuid_weeks[""num_of_weeks""])))
 num_of_uuids = list(uuid_weeks[""num_of_uuids""])
 plt.figure(figsize=(6,6))
 plt.barh(week_count_label, num_of_uuids)
 plt.yticks(week_count_label)
 plt.ylabel(""How many weeks of data are included"")
 plt.xlabel(""Number of UUIDs"")
 for i, v in enumerate(num_of_uuids):
  plt.text(v + 1, i - 0.4, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file353,17,"print(str(np.array(num_of_uuids)[-6:].sum()), ""out of"", total_uuid, ""uuids have no less than 30 weeks of datapoints."")
 print(str(np.array(num_of_uuids)[:9].sum()), ""out of"", total_uuid, ""uuids have no more than 10 weeks of datapoints."")",data exploration,,
file353,18,"# query the number of datapoints for each week, use DISTINCT on uu_id to remove duplication
 QUERY = """"""
 SELECT week_number, Count(DISTINCT(uu_id)) as count
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 GROUP BY week_number
 ORDER BY week_number
 """"""
 query_job = bigquery_client.query(QUERY)
 week_number_count = query_job.to_dataframe()
 display(week_number_count.T)
 print(""week_number: \n"" + str(week_number_count.T.to_numpy()[0]))
 print(""count: \n"" + str(week_number_count.T.to_numpy()[1]))",load data,data exploration,
file353,19,"# fill the missing weeks' count with 0 for week 4 and week 23
 i = 0
 week_number_count_filled = []
 for w_c in week_number_count.to_numpy():
  i = i + 1
  if w_c[0] == i:
  week_number_count_filled.append(list(w_c))
  else:
  week_number_count_filled.append(list([i, 0]))
  week_number_count_filled.append(list(w_c))
  i = i + 1
 print(""Total number of weeks:"", i)",data preprocessing,,
file353,20,"# plot the filled result
 week_number = list(np.array(week_number_count_filled)[:,0])
 week_count = list(np.array(week_number_count_filled)[:,1])
 plt.figure(figsize=(8,6))
 plt.barh(week_number, week_count)
 plt.yticks(week_number)
 plt.ylabel(""Week number"") 
 plt.xlabel(""Number of Datapoints"")
 for i, v in enumerate(week_count):
  plt.text(v + 2, i + 0.7, str(v), size=""small"")
 plt.tight_layout()
 plt.show()",result visualization,,
file353,21,"# select 10 of 96 uuids with 3 weeks' data
 QUERY = """"""
 SELECT uu_id
 FROM (
  SELECT uu_id, COUNT(DISTINCT(week_number)) as week_count
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  GROUP BY uu_id
 )
 WHERE week_count = 35
 LIMIT 10
 """"""
 query_job = bigquery_client.query(QUERY)
 ten_uuid_with_35_weeks = query_job.to_dataframe()
 ten_uuid_with_35_weeks",load data,data preprocessing,
file353,22,"# query the 10 selected UUID data
 ten_35w_data = []
 for uu_id in ten_uuid_with_35_weeks[""uu_id""]:
  QUERY=""""""
  SELECT uu_id, week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uu_id)
  query_job = bigquery_client.query(QUERY)
  ten_35w_data.append(query_job.to_dataframe().drop_duplicates(ignore_index=True))",load data,,
file353,23,"# manually add week 4 and week 23 missing data
 def fill_week4_and_week23(df, method, replace=False):
  """"""
  Fill or replace the week 4 and week 23 missing total_claims data.
  Args:
  df: Dataframe
  The input dataframe with missing total claims or to be replaced.
  method: string
  prev - use previous week's value
  next - use next week's value
  mid - use mean value of previous and next weeks
  replace: bool
  True - replace the existing week 4 and week 23 total_claim values
  False - add the missing values
  Returns:
  Dataframe:
  Dataframe with added total_claim values or replaced values.
  """"""
  uuid = df[""uu_id""][0]
  week_list = list(df[""week_number""])
  if 4 in week_list and 23 in week_list and not replace:
  print(""Week 4 and week 23 data already exist, please use replace=True"")
  return df
  if 4 not in week_list and 23 not in week_list and replace:
  print(""Week 4 and week 23 data do not exist, replace failed"")
  return df
  if 4 not in week_list and 23 not in week_list and not replace:
  print(""Fill uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  if 4 in week_list and 23 in week_list and replace:
  print(""Replace uuid"", uuid, ""week 4 and week 23 data with"", method, ""values"")
  # remove exisiting value and insert again
  df = df.drop(df.index[[3, 22]])
  df = df.sort_index().reset_index(drop=True)
  
  if method == ""prev"":
  # use previous avaliable week's value
  val_4 = df[""total_claims""][2]
  val_23 = df[""total_claims""][20]
  if method == ""next"":
  # use next avaliable week's value
  val_4 = df[""total_claims""][3]
  val_23 = df[""total_claims""][21]
  if method == ""mid"":
  # use mean value of previous and next avaliable weeks
  val_4 = int((df[""total_claims""][2] + df[""total_claims""][3]) * 0.5)
  val_23 = int((df[""total_claims""][20] + df[""total_claims""][21]) * 0.5)
  
  # week 4
  df.loc[2.5] = [uuid, 4, val_4]
  # week 23
  df.loc[20.5] = [uuid, 23, val_23]
  return df.sort_index().reset_index(drop=True)",helper functions,,
file353,24,"# fill the missing data using mean value of prev week and next week for all 10 uuids
 for i in range(len(ten_35w_data)):
  ten_35w_data[i] = fill_week4_and_week23(ten_35w_data[i], ""mid"")",data preprocessing,,
file353,25,"# plot the 10 uuid total_claim data from week 1 to week 37
 plt.figure(figsize=(12,4))
 plt.title(""10 selected uuid with 37 weeks of data total_claim plot"")
 for i in range(len(ten_35w_data)):
  plt.plot(list(map(str, ten_35w_data[i][""week_number""])), 
  ten_35w_data[i][""total_claims""], 
  ""o-"", linewidth=1, markersize=5, alpha=0.7,
  label=ten_35w_data[i][""uu_id""][0])
  plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)
 plt.xlabel(""week_number"")
 plt.ylabel(""total_claims"")
 plt.grid(axis=""y"")
 plt.tight_layout()
 plt.show()",data preprocessing,,
file353,26,"# find out which weeks are available for these uuids
 week_available = []
 for i in range(len(data_less_10w)):
  week_available.append(list(data_less_10w[i][""week_number""]))",data preprocessing,,
file353,27,"# plot the avaliable weeks vs uu_id
 plt.figure(figsize=(12,4))
 plt.title(""week_number distribution for uuid with no more than 10 weeks of data"")
 colors = ['C{}'.format(i) for i in range(total_uuid_less_10w)]
 plt.eventplot(week_available, orientation='vertical', linelengths=0.2, linewidths=4, colors=colors)
 plt.yticks([i for i in range(1,38,2)])
 plt.ylabel(""week_number"")
 plt.xticks([i for i in range(total_uuid_less_10w)])
 plt.xlabel(""uu_id"")
 plt.grid(axis=""x"")
 plt.axhline(4, alpha=0.5)
 plt.axhline(23, alpha=0.5)
 plt.axhline(37, alpha=0.5)
 plt.tight_layout()
 plt.show()",result visualization,,
file353,28,"# query all total_claims data and store in raw_total_claims_data dict
 def query_all_total_claims():
  temp_query_list = prediction_list[""uu_id""] # the list of uuids to be queried
  raw_total_claims_data = {uuid:[] for uuid in temp_query_list} # dict to store all query results
  total_week = 37
 

  progress = 0
  total = len(temp_query_list)
  for uuid in temp_query_list:
  progress = progress + 1
  print('\r', ""Querying"", str(progress) + ""/"" + str(total), ""uuid's data..."", end='\r')
  QUERY=""""""
  SELECT week_number, total_claims
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uuid)
  query_job = bigquery_client.query(QUERY)
  res = query_job.to_dataframe().drop_duplicates(ignore_index=True).to_numpy()
 

  # Fill the missing total_claims between week 1 and week 37 with value 0
  index = 0
  for w in range(1,total_week + 1):
  if index == len(res):
  raw_total_claims_data[uuid].append({w:0})
  continue
  if w == res[index][0]:
  raw_total_claims_data[uuid].append({w:res[index][1]})
  index = index + 1
  else:
  raw_total_claims_data[uuid].append({w:0})
  return raw_total_claims_data",data preprocessing,,
file353,29,"# commented for only excuting once
 raw_total_claims_data = query_all_total_claims()",data preprocessing,,
file353,30,"# save query result into csv file
 raw_total_claims_df = pd.DataFrame.from_dict(raw_total_claims_data)
 raw_total_claims_df.to_csv(""raw_total_claims.csv"", index=False)",data preprocessing,save results,
file353,31,"raw_total_claims_df = pd.read_csv(""raw_total_claims.csv"")",data preprocessing,,
file353,32,"for i in range(0, 37):
  for j in range(0, total_uuid):
  d = raw_total_claims_df.iloc[i,j]
  # print(int(d.split("":"")[1].split(""}"")[0]))
  raw_total_claims_df.iloc[i,j] = int(d.split("":"")[1].split(""}"")[0])",data preprocessing,,
file353,33,"raw_total_claims_list = raw_total_claims_df.T.to_numpy()
 raw_total_claims_list = raw_total_claims_list[:]",data preprocessing,,
file353,34,"x_labels = [w for w in range(1, 38)]
 NUM_PLOTS = len(raw_total_claims_list)",data preprocessing,,
file353,35,"plt.figure(figsize=(15,5))
 plt.xticks(x_labels)
 for i in range(0, NUM_PLOTS):
  plt.plot(x_labels, raw_total_claims_list[i], "".-"")",result visualization,,
file353,36,"# fill week 4 and week 23 data with mean values for all uuids
 for row in raw_total_claims_list:
  temp_sum = 0
  w4_val = round(0.5 * (row[2] + row[4]) + 0.01)
  w23_val = round(0.5 * (row[21] + row[23]) + 0.01)
  row[3] = w4_val
  row[22] = w23_val",data preprocessing,,
file353,37,"def mse_loss(y_true, y_pred):
  """"""
  MSE loss function
  Inputs param
  -------------------------
  y_true: list
  The ground truth values
  y_pred: list
  The predicted values
  -------------------------
  return: number
  mean square error of the two input lists
  """"""
  if(len(y_true) != len(y_pred)):
  print(""True label len:"" + str(len(y_true)) + "", Predict label len: "" + str(len(y_pred)))
  raise Exception(""Input lists have different length"")
  mse = np.mean(np.array(y_true) - np.array(y_pred))**2
  return mse",data preprocessing,,
file353,38,"uuid_list = list(raw_total_claims_df.columns)
 pred_results = [round(np.array(row).mean() + 0.01) for row in raw_total_claims_list]",data preprocessing,,
file353,39,"# Mean value as the prediction result
 result_dict = {
  'uu_id':uuid_list, 
  'total_claims': pred_results,
  'week_number': [39 for i in range(total_uuid)]
 }
 result_to_csv = pd.DataFrame(result_dict)
 result_to_csv",save results,,
file353,40,"print(uuid_list[0])
 print(list(raw_total_claims_list[0]))",data exploration,,
file353,41,"print(list(raw_total_claims_df[""8ba19786b86ae124a9d7eaa054f15d23""]))",data exploration,,
file353,42,"result_to_csv.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file353,43,temp_res = [row[-1] for row in raw_total_claims_list],data preprocessing,,
file353,44,"mae_loss(temp_res, pred_results)",evaluation,,
file353,45,"query = """"""
 SELECT Count(DISTINCT(uu_id)) as N_UUID
 FROM ironhacks-data.ironhacks_competition.unemployment_data
 WHERE week_number = 37
 """"""
 query_job = bigquery_client.query(query)
 week_39_count = query_job.to_dataframe()
 week_39_count",load data,data preprocessing,
file353,46,"i = 0
 data = []
 for uu_id in prediction_list[""uu_id""]:
  i = i + 1
  query = """"""
  SELECT *
  FROM ironhacks-data.ironhacks_competition.unemployment_data
  WHERE uu_id=""%s""
  ORDER BY week_number
  """"""%(uu_id)
  print(""Querying #"" + str(i) + "":"", uu_id, end='\r')
  query_job = bigquery_client.query(query)
  data.append(query_job.to_dataframe())",data preprocessing,data exploration,
file353,47,"query = """"""
 SELECT week_number, sum(total_claims) as all_claims
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY week_number
 ORDER BY week_number
 """"""",load data,,
file353,48,bigquery_client.query(query).to_dataframe(),data preprocessing,,
file353,49,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file354,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file354,1,"get_ipython().run_cell_magic('capture', '', ""#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install google-cloud-bigquery\n!pip install google-cloud-bigquery[pandas]\n"")",helper functions,,
file354,2,"#- IMPORT THE LIBRARIES YOU WILL USE
 #------------------------------------------
 # You only need to import packages one time per notebook session. To keep your
 # notebook clean and organized you can handle all imports at the top of your file.
 # The following are included for example purposed, feel free to modify or delete 
 # anything in this section.
 import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import statsmodels.api as sm
 import itertools
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import RandomForestClassifier",helper functions,,
file354,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file354,4,"#query 3: overview of employment_data(week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number ASC;
 """"""
 query_job = bigquery_client.query(query)
 overview = query_job.to_dataframe()
 overview.head()",load data,data preprocessing,
file354,5,"#query 3: overview of prediction list (week 41)
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 query_job = bigquery_client.query(query)
 predn = query_job.to_dataframe()
 predn.head()
 print(predn.head())",load data,data preprocessing,
file354,6,"fig = plt.figure()
 plt.plot([employ.week_number], [employ.total_claims],'bs')
 plt.title('Distribution of claims through week')
 plt.xlabel('Weeks')
 plt.ylabel('Total claims')
 plt.show()",result visualization,,
file354,7,"labels = np.array(overview['total_claims'])
 features = employ.drop(['uu_id'], axis=1)
 feature_list = list(features.columns)
 features = np.array(features)",data preprocessing,,
file354,8,"#split data into train and test sets, split first 20% data
 x_train, x_test, y_train,y_test = train_test_split(features, labels, test_size = 0.20, random_state = 42)",data preprocessing,,
file354,9,"print(f'Training Features Shape: {x_train.shape}')
 print(f'Testing Features Shape: {x_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data exploration,,
file354,10,"from sklearn.ensemble import RandomForestRegressor
 regressor = RandomForestRegressor(n_estimators=1000, random_state=42)
 x = x_train
 y = y_train
 regressor.fit(x,y)",modeling,,
file354,11,"#visualizing the decision tree from the regressor
 from sklearn import tree
 tree.plot_tree(regressor.estimators_[0])",helper functions,,
file354,12,"predictions = regressor.predict(x_test).astype(int)
 predictions = np.round(predictions,decimals = 0, out = None)
 print(predictions)",prediction,data exploration,
file354,13,"errors = abs(y_test - predictions)
 print(f'List of Errors: {errors}')
 print(f'Mean Absolute Error: {np.mean(errors)*10:.4f}%')",evaluation,data exploration,
file354,14,"df = pd.DataFrame(predictions, columns=['total_claims'])
 week41 = predn.join(df).iloc[:,[0,2,1]]
 print(week41)
 print(f'Total predicting number of unemployment claims of week 41: {sum(predictions):.0f}')",data preprocessing,data exploration,
file354,15,"csv_data = week41.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file355,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file355,1,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file355,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file355,3,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file355,4,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file355,5,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file355,6,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",load data,,
file355,7,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file355,8,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file355,9,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file355,10,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file355,11,"def example_function():
  print('Hello World')",helper functions,,
file355,12,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file355,13,"model.fit(x,y)
 LinearRegression",modeling,,
file355,14,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file355,15,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,,
file355,16,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",evaluation,,
file355,17,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file355,18,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,data exploration,
file355,19,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file355,20,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,data exploration,
file355,21,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file355,22,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file355,23,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file355,24,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file355,25,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file355,26,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file355,27,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file355,28,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file355,29,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",evaluation,,
file355,30,"print(f""slope: {new_model.coef_}"")",evaluation,,
file355,31,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,,
file355,32,"# Test Linear Regression
 x",data exploration,,
file355,33,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file355,34,"# Test Linear Regression
 results = model.fit()",modeling,,
file355,35,"# Test Linear Regression
 print(results.summary())",evaluation,,
file355,36,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file355,37,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file355,38,prestige.head(),data exploration,,
file355,39,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file355,40,"get_ipython().run_cell_magic('capture', '', ""\n# INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n# ------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n!python3 -m pip install [ensorflow]\n!python3 -m pip install [Keras]\n!python3 -m pip install [Pytorch]\n"")",helper functions,,
file355,41,get_ipython().system('pip install db-dtypes'),helper functions,,
file355,42,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math",helper functions,,
file355,43,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file355,44,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file355,45,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'",load data,,
file355,46,bigquery.Client(project=BIGQUERY_PROJECT),load data,,
file355,47,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file355,48,"import numpy as np
 from sklearn.linear_model import LinearRegression",helper functions,,
file355,49,"# DEFINING THE DATA; INPUTS (REGRESSOR.X) AND OUTPUTS (RESPONSE.Y)
 x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
 y = np.array([5, 20, 14, 32, 22, 38])",data preprocessing,,
file355,50,"def example_function():
  print('Hello World')",helper functions,,
file355,51,"# CREATE A LINEAR REGRESSION MODEL
 model = LinearRegression()",modeling,,
file355,52,"model.fit(x,y)
 LinearRegression",modeling,,
file355,53,"# GET RESULTS - NOW THAT THE MODEL HAS BEEN FITTED; LET SEE IF IT WORKS!
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file355,54,"new_model = LinearRegression().fit(x,y.reshape((-1,1)))
 print(f""intercept: {new_model.intercept_}"")",modeling,,
file355,55,"# DETERMIINE HOW TO GET b0 and b1 (scikit-learn)
 # intercept and coefficient are estimated values
 print(f""slope: {new_model.coef_}"")",data exploration,,
file355,56,"# NOW THAT WE KNOW OUR MODEL IS WORKING; LET'S PREDICT A RESPONSE!
 y_pred = model.predict(x)
 print(f""predicted response:\n{y_pred}"")",prediction,,
file355,57,"y_pred = model.intercept_ + model.coef_ * x
 print(f""predicted response:\n{y_pred}"")",prediction,,
file355,58,"x_new = np.arange(5). reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file355,59,"# REGRESSION MODELS ARE TYPCIALLY USED FOR FORECASTS
 # FITTED MODELS CAN BE USED TO CALCULATE THE OUTPUTS
 y_new = model.predict(x_new)
 y_new",prediction,data exploration,
file355,60,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file355,61,"query = """"""
 SELECT
 x.*,
 y.average_wage
 FROM
 (SELECT
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) x
 JOIN `ironhacks-data.ironhacks_competition.wage_data` y
 ON x.uu_id = y.uu_id
 """"""",load data,,
file355,62,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file355,63,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file355,64,"# START USING A MODEL - Same statement written differently
 model = LinearRegression().fit(x,y)",modeling,,
file355,65,"# Linear Regression
 model.fit(x, y)
 LinearRegression()",modeling,,
file355,66,"# Test Linear Regression
 model = LinearRegression().fit(x, y)",modeling,,
file355,67,"# Test Linear Regression
 r_sq = model.score(x, y)
 print(f""coefficient of determination: {r_sq}"")",evaluation,,
file355,68,"# Test Linear Regression
 print(f""predicted response:\n{y_pred}"")",prediction,,
file355,69,"print(f""slope: {new_model.coef_}"")",evaluation,,
file355,70,"# Test Linear Regression
 x_new = np.arange(5).reshape((-1, 1))
 x_new",data preprocessing,data exploration,
file355,71,"# Test Linear Regression
 x",data exploration,,
file355,72,"# Test Linear Regression
 model = sm.OLS(y, x)",modeling,,
file355,73,"# Test Linear Regression
 results = model.fit()",modeling,,
file355,74,"# Test Linear Regression
 print(results.summary())",evaluation,,
file355,75,"from statsmodels.compat import lzip
 import numpy as np
 import matplotlib.pyplot as plt
 import statsmodels.api as sm
 from statsmodels.formula.api import ols",helper functions,,
file355,76,"plt.rc(""figure"", figsize=(16, 8))
 plt.rc(""font"", size=14)",helper functions,,
file355,77,prestige.head(),data exploration,,
file355,78,"prestige_model = ols(""prestige ~ income + education"", data=prestige).fit()",modeling,,
file355,79,dta = sm.datasets.statecrime.load_pandas().data,load data,,
file355,80,print(prestige_model.summary()),evaluation,,
file355,81,"crime_model = ols(""claim ~ uu_id + tract + age + single"", data=dta).fit()
 print(crime_model.summary())",modeling,evaluation,
file356,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file356,1,"mport os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file356,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file356,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data preprocessing,
file356,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file356,5,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file356,6,print(covid19_cases_data),data exploration,,
file356,7,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file356,8,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file357,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')
 get_ipython().system('python3 -m pip install google.cloud')
 get_ipython().system('python3 -m pip install pandas')
 get_ipython().system('python3 -m pip install numpy')
 get_ipython().system('python3 -m pip install scikit-learn')
 get_ipython().system('python3 -m pip install plotly')",helper functions,,
file357,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt  
 import os
 import seaborn as sns
 from pandas import Series, DataFrame",helper functions,,
file357,2,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file357,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file357,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file357,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file357,6,wage_data[wage_data.isnull().any(axis=1)],data exploration,,
file357,7,"wage_data2 = wage_data.fillna({'average_wage': wage_data.average_wage.mean()})
 wage_data2.head()",data preprocessing,data exploration,
file357,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""",load data,,
file357,9,"#Merge the data
 unemployment_wage_data = pd.merge(unemployment_data, wage_data, on=['uu_id'], how='inner')
 unemployment_wage_data = unemployment_wage_data.drop(['countyfips_y', 'tract_y', 'tract_name_y'], axis=1)
 unemployment_wage_data = unemployment_wage_data.fillna(0)
 unemployment_wage_data.describe()",data preprocessing,data exploration,
file357,10,unemployment_wage_data.head(),data exploration,,
file357,11,"#Check for duplicated rows
 duplicated_rows = sum(unemployment_wage_data.duplicated()) 
 unemployment_wage_data = unemployment_wage_data.drop_duplicates()",data preprocessing,,
file357,12,unemployment_wage_data[unemployment_wage_data.isnull().any(axis=1)],data exploration,,
file357,13,"#heat map for correlations
 plt.figure(figsize=(25,10))
 cor = unemployment_wage_data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds) 
 plt.show()",result visualization,,
file357,14,unemployment_wage_data.columns,data exploration,,
file357,15,"X = unemployment_wage_data.drop(['total_claims', 'week_number'], axis = 1)
 y = unemployment_wage_data.total_claims",data preprocessing,,
file357,16,"uuid, label = unemployment_wage_data['uu_id'].factorize(sort=True)",data preprocessing,,
file357,17,X['uu_id'] = uuid,data preprocessing,,
file357,18,"X['tract_name_x'] = X['tract_name_x'].factorize()[0]
 X['top_category_employer1'] = X['top_category_employer1'].factorize()[0]
 X['top_category_employer2'] = X['top_category_employer2'].factorize()[0]
 X['top_category_employer3'] = X['top_category_employer3'].factorize()[0]",data preprocessing,,
file357,19,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",data preprocessing,,
file357,20,"from sklearn.ensemble import RandomForestRegressor
 rf = RandomForestRegressor(n_estimators=1000, random_state=42)
 rf.fit(X_train, y_train)",modeling,,
file357,21,"rf.score(X_test, y_test)",evaluation,,
file357,22,prediction_list,data exploration,,
file357,23,"X['uu_id'] = label[X[""uu_id""]]
 X",data preprocessing,data exploration,
file357,24,"for col in X.columns[2:]:
  li = []
  for i in prediction_list['uu_id']:
  li.append(X.loc[X['uu_id'] == i, col].mean())
  prediction_list[col] = li
 prediction_list",data preprocessing,data exploration,
file357,25,prediction_list['uu_id'] = prediction_list['uu_id'].factorize(sort=True)[0],data preprocessing,,
file357,26,"claims_predict = rf.predict(prediction_list)
 claims_predict",prediction,data exploration,
file357,27,submission_df = pd.DataFrame(),data preprocessing,,
file357,28,"submission_df[""uu_id""] = prediction_list[""uu_id""]
 submission_df[""week_number""] = prediction_list[""week_number""]
 submission_df[""total_claims""] = claims_predict",data preprocessing,,
file357,29,submission_df,data exploration,,
file357,30,"submission_df[""uu_id""] = label[submission_df[""uu_id""]]",data preprocessing,,
file357,31,"submission_df.to_csv('submission2_prediction_output.csv', index=False)",save results,,
file358,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file358,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file358,2,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",load data,data preprocessing,
file358,3,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file359,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how use \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file359,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas\n"")",helper functions,,
file359,2,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file359,3,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import plotly.express as px
 import csv",helper functions,,
file359,4,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file359,5,"def querydb(request):
  query_job = bigquery_client.query(request)
  data = query_job.to_dataframe()
  return data",load data,,
file359,6,"def createSubmission():
  return",data preprocessing,,
file359,7,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file359,8,"#Unemployment Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""
 unemployment_data = querydb(query)",load data,,
file359,9,"# Prediction Data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""
 prediction_data = querydb(query)",load data,,
file359,10,"querydb(""""""SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`"""""")",load data,,
file359,11,df_wage = wage_data,data preprocessing,,
file359,12,"#should add another column called previous week claims, this would probably be a good predictor for the next week of claims
 df_un = unemployment_data",data preprocessing,,
file359,13,df_pred = prediction_data,,,
file359,14,"df_un[df_un[""uu_id""] == ""bbcb018f0e5e49e13636f6e78ce9f60f""].sort_values(by = [""week_number""]).drop_duplicates()",data exploration,,
file359,15,"df_pred[""uu_id""]",data exploration,,
file359,16,"prediction_dict = {}
 for i, tract in enumerate(df_pred[""uu_id""]):
  df_working = df_un[df_un[""uu_id""] == tract].sort_values(by = [""week_number""]).drop_duplicates()
  mean = df_working[""total_claims""].mean()
  prediction_dict[tract] = mean",data preprocessing,,
file359,17,"header = [""uu_id"",""total_claims"",""week_number""]
 week = 38
 fname = ""submission_prediction_output.csv""
 with open(fname, 'w', encoding = ""UTF8"", newline="""") as f:
  writer = csv.writer(f)
  writer.writerow(header)
  for k, v in prediction_dict.items():
  writer.writerow([k, v, week])",data preprocessing,,
file359,18,total/count,data exploration,,
file359,19,7.5/21.5,data exploration,,
file359,20,"# how can I pull this from just pandas
 tract_dict = {}",data preprocessing,,
file359,21,"for i, tract in enumerate(df_pred[""uu_id""]):
  tract_dict[i] = df_un[df_un[""uu_id""] == tract].sort_values(by = [""week_number""]).drop_duplicates()",data preprocessing,,
file359,22,"df_wage = wage_data
 print(df_wage)",data preprocessing,data exploration,
file359,23,"# dictionary of df's for each uu_id
 tract_dict = {}",data preprocessing,,
file359,24,tract[0].head(),data exploration,,
file359,25,tract[0],data exploration,,
file359,26,tract_dict[1].head(38),data exploration,,
file359,27,"# compute the simple moving average over 4 weeks for each frame
 for key, val in tract_dict.items():
  tract_dict[key][""SMA4""] = tract_dict[key][""total_claims""].rolling(4).mean()",data preprocessing,,
file359,28,"tract_dict[0][[""SMA4"", ""total_claims""]].plot()",result visualization,,
file359,29,"tract_dict[0][[""total_claims"", ""SMA4"", ""y_bar""]].plot()",result visualization,,
file359,30,"import csv
 import pandas as pd
 import numpy as np
 import seaborn as sns
 import plotly.express as px
 import csv
 import random",helper functions,,
file359,31,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file359,32,"# compute the simple moving average over 4 weeks for each frame
 for key, val in tract_dict.items():
  tract_dict[key][""SMA4""] = tract_dict[key][""total_claims""].rolling(4).mean()
  tract_dict[key][""EMA4""] = tract_dict[key][""total_claims""].ewm(4).mean()
  tract_dict[key][""y_bar""] = tract_dict[key][""total_claims""].mean()",data preprocessing,,
file359,33,"#compare the previous predictor to the new predictor
 tract_dict[0][[""total_claims"", ""SMA4"", ""EMA4"",""y_bar""]].plot()",result visualization,,
file359,34,"tract_dict[0][""EMA4""].iat[-1]",data exploration,,
file360,0,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY tract_name
 ORDER BY week_number 
 """"""",load data,,
file360,1,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.drop_duplicates(inplace = True)
 print(unemployment_data)",load data,data exploration,
file360,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file360,3,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file360,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT week_number, COUNT(total_claims) AS total_claims, COUNT(edu_8th_or_less) AS eigth, COUNT(edu_grades_9_11) AS nine_eleven, COUNT(edu_hs_grad_equiv) AS hs_diploma, COUNT(edu_post_hs) AS post_hs, COUNT(edu_unknown) AS edu_unknown, COUNT(gender_female) AS female, COUNT(gender_male) AS male, COUNT(gender_na) AS gender_na, COUNT(race_amerindian) AS american, COUNT(race_asian) AS asian, COUNT(race_black) AS black, COUNT(race_noanswer) AS r_noAns, COUNT(race_hawaiiannative) AS hawaii, COUNT(race_other) AS other, COUNT(race_white) AS white
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY week_number
 ORDER BY week_number ASC
 """"""
 #
 #SELECT week_number, COUNT(total_claims) AS total_claims, COUNT(edu_8th_or_less) AS 8th, COUNT(edu_grades_9_11) AS 9_11, COUNT(edu_hs_grad_equiv) AS hs_diploma, COUNT(edu_post_hs) AS post_hs, COUNT(edu_unknown) AS edu_unknown, COUNT(top_category_employer1) AS top_category_employer1, COUNT(top_category_employer2) AS top_category_employer2, COUNT(top_category_employer3) AS top_category_employer3, COUNT(gender_female) AS female, COUNT(gender_male) AS male, COUNT(gender_na) AS gender_na, COUNT(race_amerindian) AS american, COUNT(race_asian) AS asian, COUNT(race_black) AS black, COUNT(race_noanswer) AS r_noAns, COUNT(race_hawaiiannative) AS hawaii, COUNT(race_other) AS other, COUNT(race_white) AS white
 #GROUP BY week_number",load data,,
file360,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.drop_duplicates(inplace = True)
 #unemployment_data.plot(subplots=True, figsize=(20,24))
 cor_target = abs(cor[""total_claims""])
 relevant_features = cor_target[cor_target>0.9]
 print(relevant_features)",load data,data preprocessing,
file360,6,"query = """"""
 SELECT 
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file360,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 wage_data = query_job.to_dataframe()
 print(wage_data)",load data,data preprocessing,
file360,8,print(relevant_features),data exploration,,
file360,9,print(unemployment_data),data exploration,,
file360,10,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT week_number,tract_name, COUNT(total_claims) AS total_claims 
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 GROUP BY tract_name
 ORDER BY week_number 
 """"""
 #
 #SELECT week_number, COUNT(total_claims) AS total_claims, COUNT(edu_8th_or_less) AS 8th, COUNT(edu_grades_9_11) AS 9_11, COUNT(edu_hs_grad_equiv) AS hs_diploma, COUNT(edu_post_hs) AS post_hs, COUNT(edu_unknown) AS edu_unknown, COUNT(top_category_employer1) AS top_category_employer1, COUNT(top_category_employer2) AS top_category_employer2, COUNT(top_category_employer3) AS top_category_employer3, COUNT(gender_female) AS female, COUNT(gender_male) AS male, COUNT(gender_na) AS gender_na, COUNT(race_amerindian) AS american, COUNT(race_asian) AS asian, COUNT(race_black) AS black, COUNT(race_noanswer) AS r_noAns, COUNT(race_hawaiiannative) AS hawaii, COUNT(race_other) AS other, COUNT(race_white) AS white
 #GROUP BY week_number",load data,,
file360,11,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)
 query = """"""
 SELECT week_number,tract_name , COUNT(total_claims) AS total_claims
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 ORDER BY week_number 
 """"""",load data,,
file361,0,"#Code for BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file361,1,"#importing libraries
 import numpy as np
 import matplotlib.pyplot as plt
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file361,2,"#Query Unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 order by uu_id
 """"""",load data,,
file361,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file361,4,"query_job = bigquery_client.query(query2)
 wage_data = query_job.to_dataframe()",load data,data preprocessing,
file361,5,"get_ipython().run_cell_magic('capture', '', '\n#Query Unemployment data\nquery = """"""\nSELECT *\nFROM `ironhacks-data.ironhacks_competition.unemployment_data`\norder by tract, week_number\n\n""""""\n\n\n# QUERY THE DATA ONCE\nquery_job = bigquery_client.query(query)\nunemployment_data = query_job.to_dataframe()\nunemployment_data.drop_duplicates()\n')",helper functions,,
file361,6,wage_data.head(),data exploration,,
file361,7,"#Retain relevant columns and merge Data
 unemployment_data=unemployment_data.drop('timeperiod','countyfips','tract','tract_name','top_category_employer1','top_category_employer2','top_category_employer3')
 wage_data=wage_data.drop('countyfips','tract','tract_name')",data preprocessing,,
file361,8,unemployment_data.head(),data exploration,,
file361,9,"#Fill missing data using columnwise linear interpolation in unemployment data
 unemployment_data.interpolate(method='linear', axis=0)",data preprocessing,,
file361,10,type(unemployment_data),data exploration,,
file361,11,type(unemployment_data['race_white']),data exploration,,
file361,12,"#Fill missing data using columnwise linear interpolation in unemployment data
 num_data=unemployment_data.iloc[:,3:-1].astype(int32)
 num_data.interpolate(method='linear', limit_direction='both', axis=0)",data preprocessing,,
file361,13,"#Fill missing data using columnwise linear interpolation in unemployment data
 from sklearn.preprocessing import LabelEncoder",helper functions,,
file361,14,"unemployment_data=LabelEncoder.fit_transform(unemployment_data)
 series=unemployment_data.iloc[:,3].interpolate(method='linear', axis=0)",data preprocessing,,
file361,15,"series=LabelEncoder.fit_transform(unemployment_data.iloc[:,3])
 series=series.interploate(method='linear')",modeling,,
file361,16,"#Fill missing data in unemployment data with 0
 unemployment_data.fillna(0)",data preprocessing,,
file361,17,"#Fill missing data in unemployment data with 0
 unemployment_data.fillna(0)
 #Fill missing data in wage data with pad
 wage_data.fillna(method='pad')",data preprocessing,,
file361,18,"work_data=pd.merge(unemployment_data, wage_data, on='uu_id')
 work_data.head()",data preprocessing,data exploration,
file361,19,"#Separate by uu_id
 work_data_collection={}
 uu_id_list=work_data['uu_id].drop_dulpicates()",data preprocessing,,
file361,20,"for id in uu_id_list:
  work_data_collection[id]=work_data[work_data['uu_id']==id]
  work_data_collection[id].drop(columns=['uu_id'])",data preprocessing,,
file361,21,work_data_collection[uu_id_list[0]],data exploration,,
file361,22,"for id in uu_id_list:
  work_data_collection[id]=work_data[work_data['uu_id']==id]
  work_data_collection[id]=work_data_collection[id].drop(columns=['uu_id'])
  work_data_collection[id]=work_data_collection[id].drop_duplicates()",data preprocessing,,
file361,23,"import seaborn as sns
 sns.heatmap(work_data_collection[uu_id_list[0]])",helper functions,,
file361,24,"work_data=work_data.drop_duplicates()
 X_data=work_data[columns=['week_num','average_wage']]
 y_data=work_data[columns=['total_claims']]",data preprocessing,,
file361,25,"from sklearn import linear_model
 lm = linear_model.LinearRegression()
 model=lm.fit(X,y)",modeling,,
file361,26,"lm.score(X,y)",evaluation,,
file361,27,"lm.score(X_data,y_data)",evaluation,,
file362,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file362,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file362,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file362,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file362,4,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file362,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file362,6,print(covid19_cases_data),data exploration,,
file362,7,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file362,8,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file363,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file363,1,"# Run these terminal commands when your Notebook Session begins
 get_ipython().system('gcloud auth login')
 get_ipython().system('gcloud auth application-default set-quota-project ironhacks-data')",helper functions,,
file363,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file363,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file363,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file363,5,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file363,6,import db-dtypes,helper functions,,
file363,7,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file364,0,"# Importing the required libraries
 from wordcloud import WordCloud, STOPWORDS
 import matplotlib.pyplot as plt",helper functions,,
file364,1,"# Replace end of line character with space
 text_raw.replace('\n', ' ')",data preprocessing,,
file364,2,"# Save a lower-case version of each word to a list
 words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']",data preprocessing,,
file364,3,"# Eliminate non alpha elements
 text_list = [word.lower() for word in words_list if word.isalpha()]",data preprocessing,,
file364,4,"# Transforming the list into a string for displaying
 text_str = ' '.join(text_list)",data preprocessing,,
file364,5,"# Defining the wordcloud parameters
 wc = WordCloud(background_color=""black"", max_words=2000,stopwords=stpwords)",data preprocessing,,
file364,6,"# Generate word cloud
 wc.generate(text_str)",result visualization,,
file364,7,"# Show the cloud
 plt.imshow(wc)
 plt.axis('off')
 # plt.show()",result visualization,,
file364,8,"words_list.plot.bar(x=""word"",y=""freq"")
 plt.show()",result visualization,,
file364,9,"df = pd.DataFrame(words_list)
 print(df)
 # df.plot.bar(x=""word"",y=""freq"")
 # plt.show()",data exploration,,
file364,10,"words = nltk.tokenize.word_tokenize(words_list)
 word_dist = nltk.FreqDist(words)
 rslt = pd.DataFrame(word_dist.most_common(top_N),
  columns=['Word', 'Frequency'])
 print(rslt)",data preprocessing,data exploration,
file364,11,"df = pd.DataFrame.from(words_list,columns=['words'])
 # print(df)
 df.plot.bar(x=""words"")
 plt.show()",result visualization,,
file364,12,"# Crating and updating the stopword list
 # stpwords = set(STOPWORDS)
 # stpwords.add('will')
 # stpwords.add('said')
 top_N=7
 words = nltk.tokenize.word_tokenize(text_str)
 word_dist = nltk.FreqDist(words)
 rslt = pd.DataFrame(word_dist.most_common(top_N),
  columns=['Word', 'Frequency'])
 print(rslt)",data preprocessing,,
file364,13,"# df = pd.DataFrame.from(words_list,columns=['words'])
 # print(df)
 plt.to_file('Trump.png')
 rslt.plot.bar(x=""Word"",y=""Frequency"")
 plt.show()",result visualization,,
file364,14,"rslt.plot.bar(x=""Word"",y=""Frequency"")
 plt.show()
 plt.savefig('Trump.png')",result visualization,,
file365,0,data.info(),data exploration,,
file365,1,data.describe(),data exploration,,
file365,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file365,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file365,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file365,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file365,6,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 # data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file365,7,"final_training_data = pd.get_dummies(data, columns=['uu_id', 'tract_name', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3'])
 final_training_data.head()",data preprocessing,data exploration,
file365,8,data_3 = final_training_data.dropna(axis=0),data preprocessing,,
file365,9,"X = data_3.drop(""total_claims"",1)  #Feature Matrix",data preprocessing,,
file365,10,"y = data_3[""total_claims""] #Target Variable",data preprocessing,,
file365,11,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file365,12,y_pred = model.predict(X),prediction,,
file365,13,y_pred,data exploration,,
file365,14,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file366,0,"get_ipython().system('pip install google-cloud-bigquery')
 get_ipython().system('pip install google-cloud-bigquery[pandas]')",helper functions,,
file366,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file366,2,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file366,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file366,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file366,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file366,6,"X = data.drop(""wind_speed"",1)  #Feature Matrix
 X = data.drop(""date"",1)
 y = data[""wind_speed""] #Target Variable",data preprocessing,,
file366,7,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file366,8,"#Correlation with output variable
 cor_target = abs(cor[""potential_water_deficit""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.5]
 print(relevant_features)",data preprocessing,,
file366,9,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file366,10,"min(data['date']),max(data['date'])",data exploration,,
file366,11,data.dtypes,data exploration,,
file366,12,data = data.set_index('date'),data preprocessing,,
file366,13,data.index,data exploration,,
file366,14,"data['Year'] = data.index.year
 data['Month'] = data.index.month
 # Display a random sampling of 5 rows
 data.sample(5, random_state=0)",data preprocessing,,
file366,15,data.loc['2019-08'],data exploration,,
file366,16,"sns.set(rc={'figure.figsize':(11, 4)})",result visualization,,
file366,17,data['precipitation_data'].plot(linewidth=0.5);,result visualization,,
file366,18,"cols_plot = ['max_rel_humidity','max_temperature','mean_temperature','min_rel_humidity','min_temperature','potential_water_deficit','precipitation_data','wind_speed']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('Precipitation')",result visualization,,
file366,19,"import matplotlib.dates as mdates
 fig, ax = plt.subplots()
 ax.plot(data.loc['2019-08':'2019-12', 'precipitation_data'], marker='o', linestyle='-')
 ax.set_ylabel('Precipitation')
 ax.set_title('Aug 2019-2020 Precipiation Data')",result visualization,,
file366,20,"fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
  sns.boxplot(data=data, x='Month', y=name, ax=ax)
  ax.set_ylabel('precipitation')
  ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
  if ax != axes[-1]:
  ax.set_xlabel('')",result visualization,,
file366,21,"sns.boxplot(data=data, x='Month', y='wind_speed');",result visualization,,
file366,22,"from statsmodels.graphics.tsaplots import plot_acf
 plot_acf(x=data['max_temperature'], lags=50)",result visualization,,
file366,23,"from sklearn.linear_model import LinearRegression
 model = LinearRegression()
 model.fit(X,y)",modeling,,
file366,24,"y_pred = pd.Series(model.predict(X), index=X.index)",prediction,,
file366,25,"ax = y.plot(alpha=0.5)
 ax = y_pred.plot(ax=ax, linewidth=3)",result visualization,,
file366,26,lag_1 = data['precipitation_data'].shift(1),data preprocessing,,
file366,27,data['lag_1']=lag_1,data preprocessing,,
file367,0,import csv,helper functions,,
file367,1,"datContent = [i.strip().split() for i in open(""101_EyeTracking_PreProcessed.dat"").readlines()]",data preprocessing,,
file367,2,"# write it as a new CSV file
 with open(""./flash.csv"", ""wb"") as f:
  writer = csv.writer(f)
  writer.writerows(datContent)",data preprocessing,,
file367,3,"# datContent = [i.strip().split() for i in open(""101_EyeTracking_PreProcessed.dat"").readlines()]
 datContent = np.fromfile(""101_EyeTracking_PreProcessed.dat"", dtype=""byte"")
 datContent",data preprocessing,data exploration,
file367,4,"import csv
 import numpy as np",helper functions,,
file367,5,"dat_file = ""101_EyeTracking_PreProcessed.dat""",data preprocessing,,
file367,6,"with open(dat_file, 'r') as file:
  text = file.read()
  print(text)",load data,data exploration,
file367,7,"data = np.genfromtxt('101_EyeTracking_PreProcessed.dat',
  skip_header=1,
  skip_footer=1,
  names=True,
  dtype=None,
  delimiter=' ')
 print(data)",data preprocessing,data exploration,
file368,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file368,1,"get_ipython().system('pip install db-dtypes')
 get_ipython().system('python3 -m pip install tensorflow')
 get_ipython().system('python3 -m pip install keras')",helper functions,,
file368,2,"import csv
 import pandas as pd
 pd.set_option('display.max_columns', None)
 import numpy as np",helper functions,,
file368,3,"from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file368,4,"from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 from sklearn import linear_model
 from sklearn.experimental import enable_iterative_imputer
 from sklearn.impute import IterativeImputer
 from sklearn.preprocessing import StandardScaler
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import OneHotEncoder",helper functions,,
file368,5,"# REGRESSIONS
 from sklearn.linear_model import LogisticRegression
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import SGDRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.kernel_ridge import KernelRidge
 from sklearn import svm",helper functions,,
file368,6,"# KERAS
 import keras
 from keras import initializers, regularizers
 from keras.layers import Dense, Dropout
 from keras.models import Sequential",helper functions,,
file368,7,"# PLOTS
 import seaborn as sns
 import matplotlib.pyplot as plt",helper functions,,
file368,8,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file368,9,"# QUERY FUNCTION
 def query_from_statement(query):
  query_job = bigquery_client.query(query) # BIGQUERY 
  df = query_job.to_dataframe() # TURNING INTO PANDAS DF
  return df",load data,,
file368,10,"# VALUE INPUTATION FUNCTION
 def value_inputation(g):
  tmp = g.loc[:, g.columns != 'uu_id'].copy()
  tmp = np.clip(np.round(imp.transform(tmp.values)), 0, None)
  g.loc[:, g.columns != 'uu_id'] = tmp.copy()
  return g",data preprocessing,,
file368,11,"u_claims_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 Where week_number between 1 and 39
 """"""",load data,,
file368,12,"unemployment_claims_data = query_from_statement(u_claims_query)
 unemployment_claims_data = unemployment_claims_data.sort_values(by=['uu_id', 'week_number'])",data preprocessing,,
file368,13,"wage_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""",load data,,
file368,14,unemployment_wage_data = query_from_statement(wage_query),data preprocessing,,
file368,15,"duplicated_rows = sum(unemployment_claims_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE 3079 DUPLICATED ROWS BETWEEN WEEKS 1 and 37
 unemployment_claims_data = unemployment_claims_data.drop_duplicates()",data preprocessing,,
file368,16,"duplicated_rows = sum(unemployment_wage_data.duplicated()) # CHECKING FOR DUPLICATED ROWS
 # THERE ARE NO DUPLICATES",data preprocessing,,
file368,17,"data = unemployment_claims_data.join(unemployment_wage_data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING 
 data = data.drop(['countyfips_other', 'tract_other','tract_name_other'], axis=1) # REMOVING REPEATED COLUMNS IN BOTH TABLES",data preprocessing,,
file368,18,"data['tract_name'] = [i.split(',')[1].strip().split(' ')[0] for i in data['tract_name']]",data preprocessing,,
file368,19,"tract_name_encoder = OneHotEncoder(sparse=False)
 tract_name_encoder.fit(data['tract_name'].values.reshape(-1, 1))
 #tract_name_encoder.categories_[0]
 tract_name_dataset = pd.DataFrame(tract_name_encoder.transform(data['tract_name'].values.reshape(-1, 1)), index=data.index, columns= tract_name_encoder.categories_[0])
 data = data.drop(['tract_name'], axis=1)",data preprocessing,,
file368,20,"data = data.drop(['top_category_employer1', 'top_category_employer2',
  'top_category_employer3', 'timeperiod'], axis=1)
 print(data.shape)
 display(data.tail(n=5))",data preprocessing,data exploration,
file368,21,"data = data.apply(pd.to_numeric, errors='ignore')
 data = data.astype(float, errors='ignore')",data preprocessing,,
file368,22,"imp = IterativeImputer(max_iter=10, random_state=0)
 imp.fit(data.loc[:, data.columns != 'uu_id'].values)",data preprocessing,,
file368,23,data = data.groupby(by='uu_id').apply(value_inputation),data preprocessing,,
file368,24,"data = pd.concat([data, tract_name_dataset], axis=1)
 display(data)",data preprocessing,data exploration,
file368,25,"y = np.array(data['total_claims'].values).reshape(-1,1)",data preprocessing,,
file368,26,"input_data_no_claims = data.drop(['total_claims', 'uu_id'], axis=1)
 X = input_data_no_claims.values",data preprocessing,,
file368,27,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
 print(f'Training Features Shape: {X_train.shape}')
 print(f'Testing Features Shape: {X_test.shape}')
 print(f'Training Labels Shape: {y_train.shape}')
 print(f'Testing Labels Shape: {y_test.shape}')",data preprocessing,data exploration,
file368,28,"kernel_init = initializers.RandomNormal(seed=0)
 bias_init = initializers.Zeros()",modeling,,
file368,29,"nn_model = Sequential()
 nn_model.add(Dense(75, activation='relu', use_bias = True, input_shape=(X_train.shape[1], ), kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(50, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(25, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))
 nn_model.add(Dropout(0.1))
 nn_model.add(Dense(1, activation='relu', use_bias = True, kernel_initializer=kernel_init, bias_initializer=bias_init))",modeling,,
file368,30,optimizer = keras.optimizers.Adam(learning_rate=0.001),modeling,,
file368,31,"nn_model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])
 nn_model.summary()",evaluation,,
file368,32,"history = nn_model.fit(X_train, y_train, validation_split=0.1, shuffle=False, epochs=10)",modeling,,
file368,33,"lin_model = LinearRegression().fit(X_train, y_train.ravel())",modeling,,
file368,34,"rf_model = RandomForestRegressor(max_depth=300, random_state=0).fit(X_train, y_train.ravel())",modeling,,
file368,35,"lin_y_pred = lin_model.predict(X_test)
 #svm_y_pred = svm_model.predict(X_test)
 nn_y_pred = nn_model.predict(X_test)
 #krr_y_pred = krr_model.predict(X_test)
 #lasso_y_pred = lasso_model.predict(X_test)
 #logistic_y_pred = logistic_model.predict(X_test)
 #sgd_y_pred = sgd_model.predict(X_test)
 rf_y_pred = rf_model.predict(X_test)",prediction,,
file368,36,"fig, ax = plt.subplots(3,2,figsize=(10,10))
 ax = ax.flatten()",result visualization,,
file368,37,"l_mape = metrics.mean_absolute_percentage_error(y_test, lin_y_pred)
 ax[0].scatter(y_test, lin_y_pred, color='gray', label='Linear Model ' + ""MAPE: "" + str(l_mape.round(2)))
 ax[0].legend()",result visualization,,
file368,38,plt.show(),result visualization,,
file368,39,"prediction_query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file368,40,unemployment_prediction_data = query_from_statement(prediction_query),data preprocessing,,
file368,41,"complete_unemployment_prediction_data = unemployment_prediction_data.join(data.set_index('uu_id'), on='uu_id', rsuffix='_other') # JOINING
 complete_unemployment_prediction_data = complete_unemployment_prediction_data.drop_duplicates(subset=['uu_id'], keep='last')",data preprocessing,,
file368,42,"final_prediction_data = complete_unemployment_prediction_data.drop(['uu_id', 'week_number_other', 'total_claims'], axis=1)
 print(final_prediction_data.shape)
 print(final_prediction_data.columns)",data preprocessing,,
file368,43,"future = final_prediction_data.values
 future_weeks_pred = future_regressor.predict(future)
 print(future_weeks_pred.shape)",prediction,data exploration,
file368,44,"unemployment_prediction_data['total_claims'] = future_weeks_pred.astype(int)
 display(unemployment_prediction_data)",data preprocessing,data exploration,
file368,45,"unemployment_prediction_data.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file368,46,data['average_wage'] = data['average_wage']/1000,data preprocessing,,
file369,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file369,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file369,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file369,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file369,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head()",load data,data preprocessing,
file369,5,unemployment_data.total_claims.describe(),data exploration,,
file369,6,"avg = 22.64
 prediction_list[""total_claims""] = avg",data preprocessing,,
file369,7,prediction_list.head(),data exploration,,
file369,8,"prediction_list = prediction_list[['uu_id', 'total_claims', 'week_number']]",data preprocessing,,
file369,9,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file370,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file370,1,from google.cloud import bigquery,helper functions,,
file370,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file370,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file370,4,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file370,5,unemployment_data,data exploration,,
file370,6,unemployment_data['total_claims'].head,data exploration,,
file370,7,unemployment_data['week_number'].head,data exploration,,
file370,8,unemployment_data.describe(),data exploration,,
file370,9,unmeployment_data[['total_claims']].groupby('week_number').mean(),data exploration,,
file370,10,unemployment_data.groupby('week_number')['total_claims'].mean(),data exploration,,
file370,11,wage_data.head,data exploration,,
file370,12,prediction_list.head(,data exploration,,
file370,13,"merged_data = prediction_list.merge(unemployment_data, on='uu_id',how='left')",data preprocessing,,
file370,14,merged_data,data exploration,,
file370,15,"merged_data.shape,prediction_list.shape",data exploration,,
file370,16,"merged_data.shape,prediction_list.shape,unemployment_data.shape",data exploration,,
file370,17,import pandas as pd,helper functions,,
file370,18,merged_data.shape,data exploration,,
file370,19,merged_data.head(),data exploration,,
file370,20,unemplyment_data.value_counts(),data exploration,,
file371,0,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file371,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file371,2,"import db_dtypes
 import matplotlib.pyplot as plt
 import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import RandomForestRegressor
 from sklearn import metrics
 import numpy as np
 import seaborn as sns",helper functions,,
file371,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file371,4,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file371,5,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file371,6,"unemployment_data = unemployment_data.drop_duplicates()
 unemployment_data.fillna(0, inplace=True)",data preprocessing,,
file371,7,unemployment_data,data exploration,,
file371,8,unemployment_data.isnull().sum(),data exploration,,
file371,9,"unemployment_data = unemployment_data.sort_values(by=['uu_id','week_number']).reset_index()",data preprocessing,,
file371,10,"unemployment_data = unemployment_data.drop('index', axis=1)",data preprocessing,,
file371,11,uuids = unemployment_data.uu_id.unique(),data preprocessing,,
file371,12,"def predict_claims(uuid, week):
  data = unemployment_data[unemployment_data.uu_id == uuid]
 

  plt.plot(data.week_number, data.total_claims)
  plt.show()
  
  X = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]
 

  y = df[['price']]
  
  # Splitting data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 101)
  # Train the Model
 

  regr = RandomForestRegressor(n_estimators = 10, max_depth = 10, random_state = 101)
  regr.fit(X_train, y_train.values.ravel())
  
  predictions = regr.predict(X_test)
  
  result = X_test
  result['total_claims'] = y_test
  result['prediction'] = predictions.tolist()
  
  x_axis = X_test.week_number
  
  plt.scatter(x_axis, y_test, c = 'b', alpha = 0.5, marker = '.', label = 'Real')
  plt.scatter(x_axis, predictions, c = 'r', alpha = 0.5, marker = '.', label = 'Predicted')
 

  plt.xlabel('Week Number')
  plt.ylabel('Total Claims')
  plt.title('Tract: '+uuid)
 

  plt.grid(color = '#D3D3D3', linestyle = 'solid')
 

  plt.legend(loc = 'lower right')
 

  plt.show()
  
  result = result.sort_values(by = 'week_number')
  
  return result.prediction.iloc[-1].round()",modeling,prediction,
file371,13,,,,
file371,14,"predict_claims('0392ee82d61e6b95e117d22d8f732b12',39)",data exploration,,
file372,0,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file372,1,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file372,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file372,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file373,0,"visual_data = data.groupby(['uu_id'])['total_claims'].sum().reset_index().merge(
  data_dict['wage_data'],
  on=['uu_id'],
  how='inner',
 )
 sns.scatterplot(data=visual_data, x='total_claims', y='average_wage', marker='+')",result visualization,,
file373,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file373,2,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!pip install db-dtypes pmdarima 'google-cloud-bigquery[pandas]' tqdm\n"")",helper functions,,
file373,3,"import numpy as np
 import pandas as pd
 from tqdm.auto import tqdm
 import seaborn as sns
 from google.cloud import bigquery",helper functions,,
file373,4,from pmdarima.arima import AutoARIMA,helper functions,,
file373,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file373,6,"def print_missing(data: pd.DataFrame):
  """"""Show how much missing data in each column of the input DataFrame.""""""
  for cname, cvalues in data.items():
  print('Column {} has {} ({}%) missing value(s)'.format(
  cname,
  cvalues.isna().sum(),
  round(100.0 * cvalues.isna().sum() / len(cvalues), 2),
  ))",data exploration,,
file373,7,"data_tables = bigquery_client.query(f""""""
  SELECT table_catalog, table_schema, table_name
  FROM `ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """""").to_dataframe()
 print(data_tables)",load data,,
file373,8,"# Read all data tables in ironhacks-data.ironhacks_competition
 data_dict = {
  table_name: bigquery_client.query(f""""""
  SELECT * FROM `ironhacks-data.ironhacks_competition.{table_name}`
  """""").to_dataframe()
  for table_name in data_tables['table_name'].tolist()
 }",load data,,
file373,9,"common_cols = list(set(data_dict['unemployment_data'].columns) & set(data_dict['wage_data'].columns))
 print(f'Common columns: {common_cols}')
 data = data_dict['unemployment_data'].merge(
  data_dict['wage_data'],
  on=common_cols,
  how='left',
 ).sort_values(['countyfips', 'week_number']).drop_duplicates().reset_index(drop=True)
 data['timeperiod'] = pd.to_datetime(data['timeperiod'], format='%Y%m%d')
 data",data preprocessing,,
file373,10,"print(data.columns)
 print_missing(data)",data exploration,,
file373,11,"for cname in ['edu', 'gender', 'race']:
  cols = [c for c in data.columns if c.startswith(cname)]
  data[cols] = data[cols].fillna(0)
  data[f'{cname}_missing'] = data['total_claims'] - data[cols].sum(axis=1)",data preprocessing,,
file373,12,"cols = [c for c in data.columns if c.startswith('top_category_employer')]
 data[cols] = data[cols].replace('N/A', None)
 # data = pd.get_dummies(data, columns=cols, dummy_na=True)
 print_missing(data)",data preprocessing,data exploration,
file373,13,"cols_race = [c for c in data.columns if c.startswith('race')]
 data.groupby(['timeperiod'])[cols_race].sum().plot(legend=True)",data preprocessing,,
file373,14,"for cname in ['edu', 'gender', 'race']:
  plt.figure()
  cols = [c for c in data.columns if c.startswith(cname)]
  data.groupby(['timeperiod'])[cols_race].sum().plot(legend=True)",data preprocessing,,
file373,15,"# For our own convenience, create a correspondance DataFrame for `week_number` and `timeperiod`.
 wt = pd.Series(
  range(1, 53),
  name='week_number',
  index=pd.date_range('2022-01-01', periods=52, freq='W-SAT').rename('timeperiod'),
 ).reset_index()
 wt['timeperiod'] = wt['timeperiod'].astype(str)
 wt.head()",data preprocessing,,
file374,0,"get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\n!pip install keras\n!pip install tensorflow\n')",helper functions,,
file374,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file374,2,"get_ipython().run_cell_magic('capture', '', 'import pandas as pd\nimport numpy as np\nimport os\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.bigquery import magics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\n')",helper functions,,
file374,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file374,4,"query_main = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file374,5,"query_job = bigquery_client.query(query_main)
 unemployment_data = query_job.to_dataframe()",load data,data preprocessing,
file374,6,"print(unemployment_data.info())
 print(wage_data.info())",data exploration,,
file374,7,"# check shape of both frames to see if they are joinable
 print('Unemployment df size:', unemployment_data.shape)
 print('Wage df size:', wage_data.shape)",data exploration,,
file374,8,unemployment_data.isnull().sum() / len(unemployment_data) * 100,data exploration,,
file374,9,wage_data.isnull().sum() / len(wage_data) * 100,data exploration,,
file374,10,"# replace values with 0
 clean_unemploymentDf = unemployment_data.copy()
 clean_unemploymentDf.fillna(0, inplace=True)
 clean_unemploymentDf",data preprocessing,data exploration,
file374,11,clean_unemploymentDf.isnull().sum() #check,data exploration,,
file374,12,"# check correlation
 correlation = clean_unemploymentDf.corr()
 mask = np.triu(np.ones_like(correlation, dtype=bool))
 plt.figure(figsize=(15,10))
 sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')",data preprocessing,result visualization,
file374,13,"# check wage information
 wage_data[wage_data['average_wage'].isnull()] # there's 3 nulls here - might as well drop them and use this tract to attempt to join the datasets together; or impute with mean",data exploration,,
file374,14,"#wage_data.dropna(axis=0, inplace=True)
 wage_data['average_wage'].fillna(wage_data['average_wage'].mean(), inplace=True)
 wage_data.isnull().sum()",data preprocessing,data exploration,
file374,15,"wage_dupl = wage_data[wage_data.duplicated()]
 print('Duplicate rows: ', wage_dupl)",data preprocessing,,
file374,16,"# join df on tract
 main_df = pd.merge(clean_unemploymentDf, wage_data, on=['tract', 'uu_id'], how='outer')",data preprocessing,,
file374,17,main_df,data exploration,,
file375,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file375,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n#!python3 -m pip install pandas\n"")",helper functions,,
file375,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file375,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file375,4,"!pip install db-dtypes
 query_unemployment = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` 
 """"""
 query = bigquery_client.query(query_unemployment)
 df_unemployment = query.to_dataframe()
 #df_unemployment.head()",load data,,
file375,5,"query_wage = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.wage_data`
 """"""
 query = bigquery_client.query(query_wage)
 df_wage = query.to_dataframe()
 #df_wage.head()",load data,data preprocessing,
file375,6,"df_three_col = df_unemployment[[""uu_id"", ""week_number"", ""total_claims""]]
 #df_three_col.shape",data preprocessing,,
file375,7,"df_three_col = df_three_col.drop_duplicates()
 #print(df_three_col.shape)",data preprocessing,,
file375,8,"df_three_col.sort_values(by=['uu_id', ""week_number""],inplace=True)
 #df_three_col.head(21)
 #df_three_col.tail(3)
 #uu_id week_number total_claims
 #1876 001cd9ae23064d7f0fd3cd327c873d8d 31 34",data preprocessing,,
file375,9,"tmp_df = df_three_col['week_number']
 tmp_df = tmp_df.drop_duplicates()
 print(tmp_df)",data preprocessing,data exploration,
file375,10,"res = pd.DataFrame(columns = ['uu_id', 'total_claims', 'week_number'])",data preprocessing,,
file375,11,"for cur_uu_id in df_pred_list['uu_id']:
  #print(uu_id)
  test_data = df_three_col[df_three_col[""uu_id""].isin([cur_uu_id]) ]
  #test_data = test_data.tail(3)
  week_list = test_data['week_number'].tolist()
  y_pred = -1
  count = 0
  sum = 0
  for week_id in range(33,38):
  if week_id in week_list:
  count+=1
  tmp_row = test_data[test_data['week_number'] == week_id ]
  sum+= int( tmp_row['total_claims'])
  if count > 0:
  y_pred = int(sum/count)
  else:
  max_week = max(week_list)
  tmp_row = test_data[test_data['week_number'] == max_week ] 
  y_pre = tmp_row['total_claims']
  cur_row = pd.DataFrame([[cur_uu_id, y_pred, 42]], columns=['uu_id', 'total_claims', 'week_number'] )
  res = pd.concat([res,cur_row] ,ignore_index = True)",data preprocessing,,
file375,12,"res.to_csv(""Nov28_submission_prediction_output.csv"", index=False)",save results,,
file376,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file376,1,get_ipython().system('python3 -m pip install db-dtypes pmdarima'),helper functions,,
file376,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn import metrics
 import matplotlib.pyplot as plt
 import seaborn as sns
 import pmdarima as pm
 from pmdarima.arima import auto_arima",helper functions,,
file376,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file376,4,"query = """"""
 Select a.*, b.average_wage FROM 
 (SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 order by uu_id, week_number
 """"""",load data,,
file376,5,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 empdata = query_job.to_dataframe().drop_duplicates().fillna(0)",load data,data preprocessing,
file376,6,empdata.head().transpose(),data exploration,,
file376,7,"plt.figure(figsize=(20,15))
 cor = empdata.corr(numeric_only=True)
 sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file376,8,"empdata = empdata[['uu_id', 'week_number', 'total_claims']]
 empdata.head(10)",data preprocessing,data exploration,
file376,9,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""].plot(x='week_number', y='total_claims')",data exploration,,
file376,10,empdata[['total_claims']].describe(),data exploration,,
file376,11,plt.plot(np.sort(empdata['total_claims'].values)),result visualization,,
file376,12,"meancpw = []
 for i in np.sort(empdata['week_number'].unique()):
  meancpw.append(empdata.loc[empdata['week_number'] == i][['total_claims']].mean())
 plt.plot(meancpw)",data preprocessing,result visualization,
file376,13,"empdata.plot.box('week_number', figsize=(25,15))",result visualization,,
file376,14,"empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""]",data exploration,,
file376,15,"testuu_id = empdata.loc[empdata['uu_id'] == ""005be9532fd717dc36d4be318fd9ad25""][['week_number', 'total_claims']]
 testuu_id = testuu_id.set_index('week_number')
 allweeks = pd.DataFrame({'week_number':range(1,37+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id.median()))
 allweeks",data preprocessing,data exploration,
file376,16,"m = pm.auto_arima(allweeks['total_claims'].values, error_action='ignore')
 m.predict(n_periods=6)[5]",modeling,prediction,
file376,17,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file376,18,uupred.head(),data exploration,,
file376,19,"uupred['total_claims'] = 0
 uupred.head()",data preprocessing,data exploration,
file376,20,"def dopred(lastw, predw):
  for uu in uupred['uu_id'].values:
  testuu_id = empdata.loc[empdata['uu_id'] == uu][['week_number', 'total_claims']]
  testuu_id = testuu_id.set_index('week_number')
  allweeks = pd.DataFrame({'week_number':range(1,lastw+1)}).join(testuu_id, on='week_number').fillna(int(testuu_id['total_claims'].median()))
  m = pm.auto_arima(allweeks['total_claims'].values[:lastw], seasonal=False, error_action='ignore')
  pred = int(m.predict(n_periods=predw-lastw)[predw-lastw-1])
  uupred.loc[uupred['uu_id'] == uu, ['total_claims']] = pred
  print(uu, int(pred))",modeling,prediction,
file376,21,"dopred(37,43)",prediction,,
file376,22,"uupred = uupred[['uu_id', 'total_claims', 'week_number']]
 uupred.to_csv('submission_prediction_output.csv', index=False)",save results,,
file376,23,1,,,
file377,0,"import numpy as np
 import pandas as pd
 import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 pd.set_option('display.max_columns', None)",helper functions,,
file377,1,"get_ipython().run_line_magic('pip', 'install db-dtypes')",helper functions,,
file377,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file377,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file377,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 df = query_job.to_dataframe()",load data,data preprocessing,
file377,5,df.head(),data exploration,,
file377,6,df.info,data exploration,,
file377,7,df.describe(),data exploration,,
file378,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file378,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file378,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file378,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file378,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data preprocessing,
file378,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file378,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file378,7,print(covid19_cases_data),data exploration,,
file378,8,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file379,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file379,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file379,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file379,3,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file379,4,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 covid19_cases_data.head()",load data,data preprocessing,
file379,5,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file379,6,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file379,7,print(covid19_cases_data),data exploration,,
file379,8,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file379,9,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
file380,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics
 import matplotlib.pyplot as plt",helper functions,,
file380,1,print(X_train),data exploration,,
file380,2,"import os
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file380,3,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_training.weather_data`
 """"""",load data,,
file380,4,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file380,5,"X = data['min_temperature'].values.reshape(-1,1)
 y = data['max_temperature'].values.reshape(-1,1)",data preprocessing,,
file380,6,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file380,7,class(X_train),data exploration,,
file381,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file381,1,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n\n!python3 -m pip install pandas db-dtypes\n"")",helper functions,,
file381,2,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file381,3,"def example_function():
  print('Hello World')",helper functions,,
file381,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file381,5,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file381,6,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 print(unemployment_data)",load data,data preprocessing,
file381,7,"query_job = bigquery_client.query(query)
 prediction_list = query_job.to_dataframe()
 print(prediction_list)",load data,data preprocessing,
file381,8,"len(pd.unique(unemployment_data[""uu_id""]))",data exploration,,
file381,9,print(unemployment_data.loc[0]),data exploration,,
file381,10,"# remove unnecessary colomns and combine unemployment data together with wages
 unemployment_sample = unemployment_data.copy()
 unemployment_sample = unemployment_sample.drop(['timeperiod', 'tract', 'tract_name', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs',\
  'edu_unknown', 'top_category_employer1', 'top_category_employer2', 'top_category_employer3', 'gender_female', 'gender_male',\
  'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 'race_hawaiiannative',\
  'race_other', 'race_white'], axis=1)
 wage_sample = wage_data.copy()
 wage_sample.drop(['tract', 'tract_name'], axis=1)
 prediction_sample = prediction_list.copy()
 wages_dict = {i:wage_sample[""average_wage""].values[k] for k,i in enumerate(wage_sample[""uu_id""])}
 wage_list = [wages_dict[i] for i in unemployment_sample[""uu_id""]]
 # print(wages_dict)
 unemployment_sample[""average_wage""] = wage_list",data preprocessing,,
file381,11,"tract_dic = {i:k for k,i in enumerate(wage_sample[""uu_id""])}",data preprocessing,,
file381,12,"unemployment_sample[""uu_id""] = [tract_dic[i] for i in unemployment_sample[""uu_id""]]",data preprocessing,,
file381,13,"X = unemployment_sample.drop([""total_claims""], axis=1).to_numpy() #to_numpy() values
 y = unemployment_sample['total_claims'].to_numpy()  #to_numpy() values
 # print(X.shape,y.shape)",data preprocessing,,
file381,14,"from sklearn.model_selection import train_test_split 
 from sklearn.linear_model import LinearRegression
 from sklearn import metrics",helper functions,,
file381,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",data preprocessing,,
file381,16,"regressor = LinearRegression()  
 regressor.fit(X_train, y_train) #training the algorithm",modeling,,
file381,17,print(X),data exploration,,
file381,18,"for k,i in enumerate(X):
  if np.nan in i:
  print(k,i)",data preprocessing,,
file381,19,"for k,i in enumerate(X):
  for j in i:
  if pd.isna(j)
  print(k,i)",data preprocessing,,
file381,20,"for k,i in enumerate(wage_list):
  if pd.isna(i):
  print(k,i)
  # for j in i:
  #  if pd.isna(j):
  #  print(k,i)",data preprocessing,,
file381,21,unemployment_sample = unemployment_sample.dropna(),data preprocessing,,
file381,22,"#To retrieve the intercept:
 print(regressor.intercept_)",evaluation,,
file381,23,"#For retrieving the slope:
 print(regressor.coef_)",evaluation,,
file381,24,"y_pred = regressor.predict(X_test)
 df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
 df",prediction,,
file381,25,"df1 = df.head(25)
 df1.plot(kind='bar',figsize=(16,10))
 plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
 plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
 plt.show()",result visualization,,
file381,26,"plt.scatter(X_test, y_test, color='gray')
 plt.plot(X_test, y_pred, color='red', linewidth=2)
 plt.show()",result visualization,,
file381,27,"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
 print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
 print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))",data exploration,,
file382,0,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file382,1,"# QUERY THE DATA ONCE
 query1_job = bigquery_client.query(query1)
 pdf = query1_job.to_dataframe()
 pdf.head()",load data,data preprocessing,
file382,2,get_ipython().system('pip install db-dtypes'),helper functions,,
file382,3,"import os
 import pandas
 import pandas as pd
 import numpy as np
 import matplotlib
 import itertools
 import matplotlib.pyplot as plt
 import seaborn as sns
 # import lightgbm as lgb
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split",helper functions,,
file382,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file382,5,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` d
 LEFT JOIN `ironhacks-data.ironhacks_competition.wage_data` i
 ON d.uu_id = i.uu_id;
 """"""",load data,,
file382,6,"# saving data to avoid pulling it each time from the server
 wdf.to_csv(""wage.csv"")
 udf.to_csv(""unemploy.csv"")
 Mdf.to_csv(""mixed.csv"")
 pdf.to_csv(""pred.csv"")",data preprocessing,,
file382,7,"# reading from the saved data
 udf = pd.read_csv(""unemploy.csv"")
 wdf = pd.read_csv(""wage.csv"")
 Mdf = pd.read_csv(""mixed.csv"")
 pdf = pd.read_csv(""pred.csv"")",load data,,
file382,8,"print(udf[""week_number""].unique())
 print(Mdf[""week_number""].unique())
 print(pdf[""week_number""].unique())
 # udf.nunique()",data exploration,,
file382,9,"# cleaning the data by imputing null and nan with 0
 mdf = Mdf.replace(np.nan, 0)
 udf = udf.replace(np.nan, 0)",data preprocessing,,
file382,10,"# understanding the datatypes for each columns of the dataframe
 mdf.dtypes",data exploration,,
file382,11,"# removing unwanted columns
 mdf_1 = mdf.drop([""uu_id_1"",""countyfips_1"",""tract_1"",""tract_name_1"",""tract_name""],axis = 1)
 mdf_1.columns",data preprocessing,data exploration,
file382,12,"# there was a N/A value left that we caught while imputing and converting values to float
 mdf_1[""top_category_employer1""].unique()",data exploration,,
file382,13,"# imputing and converting values to float
 emp_cols = [""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('31-33', ""31.5"")
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('48-49', ""48.5"")
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('44-45', ""44.5"")
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('N/A', ""0.0"")",data preprocessing,,
file382,14,"mdf_1[""top_category_employer1""] = pd.to_numeric(mdf_1[""top_category_employer1""])
 mdf_1[""top_category_employer2""] = pd.to_numeric(mdf_1[""top_category_employer2""])
 mdf_1[""top_category_employer3""] = pd.to_numeric(mdf_1[""top_category_employer3""])",data preprocessing,,
file382,15,mdf_1.dtypes,data exploration,,
file382,16,mdf_1.describe(),data exploration,,
file382,17,mdf_1.info(),data exploration,,
file382,18,"mdf_2 = mdf_1.groupby(""uu_id"").median()
 mdf_2.head(5)",data preprocessing,data exploration,
file382,19,"plt.figure(figsize=(20,5))
 plt.plot(mdf_2['total_claims'])
 plt.show()",result visualization,,
file382,20,"k = 0
 u_uuid = []
 for i, row in mdf_1.iterrows():
  if row[""uu_id""] not in u_uuid:
  if row[""week_number""] in [37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18]:
  k+=1
  u_uuid.append(row[""uu_id""])",data preprocessing,,
file382,21,"print(len(u_uuid))
 k",data exploration,,
file382,22,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file382,23,"# QUERY THE DATA ONCE
 query1_job = bigquery_client.query(query1)
 pdf = query1_job.to_dataframe()
 print(len(pdf))
 pdf.head()",load data,data preprocessing,
file382,24,"import os
 import pandas
 import pandas as pd
 import numpy as np
 import matplotlib
 import itertools
 import matplotlib.pyplot as plt
 import seaborn as sns
 # import lightgbm as lgb
 import statsmodels.api as sm
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 from sklearn import linear_model
 from sklearn.model_selection import train_test_split",helper functions,,
file382,25,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file382,26,"query1 = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data` d
 LEFT JOIN `ironhacks-data.ironhacks_competition.wage_data` i
 ON d.uu_id = i.uu_id;
 """"""",load data,,
file382,27,"# saving data to avoid pulling it each time from the server
 wdf.to_csv(""wage.csv"")
 udf.to_csv(""unemploy.csv"")
 Mdf.to_csv(""mixed.csv"")
 pdf.to_csv(""pred.csv"")",data preprocessing,,
file382,28,"# reading from the saved data
 udf = pd.read_csv(""unemploy.csv"")
 wdf = pd.read_csv(""wage.csv"")
 Mdf = pd.read_csv(""mixed.csv"")
 pdf = pd.read_csv(""pred.csv"")",load data,,
file382,29,"print(udf[""week_number""].unique())
 print(Mdf[""week_number""].unique())
 print(pdf[""week_number""].unique())
 # udf.nunique()",data exploration,,
file382,30,"# cleaning the data by imputing null and nan with 0
 mdf = Mdf.replace(np.nan, 0)
 udf = udf.replace(np.nan, 0)",data preprocessing,,
file382,31,"# understanding the datatypes for each columns of the dataframe
 mdf.dtypes",data exploration,,
file382,32,"# removing unwanted columns
 mdf_1 = mdf.drop([""uu_id_1"",""countyfips_1"",""tract_1"",""tract_name_1"",""tract_name""],axis = 1)
 mdf_1.columns",data preprocessing,data exploration,
file382,33,"# there was a N/A value left that we caught while imputing and converting values to float
 mdf_1[""top_category_employer1""].unique()",data exploration,,
file382,34,"# imputing and converting values to float
 emp_cols = [""top_category_employer1"", ""top_category_employer2"" ,""top_category_employer3""]
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('31-33', ""31.5"")
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('48-49', ""48.5"")
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('44-45', ""44.5"")
 mdf_1[emp_cols] = mdf_1[emp_cols].replace('N/A', ""0.0"")",data preprocessing,,
file382,35,"mdf_1[""top_category_employer1""] = pd.to_numeric(mdf_1[""top_category_employer1""])
 mdf_1[""top_category_employer2""] = pd.to_numeric(mdf_1[""top_category_employer2""])
 mdf_1[""top_category_employer3""] = pd.to_numeric(mdf_1[""top_category_employer3""])",data preprocessing,,
file382,36,mdf_1.dtypes,data exploration,,
file382,37,mdf_1.describe(),data exploration,,
file382,38,mdf_1.info(),data exploration,,
file382,39,"mdf_2 = mdf_1.groupby(""uu_id"").median()
 mdf_2.head(5)",data preprocessing,data exploration,
file382,40,"plt.figure(figsize=(20,5))
 plt.plot(mdf_2['total_claims'])
 plt.show()",result visualization,,
file382,41,"k = 0
 u_uuid = []
 for i, row in mdf_1.iterrows():
  if row[""uu_id""] not in u_uuid:
  if row[""week_number""] in [37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,21,20,19,18,17,16]:
  k+=1
  u_uuid.append(row[""uu_id""])",data preprocessing,,
file382,42,"print(len(u_uuid))
 k",data exploration,,
file382,43,"# finding the uuids that do not have week 37
 mdf_5 = mdf_1.groupby(['uu_id'], sort=False)['week_number'].max()
 u_1uid = []",data preprocessing,data exploration,
file382,44,"for row in mdf_5.index:
  if mdf_5[row] != 37:
  u_1uid.append(row)",data preprocessing,,
file382,45,"print(len(u_1uid))
 len(u_1uid)",data exploration,,
file382,46,"arr = [0]*93
 arr_week = [37]*93
 dct = {}
 coll = mdf_1.columns",data preprocessing,,
file382,47,"for cal in coll:
  if cal == 'uu_id':
  dct[cal] = u_1uid
  elif cal == 'week_number':
  dct[cal] = arr_week
  else:
  dct[cal] = arr",data preprocessing,,
file382,48,"mdf_6 = pd.DataFrame(dct)
 mdf_6 = mdf_6.append(mdf_1, ignore_index = True)
 len(mdf_6)",data preprocessing,data exploration,
file382,49,"# understanding the datatypes for each columns of the dataframe
 mdf = mdf.drop(""Unnamed: 0"", axis =1)
 mdf.dtypes",data preprocessing,data exploration,
file382,50,"print(mdf_1.columns())
 dct.keys()",data exploration,,
file382,51,"mdf_p = mdf_6[mdf_6[""week_number""] == 37]
 mdf_r = mdf_6[mdf_6[""week_number""] != 37]
 mdf_r = mdf_r.drop_duplicates(subset=[""uu_id"", ""week_number""])
 mdf_p = mdf_p.groupby(""uu_id"").max()
 mdf_p.head(5)",data preprocessing,,
file382,52,"f1_r = mdf_r[""week_number""] - 7
 mdf1_r = mdf_r.copy()
 mdf1_r[""week_number""] = f1_r
 mdf1_r[""week_number""] 
 mdf2_r = mdf1_r[mdf1_r[""week_number""] > 0]
 mdf2_r.head(5)",data preprocessing,data exploration,
file382,53,"k = mdf_r[""week_number""].max()+1
 dct1 = {}
 inx = []
 for x in mdf_p.index:
  k+=1
  dct1[x] = k
  inx.append(k)",data preprocessing,,
file382,54,"mdf_p[""int_uu_id""]= inx
 mdf_p[""nwk""]= mdf_p[""int_uu_id""] + mdf_p[""week_number""]
 mdf_p.head(5)",data preprocessing,data exploration,
file382,55,"inx1 = []
 for i,rows in mdf_r.iterrows():
  inx1.append(dct1[rows[""uu_id""]])",data preprocessing,,
file382,56,"# train test split
 X = mdf_r.drop([""uu_id"",""week_number"",""total_claims""], axis = 1)
 y = mdf_r[""total_claims""]",data preprocessing,,
file382,57,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)",data preprocessing,,
file382,58,"# training and fitting the model
 regr = linear_model.LinearRegression()
 regr.fit(X_train, y_train)
 regr.score(X_test,y_test)",modeling,evaluation,
file382,59,"# prepare the data to predict
 pre = mdf_p.drop([""week_number"",""total_claims""], axis = 1)
 predict = regr.predict(pre)
 print(len(predict))",prediction,data exploration,
file382,60,"lxt1 = [i for i in range(1,501)]
 wk = [44]*500
 d = {'uu_id': mdf_p.index, ""total_claims"": predict, 'week_number': wk}
 rmdf = pd.DataFrame(d)
 rmdf.index = lxt1
 print(len(rmdf))
 rmdf.head(5)",data preprocessing,,
file382,61,"rmdf.to_csv(""submission_prediction_output.csv"",index=False)",save results,,
file382,62,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file383,0,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import statsmodels.api as sm
 import math
 import plotly.express as px
 from pmdarima.arima import auto_arima
 import collections.abc
 #hyper needs the four following aliases to be done manually.
 collections.Iterable = collections.abc.Iterable
 collections.Mapping = collections.abc.Mapping
 collections.MutableSet = collections.abc.MutableSet
 collections.MutableMapping = collections.abc.MutableMapping
 import hts
 from hts.hierarchy import HierarchyTree
 from hts.model import AutoArimaModel
 from hts import HTSRegressor",helper functions,,
file383,1,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file383,2,"get_ipython().run_cell_magic('capture', '', ""\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\n#------------------------------------------\n# This is normally not required. The hub environment comes preinstaled with \n# many packages that you can already use without setup. In case there is some\n# other library you would like to use that isn't on the list you run this command\n# once to install them. If it is already installed this command has no effect.\n!pip install db-dtypes\n!python3 -m pip install pandas\n!pip install pmdarima\n!pip install plotly==5.11.0\n!pip install scikit-hts[auto-arima]\n"")",helper functions,,
file383,3,"def dataExplore(data):
  '''
  Explore dataframe
  '''
  print(""# of observations: "", data.shape[0])
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  print(""# of %s: %s"" % (col, len(pd.unique(data[col]))))
  else:
  print(""Unique value of %s: %s"" % (col, pd.unique(data[col])))",data exploration,,
file383,4,"def dataBalanceCheck(data):
  '''
  Check the balance of data frame
  '''
  unbalance_count = 0
  print(""# of observations in complete time series: "", len(pd.unique(data[""week_number""])))
  for id in pd.unique(data[""uu_id""]):
  if len(data[data[""uu_id""] == id]) < len(pd.unique(data[""week_number""])):
  print(id, len(data[data[""uu_id""] == id]))
  unbalance_count += 1
  print(""% of tracts with incomplete time series: "", unbalance_count / len(pd.unique(data[""uu_id""]))*100)",data preprocessing,data exploration,
file383,5,"def dataFillNa(data, value):
  """"""
  fill NA with given value in the dataframe
  """"""
  for col in data.columns:
  if col in [""uu_id"", ""timeperiod"", ""week_number"", ""countyfips"", ""tract"", ""tract_name"", ""date""]:
  pass
  elif col in [""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3""]:
  data[col] = data[col].replace({'N/A':str(value)})
  else:
  data[col] = data[col].fillna(value)
  return(data)",data preprocessing,,
file383,6,"def dataIdentifyDWM(data):
  '''
  Input: # of week. Output: data for the first day, its month and week order in the month
  '''
  data[""date""] = pd.to_datetime(2022 * 1000 + (1+(data[""week_number""]-1)*7), format='%Y%j')
  data[""month""] = pd.DatetimeIndex(data[""date""]).month
  data[""weekofmonth""]= pd.to_numeric(data[""date""].dt.day/7)
  data['weekofmonth'] = data['weekofmonth'].apply(lambda x: math.ceil(x))
  return(data)",data preprocessing,,
file383,7,"def MSPE(s1, s2):
  return(sum((s1 - s2)**2)/len(s1))",data preprocessing,,
file383,8,"def ARIMA_predict(df_input, cutoff_rate = 0.8, n_period = 15):
  cutoff = int(cutoff_rate * len(df_input))
  if cutoff_rate < 1:
  valid = df_input[cutoff:]
  train = df_input[:cutoff]
  model = auto_arima(train, trace=False, error_action='ignore', suppress_warnings=True)
  model.fit(train)
  forecast = model.predict(n_period)
  return(forecast)",modeling,prediction,
file383,9,"# Obtain data using BigQuery
 BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file383,10,"query = """"""
 SELECT
 a.*,
 b.average_wage
 FROM 
 (SELECT 
 *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`) a
 JOIN `ironhacks-data.ironhacks_competition.wage_data` b 
 ON a.uu_id=b.uu_id
 """"""",load data,,
file383,11,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()",load data,data preprocessing,
file383,12,"query_pred = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.prediction_list`
 """"""",load data,,
file383,13,"# Explore input data for NA and special values
 # dataExplore(data)
 # dataExplore(data_pred_query)
 # data_pred_query.head()
 data.head()",data exploration,,
file383,14,"# Further check tracts with average_wage as Nan
 # I find three tracts with all average_wage as Nan. If I drop these tracts due to Nan value, they cannot be predicted
 for id in pd.unique(data[data['average_wage'].isna()][""uu_id""]):
  print(id)
  print(""All value are nan?"", data[data['uu_id'] == id][""average_wage""].isnull().all())
  print(""Included in prediction list?"", len(data_pred_query[data_pred_query['uu_id'] == id]) > 0)",data exploration,,
file383,15,"# Backup the data before pre-treatment
 data_backup = data.copy()
 data_pred_query_backup = data_pred_query.copy()",data preprocessing,,
file383,16,"# Pretreatment: convert week_number to month and week of month, to capture seasonality
 data = dataIdentifyDWM(data)",data preprocessing,,
file383,17,"# Check if the dataset is a balance panel (all tracts have value for all time periods)
 # 54% of tracts has less than 35 observations (total number of full time series), indicating it is unbalanced
 # Even if only checking data afer 2022/6/1, there are still 36% of tracts with incomplete series
 dataBalanceCheck(data)
 dataBalanceCheck(data[data[""date""] > ""2022-06-01""])",data preprocessing,,
file383,18,"# To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file383,19,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,,
file383,20,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file383,21,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file383,22,dataBalanceCheck(data_balance),data preprocessing,,
file383,23,"# Data clean up: convert NA to 0 for gender, race, education and top employer and recalculate unknown category
 # Based on the check of Nan in average_wage above, I also convert Nan to zero as well, but try models with and without ""average_wage"" variable
 data = dataFillNa(data, 0)",data preprocessing,,
file383,24,"# Split data to training and validaton sets
 # Max trainweek is 37, use a 80 / 20 rule
 train_week = int(max(pd.unique(data[""week_number""]))*0.8)",data preprocessing,,
file383,25,"data_train = data[data[""week_number""] < train_week]
 data_valid = data[data[""week_number""] >= train_week]",data preprocessing,,
file383,26,"data_train_x = data_train.drop(""total_claims"",1)
 data_train_y = data_train[""total_claims""]",data preprocessing,,
file383,27,"data_balance_valid_x = data_balance_valid.drop(""total_claims"",1)
 data_balance_valid_y = data_balance_valid[""total_claims""]",data preprocessing,,
file383,28,data_balance_valid_y.shape,data exploration,,
file383,29,"# Model 1 : Poisson regression with unbalanced data
 data_train_x_m1 = data_train_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_train_x_m1[""month""] = data_train_x_m1[""month""].astype(str)
 data_train_x_m1[""weekofmonth""] = data_train_x_m1[""weekofmonth""].astype(str)
 data_train_x_m1[""week_number2""] = data_train_x_m1[""week_number""]**2
 data_train_x_m1 = pd.get_dummies(data_train_x_m1)",data preprocessing,,
file383,30,"data_train_x_m1[""month_8""] = 0
 data_train_x_m1[""month_9""] = 0",data preprocessing,,
file383,31,"for i in range(8):
  data_valid_x_m1[""month_""+str(1+i)] = 0",data preprocessing,,
file383,32,"for i in range(5):
  if i == 1:
  pass
  data_valid_x_m1[""weekofmonth_""+str(1+i)] = 0",data preprocessing,,
file383,33,"poission_model = sm.GLM(data_train_y.astype(int), data_train_x_m1.astype(float), family=sm.families.Poisson())
 result = poission_model.fit()
 result.summary()",modeling,,
file383,34,"data_estimate_m1 = result.predict(data_valid_x_m1.astype(float))
 print(""MAPE: "", MAPE(data_estimate_m1, data_valid_y))
 print(""MSPE: "", MSPE(data_estimate_m1, data_valid_y))",prediction,data exploration,
file383,35,"data_balance_valid_x_m1 = data_balance_valid_x[[""week_number"",""month"", ""weekofmonth"", ""average_wage""]]
 data_balance_valid_x_m1[""month""] = data_balance_valid_x_m1[""month""].astype(str)
 data_balance_valid_x_m1[""weekofmonth""] = data_balance_valid_x_m1[""weekofmonth""].astype(str)
 data_balance_valid_x_m1[""week_number2""] = data_balance_valid_x_m1[""week_number""]**2
 data_balance_valid_x_m1 = pd.get_dummies(data_balance_valid_x_m1)",data preprocessing,,
file383,36,"# Model 3 time series
 # First, visualize average total_claim
 data_balance_ave = data_balance[[""week_number"", ""total_claims"", ""uu_id""]]
 data_balance_ave = data_balance_ave.groupby(['week_number']).mean()
 data_balance_ave = data_balance_ave.reset_index()
 data_balance_ave['MA4'] = data['total_claims'].rolling(4).mean()
 fig = px.line(data_balance_ave, x=""week_number"", y=[""total_claims"", ""MA4""])
 fig.show()",data preprocessing,,
file383,37,"# model train and validation
 MAPE_list = []
 MSPE_list = []",data preprocessing,,
file383,38,"uu_id_list = pd.unique(data_balance[""uu_id""])",data preprocessing,,
file383,39,"for i in range(len(uu_id_list)):
  print(i)
  data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[i]]
  data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
  data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
  forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)
  
  df_forecast = pd.DataFrame(forecast)
  df_forecast.index.name = ""week_number""
  df_forecast.columns = [""total_claim_pred""]
  
  data_balance_ave_valid_check = data_balance_ave_valid.merge(df_forecast, on = ""week_number"")
  MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
  MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
  
  MAPE_list.append(MAPE_series)
  MSPE_list.append(MSPE_series)",data preprocessing,prediction,
file383,40,MAPE_list,data exploration,,
file383,41,"i = 0
 data_balance_tract = data_balance[data_balance[""uu_id""] == uu_id_list[i]]
 data_balance_tract_model = data_balance_tract[[""week_number"",""total_claims""]]
 data_balance_tract_model = data_balance_tract_model.set_index(""week_number"")
 forecast = ARIMA_predict(data_balance_tract_model, cutoff_rate = 0.8, n_period = 15)",data preprocessing,prediction,
file383,42,"df_forecast = pd.DataFrame(forecast)
 df_forecast.index.name = ""week_number""
 df_forecast.columns = [""total_claim_pred""]",data preprocessing,,
file383,43,"data_balance_ave_valid_check = data_balance_tract_model.merge(df_forecast, on = ""week_number"")
 MAPE_series = MAPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])
 MSPE_series = MSPE(data_balance_ave_valid_check[""total_claims""], data_balance_ave_valid_check[""total_claim_pred""])",data preprocessing,,
file383,44,data_balance_ave_valid_check,data exploration,,
file383,45,"# validation
 print(""MAPE: "", sum(MAPE_list)/len(MAPE_list))
 print(""MSPE: "", sum(MSPE_list)/len(MSPE_list))",data exploration,,
file383,46,data_balance.head(),data exploration,,
file383,47,"data_balance.sort_values(by='uu_id',inplace=True)
 data_balance[""countyfips""].fillna(method='ffill')",data preprocessing,,
file383,48,"# To balance the dataset as panel data
 data_balance = data.set_index('week_number')
 data_balance = data_balance.sort_index(ascending=False)
 data_balance = data_balance.set_index('uu_id',append=True)
 data_balance = data_balance[~data_balance.index.duplicated(keep='first')]",data preprocessing,,
file383,49,"data_balance = data_balance.reset_index(level=['week_number'])
 data_balance = (data_balance.set_index('week_number',append=True).reindex(pd.MultiIndex.from_product([data_balance.index.unique(),
  range(data_balance.week_number.min(),data_balance.week_number.max()+1)],
  names=['uu_id','week_number'])).reset_index(level=1))",data preprocessing,,
file383,50,"data_balance = data_balance.set_index('week_number',append=True)
 data_balance['total_claims'] = data_balance['total_claims'].fillna(0)
 data_balance['average_wage'] = data_balance['average_wage'].interpolate(method = ""linear"")",data preprocessing,,
file383,51,"data_balance = data_balance.reset_index(level=['uu_id', ""week_number""])
 data_balance = dataIdentifyDWM(data_balance)",data preprocessing,,
file383,52,data_balance,data exploration,,
file383,53,"data_balance.sort_values(by='uu_id',inplace=True)
 data_balance = data_balance.fillna(method='ffill')
 data_balance.sort_index()",data preprocessing,data exploration,
file383,54,"# Split data to training and validaton sets
 # Max trainweek is 37, use a 80 / 20 rule
 train_week = int(max(pd.unique(data[""week_number""]))*0.8)",data preprocessing,,
file383,55,"data_train = data[data[""week_number""] < train_week]
 data_valid = data[data[""week_number""] >= train_week]",data preprocessing,,
file383,56,"data_train_x = data_train.drop(""total_claims"",1)
 data_train_y = data_train[""total_claims""]",data preprocessing,,
file383,57,"data_balance_valid_x = data_balance_valid.drop(""total_claims"",1)
 data_balance_valid_y = data_balance_valid[""total_claims""]",data preprocessing,,
file383,58,data_balance_valid_y.shape,data exploration,,
file383,59,"# Model 4 Hierarchical ARIMA model
 data_balance.groupby(""countyfips"")[""uu_id""].apply(set).to_frame()",data preprocessing,,
file383,60,"# Model 4 Hierarchical ARIMA model
 # data_balance.groupby(""countyfips"")[""uu_id""].apply(set).to_frame()
 data_balance[""county_tract""] = data_balance.apply(lambda x: f""{x['county']}_{x['uu_id']}"", axis=1)",data preprocessing,,
file383,61,"df_bottom_level = data_balance.pivot(index=""week_number"", columns=""county_tract"", values=""total_claims"")
 df_middle_level = data_balance.groupby([""week_number"", ""state""]) \
  .sum() \
  .reset_index(drop=False) \
  .pivot(index=""week_number"", columns=""countyfips"", values=""total_claims"")
 df_total = data_balance.groupby(""week_number"")[""total_claims""] \
  .sum() \
  .to_frame() \
  .rename(columns={""total_claims"": ""total""})",data preprocessing,,
file383,62,"print(f""Number of time series at the bottom level: {df_bottom_level.shape[1]}"")
 print(f""Number of time series at the middle level: {df_middle_level.shape[1]}"")",data exploration,,
file383,63,hierarchy_df,data exploration,,
file383,64,"# Model 4 Hierarchical ARIMA model
 # data_balance.groupby(""countyfips"")[""uu_id""].apply(set).to_frame()
 data_balance[""county_tract""] = data_balance.apply(lambda x: f""{x['countyfips']}_{x['uu_id']}"", axis=1)
 data_balance_hts = data_balance.copy()
 data_balance_hts = data_balance_hts[[""week_number"", ""uu_id"", ""countyfips"", ""total_claims"", ""county_tract""]]",data preprocessing,,
file383,65,hierarchy_df = Null,data preprocessing,,
file383,66,"data_balance_hts.sort_values(by='week_number',inplace=True)",data preprocessing,,
file383,67,hierarchy_df.head(),data exploration,,
file383,68,df_bottom_level,data exploration,,
file383,69,df_middle_level,data exploration,,
file383,70,"hierarchy_df[""total""].plot(title=""Trips - total level"");",result visualization,,
file383,71,"ax = hierarchy_df[hierarchy['total']].plot(title=""Trips - state level"")
 ax.legend(bbox_to_anchor=(1.0, 1.0));",result visualization,,
file383,72,"county = data_balance_hts[""countyfips""].unique()
 tract = data_balance_hts[""county_tract""].unique()",data preprocessing,,
file383,73,"total = {'total': list(county)}
 county = {k: [v for v in tract if v.startswith(k)] for k in county}
 hierarchy = {**total, **county}",data preprocessing,,
file383,74,data_balance_hts,data exploration,,
file383,75,"model_ols_arima = hts.HTSRegressor(model='auto_arima', revision_method='OLS', n_jobs=0)
 model_ols_arima = model_ols_arima.fit(hierarchy_df, hierarchy)
 pred_ols_arima = model_ols_arima.predict(steps_ahead=10)",modeling,prediction,
file383,76,"df_bottom_level = data_balance_hts.pivot(index=""date"", columns=""county_tract"", values=""total_claims"")
 df_middle_level = data_balance_hts.groupby([""date"", ""countyfips""]) \
  .sum() \
  .reset_index(drop=False) \
  .pivot(index=""date"", columns=""countyfips"", values=""total_claims"")
 df_total = data_balance_hts.groupby(""date"")[""total_claims""] \
  .sum() \
  .to_frame() \
  .rename(columns={""total_claims"": ""total""})
 hierarchy_df = df_bottom_level.join(df_middle_level) \
  .join(df_total)
 hierarchy_df.index = pd.to_datetime(hierarchy_df.index)
 hierarchy_df = hierarchy_df.resample(""7D"") \
  .sum()",data preprocessing,,
file383,77,pred_ols_arima,prediction,,
file383,78,"# Based on MAPE and MSPE, now the ARIMA model has best prediction, so the following prediction is based on ARIMA model
 data_pred = data_pred_query.copy()",data preprocessing,,
file383,79,"# Prediction with Hierarchical ARIMA model, for week 43
 data_pred[""total_claims""] = 0
 data_pred = dataIdentifyDWM(data_pred)",data preprocessing,,
file383,80,data_pred,data exploration,,
file383,81,pred_ols_arima.columns(),data exploration,,
file383,82,"pred_col = pred_ols_arima.columns
 pred = pred_ols_arima.copy()
 pred = pred.reset_index()
 pd.melt(pred, id_vars='date', value_vars=pred_col)",data preprocessing,,
file383,83,"pred_col = pred_ols_arima.columns
 pred = pred_ols_arima.copy()
 pred = pred.reset_index()",data preprocessing,,
file383,84,pred,data exploration,,
file383,85,"pd.melt(pred, id_vars='index', value_vars=pred_col)",data preprocessing,,
file383,86,"pred_long[""uu_id""]= pred_long[""variable""].str.split(""_"", n = 1, expand = False)",data preprocessing,,
file383,87,pred_long,data exploration,,
file383,88,"pred_long[""new_var""]= pred_long[""variable""].str.split(""_"", n = 1, expand = False)
 pred_long[""uu_id""] = pred_long[""new_var""][-1]",data preprocessing,,
file383,89,pred_long = pred_long[pred_long['variable'].str.contains('_')],data preprocessing,,
file383,90,new,data exploration,,
file383,91,new[1],data exploration,,
file383,92,"pred_long[""uu_id""] = new[1]",data preprocessing,,
file383,93,"pred_long[""uu_id""] = new[1]
 pred_long.rename(columns={""index"": ""date""})",data preprocessing,,
file383,94,"data_pred_join = pd.merge（data_pred, pred_long, how='left', left_on=['date','uu_id'],",data preprocessing,,
file383,95,data_pred_join,data exploration,,
file383,96,"data_pred_join = data_pred_join.rename(columns={""value"": ""total_claims""})",data preprocessing,,
file383,97,"data_pred_join.loc[data_pred_join['total_claims']<0,'B']=0",data exploration,,
file383,98,"data_pred = data_pred_join[[""uu_id"", ""total_claims"", ""week_number""]]",data preprocessing,,
file383,99,"## This can also be a good place for you to cleanup any input/output and export your results to a file.
 data_pred.to_csv(""submission_prediction_output.csv"", index=False)",save results,,
file383,100,data_pred_query,data exploration,,
file383,101,"# Based on MAPE and MSPE, now the ARIMA model has best prediction, so the following prediction is based on ARIMA model
 data_pred = data_pred_query.copy()",data preprocessing,,
file383,102,data_pred,data exploration,,
file383,103,data_pred = dataIdentifyDWM(data_pred),data preprocessing,,
file383,104,"# Prediction with Hierarchical ARIMA model, for week 43
 pred_col = pred_ols_arima.columns
 pred = pred_ols_arima.copy()
 pred = pred.reset_index()
 pred_long = pd.melt(pred, id_vars='index', value_vars=pred_col)",data preprocessing,,
file383,105,"pred_long.sort_values('value', ascending=False).drop_duplicates('uu_id').sort_index()",data preprocessing,,
file383,106,"pred_long = pred_long[pred_long[date] == ""2022-10-22""]",data preprocessing,,
file383,107,"new = pred_long[""variable""].str.split(""_"", n = 1, expand = True)",data preprocessing,,
file383,108,pred_long,data exploration,,
file384,0,"get_ipython().run_cell_magic('capture', '', ""%logstop\n%logstart -t -r -q ipython_command_log.py global\n\n#- IRONHACKS RESEARCH TRACKING CODE\n#----------------------------------\n# The following code is used to help our research team understand how you \n# our notebook environment. We do not collect any personal information with\n# the following code, it is used to measure when and how often you work on\n# your submission files.\n\nimport os\nfrom datetime import datetime\nimport IPython.core.history as history\n\nha = history.HistoryAccessor()\nha_tail = ha.get_tail(1)\nha_cmd = next(ha_tail)\nsession_id = str(ha_cmd[0])\ncommand_id = str(ha_cmd[1])\ntimestamp = datetime.utcnow().isoformat()\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\n'\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\nlogfile.write(history_line)\nlogfile.close()\n"")",helper functions,,
file384,1,"import csv
 import pandas as pd
 import numpy as np
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file384,2,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file384,3,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks-data.ironhacks_competition`
 """"""",load data,,
file384,4,"query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()
 print(covid19_cases_data)",load data,data preprocessing,
file384,5,"query = """"""
 SELECT COUNT(*)
 FROM `ironhacks_competition.unemployment_data`
 """"""",load data,,
file385,0,get_ipython().system('pip install db-dtypes'),helper functions,,
file385,1,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file385,2,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file385,3,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file385,4,"query = """"""
 SELECT * FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file385,5,"query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 data.head()",load data,data preprocessing,
file385,6,data.describe(),data exploration,,
file385,7,data.shape,data exploration,,
file385,8,data.info(),data exploration,,
file385,9,y=data['total_claims'],data preprocessing,,
file385,10,"l=[]
 for i in data.columns:
  if sum(data[i].isnull())>0:
  l.append(i)
  print('The null values in',i,'are',sum(data[i].isnull()))",data preprocessing,,
file385,11,data.isnull().sum(axis=0),data exploration,,
file385,12,"for i in data.columns:
  if data[i].isnull().sum()>= 0.4*len(data):
  data=data.drop(i,axis=1)",data preprocessing,,
file385,13,"for i in data.columns:
  if data[i].isnull().sum()>0:
  print('The value counts of feature',i)
  print(data[i].value_counts(),'\n')",data preprocessing,,
file385,14,"for i in data.columns:
  print('The unique values in',i,'are',len(data[i].value_counts()))",data exploration,,
file385,15,data.corr(),data exploration,,
file385,16,"plt.figure(figsize=(20,15))
 cor = data.corr()
 sns.heatmap(cor,annot=True, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file386,0,"get_ipython().system('pip install transformers')
 get_ipython().system('pip install torch')
 get_ipython().system('pip install -r requirements.txt')",helper functions,,
file386,1,"import torch
 from transformers import AutoTokenizer, AutoModel
 tokenizer = AutoTokenizer.from_pretrained(""GanjinZero/UMLSBert_ENG"")
 model = AutoModel.from_pretrained(""GanjinZero/UMLSBert_ENG"")
 import pandas as pd",helper functions,,
file386,2,"# read in scraped mayo symptoms
 with open(""mayo_data.json"", 'r') as j:
  data = json.loads(j.read())",load data,,
file386,3,"symptoms=[]
 specialties=[]
 diseases=[]",data preprocessing,,
file386,4,"for disease in list(data.keys()):
  list_of_symptoms=data[disease]['symptoms']
  if len(list_of_symptoms)==0:
  pass
  symptoms.append(list_of_symptoms)
  diseases.append([disease]*len(list_of_symptoms))
  list_of_specialties=data[disease]['specialties']
  specialtystr=','.join(list_of_specialties)
  specialties.append([specialtystr]*len(list_of_symptoms))",data preprocessing,,
file386,5,"def flatten(l):
  return [item for sublist in l for item in sublist]",data exploration,,
file386,6,"diseases=flatten(diseases)
 symptoms=flatten(symptoms)
 specialties=flatten(specialties)",data preprocessing,,
file386,7,"lst = [diseases,symptoms,specialties]
 df = pd.DataFrame(
  {'disease': diseases,
  'symptoms': symptoms,
  'specialties': specialties
  })",data preprocessing,,
file386,8,"def get_pooled_embedding(text, model, tokenizer):
  tokenized_input = tokenizer(text, return_tensors='pt')
  output = model(**tokenized_input)
  
  return output.pooler_output.detach().numpy()[0]",data preprocessing,,
file386,9,"def create_pooled_embedding_df(model, tokenizer, df):
  df['pooled_embedding'] = df.symptoms.apply(get_pooled_embedding, args=(model, tokenizer))
  
  return df",data preprocessing,,
file386,10,"# add embedding column and save to pickle obj
 df= create_pooled_embedding_df(model, tokenizer, df)
 print(df)
 #df.to_pickle(""mayo_symptoms_embeddings.plk"")",data preprocessing,data exploration,
file387,0,"get_ipython().run_cell_magic('capture', 'cont', '!pip install db-dtypes\n')",helper functions,,
file387,1,cont.show(),data exploration,,
file387,2,"import pandas as pd
 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 import seaborn as sns
 get_ipython().run_line_magic('matplotlib', 'inline')",helper functions,,
file387,3,"import os
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file387,4,"import statsmodels.api as sm
 import itertools",helper functions,,
file387,5,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file387,6,"#obtaining the unemployment data
 query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file387,7,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 data = query_job.to_dataframe()
 #data['date']= pd.to_datetime(data['date'])
 data.head()",load data,data preprocessing,
file387,8,"data.info()  #Feature Matrix
 #X = data.drop(""date"",1)
 #y = data[""wind_speed""] #Target Variable",data exploration,data preprocessing,
file387,9,submit.info(),data exploration,,
file387,10,"data.drop_duplicates(inplace=True, ignore_index=True)
 data.info()",data preprocessing,data exploration,
file387,11,data.uu_id.unique().size,data exploration,,
file387,12,"data.fillna(0, inplace=True)",data exploration,,
file387,13,data.info(),data exploration,,
file387,14,"test=data[data['uu_id']=='bbcb018f0e5e49e13636f6e78ce9f60f']
 len(test)",data preprocessing,data exploration,
file387,15,"data['timeperiod']= pd.to_datetime(data['timeperiod'], format='%Y%m%d')",data preprocessing,,
file387,16,"plt.figure(figsize=(12,10))
 cor = data.corr()
 sns.heatmap(cor, cmap=plt.cm.Reds)
 plt.show()",result visualization,,
file387,17,"#Correlation with output variable
 cor_target = abs(cor[""total_claims""])
 #Selecting highly correlated features
 relevant_features = cor_target[cor_target>0.2]
 print(relevant_features)",data preprocessing,data exploration,
file387,18,columns_rel = relevant_features.index.to_list(),data preprocessing,,
file387,19,"data.plot(subplots=True, figsize=(20,24))",result visualization,,
file387,20,"min(data['timeperiod']),max(data['timeperiod'])",data exploration,,
file387,21,data.dtypes,data exploration,,
file387,22,data.index,data exploration,,
file387,23,"sns.set(rc={'figure.figsize':(11, 4)})",helper functions,,
file387,24,data['total_claims'].plot(linewidth=.5);,result visualization,,
file387,25,"cols_plot = ['week_number','countyfips','tract','total_claims','edu_8th_or_less','edu_grades_9_11','edu_hs_grad_equiv','edu_post_hs',
  'edu_unknown', 'gender_female', 'gender_male', 'gender_na', 'race_amerindian', 'race_asian', 'race_black', 'race_noanswer', 
  'race_hawaiiannative', 'race_other', 'race_white']
 axes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(30, 25), subplots=True)
 for ax in axes:
  ax.set_ylabel('features')",result visualization,,
file387,26,"#fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)
 #for name, ax in zip(['precipitation_data', 'min_temperature', 'max_temperature'], axes):
 # sns.boxplot(data=data, x='Month', y=name, ax=ax)
 # ax.set_ylabel('precipitation')
 # ax.set_title(name)
  # Remove the automatic x-axis label from all but the bottom subplot
 # if ax != axes[-1]:
 # ax.set_xlabel('')",result visualization,,
file387,27,"columns_rel.append('timeperiod')
 columns_rel.append('uu_id')
 columns_rel",data preprocessing,data exploration,
file387,28,data,data exploration,,
file387,29,"data_arima=data[columns_rel]
 data_arima",data preprocessing,data exploration,
file387,30,"get_ipython().run_cell_magic('capture', '', ""unique_id=list(data_arima['uu_id'].unique())\ndata_dict = {}\n\nfor i in unique_id:\n j = data_arima[data_arima['uu_id']==i].groupby('timeperiod')['total_claims'].sum().reset_index()\n j = j.set_index('timeperiod')\n data_dict[i] = j\n"")",helper functions,,
file387,31,"p = d = q = range(0, 2)
 pdq = list(itertools.product(p, d, q))
 seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
 print('Examples of parameter combinations for Seasonal ARIMA...')
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
 print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
 print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))",data preprocessing,data exploration,
file387,32,"import warnings
 warnings.filterwarnings(""ignore"")",helper functions,,
file387,33,with io.capture_output() as captured:,helper functions,,
file387,34,import csv,helper functions,,
file387,35,"filename =""param.csv""",data preprocessing,,
file387,36,"# opening the file using ""with""
 # statement
 with open(filename, 'r') as data:
  for line in csv.DictReader(data):
  print(line)",data exploration,,
file387,37,csv.DictReader(data),data preprocessing,,
file387,38,"filename =pd.read_csv(""param.csv"")",data preprocessing,,
file387,39,filename.head(),data exploration,,
file387,40,filename.to_dict(),data exploration,,
file387,41,"param_dict=file_param.to_dict('list')
 seasonal_dict=file_seasonal.to_dict('list')",data preprocessing,,
file387,42,"file_param =pd.read_csv(""param.csv"")
 file_seasonal =pd.read_csv(""seasonal.csv"")
 # opening the file using ""with""
 # statement
 #with open(filename, 'r') as data:
 # for line in csv.DictReader(data):
 # print(line)",load data,,
file387,43,len(data_dict),data exploration,,
file387,44,"hundred=list(data_dict.keys())[:100]
 two_hundred=list(data_dict.keys())[100:200]
 three_hundred=list(data_dict.keys())[200:300]
 four_hundred=list(data_dict.keys())[300:400]
 five_hundred=list(data_dict.keys())[500:573]",data preprocessing,,
file387,45,"#results = {}
 for key in hundred:
  y=data_dict[key]['total_claims']
  mod = sm.tsa.statespace.SARIMAX(y.astype(float),
  order=tuple(param_dict[key]),
  seasonal_order=tuple(seasonal_dict[key]),
  enforce_stationarity=False,
  enforce_invertibility=False)
  results[key] = mod.fit(disp=False)
  #print(results.summary().tables[1])",data preprocessing,,
file387,46,len(results),data exploration,,
file387,47,list(results.keys()),data exploration,,
file387,48,"results[x].plot_diagnostics(figsize=(16, 8))
 plt.show()",result visualization,,
file387,49,data_dict[x]['total_claims'],data exploration,,
file387,50,"pred = results[x].get_prediction(start=pd.to_datetime('2022-09-26'), dynamic=False)",prediction,,
file387,51,pred = results[x].get_prediction( dynamic=False),prediction,,
file387,52,"pred_ci = pred.conf_int()
 ax = data_dict[x]['total_claims'].plot(label='observed')
 pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))
 ax.fill_between(pred_ci.index,
  pred_ci.iloc[:, 0],
  pred_ci.iloc[:, 1], color='k', alpha=.2)
 ax.set_xlabel('Date')
 ax.set_ylabel('Furniture Sales')
 plt.legend()
 plt.show()",result visualization,,
file387,53,pred = results[x].forecast(),data preprocessing,,
file387,54,pred,data exploration,,
file387,55,"pred = []
 for key in hundred:
  i=results[key].forecast()
  pred.append(i)
  #print(results.summary().tables[1])",data preprocessing,,
file387,56,pred.values,data exploration,,
file387,57,len(pred),data exploration,,
file387,58,hundred+two_hundred,data exploration,,
file387,59,uu_id=hundred+two_hundred+three_hundred+four_hundred+five_hundred+six_hundred,data preprocessing,,
file387,60,len(uu_id),data exploration,,
file387,61,submit.head(),data exploration,,
file387,62,"first_trial = pd.DataFrame(uu_id, columns=['uu_id'])
 first_trial.head()",data preprocessing,data exploration,
file387,63,first_trial['total_claims']=pred,data preprocessing,,
file387,64,first_trial.head(),data exploration,,
file387,65,"pd.merge(submit, first_trial, on='uu_id', how='outer')",data preprocessing,,
file387,66,"final_file=pd.merge(submit, first_trial, on='uu_id', how='outer')
 final_file.to_csv(./'submission.csv', index=False)",save results,,
file388,0,"import numpy as np
 import pandas as pd",helper functions,,
file389,0,"print(""Hello World :)"")",helper functions,,
file389,1,get_ipython().system('pip install db-dtypes'),helper functions,,
file389,2,"import os
 import pandas as pd
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics
 import matplotlib.pyplot as plt
 import numpy as np",helper functions,,
file389,3,"# Constants
 PRED_WEEK = 40",data preprocessing,,
file389,4,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)",load data,,
file389,5,"query_get_tables = """"""
 SELECT * 
 FROM `ironhacks-data.ironhacks_competition.INFORMATION_SCHEMA.TABLES`
 """"""
 query_job = bigquery_client.query(query_get_tables)
 query_job.to_dataframe().head()",load data,data preprocessing,
file389,6,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 """"""",load data,,
file389,7,"query_job = bigquery_client.query(query)
 unemployment_data = query_job.to_dataframe()
 unemployment_data.head(2)",load data,data preprocessing,
file389,8,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_competition.unemployment_data`
 WHERE uu_id IN (SELECT uu_id
  FROM `ironhacks-data.ironhacks_competition.prediction_list`
  WHERE week_number = (SELECT MAX(week_number) FROM `ironhacks-data.ironhacks_competition.prediction_list`))
 ORDER BY uu_id,week_number
 """"""",load data,,
file389,9,relevant_unemployment_df.head(3),data exploration,,
file389,10,"# Drop duplicate columns
 relevant_unemployment_df.drop_duplicates(inplace=True)
 relevant_unemployment_df.drop(['timeperiod', 'tract_name'], axis=1, inplace=True)",data preprocessing,,
file389,11,"# Drop columns with excessive null values
 # NOTE: Revisit this, these columns may still be useful, especially those that aren't missing too many values
 relevant_unemployment_df.dropna(axis='columns', inplace=True)
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()
 relevant_unemployment_df['countyfips'] = relevant_unemployment_df['countyfips'].astype(str)
 relevant_unemployment_df['tract'] = relevant_unemployment_df['tract'].astype(str)
 relevant_unemployment_df['real_data'] = True
 relevant_unemployment_df = relevant_unemployment_df.convert_dtypes()",data preprocessing,,
file389,12,"# Add Wage data
 relevant_unemployment_df['wage_data'] = relevant_unemployment_df['uu_id'].map(wage_data.set_index('uu_id')['average_wage'])
 relevant_unemployment_df.info()
 bad_wage_uu_ids = list(set(relevant_unemployment_df.loc[relevant_unemployment_df['wage_data'].isna(),'uu_id']))",data preprocessing,,
file389,13,"for uuid in set(relevant_unemployment_df[""uu_id""]):
  num_tracts = 0
  tracts=[]
  for tract in set(relevant_unemployment_df.loc[relevant_unemployment_df[""uu_id""]==uuid,'wage_data']):
  num_tracts += 1
  tracts.append(tract)
  if num_tracts > 1:
  print(uuid,num_tracts,tracts)
 relevant_unemployment_df",data preprocessing,data exploration,
file389,14,"import warnings
 warnings.simplefilter(action='ignore', category=FutureWarning)",helper functions,,
file389,15,"temp_df = pd.DataFrame(columns=list(relevant_unemployment_df.columns))
 my_ids = list(set(relevant_unemployment_df['uu_id']))
 for UUid in my_ids:
  ctyfips = None
  wage_data = None
  tract_vote = [None, 0]
  top_cat_1_vote = [None, 0]
  top_cat_2_vote = [None, 0]
  top_cat_3_vote = [None, 0]
  total_claims_avg = []
  total_claims_num = 0
  
  for week in range(1,PRED_WEEK):
  row = relevant_unemployment_df.loc[((relevant_unemployment_df.uu_id==UUid) 
  & (relevant_unemployment_df.week_number == week)),]
  if len(row) == 1:
  temp_df = pd.concat([temp_df, row])
  
  ### Store information about values to infer for rows without data
  ## Consistent within uu_id values
  ctyfips = row['countyfips'].values[0]
  wage_data = row['wage_data'].values[0]
  
  ## Inconsistent within uu_id values
  # Categorical 
 

  if ((tract_vote[0] == None) or ((tract_vote[0] != row['tract'].values[0]) and (tract_vote[1] == 1))):
  tract_vote = [row['tract'].values[0], 1]
  elif tract_vote[0] == row['tract'].values[0]:
  tract_vote[1] += 1
  else:
  tract_vote[1] -= 1
  
  if ((top_cat_1_vote[0] == None) or ((top_cat_1_vote[0] != row['top_category_employer1'].values[0]) and (top_cat_1_vote[1] == 1))):
  top_cat_1_vote = [row['top_category_employer1'].values[0], 1]
  elif top_cat_1_vote[0] == row['top_category_employer1'].values[0]:
  top_cat_1_vote[1] += 1
  else:
  top_cat_1_vote[1] -= 1
  
  if ((top_cat_2_vote[0] == None) or ((top_cat_2_vote[0] != row['top_category_employer2'].values[0]) and (top_cat_2_vote[1] == 1))):
  top_cat_2_vote = [row['top_category_employer2'].values[0], 1]
  elif top_cat_2_vote[0] == row['top_category_employer2'].values[0]:
  top_cat_2_vote[1] += 1
  else:
  top_cat_2_vote[1] -= 1
  
  if ((top_cat_3_vote[0] == None) or ((top_cat_3_vote[0] != row['top_category_employer3'].values[0]) and (top_cat_3_vote[1] == 1))):
  top_cat_3_vote = [row['top_category_employer3'].values[0], 1]
  elif top_cat_3_vote[0] == row['top_category_employer3'].values[0]:
  top_cat_3_vote[1] += 1
  else:
  top_cat_3_vote[1] -= 1
  
  # Numerical
  total_claims_avg.append(row['total_claims'].values[0])
  if len(total_claims_avg) > 0:
  total_claims_num = np.mean(total_claims_avg)
 

  continue
  temp_df = pd.concat([temp_df, pd.DataFrame.from_records([{'uu_id': UUid, 
  'week_number': week, 
  'countyfips': ctyfips, 
  'tract': tract_vote[0],
  'total_claims': total_claims_num, 
  'top_category_employer1': top_cat_1_vote[0],
  'top_category_employer2': top_cat_2_vote[0], 
  'top_category_employer3': top_cat_3_vote[0],
  'real_data': False, 
  'wage_data': wage_data}])])",helper functions,,
file389,16,"temp_df.reset_index(inplace=True)
 temp_df.drop('index', axis=1,inplace=True)
 temp_df.info()",data preprocessing,data exploration,
file389,17,temp_df.head(),data exploration,,
file389,18,"non_null_cols = [""countyfips"", ""tract"", ""top_category_employer1"", ""top_category_employer2"", ""top_category_employer3"", ""wage_data""]",data preprocessing,,
file389,19,"for UUid2 in my_ids[::-1]:
  for week_num in range(PRED_WEEK-1,0,-1):
  row = temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num)),]
  if row['countyfips'].values[0] == None:
  for col in non_null_cols:
  temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num)), col] = temp_df.loc[((temp_df['uu_id'] == UUid2) & (temp_df[""week_number""] == week_num+1)), col].values[0]",data preprocessing,,
file389,20,temp_df.info(),data exploration,,
file389,21,"temp_df = temp_df.convert_dtypes()
 temp_df.info()",data preprocessing,data exploration,
file389,22,"bad_wage_ids=set(temp_df.loc[temp_df['wage_data'].isna(),""uu_id""])",data preprocessing,,
file389,23,"data = temp_df
 data.rename(columns={""uu_id"": ""id"", ""week_number"": ""week"", ""countyfips"": ""fips"", ""top_category_employer1"": ""emp1""}, inplace=True)
 data.rename(columns={""top_category_employer2"": ""emp2"", ""top_category_employer3"": ""emp3"", ""real_data"": ""real"", ""wage_data"":""wages""}, inplace=True)
 for col in ['fips', 'tract', 'emp1', 'emp2', 'emp3']:
  data[col] = pd.factorize(data[col])[0]",data preprocessing,,
file389,24,data.head(),data exploration,,
file389,25,data.info(),data exploration,,
file389,26,abs(data.corr()['total_claims']),data exploration,,
file389,27,"ids = list(set(data['id']))
 weeks = list(set(data['week']))
 test_weeks = weeks[PRED_WEEK-10:]",data preprocessing,,
file389,28,from sklearn.linear_model import LinearRegression,helper functions,,
file389,29,"predictions = dict()
 actual = dict()
 for pred_id in ids[:1]:
  predictions[pred_id] = []
  actual[pred_id] = []
  
  for wk in test_weeks:
  # Train test split
  if id not in bad_wage_ids:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3"", ""wages""]]
  else:
  train_x = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  test_x = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), 
  [""fips"", ""tract"", ""emp1"", ""emp2"", ""emp3""]]
  train_y = data.loc[((data['week'] < wk) & (data[""id""] == pred_id)), ""total_claims""]
  test_y = data.loc[((data['week'] == wk) & (data[""id""] == pred_id)), ""total_claims""]
  actual[pred_id].append(test_y)
  
  
  ## Prediction
  # Linear Model
  reg = LinearRegression().fit(train_x, train_y)
  prediction = reg.predict(test_x)
  predictions[pred_id].append(prediction)",modeling,prediction,
file389,30,,,,
file389,31,predictions,data exploration,,
file389,32,actual,data exploration,,
file389,33,temp_df,data exploration,,
file389,34,"from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error
 from sklearn.metrics import mean_absolute_error",helper functions,,
file389,35,MSE,data exploration,,
file389,36,MAE,data exploration,,
file389,37,bad_ids,data exploration,,
file389,38,bad_wage_ids,data exploration,,
file389,39,train_x,data exploration,,
file390,0,"BIGQUERY_PROJECT = 'ironhacks-data'
 bigquery_client = BIGQUERY_PROJECT.Client(project=BIGQUERY_PROJECT)",load data,,
file390,1,"import os
 import pandas
 from google.cloud import bigquery
 from google.oauth2 import service_account
 from google.cloud.bigquery import magics",helper functions,,
file390,2,"query = """"""
 SELECT *
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 """"""",load data,,
file390,3,"# QUERY THE DATA ONCE
 query_job = bigquery_client.query(query)
 covid19_cases_data = query_job.to_dataframe()",load data,data preprocessing,
file390,4,"# THEN WORK BELOW TO DO SOMETHING THE RESULTS
 print(""Columns:"")
 print('\n'.join(covid19_cases_data.columns))
 print(""\nResults:"")
 print(covid19_cases_data.head())",data exploration,,
file390,5,print(covid19_cases_data),data exploration,,
file390,6,"query = """"""
 SELECT 
 week_number,
 cases 
 FROM `ironhacks-data.ironhacks_training.covid19_cases`
 Where week_number between 1 and 3
 order by week_number
 """"""",load data,,
file390,7,"df = pandas.DataFrame(covid19_cases_data, columns = ['cases'])
 print(""mean: "", df.mean());",data preprocessing,data exploration,
