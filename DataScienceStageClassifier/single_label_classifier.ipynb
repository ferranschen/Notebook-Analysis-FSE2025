{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('./dataset.csv')\n",
    "\n",
    "\n",
    "\n",
    "# # focus on columns 'code', 'Primary-label'\n",
    "df = df[['code', 'Primary-label', 'Secondary-label','cell number', 'filename']]\n",
    "# df = df[['code', 'Primary-label', 'Secondary-label']]\n",
    "\n",
    "# drop data with no label or no code\n",
    "df = df.dropna(subset=['Primary-label', 'code'])   \n",
    "\n",
    "\n",
    "# fill \"None\" if there is no secondary label\n",
    "df['Secondary-label'] = df['Secondary-label'].fillna('none')\n",
    "\n",
    "\n",
    "# give a unique id to each data\n",
    "df['id'] = df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head of train\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show total number of data\n",
    "print('Total number of train data: ', len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show total number of data\n",
    "print('Total number of train data: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction (use regex to extract function name)\n",
    "# for instance, model.fit() -> fit, df.dropna() -> dropna\n",
    "# df.head -> head\n",
    "def extract_func_name(code):\n",
    "    # Split the code into lines\n",
    "    lines = code.splitlines()\n",
    "\n",
    "    # Remove import statements from the lines\n",
    "    non_import_lines = [\n",
    "        line for line in lines if not line.strip().startswith(('import', 'from'))]\n",
    "\n",
    "    # remove comments\n",
    "    non_import_lines = [re.sub(r'#.*', '', line) for line in non_import_lines]\n",
    "\n",
    "    # Join the non-import lines back into a single string\n",
    "    non_import_code = '\\n'.join(non_import_lines)\n",
    "\n",
    "    # Use regex to match the function, class, and attribute name patterns\n",
    "    pattern = r'(?:^|\\s|\\.)((?:\\w+\\()|(?:\\w+)(?=\\(|\\[|\\.|$))'\n",
    "    matches = re.finditer(pattern, non_import_code)\n",
    "\n",
    "    # If matches are found, return a list of function names\n",
    "    if matches:\n",
    "        func_names = [match.group(1).replace(\"(\", \"\") for match in matches]\n",
    "        return func_names\n",
    "    # Otherwise, return an empty list\n",
    "    else:\n",
    "        return []\n",
    "# Example\n",
    "code = \"empdata.loc[empdata['uu_id']; LinearRegression().fit; df.head\"\n",
    "print(extract_func_name(code))  # Output: ['loc', 'LinearRegression', 'fit', 'head']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction ( use regex to extract number of import)\n",
    "def extract_num_import(code):\n",
    "    # Use regex to match the import statements\n",
    "    pattern = r'\\bimport\\b'\n",
    "    matches = re.finditer(pattern, code)\n",
    "\n",
    "    # If matches are found, return the count\n",
    "    if matches:\n",
    "        import_count = sum(1 for _ in matches)\n",
    "        return import_count\n",
    "    # Otherwise, return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# # Example\n",
    "# code = '''\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# '''\n",
    "\n",
    "# print(extract_num_import(code))  # Output: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction ( use regex to extract number of import)\n",
    "def extract_num_comments(code):\n",
    "    # Use regex to match the single-line comments\n",
    "    pattern = r'\\s*#.*$'\n",
    "    matches = re.finditer(pattern, code, re.MULTILINE)\n",
    "\n",
    "    # If matches are found, return the count\n",
    "    if matches:\n",
    "        comment_count = sum(1 for _ in matches)\n",
    "        return comment_count\n",
    "    # Otherwise, return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num_lines(code):\n",
    "    # Split the code into lines\n",
    "    lines = code.splitlines()\n",
    "\n",
    "    return len(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract function names\n",
    "df['func_names'] = df['code'].apply(extract_func_name)\n",
    "# extract number of function names\n",
    "df['num_func'] = df['func_names'].apply(lambda x: len(x))\n",
    "# extract number of import\n",
    "df['num_import'] = df['code'].apply(extract_num_import)\n",
    "# extract number of comments\n",
    "df['num_comments'] = df['code'].apply(extract_num_comments)\n",
    "# extract number of lines\n",
    "df['num_lines'] = df['code'].apply(extract_num_lines)\n",
    "\n",
    "df.sort_values(by=['filename', 'cell number'], inplace=True)\n",
    "\n",
    "# Create new columns representing the next row's data\n",
    "df['next_num_func'] = df.groupby('filename')['num_func'].shift(-1)\n",
    "df['next_num_import'] = df.groupby('filename')['num_import'].shift(-1)\n",
    "df['next_num_comments'] = df.groupby('filename')['num_comments'].shift(-1)\n",
    "df['next_num_lines'] = df.groupby('filename')['num_lines'].shift(-1)\n",
    "# Create new columns representing the previous row's data\n",
    "df['previous_num_func'] = df.groupby('filename')['num_func'].shift(1)\n",
    "df['previous_num_import'] = df.groupby('filename')['num_import'].shift(1)\n",
    "df['previous_num_comments'] = df.groupby('filename')['num_comments'].shift(1)\n",
    "df['previous_num_lines'] = df.groupby('filename')['num_lines'].shift(1)\n",
    "\n",
    "# Handling missing values\n",
    "# You can fill the missing values (NaNs for the last row of each file) with zeros, or other values depending on your requirement:\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# cell number data preprocessing\n",
    "max_cell_numbers = df.groupby('filename')['cell number'].max() + 1\n",
    "df['normalized cell number'] = df.apply(\n",
    "    lambda row: row['cell number'] / max_cell_numbers[row['filename']], axis=1)\n",
    "\n",
    "# Extract numerical features\n",
    "X_num = df[['num_func', 'num_import', 'num_comments',\n",
    "            'num_lines', 'normalized cell number', 'next_num_func', 'next_num_import', 'next_num_comments', 'next_num_lines', 'previous_num_func', 'previous_num_import', 'previous_num_comments', 'previous_num_lines']].values\n",
    "\n",
    "\n",
    "\n",
    "# drop unnecessary columns\n",
    "df = df.drop(['cell number', 'filename'], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "df[['num_func', 'num_import', 'num_comments', 'num_lines', 'normalized cell number', 'next_num_func', 'next_num_import', 'next_num_comments', 'next_num_lines', 'previous_num_func', 'previous_num_import', 'previous_num_comments', 'previous_num_lines']] = X_num_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# extract unique function names from all data\n",
    "unique_func_names = set()\n",
    "\n",
    "for func_names in df['func_names']:\n",
    "    unique_func_names.update(func_names)\n",
    "\n",
    "# Convert the set to a list\n",
    "unique_func_names_list = list(unique_func_names)\n",
    "\n",
    "def has_func_name(row, func_name):\n",
    "    # Check if the function name is in the list of function names\n",
    "    if func_name in row['func_names']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Create a binary feature for each unique function name\n",
    "for func_name in unique_func_names_list:\n",
    "    df[f'has_{func_name}'] = df.apply(\n",
    "        has_func_name, args=(func_name,), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# encode labels\n",
    "labels = df['Primary-label'].unique().tolist()\n",
    "\n",
    "# print labels name and their corresponding integer values\n",
    "print(labels)\n",
    "\n",
    "# encode them as integers\n",
    "df['Primary-label'] = df['Primary-label'].apply(lambda x: labels.index(x))\n",
    "\n",
    "# split data into train and test, with stratification\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Primary-label'])\n",
    "\n",
    "x_train = train.drop(['Primary-label', 'Secondary-label', 'code', 'func_names', 'id'], axis=1)\n",
    "y_train = train['Primary-label']\n",
    "x_test = test.drop(['Primary-label', 'Secondary-label', 'code', 'func_names', 'id'], axis=1)\n",
    "y_test = test['Primary-label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [1, 10],\n",
    "    'gamma': [0.1, 1, 10], # expanded range for gamma\n",
    "    'kernel': ['rbf'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'shrinking': [True, False], # included False for experimentation\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "    'decision_function_shape': ['ovr', 'ovo'],\n",
    "    'max_iter': [1000],\n",
    "}\n",
    "# Create an SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Create a grid search object for each classifier\n",
    "clf_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "# Train the grid search objects on the training data\n",
    "print(\"training SVM\")\n",
    "clf_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for each classifier\n",
    "print('Best hyperparameters for SVM:', clf_search.best_params_)\n",
    "\n",
    "# Evaluate each classifier on the testing data\n",
    "y_pred = clf_search.predict(x_test)\n",
    "\n",
    "\n",
    "print('Accuracy of SVM: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# f1 score\n",
    "print('F1 score of SVM: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# precision score\n",
    "print('Precision score of SVM: ', precision_score(\n",
    "    y_test, y_pred, average='weighted'))\n",
    "\n",
    "# recall score\n",
    "print('Recall score of SVM: ', recall_score(\n",
    "    y_test, y_pred, average='weighted'))\n",
    "\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],       \n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [10],\n",
    "    'min_samples_leaf': [1],\n",
    "    'bootstrap': [True],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'max_leaf_nodes': [10, 20, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "    'random_state': [42],\n",
    "}\n",
    "\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Create a grid search object for the Random Forest classifier\n",
    "rf_search = GridSearchCV(rf_clf, param_grid_rf, cv=5)\n",
    "\n",
    "# Train the grid search object on the training data\n",
    "print(\"Training Random Forest\")\n",
    "rf_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for the Random Forest classifier\n",
    "print('Best hyperparameters for Random Forest:', rf_search.best_params_)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "y_pred = rf_search.predict(x_test)\n",
    "\n",
    "print('Accuracy of Random Forest: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# Create a Logistic Regression classifier\n",
    "lr_clf = LogisticRegression(random_state=5)\n",
    "\n",
    "# Create a grid search object for the Logistic Regression classifier\n",
    "lr_search = GridSearchCV(lr_clf, param_grid_lr, cv=5)\n",
    "\n",
    "# Train the grid search object on the training data\n",
    "print(\"Training Logistic Regression\")\n",
    "lr_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for the Logistic Regression classifier\n",
    "print('Best hyperparameters for Logistic Regression:', lr_search.best_params_)\n",
    "# Evaluate the classifier on the testing data\n",
    "y_pred = lr_search.predict(x_test)\n",
    "\n",
    "print('Accuracy of Logistic Regression: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [30, 50, 100],\n",
    "    'p': [1, 2],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'metric_params': [None],\n",
    "    'n_jobs': [None, -1]\n",
    "}\n",
    "\n",
    "# Create a K-Nearest Neighbors classifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# Create a grid search object for the K-Nearest Neighbors classifier\n",
    "knn_search = GridSearchCV(knn_clf, param_grid_knn, cv=5)\n",
    "\n",
    "# Train the grid search object on the training data\n",
    "print(\"Training K-Nearest Neighbors\")\n",
    "knn_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for the K-Nearest Neighbors classifier\n",
    "print('Best hyperparameters for K-Nearest Neighbors:', knn_search.best_params_)\n",
    "# Evaluate the classifier on the testing data\n",
    "y_pred = knn_search.predict(x_test)\n",
    "\n",
    "print('Accuracy of K-Nearest Neighbors: ', accuracy_score(y_test, y_pred))\n",
    "8\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "# Create a grid search object for the Decision Tree classifier\n",
    "dt_search = GridSearchCV(dt_clf, param_grid_dt, cv=5)\n",
    "\n",
    "# Train the grid search object on the training data\n",
    "print(\"Training Decision Tree\")\n",
    "dt_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for the Decision Tree classifier\n",
    "print('Best hyperparameters for Decision Tree:', dt_search.best_params_)\n",
    "\n",
    "# Evaluate the classifier on the testing data\n",
    "y_pred = dt_search.predict(x_test)\n",
    "\n",
    "print('Accuracy of Decision Tree: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06],\n",
    "    'priors': [None, [0.2, 0.8], [0.5, 0.5]],\n",
    "}\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "# Create a grid search object for the Naive Bayes classifier\n",
    "nb_search = GridSearchCV(nb_clf, param_grid_nb, cv=5)\n",
    "\n",
    "# Train the grid search object on the training data\n",
    "print(\"Training Naive Bayes\")\n",
    "nb_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for the Naive Bayes classifier\n",
    "print('Best hyperparameters for Naive Bayes:', nb_search.best_params_)\n",
    "# Evaluate the classifier on the testing data\n",
    "y_pred = nb_search.predict(x_test)\n",
    "\n",
    "print('Accuracy of Naive Bayes: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_gb = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [1.0, 0.8, 0.5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'warm_start': [False, True]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# Create a grid search object for the Gradient Boosting classifier\n",
    "gb_search = GridSearchCV(gb_clf, param_grid_gb, cv=5)\n",
    "\n",
    "# Train the grid search object on the training data\n",
    "print(\"Training Gradient Boosting\")\n",
    "gb_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters for the Gradient Boosting classifier\n",
    "print('Best hyperparameters for Gradient Boosting:', gb_search.best_params_)\n",
    "# Evaluate the classifier on the testing data\n",
    "y_pred = gb_search.predict(x_test)\n",
    "\n",
    "print('Accuracy of Gradient Boosting: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
