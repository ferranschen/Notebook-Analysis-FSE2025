# Jupyter Notebook Analysis

Paper Title: **From Errors to Resolutions: Understanding Fine-Grained Debugging Patterns of Data Science Programmers in Jupyter Notebooks.** [\[pdf\]](paper.pdf)

This repository contains code and data used to analyze the debugging patterns of data science programmers in Jupyter Notebooks.

## Table of Contents

- [Jupyter Notebook Analysis](#jupyter-notebook-analysis)
  - [Table of Contents](#table-of-contents)
  - [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
    - [Recommended Execution Order](#recommended-execution-order)
  - [Components](#components)
    - [NotebookFiles](#notebookfiles)
    - [Dataset](#dataset)
    - [AutoExecuteNotebooks](#autoexecutenotebooks)
    - [ErrorsPreprocessing](#errorspreprocessing)
    - [DeduplicateErrors](#deduplicateerrors)
    - [EditPatternClassifier](#editpatternclassifier)
    - [DataScienceStageClassifier](#datasciencestageclassifier)
    - [DebuggingActivity](#debuggingactivity)
  - [Directory Structure](#directory-structure)

## Getting Started

### Prerequisites

- Python 3.0 or higher
  > :bulb: **_NOTE:_** You may want to use a virtual environment to run the components since the notebooks may contain different python packages.
- Please refer to the [gumtree repo](https://github.com/GumTreeDiff/gumtree) for the installation of GumTree. We have put the executable file in the `gumtree` directory but it may not work on your machine. If it does not work, you need to follow the instructions in the GumTreeDiff repository to build the executable file.

### Recommended Execution Order

Currently, each component listed above can be run independently. However, If you want to follow the steps in the paper, you need to run the components in the following order:

1. **AutoExecuteNotebooks**
2. **ErrorsPreprocessing**
3. **DeduplicateErrors**
4. **EditPatternClassifier**
5. **DebuggingActivity**
6. **DataScienceStageClassifier**

> :bulb: **_NOTE:_** Sometimes you need to move the dependent files to the correct directory before running the component. Most of the dependent files are located in the `Dataset` and `NotebookFiles` directories. There is no dependency between step 4. to 6. You can run them in any order.

## Components

### NotebookFiles

This directory contains the Jupyter Notebooks used for the analysis. The notebooks are organized into two subdirectories: `executed` and `original`. The `executed` directory contains the notebooks that were executed using the [AutoExecuteNotebooks](#autoexecutenotebooks) component. The `original` directory contains the notebooks that were not executed. **These notebooks have combined with the system logs** so we can observe how a bug is fixed by a programmer.

### Dataset

This directory contains the dataset used for the analysis. The file `dataset.csv` contains the dataset in CSV format. There are a total of 390 notebook data in the dataset. The columns in the dataset are as follows: filename, cell number, code, Primary-label, and Secondary-label. This dataset is used for training the [DataScienceStageClassifier](#datasciencestageclassifier).
The file `error_analysis.csv` contains the error analysis data in CSV format. There are a total of 839 raw bugs in the dataset. The columns in the dataset are as follows: File,Original_Index,Error_Cell,Fixed_Index,Fixed_Cell,Error_Type. It can be generated by running the [ErrorsPreprocessing](#errorspreprocessing) component. This is the raw error analysis data. The file `error_analysis_primary.csv` contains the primary bugs in the dataset after deduplication by running the [DeduplicateErrors](#deduplicateerrors) component. There are a total of 529 primary bugs in the dataset. The columns in the dataset are as follows: File,Original_Index,Error_Cell,Fixed_Index,Fixed_Cell,Error_Type. This is also the primary error analysis data used in the [EditPatternClassifier](#editpatternclassifier) component.

### AutoExecuteNotebooks

The first step to collect error output is to execute the notebooks. This component executes the notebooks in the `original` directory and saves the executed notebooks in the `executed` directory. To execute the notebooks, run the following command:

```bash
python AutoExecuteNotebooks/auto.py
```

> :bulb: **_NOTE:_** You need to put the `original` directory in the same directory as the `auto.py` file.

### ErrorsPreprocessing

This directory contains the code for preprocessing the error output. It extracts the following 15 cells after the buggy cell to identify the fixed cell. It also outputs the `error_analysis.csv`, which contains every bug and its corresponding fixed cell. To run the preprocessing, execute the following command:

```bash
python ErrorsPreprocessing/main.py
```

### DeduplicateErrors

This directory contains the code for deduplicating the error output. It takes the `error_analysis.csv` file as input and outputs the `error_analysis_primary.csv` file. To run the deduplication, execute the following command:

```bash
python DeduplicateErrors/Deduplicate.py
```

> :bulb: **_NOTE:_** You need to put the `error_analysis.csv` file in the same directory as the `Deduplicate.py` file.

### EditPatternClassifier

This directory contains the code for the Edit Pattern Classifier. To run the classifier, run the following command:

```bash
python EditPatternClassifier/main.py
```

To know more about the implementation details of the classifier, please refer to the `EditPatternClassifier.py` file.

### DataScienceStageClassifier

This directory contains the code for the Data Science Stage Classifier. To train the classifier, open the `single_label_classifier.ipynb notebook` and run the cells. The training dataset is located in the `dataset` directory.

> :bulb: **_NOTE:_** You need to put the `dataset.csv` file in the same directory as the `single_label_classifier.ipynb` file.

### DebuggingActivity

There are two subdirectories in this directory: `DebuggingPattern_Extract` and `DebuggingPattern_Analysis`. The `DebuggingPattern_Extract` directory contains the code for extracting the debugging traces. The `DebuggingPattern_Analysis` directory contains the code for analyzing the debugging traces.
First, run the following command to extract the debugging traces:

```bash
python ./DebuggingPattern_Extract/TraceExtractor.py
```

> :bulb: **_NOTE:_** You need to move the `executed/` directory to the `DebuggingPattern_Extract` directory before running the command. It will generate the `"debug_sequence_" + str(datetime.now().time()) + ".txt"` file in the `DebuggingPattern_Extract` directory. You need to rename the file to `debug_sequence.txt` and put it in the `DebuggingPattern_Analysis` directory.

Then, run the following command to analyze the debugging traces:

```bash
python ./DebuggingPattern_Analysis/PatternMiner.py
```

## Directory Structure

```bash
├── AutoExecuteNotebooks
│   ├── GenerateCSVfromNB.py
│   ├── auto.py
│   └── distance.py
├── DataScienceStageClassifier
│   ├── dataset.csv
│   └── single_label_classifier.ipynb
├── Dataset
│   ├── dataset.csv
│   └── error_analysis_primary.csv
├── DebuggingActivity
│   ├── DebuggingPattern_Analysis
│   └── DebuggingPattern_Extract
├── DeduplicateErrors
│   ├── Deduplicate.py
│   ├── error_analysis.csv
│   └── error_analysis_primary.csv
├── EditPatternClassifier
│   ├── EditPatternClassifier.py
│   ├── error_analysis_primary.csv
│   ├── gumtree
│   └── main.py
├── ErrorsPreprocessing
│   ├── GumTreeAnalysis.py
│   ├── GumTreeDiff.py
│   ├── distance.py
│   ├── error_analysis.csv
│   ├── format.py
│   ├── gumtree-3.1.0-SNAPSHOT
│   └── main.py
├── InstrumentationExample
│   └── example.ipynb
├── InterviewStudy
│   ├── NotebookAnalysis Coding.xlsx
│   ├── NotebookAnalysis Consent.docx
│   └── NotebookAnalysis Interview Guide.docx
├── NotebookFiles
│   ├── executed
│   └── original
└── README.md
```
