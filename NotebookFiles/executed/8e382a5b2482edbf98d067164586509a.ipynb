{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d590018",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37aeff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.811391Z",
     "iopub.status.busy": "2023-05-10T00:51:37.811247Z",
     "iopub.status.idle": "2023-05-10T00:51:37.891323Z",
     "shell.execute_reply": "2023-05-10T00:51:37.891024Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315bc17",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96da4815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.892931Z",
     "iopub.status.busy": "2023-05-10T00:51:37.892822Z",
     "iopub.status.idle": "2023-05-10T00:51:37.899422Z",
     "shell.execute_reply": "2023-05-10T00:51:37.899159Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace end of line character with space\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_raw\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae18bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.900903Z",
     "iopub.status.busy": "2023-05-10T00:51:37.900802Z",
     "iopub.status.idle": "2023-05-10T00:51:37.903808Z",
     "shell.execute_reply": "2023-05-10T00:51:37.903539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bdedc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.905270Z",
     "iopub.status.busy": "2023-05-10T00:51:37.905189Z",
     "iopub.status.idle": "2023-05-10T00:51:37.906879Z",
     "shell.execute_reply": "2023-05-10T00:51:37.906665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b408b1b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.908287Z",
     "iopub.status.busy": "2023-05-10T00:51:37.908200Z",
     "iopub.status.idle": "2023-05-10T00:51:37.909833Z",
     "shell.execute_reply": "2023-05-10T00:51:37.909620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64143e",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b46104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.911175Z",
     "iopub.status.busy": "2023-05-10T00:51:37.911079Z",
     "iopub.status.idle": "2023-05-10T00:51:37.917587Z",
     "shell.execute_reply": "2023-05-10T00:51:37.917373Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f624a7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.918996Z",
     "iopub.status.busy": "2023-05-10T00:51:37.918914Z",
     "iopub.status.idle": "2023-05-10T00:51:37.925412Z",
     "shell.execute_reply": "2023-05-10T00:51:37.925172Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4afbc",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb72f144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.926754Z",
     "iopub.status.busy": "2023-05-10T00:51:37.926673Z",
     "iopub.status.idle": "2023-05-10T00:51:37.948299Z",
     "shell.execute_reply": "2023-05-10T00:51:37.947529Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0791f",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f805fc36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.950430Z",
     "iopub.status.busy": "2023-05-10T00:51:37.950283Z",
     "iopub.status.idle": "2023-05-10T00:51:37.957436Z",
     "shell.execute_reply": "2023-05-10T00:51:37.957163Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96798a2a",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d443121b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.958993Z",
     "iopub.status.busy": "2023-05-10T00:51:37.958901Z",
     "iopub.status.idle": "2023-05-10T00:51:37.965444Z",
     "shell.execute_reply": "2023-05-10T00:51:37.965109Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace end of line character with space\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_raw\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd95867a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.966960Z",
     "iopub.status.busy": "2023-05-10T00:51:37.966830Z",
     "iopub.status.idle": "2023-05-10T00:51:37.969847Z",
     "shell.execute_reply": "2023-05-10T00:51:37.969478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac0b2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.971365Z",
     "iopub.status.busy": "2023-05-10T00:51:37.971283Z",
     "iopub.status.idle": "2023-05-10T00:51:37.973034Z",
     "shell.execute_reply": "2023-05-10T00:51:37.972736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa83c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.974388Z",
     "iopub.status.busy": "2023-05-10T00:51:37.974310Z",
     "iopub.status.idle": "2023-05-10T00:51:37.976071Z",
     "shell.execute_reply": "2023-05-10T00:51:37.975769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754e389",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf4a1fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.977381Z",
     "iopub.status.busy": "2023-05-10T00:51:37.977301Z",
     "iopub.status.idle": "2023-05-10T00:51:37.984358Z",
     "shell.execute_reply": "2023-05-10T00:51:37.984049Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec75b86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.986118Z",
     "iopub.status.busy": "2023-05-10T00:51:37.985996Z",
     "iopub.status.idle": "2023-05-10T00:51:37.992456Z",
     "shell.execute_reply": "2023-05-10T00:51:37.992193Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c6d3a",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10654efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:37.993833Z",
     "iopub.status.busy": "2023-05-10T00:51:37.993752Z",
     "iopub.status.idle": "2023-05-10T00:51:38.000325Z",
     "shell.execute_reply": "2023-05-10T00:51:38.000018Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25112300",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccca0f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.002075Z",
     "iopub.status.busy": "2023-05-10T00:51:38.001926Z",
     "iopub.status.idle": "2023-05-10T00:51:38.009249Z",
     "shell.execute_reply": "2023-05-10T00:51:38.009007Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be148d0",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fb6af",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba11d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.010754Z",
     "iopub.status.busy": "2023-05-10T00:51:38.010678Z",
     "iopub.status.idle": "2023-05-10T00:51:38.013310Z",
     "shell.execute_reply": "2023-05-10T00:51:38.013073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a109f78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.014892Z",
     "iopub.status.busy": "2023-05-10T00:51:38.014769Z",
     "iopub.status.idle": "2023-05-10T00:51:38.017246Z",
     "shell.execute_reply": "2023-05-10T00:51:38.016869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b620746d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.018860Z",
     "iopub.status.busy": "2023-05-10T00:51:38.018746Z",
     "iopub.status.idle": "2023-05-10T00:51:38.020446Z",
     "shell.execute_reply": "2023-05-10T00:51:38.020172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e06ca6",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2aca993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.021777Z",
     "iopub.status.busy": "2023-05-10T00:51:38.021698Z",
     "iopub.status.idle": "2023-05-10T00:51:38.028298Z",
     "shell.execute_reply": "2023-05-10T00:51:38.028047Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,stopwords\u001b[38;5;241m=\u001b[39mstpwords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3ce2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.029620Z",
     "iopub.status.busy": "2023-05-10T00:51:38.029536Z",
     "iopub.status.idle": "2023-05-10T00:51:38.036156Z",
     "shell.execute_reply": "2023-05-10T00:51:38.035893Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f487b",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d99165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.037680Z",
     "iopub.status.busy": "2023-05-10T00:51:38.037574Z",
     "iopub.status.idle": "2023-05-10T00:51:38.044028Z",
     "shell.execute_reply": "2023-05-10T00:51:38.043791Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c11ffc",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c0c9de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.045425Z",
     "iopub.status.busy": "2023-05-10T00:51:38.045347Z",
     "iopub.status.idle": "2023-05-10T00:51:38.051629Z",
     "shell.execute_reply": "2023-05-10T00:51:38.051374Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7a38e",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeadcff",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "713a44be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.053245Z",
     "iopub.status.busy": "2023-05-10T00:51:38.053122Z",
     "iopub.status.idle": "2023-05-10T00:51:38.056328Z",
     "shell.execute_reply": "2023-05-10T00:51:38.055981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a8edb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.057759Z",
     "iopub.status.busy": "2023-05-10T00:51:38.057678Z",
     "iopub.status.idle": "2023-05-10T00:51:38.059457Z",
     "shell.execute_reply": "2023-05-10T00:51:38.059117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a4d9a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.060943Z",
     "iopub.status.busy": "2023-05-10T00:51:38.060862Z",
     "iopub.status.idle": "2023-05-10T00:51:38.062401Z",
     "shell.execute_reply": "2023-05-10T00:51:38.062163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bfa03",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e42f64",
   "metadata": {},
   "source": [
    "Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,stopwords=stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "243800d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.063745Z",
     "iopub.status.busy": "2023-05-10T00:51:38.063663Z",
     "iopub.status.idle": "2023-05-10T00:51:38.070855Z",
     "shell.execute_reply": "2023-05-10T00:51:38.070514Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605ede9",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a9260a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.072355Z",
     "iopub.status.busy": "2023-05-10T00:51:38.072247Z",
     "iopub.status.idle": "2023-05-10T00:51:38.078636Z",
     "shell.execute_reply": "2023-05-10T00:51:38.078397Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf241c2",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c030d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.080075Z",
     "iopub.status.busy": "2023-05-10T00:51:38.079990Z",
     "iopub.status.idle": "2023-05-10T00:51:38.087034Z",
     "shell.execute_reply": "2023-05-10T00:51:38.086752Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a8985",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09763bbe",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3b38ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.088494Z",
     "iopub.status.busy": "2023-05-10T00:51:38.088405Z",
     "iopub.status.idle": "2023-05-10T00:51:38.091119Z",
     "shell.execute_reply": "2023-05-10T00:51:38.090888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e47a901e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.092579Z",
     "iopub.status.busy": "2023-05-10T00:51:38.092477Z",
     "iopub.status.idle": "2023-05-10T00:51:38.094182Z",
     "shell.execute_reply": "2023-05-10T00:51:38.093948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85440bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.095465Z",
     "iopub.status.busy": "2023-05-10T00:51:38.095376Z",
     "iopub.status.idle": "2023-05-10T00:51:38.097056Z",
     "shell.execute_reply": "2023-05-10T00:51:38.096828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd5a73",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b97f7037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.098357Z",
     "iopub.status.busy": "2023-05-10T00:51:38.098279Z",
     "iopub.status.idle": "2023-05-10T00:51:38.105844Z",
     "shell.execute_reply": "2023-05-10T00:51:38.105579Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74360733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.107360Z",
     "iopub.status.busy": "2023-05-10T00:51:38.107253Z",
     "iopub.status.idle": "2023-05-10T00:51:38.113595Z",
     "shell.execute_reply": "2023-05-10T00:51:38.113350Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad3a1a",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12e8de7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.115968Z",
     "iopub.status.busy": "2023-05-10T00:51:38.115775Z",
     "iopub.status.idle": "2023-05-10T00:51:38.122727Z",
     "shell.execute_reply": "2023-05-10T00:51:38.122437Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce30ae2",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4125bf54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.124187Z",
     "iopub.status.busy": "2023-05-10T00:51:38.124085Z",
     "iopub.status.idle": "2023-05-10T00:51:38.130526Z",
     "shell.execute_reply": "2023-05-10T00:51:38.130267Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e7ba4",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31036f",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e27fd445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.132395Z",
     "iopub.status.busy": "2023-05-10T00:51:38.132205Z",
     "iopub.status.idle": "2023-05-10T00:51:38.135041Z",
     "shell.execute_reply": "2023-05-10T00:51:38.134793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c9de9c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.136445Z",
     "iopub.status.busy": "2023-05-10T00:51:38.136351Z",
     "iopub.status.idle": "2023-05-10T00:51:38.137977Z",
     "shell.execute_reply": "2023-05-10T00:51:38.137757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6ce5924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.139293Z",
     "iopub.status.busy": "2023-05-10T00:51:38.139217Z",
     "iopub.status.idle": "2023-05-10T00:51:38.140873Z",
     "shell.execute_reply": "2023-05-10T00:51:38.140650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab952d",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e47d74a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.142170Z",
     "iopub.status.busy": "2023-05-10T00:51:38.142097Z",
     "iopub.status.idle": "2023-05-10T00:51:38.148386Z",
     "shell.execute_reply": "2023-05-10T00:51:38.148146Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccb1aa2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.149759Z",
     "iopub.status.busy": "2023-05-10T00:51:38.149674Z",
     "iopub.status.idle": "2023-05-10T00:51:38.155778Z",
     "shell.execute_reply": "2023-05-10T00:51:38.155558Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583277cb",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a06776c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.157151Z",
     "iopub.status.busy": "2023-05-10T00:51:38.157069Z",
     "iopub.status.idle": "2023-05-10T00:51:38.163274Z",
     "shell.execute_reply": "2023-05-10T00:51:38.163052Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(wc)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486e2e4",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57b31074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.164569Z",
     "iopub.status.busy": "2023-05-10T00:51:38.164488Z",
     "iopub.status.idle": "2023-05-10T00:51:38.170730Z",
     "shell.execute_reply": "2023-05-10T00:51:38.170451Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255e06d",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fff9a",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "558fe181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.172009Z",
     "iopub.status.busy": "2023-05-10T00:51:38.171924Z",
     "iopub.status.idle": "2023-05-10T00:51:38.174469Z",
     "shell.execute_reply": "2023-05-10T00:51:38.174260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6410d238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.175744Z",
     "iopub.status.busy": "2023-05-10T00:51:38.175664Z",
     "iopub.status.idle": "2023-05-10T00:51:38.177315Z",
     "shell.execute_reply": "2023-05-10T00:51:38.177088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5904f7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.178565Z",
     "iopub.status.busy": "2023-05-10T00:51:38.178491Z",
     "iopub.status.idle": "2023-05-10T00:51:38.179973Z",
     "shell.execute_reply": "2023-05-10T00:51:38.179745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d6d30",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71d50cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.181384Z",
     "iopub.status.busy": "2023-05-10T00:51:38.181289Z",
     "iopub.status.idle": "2023-05-10T00:51:38.188136Z",
     "shell.execute_reply": "2023-05-10T00:51:38.187889Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9082d10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.189546Z",
     "iopub.status.busy": "2023-05-10T00:51:38.189463Z",
     "iopub.status.idle": "2023-05-10T00:51:38.195473Z",
     "shell.execute_reply": "2023-05-10T00:51:38.195207Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8281e5",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f9d56c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.196777Z",
     "iopub.status.busy": "2023-05-10T00:51:38.196699Z",
     "iopub.status.idle": "2023-05-10T00:51:38.202908Z",
     "shell.execute_reply": "2023-05-10T00:51:38.202666Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the cloud\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# plt.imshow(wc)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the cloud\n",
    "# plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18457bf4",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46fbf222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.204290Z",
     "iopub.status.busy": "2023-05-10T00:51:38.204215Z",
     "iopub.status.idle": "2023-05-10T00:51:38.210424Z",
     "shell.execute_reply": "2023-05-10T00:51:38.210192Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf21492",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c7fdf",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a7f689e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.211861Z",
     "iopub.status.busy": "2023-05-10T00:51:38.211778Z",
     "iopub.status.idle": "2023-05-10T00:51:38.214507Z",
     "shell.execute_reply": "2023-05-10T00:51:38.214278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb46440d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.215899Z",
     "iopub.status.busy": "2023-05-10T00:51:38.215820Z",
     "iopub.status.idle": "2023-05-10T00:51:38.217494Z",
     "shell.execute_reply": "2023-05-10T00:51:38.217269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f670cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.219315Z",
     "iopub.status.busy": "2023-05-10T00:51:38.219226Z",
     "iopub.status.idle": "2023-05-10T00:51:38.221030Z",
     "shell.execute_reply": "2023-05-10T00:51:38.220795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00b94a",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a27b640f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.222350Z",
     "iopub.status.busy": "2023-05-10T00:51:38.222272Z",
     "iopub.status.idle": "2023-05-10T00:51:38.228624Z",
     "shell.execute_reply": "2023-05-10T00:51:38.228387Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93c1fd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.229997Z",
     "iopub.status.busy": "2023-05-10T00:51:38.229922Z",
     "iopub.status.idle": "2023-05-10T00:51:38.236080Z",
     "shell.execute_reply": "2023-05-10T00:51:38.235837Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286fbdc",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d41eea",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6005aa",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1efd7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.237418Z",
     "iopub.status.busy": "2023-05-10T00:51:38.237342Z",
     "iopub.status.idle": "2023-05-10T00:51:38.243599Z",
     "shell.execute_reply": "2023-05-10T00:51:38.243362Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc6a5a",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f301934",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "421ce336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.244919Z",
     "iopub.status.busy": "2023-05-10T00:51:38.244837Z",
     "iopub.status.idle": "2023-05-10T00:51:38.247713Z",
     "shell.execute_reply": "2023-05-10T00:51:38.247333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85a0ec58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.249443Z",
     "iopub.status.busy": "2023-05-10T00:51:38.249314Z",
     "iopub.status.idle": "2023-05-10T00:51:38.251265Z",
     "shell.execute_reply": "2023-05-10T00:51:38.250878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ec5a380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.252758Z",
     "iopub.status.busy": "2023-05-10T00:51:38.252645Z",
     "iopub.status.idle": "2023-05-10T00:51:38.254283Z",
     "shell.execute_reply": "2023-05-10T00:51:38.254039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24491c",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9a73434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.255667Z",
     "iopub.status.busy": "2023-05-10T00:51:38.255594Z",
     "iopub.status.idle": "2023-05-10T00:51:38.261897Z",
     "shell.execute_reply": "2023-05-10T00:51:38.261654Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b10c726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.263322Z",
     "iopub.status.busy": "2023-05-10T00:51:38.263241Z",
     "iopub.status.idle": "2023-05-10T00:51:38.270415Z",
     "shell.execute_reply": "2023-05-10T00:51:38.270148Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6735e",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b887432",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffdfb129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.271833Z",
     "iopub.status.busy": "2023-05-10T00:51:38.271737Z",
     "iopub.status.idle": "2023-05-10T00:51:38.278195Z",
     "shell.execute_reply": "2023-05-10T00:51:38.277966Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwords_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "words_list.plot.bar(x=\"word\",y=\"freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75fd06",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d8a5108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.279601Z",
     "iopub.status.busy": "2023-05-10T00:51:38.279523Z",
     "iopub.status.idle": "2023-05-10T00:51:38.286241Z",
     "shell.execute_reply": "2023-05-10T00:51:38.286001Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25371108",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e634644",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "885d209e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.287657Z",
     "iopub.status.busy": "2023-05-10T00:51:38.287570Z",
     "iopub.status.idle": "2023-05-10T00:51:38.290358Z",
     "shell.execute_reply": "2023-05-10T00:51:38.290124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "405b45e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.291658Z",
     "iopub.status.busy": "2023-05-10T00:51:38.291578Z",
     "iopub.status.idle": "2023-05-10T00:51:38.293269Z",
     "shell.execute_reply": "2023-05-10T00:51:38.293024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae5f05c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.294541Z",
     "iopub.status.busy": "2023-05-10T00:51:38.294466Z",
     "iopub.status.idle": "2023-05-10T00:51:38.295903Z",
     "shell.execute_reply": "2023-05-10T00:51:38.295671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc90df",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8b240ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.297215Z",
     "iopub.status.busy": "2023-05-10T00:51:38.297136Z",
     "iopub.status.idle": "2023-05-10T00:51:38.303680Z",
     "shell.execute_reply": "2023-05-10T00:51:38.303441Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a1fb93f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.305119Z",
     "iopub.status.busy": "2023-05-10T00:51:38.305015Z",
     "iopub.status.idle": "2023-05-10T00:51:38.311289Z",
     "shell.execute_reply": "2023-05-10T00:51:38.311055Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d85159",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b13dfd",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bed7b009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.312646Z",
     "iopub.status.busy": "2023-05-10T00:51:38.312565Z",
     "iopub.status.idle": "2023-05-10T00:51:38.319665Z",
     "shell.execute_reply": "2023-05-10T00:51:38.319377Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list)\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list)\n",
    "df.plot.bar(x=\"word\",y=\"freq\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a358468",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03b97985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.321260Z",
     "iopub.status.busy": "2023-05-10T00:51:38.321140Z",
     "iopub.status.idle": "2023-05-10T00:51:38.327695Z",
     "shell.execute_reply": "2023-05-10T00:51:38.327441Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97451074",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ba686",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ce965c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.329109Z",
     "iopub.status.busy": "2023-05-10T00:51:38.329032Z",
     "iopub.status.idle": "2023-05-10T00:51:38.331777Z",
     "shell.execute_reply": "2023-05-10T00:51:38.331470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f2480df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.333378Z",
     "iopub.status.busy": "2023-05-10T00:51:38.333278Z",
     "iopub.status.idle": "2023-05-10T00:51:38.335488Z",
     "shell.execute_reply": "2023-05-10T00:51:38.335205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bb2229b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.336943Z",
     "iopub.status.busy": "2023-05-10T00:51:38.336859Z",
     "iopub.status.idle": "2023-05-10T00:51:38.338621Z",
     "shell.execute_reply": "2023-05-10T00:51:38.338377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d6220",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ea2be65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.339924Z",
     "iopub.status.busy": "2023-05-10T00:51:38.339848Z",
     "iopub.status.idle": "2023-05-10T00:51:38.346543Z",
     "shell.execute_reply": "2023-05-10T00:51:38.346292Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6e3ac25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.348826Z",
     "iopub.status.busy": "2023-05-10T00:51:38.348627Z",
     "iopub.status.idle": "2023-05-10T00:51:38.355094Z",
     "shell.execute_reply": "2023-05-10T00:51:38.354846Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b1782",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b29d3",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74c5bfdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.356575Z",
     "iopub.status.busy": "2023-05-10T00:51:38.356491Z",
     "iopub.status.idle": "2023-05-10T00:51:38.363573Z",
     "shell.execute_reply": "2023-05-10T00:51:38.363315Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list)\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905c501",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a01e487b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.365169Z",
     "iopub.status.busy": "2023-05-10T00:51:38.365052Z",
     "iopub.status.idle": "2023-05-10T00:51:38.371654Z",
     "shell.execute_reply": "2023-05-10T00:51:38.371406Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2b1b8",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81331861",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ff22a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.373033Z",
     "iopub.status.busy": "2023-05-10T00:51:38.372945Z",
     "iopub.status.idle": "2023-05-10T00:51:38.375635Z",
     "shell.execute_reply": "2023-05-10T00:51:38.375414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7dc983d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.376938Z",
     "iopub.status.busy": "2023-05-10T00:51:38.376864Z",
     "iopub.status.idle": "2023-05-10T00:51:38.378508Z",
     "shell.execute_reply": "2023-05-10T00:51:38.378296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08de4368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.379785Z",
     "iopub.status.busy": "2023-05-10T00:51:38.379706Z",
     "iopub.status.idle": "2023-05-10T00:51:38.381269Z",
     "shell.execute_reply": "2023-05-10T00:51:38.380983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790e345",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60dddaa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.382738Z",
     "iopub.status.busy": "2023-05-10T00:51:38.382649Z",
     "iopub.status.idle": "2023-05-10T00:51:38.389127Z",
     "shell.execute_reply": "2023-05-10T00:51:38.388877Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "538ee197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.390445Z",
     "iopub.status.busy": "2023-05-10T00:51:38.390367Z",
     "iopub.status.idle": "2023-05-10T00:51:38.396404Z",
     "shell.execute_reply": "2023-05-10T00:51:38.396149Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706afd7",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59699ef7",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58d50107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.398031Z",
     "iopub.status.busy": "2023-05-10T00:51:38.397906Z",
     "iopub.status.idle": "2023-05-10T00:51:38.404553Z",
     "shell.execute_reply": "2023-05-10T00:51:38.404311Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39mwords)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=words)\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bb139",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c20ba52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.405969Z",
     "iopub.status.busy": "2023-05-10T00:51:38.405888Z",
     "iopub.status.idle": "2023-05-10T00:51:38.412200Z",
     "shell.execute_reply": "2023-05-10T00:51:38.411967Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee77cee",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f78657",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0e5466e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.413542Z",
     "iopub.status.busy": "2023-05-10T00:51:38.413461Z",
     "iopub.status.idle": "2023-05-10T00:51:38.416285Z",
     "shell.execute_reply": "2023-05-10T00:51:38.416005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc947815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.417656Z",
     "iopub.status.busy": "2023-05-10T00:51:38.417574Z",
     "iopub.status.idle": "2023-05-10T00:51:38.419315Z",
     "shell.execute_reply": "2023-05-10T00:51:38.419095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5dbca68c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.420618Z",
     "iopub.status.busy": "2023-05-10T00:51:38.420544Z",
     "iopub.status.idle": "2023-05-10T00:51:38.421979Z",
     "shell.execute_reply": "2023-05-10T00:51:38.421752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7821497",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d98b572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.423273Z",
     "iopub.status.busy": "2023-05-10T00:51:38.423193Z",
     "iopub.status.idle": "2023-05-10T00:51:38.429418Z",
     "shell.execute_reply": "2023-05-10T00:51:38.429195Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "705b606a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.430735Z",
     "iopub.status.busy": "2023-05-10T00:51:38.430662Z",
     "iopub.status.idle": "2023-05-10T00:51:38.437062Z",
     "shell.execute_reply": "2023-05-10T00:51:38.436792Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc81ae",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912293b",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "773ca241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.438486Z",
     "iopub.status.busy": "2023-05-10T00:51:38.438388Z",
     "iopub.status.idle": "2023-05-10T00:51:38.445549Z",
     "shell.execute_reply": "2023-05-10T00:51:38.445299Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df.plot.bar(x=\"word\",y=\"freq\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "print(df)\n",
    "# df.plot.bar(x=\"word\",y=\"freq\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3db92",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc09bf4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.446865Z",
     "iopub.status.busy": "2023-05-10T00:51:38.446787Z",
     "iopub.status.idle": "2023-05-10T00:51:38.453505Z",
     "shell.execute_reply": "2023-05-10T00:51:38.453234Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a9b2b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1debd5",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f449829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.454913Z",
     "iopub.status.busy": "2023-05-10T00:51:38.454833Z",
     "iopub.status.idle": "2023-05-10T00:51:38.457485Z",
     "shell.execute_reply": "2023-05-10T00:51:38.457257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eeb6dc2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.458795Z",
     "iopub.status.busy": "2023-05-10T00:51:38.458699Z",
     "iopub.status.idle": "2023-05-10T00:51:38.460371Z",
     "shell.execute_reply": "2023-05-10T00:51:38.460133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "25144be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.461760Z",
     "iopub.status.busy": "2023-05-10T00:51:38.461685Z",
     "iopub.status.idle": "2023-05-10T00:51:38.463214Z",
     "shell.execute_reply": "2023-05-10T00:51:38.462994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552d935",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4cb7baac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.464551Z",
     "iopub.status.busy": "2023-05-10T00:51:38.464469Z",
     "iopub.status.idle": "2023-05-10T00:51:38.470844Z",
     "shell.execute_reply": "2023-05-10T00:51:38.470597Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a59bd921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.472225Z",
     "iopub.status.busy": "2023-05-10T00:51:38.472140Z",
     "iopub.status.idle": "2023-05-10T00:51:38.478167Z",
     "shell.execute_reply": "2023-05-10T00:51:38.477939Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375ecfc",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beace632",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae4e7eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.479576Z",
     "iopub.status.busy": "2023-05-10T00:51:38.479493Z",
     "iopub.status.idle": "2023-05-10T00:51:38.486392Z",
     "shell.execute_reply": "2023-05-10T00:51:38.486120Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af84e2d",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed79cb15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.487771Z",
     "iopub.status.busy": "2023-05-10T00:51:38.487694Z",
     "iopub.status.idle": "2023-05-10T00:51:38.493951Z",
     "shell.execute_reply": "2023-05-10T00:51:38.493723Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88774991",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140aec90",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "28efddd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.495281Z",
     "iopub.status.busy": "2023-05-10T00:51:38.495198Z",
     "iopub.status.idle": "2023-05-10T00:51:38.498089Z",
     "shell.execute_reply": "2023-05-10T00:51:38.497800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d353d164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.499541Z",
     "iopub.status.busy": "2023-05-10T00:51:38.499458Z",
     "iopub.status.idle": "2023-05-10T00:51:38.501318Z",
     "shell.execute_reply": "2023-05-10T00:51:38.501106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea5d539f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.502688Z",
     "iopub.status.busy": "2023-05-10T00:51:38.502611Z",
     "iopub.status.idle": "2023-05-10T00:51:38.504243Z",
     "shell.execute_reply": "2023-05-10T00:51:38.504027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120763f7",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f017ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.505510Z",
     "iopub.status.busy": "2023-05-10T00:51:38.505437Z",
     "iopub.status.idle": "2023-05-10T00:51:38.511801Z",
     "shell.execute_reply": "2023-05-10T00:51:38.511562Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a3f47e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.513149Z",
     "iopub.status.busy": "2023-05-10T00:51:38.513062Z",
     "iopub.status.idle": "2023-05-10T00:51:38.519704Z",
     "shell.execute_reply": "2023-05-10T00:51:38.519452Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f96ae9",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e39c66",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3ed8afac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.521304Z",
     "iopub.status.busy": "2023-05-10T00:51:38.521193Z",
     "iopub.status.idle": "2023-05-10T00:51:38.528579Z",
     "shell.execute_reply": "2023-05-10T00:51:38.528329Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(words_list,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46fc00",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "327124d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.529926Z",
     "iopub.status.busy": "2023-05-10T00:51:38.529848Z",
     "iopub.status.idle": "2023-05-10T00:51:38.536714Z",
     "shell.execute_reply": "2023-05-10T00:51:38.536426Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee973120",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310ea41",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a53b0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.538170Z",
     "iopub.status.busy": "2023-05-10T00:51:38.538087Z",
     "iopub.status.idle": "2023-05-10T00:51:38.540865Z",
     "shell.execute_reply": "2023-05-10T00:51:38.540649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "684089dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.542194Z",
     "iopub.status.busy": "2023-05-10T00:51:38.542118Z",
     "iopub.status.idle": "2023-05-10T00:51:38.543766Z",
     "shell.execute_reply": "2023-05-10T00:51:38.543556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1859f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.545074Z",
     "iopub.status.busy": "2023-05-10T00:51:38.544991Z",
     "iopub.status.idle": "2023-05-10T00:51:38.546712Z",
     "shell.execute_reply": "2023-05-10T00:51:38.546438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45960eca",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2960d0ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.548244Z",
     "iopub.status.busy": "2023-05-10T00:51:38.548135Z",
     "iopub.status.idle": "2023-05-10T00:51:38.555480Z",
     "shell.execute_reply": "2023-05-10T00:51:38.555212Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(words_list)\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c94af428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.556895Z",
     "iopub.status.busy": "2023-05-10T00:51:38.556797Z",
     "iopub.status.idle": "2023-05-10T00:51:38.563066Z",
     "shell.execute_reply": "2023-05-10T00:51:38.562838Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4849dbb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.564418Z",
     "iopub.status.busy": "2023-05-10T00:51:38.564341Z",
     "iopub.status.idle": "2023-05-10T00:51:38.570896Z",
     "shell.execute_reply": "2023-05-10T00:51:38.570641Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22fda6",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be302a",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "849bfa21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.572293Z",
     "iopub.status.busy": "2023-05-10T00:51:38.572195Z",
     "iopub.status.idle": "2023-05-10T00:51:38.574234Z",
     "shell.execute_reply": "2023-05-10T00:51:38.573992Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1507832862.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [113], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.DataFrame.from(words_list,columns=['words'])\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40206891",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "826d971d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.575529Z",
     "iopub.status.busy": "2023-05-10T00:51:38.575437Z",
     "iopub.status.idle": "2023-05-10T00:51:38.581987Z",
     "shell.execute_reply": "2023-05-10T00:51:38.581680Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b834b",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7832a46",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "93848b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.583712Z",
     "iopub.status.busy": "2023-05-10T00:51:38.583585Z",
     "iopub.status.idle": "2023-05-10T00:51:38.586490Z",
     "shell.execute_reply": "2023-05-10T00:51:38.586243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ce896ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.588293Z",
     "iopub.status.busy": "2023-05-10T00:51:38.587814Z",
     "iopub.status.idle": "2023-05-10T00:51:38.590002Z",
     "shell.execute_reply": "2023-05-10T00:51:38.589763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cc2f1b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.591649Z",
     "iopub.status.busy": "2023-05-10T00:51:38.591539Z",
     "iopub.status.idle": "2023-05-10T00:51:38.593165Z",
     "shell.execute_reply": "2023-05-10T00:51:38.592924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64aa5ca",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "331953a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.594570Z",
     "iopub.status.busy": "2023-05-10T00:51:38.594487Z",
     "iopub.status.idle": "2023-05-10T00:51:38.601980Z",
     "shell.execute_reply": "2023-05-10T00:51:38.601706Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mword_tokenize(words_list)\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ad8aff43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.603403Z",
     "iopub.status.busy": "2023-05-10T00:51:38.603297Z",
     "iopub.status.idle": "2023-05-10T00:51:38.609673Z",
     "shell.execute_reply": "2023-05-10T00:51:38.609400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94df0460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.611108Z",
     "iopub.status.busy": "2023-05-10T00:51:38.611026Z",
     "iopub.status.idle": "2023-05-10T00:51:38.617460Z",
     "shell.execute_reply": "2023-05-10T00:51:38.617194Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [120], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985360d",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba5cb3",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d5d51e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.618852Z",
     "iopub.status.busy": "2023-05-10T00:51:38.618765Z",
     "iopub.status.idle": "2023-05-10T00:51:38.625991Z",
     "shell.execute_reply": "2023-05-10T00:51:38.625756Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c79899",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4714040a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:38.627378Z",
     "iopub.status.busy": "2023-05-10T00:51:38.627299Z",
     "iopub.status.idle": "2023-05-10T00:51:39.116178Z",
     "shell.execute_reply": "2023-05-10T00:51:39.115892Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [122], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fb588",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e95724",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "52ae18b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.117710Z",
     "iopub.status.busy": "2023-05-10T00:51:39.117601Z",
     "iopub.status.idle": "2023-05-10T00:51:39.120233Z",
     "shell.execute_reply": "2023-05-10T00:51:39.119971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "49064dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.121560Z",
     "iopub.status.busy": "2023-05-10T00:51:39.121460Z",
     "iopub.status.idle": "2023-05-10T00:51:39.123237Z",
     "shell.execute_reply": "2023-05-10T00:51:39.122886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8205ca03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.124698Z",
     "iopub.status.busy": "2023-05-10T00:51:39.124596Z",
     "iopub.status.idle": "2023-05-10T00:51:39.126147Z",
     "shell.execute_reply": "2023-05-10T00:51:39.125909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880385da",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fa217b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.127554Z",
     "iopub.status.busy": "2023-05-10T00:51:39.127465Z",
     "iopub.status.idle": "2023-05-10T00:51:39.190998Z",
     "shell.execute_reply": "2023-05-10T00:51:39.190696Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b926b567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.192575Z",
     "iopub.status.busy": "2023-05-10T00:51:39.192458Z",
     "iopub.status.idle": "2023-05-10T00:51:39.199295Z",
     "shell.execute_reply": "2023-05-10T00:51:39.198947Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6a2f21e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.200807Z",
     "iopub.status.busy": "2023-05-10T00:51:39.200704Z",
     "iopub.status.idle": "2023-05-10T00:51:39.207057Z",
     "shell.execute_reply": "2023-05-10T00:51:39.206804Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07728fd4",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66cfdf",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0fa5f7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.208484Z",
     "iopub.status.busy": "2023-05-10T00:51:39.208395Z",
     "iopub.status.idle": "2023-05-10T00:51:39.215077Z",
     "shell.execute_reply": "2023-05-10T00:51:39.214771Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [129], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67eba3",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2c546702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.216606Z",
     "iopub.status.busy": "2023-05-10T00:51:39.216500Z",
     "iopub.status.idle": "2023-05-10T00:51:39.222947Z",
     "shell.execute_reply": "2023-05-10T00:51:39.222721Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [130], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33573ef3",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410caf4",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6d04053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.224329Z",
     "iopub.status.busy": "2023-05-10T00:51:39.224253Z",
     "iopub.status.idle": "2023-05-10T00:51:39.226970Z",
     "shell.execute_reply": "2023-05-10T00:51:39.226748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8758bab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.228322Z",
     "iopub.status.busy": "2023-05-10T00:51:39.228233Z",
     "iopub.status.idle": "2023-05-10T00:51:39.230238Z",
     "shell.execute_reply": "2023-05-10T00:51:39.229965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "00ffc8ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.232601Z",
     "iopub.status.busy": "2023-05-10T00:51:39.232450Z",
     "iopub.status.idle": "2023-05-10T00:51:39.234313Z",
     "shell.execute_reply": "2023-05-10T00:51:39.233987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340086c0",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8224eefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.236431Z",
     "iopub.status.busy": "2023-05-10T00:51:39.236304Z",
     "iopub.status.idle": "2023-05-10T00:51:39.269365Z",
     "shell.execute_reply": "2023-05-10T00:51:39.268699Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d96ca53e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.271825Z",
     "iopub.status.busy": "2023-05-10T00:51:39.271672Z",
     "iopub.status.idle": "2023-05-10T00:51:39.284023Z",
     "shell.execute_reply": "2023-05-10T00:51:39.283387Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [135], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2fff35af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.287600Z",
     "iopub.status.busy": "2023-05-10T00:51:39.287158Z",
     "iopub.status.idle": "2023-05-10T00:51:39.297287Z",
     "shell.execute_reply": "2023-05-10T00:51:39.296927Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60baee20",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007a24",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "72bb2dda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.299161Z",
     "iopub.status.busy": "2023-05-10T00:51:39.299000Z",
     "iopub.status.idle": "2023-05-10T00:51:39.306917Z",
     "shell.execute_reply": "2023-05-10T00:51:39.306466Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [137], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e708f",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "531ba9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.308465Z",
     "iopub.status.busy": "2023-05-10T00:51:39.308353Z",
     "iopub.status.idle": "2023-05-10T00:51:39.459882Z",
     "shell.execute_reply": "2023-05-10T00:51:39.459598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [138], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43mDownloaderShell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md) Download\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml) List\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq) Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloader> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"Forward raw_input to frontends\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03mRaises\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m------\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mStdinNotImplementedError if active frontend doesn't support stdin.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288031c1",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2155bc",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2fcdb8ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.461453Z",
     "iopub.status.busy": "2023-05-10T00:51:39.461347Z",
     "iopub.status.idle": "2023-05-10T00:51:39.464041Z",
     "shell.execute_reply": "2023-05-10T00:51:39.463794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "54b06202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.465474Z",
     "iopub.status.busy": "2023-05-10T00:51:39.465376Z",
     "iopub.status.idle": "2023-05-10T00:51:39.467031Z",
     "shell.execute_reply": "2023-05-10T00:51:39.466820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4905de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.468340Z",
     "iopub.status.busy": "2023-05-10T00:51:39.468246Z",
     "iopub.status.idle": "2023-05-10T00:51:39.469856Z",
     "shell.execute_reply": "2023-05-10T00:51:39.469643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f86b22",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d24d2fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.471182Z",
     "iopub.status.busy": "2023-05-10T00:51:39.471107Z",
     "iopub.status.idle": "2023-05-10T00:51:39.496946Z",
     "shell.execute_reply": "2023-05-10T00:51:39.496647Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(words_list)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "acce8730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.499240Z",
     "iopub.status.busy": "2023-05-10T00:51:39.499096Z",
     "iopub.status.idle": "2023-05-10T00:51:39.505845Z",
     "shell.execute_reply": "2023-05-10T00:51:39.505551Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [143], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9491b945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.507319Z",
     "iopub.status.busy": "2023-05-10T00:51:39.507210Z",
     "iopub.status.idle": "2023-05-10T00:51:39.514264Z",
     "shell.execute_reply": "2023-05-10T00:51:39.513195Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [144], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de23e9",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f74a2",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f88f1761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.516086Z",
     "iopub.status.busy": "2023-05-10T00:51:39.515950Z",
     "iopub.status.idle": "2023-05-10T00:51:39.522762Z",
     "shell.execute_reply": "2023-05-10T00:51:39.522495Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [145], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e661c",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a0c88d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.524250Z",
     "iopub.status.busy": "2023-05-10T00:51:39.524138Z",
     "iopub.status.idle": "2023-05-10T00:51:39.546311Z",
     "shell.execute_reply": "2023-05-10T00:51:39.545918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [146], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interactive_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43mDownloaderShell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md) Download\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml) List\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq) Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloader> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py:1174\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m\"\"\"Forward raw_input to frontends\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03mRaises\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m------\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mStdinNotImplementedError if active frontend doesn't support stdin.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527b3a0",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f925f66",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af80aa6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.548122Z",
     "iopub.status.busy": "2023-05-10T00:51:39.547996Z",
     "iopub.status.idle": "2023-05-10T00:51:39.550850Z",
     "shell.execute_reply": "2023-05-10T00:51:39.550571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "daec81f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.552361Z",
     "iopub.status.busy": "2023-05-10T00:51:39.552256Z",
     "iopub.status.idle": "2023-05-10T00:51:39.553970Z",
     "shell.execute_reply": "2023-05-10T00:51:39.553723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0604a743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.555413Z",
     "iopub.status.busy": "2023-05-10T00:51:39.555310Z",
     "iopub.status.idle": "2023-05-10T00:51:39.556905Z",
     "shell.execute_reply": "2023-05-10T00:51:39.556675Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8d42b",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cb1cc68e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.558195Z",
     "iopub.status.busy": "2023-05-10T00:51:39.558113Z",
     "iopub.status.idle": "2023-05-10T00:51:39.592326Z",
     "shell.execute_reply": "2023-05-10T00:51:39.591889Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f3568f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.594035Z",
     "iopub.status.busy": "2023-05-10T00:51:39.593905Z",
     "iopub.status.idle": "2023-05-10T00:51:39.604786Z",
     "shell.execute_reply": "2023-05-10T00:51:39.604514Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [151], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7519a745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.606272Z",
     "iopub.status.busy": "2023-05-10T00:51:39.606102Z",
     "iopub.status.idle": "2023-05-10T00:51:39.648945Z",
     "shell.execute_reply": "2023-05-10T00:51:39.648568Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [152], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519914c",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34a184",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bff5c9e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.650699Z",
     "iopub.status.busy": "2023-05-10T00:51:39.650546Z",
     "iopub.status.idle": "2023-05-10T00:51:39.657576Z",
     "shell.execute_reply": "2023-05-10T00:51:39.657301Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [153], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b9139",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0b144832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.659058Z",
     "iopub.status.busy": "2023-05-10T00:51:39.658944Z",
     "iopub.status.idle": "2023-05-10T00:51:39.666449Z",
     "shell.execute_reply": "2023-05-10T00:51:39.666159Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [154], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcac49",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a10068",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0cead039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.667957Z",
     "iopub.status.busy": "2023-05-10T00:51:39.667849Z",
     "iopub.status.idle": "2023-05-10T00:51:39.670584Z",
     "shell.execute_reply": "2023-05-10T00:51:39.670357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1bb5ba61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.671891Z",
     "iopub.status.busy": "2023-05-10T00:51:39.671800Z",
     "iopub.status.idle": "2023-05-10T00:51:39.673410Z",
     "shell.execute_reply": "2023-05-10T00:51:39.673176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a7b74a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.674841Z",
     "iopub.status.busy": "2023-05-10T00:51:39.674760Z",
     "iopub.status.idle": "2023-05-10T00:51:39.676343Z",
     "shell.execute_reply": "2023-05-10T00:51:39.676015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fa074",
   "metadata": {},
   "source": [
    "Crating and updating the stopword list\n",
    "stpwords = set(STOPWORDS)\n",
    "stpwords.add('will')\n",
    "stpwords.add('said')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1662cbb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.677698Z",
     "iopub.status.busy": "2023-05-10T00:51:39.677617Z",
     "iopub.status.idle": "2023-05-10T00:51:39.704736Z",
     "shell.execute_reply": "2023-05-10T00:51:39.704456Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [158], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      3\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      4\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a8c30313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.706302Z",
     "iopub.status.busy": "2023-05-10T00:51:39.706194Z",
     "iopub.status.idle": "2023-05-10T00:51:39.712483Z",
     "shell.execute_reply": "2023-05-10T00:51:39.712251Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [159], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "91dc1d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.714278Z",
     "iopub.status.busy": "2023-05-10T00:51:39.714048Z",
     "iopub.status.idle": "2023-05-10T00:51:39.720911Z",
     "shell.execute_reply": "2023-05-10T00:51:39.720657Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [160], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f4d6c",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea608991",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c0d56a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.722321Z",
     "iopub.status.busy": "2023-05-10T00:51:39.722216Z",
     "iopub.status.idle": "2023-05-10T00:51:39.728583Z",
     "shell.execute_reply": "2023-05-10T00:51:39.728274Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [161], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574873fc",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "32edb3cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.730371Z",
     "iopub.status.busy": "2023-05-10T00:51:39.730189Z",
     "iopub.status.idle": "2023-05-10T00:51:39.739151Z",
     "shell.execute_reply": "2023-05-10T00:51:39.737716Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [162], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf13f12",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b23e3",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dace7841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.741615Z",
     "iopub.status.busy": "2023-05-10T00:51:39.741494Z",
     "iopub.status.idle": "2023-05-10T00:51:39.744941Z",
     "shell.execute_reply": "2023-05-10T00:51:39.744512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3216ef46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.748109Z",
     "iopub.status.busy": "2023-05-10T00:51:39.747920Z",
     "iopub.status.idle": "2023-05-10T00:51:39.750165Z",
     "shell.execute_reply": "2023-05-10T00:51:39.749775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "80d5032e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.751928Z",
     "iopub.status.busy": "2023-05-10T00:51:39.751814Z",
     "iopub.status.idle": "2023-05-10T00:51:39.753689Z",
     "shell.execute_reply": "2023-05-10T00:51:39.753384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a3684319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.755284Z",
     "iopub.status.busy": "2023-05-10T00:51:39.755179Z",
     "iopub.status.idle": "2023-05-10T00:51:39.785245Z",
     "shell.execute_reply": "2023-05-10T00:51:39.784937Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [166], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "132e1ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.786840Z",
     "iopub.status.busy": "2023-05-10T00:51:39.786732Z",
     "iopub.status.idle": "2023-05-10T00:51:39.793145Z",
     "shell.execute_reply": "2023-05-10T00:51:39.792879Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [167], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c94d2f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.794519Z",
     "iopub.status.busy": "2023-05-10T00:51:39.794433Z",
     "iopub.status.idle": "2023-05-10T00:51:39.801002Z",
     "shell.execute_reply": "2023-05-10T00:51:39.800742Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [168], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac2a5fe",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6019a7c",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4673db02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.802425Z",
     "iopub.status.busy": "2023-05-10T00:51:39.802317Z",
     "iopub.status.idle": "2023-05-10T00:51:39.808790Z",
     "shell.execute_reply": "2023-05-10T00:51:39.808523Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [169], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b474b81",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a242f483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.810181Z",
     "iopub.status.busy": "2023-05-10T00:51:39.810102Z",
     "iopub.status.idle": "2023-05-10T00:51:39.816976Z",
     "shell.execute_reply": "2023-05-10T00:51:39.816719Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [170], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84497422",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b96b5",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9b64002f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.818401Z",
     "iopub.status.busy": "2023-05-10T00:51:39.818323Z",
     "iopub.status.idle": "2023-05-10T00:51:39.821052Z",
     "shell.execute_reply": "2023-05-10T00:51:39.820830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6e15a99e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.822457Z",
     "iopub.status.busy": "2023-05-10T00:51:39.822370Z",
     "iopub.status.idle": "2023-05-10T00:51:39.824092Z",
     "shell.execute_reply": "2023-05-10T00:51:39.823900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9b76af34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.825424Z",
     "iopub.status.busy": "2023-05-10T00:51:39.825339Z",
     "iopub.status.idle": "2023-05-10T00:51:39.827118Z",
     "shell.execute_reply": "2023-05-10T00:51:39.826888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d0b3fb78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.828412Z",
     "iopub.status.busy": "2023-05-10T00:51:39.828335Z",
     "iopub.status.idle": "2023-05-10T00:51:39.855966Z",
     "shell.execute_reply": "2023-05-10T00:51:39.855649Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [174], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=8\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2259fa27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.857504Z",
     "iopub.status.busy": "2023-05-10T00:51:39.857400Z",
     "iopub.status.idle": "2023-05-10T00:51:39.864379Z",
     "shell.execute_reply": "2023-05-10T00:51:39.864038Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [175], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "993640c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.866151Z",
     "iopub.status.busy": "2023-05-10T00:51:39.866034Z",
     "iopub.status.idle": "2023-05-10T00:51:39.872437Z",
     "shell.execute_reply": "2023-05-10T00:51:39.872157Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [176], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8964b8d9",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dba7da",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0b83375b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.873836Z",
     "iopub.status.busy": "2023-05-10T00:51:39.873752Z",
     "iopub.status.idle": "2023-05-10T00:51:39.880327Z",
     "shell.execute_reply": "2023-05-10T00:51:39.880023Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [177], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fa970",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "632d4095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.881938Z",
     "iopub.status.busy": "2023-05-10T00:51:39.881825Z",
     "iopub.status.idle": "2023-05-10T00:51:39.888337Z",
     "shell.execute_reply": "2023-05-10T00:51:39.888091Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [178], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf658713",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404319c5",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "479da3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.889740Z",
     "iopub.status.busy": "2023-05-10T00:51:39.889659Z",
     "iopub.status.idle": "2023-05-10T00:51:39.892343Z",
     "shell.execute_reply": "2023-05-10T00:51:39.892122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7fbcbd5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.893605Z",
     "iopub.status.busy": "2023-05-10T00:51:39.893529Z",
     "iopub.status.idle": "2023-05-10T00:51:39.895248Z",
     "shell.execute_reply": "2023-05-10T00:51:39.895020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2b54913d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.896541Z",
     "iopub.status.busy": "2023-05-10T00:51:39.896466Z",
     "iopub.status.idle": "2023-05-10T00:51:39.898172Z",
     "shell.execute_reply": "2023-05-10T00:51:39.897944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "47228657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.899534Z",
     "iopub.status.busy": "2023-05-10T00:51:39.899459Z",
     "iopub.status.idle": "2023-05-10T00:51:39.932315Z",
     "shell.execute_reply": "2023-05-10T00:51:39.931899Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [182], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4f571e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.934375Z",
     "iopub.status.busy": "2023-05-10T00:51:39.934140Z",
     "iopub.status.idle": "2023-05-10T00:51:39.941299Z",
     "shell.execute_reply": "2023-05-10T00:51:39.940986Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [183], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "51dff3e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.943144Z",
     "iopub.status.busy": "2023-05-10T00:51:39.942942Z",
     "iopub.status.idle": "2023-05-10T00:51:39.950844Z",
     "shell.execute_reply": "2023-05-10T00:51:39.950559Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [184], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9143e",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbcf82",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ce908a7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.952267Z",
     "iopub.status.busy": "2023-05-10T00:51:39.952159Z",
     "iopub.status.idle": "2023-05-10T00:51:39.958623Z",
     "shell.execute_reply": "2023-05-10T00:51:39.958364Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [185], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "df.plot.bar(x=\"words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ef3c1",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ea2648dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.960017Z",
     "iopub.status.busy": "2023-05-10T00:51:39.959935Z",
     "iopub.status.idle": "2023-05-10T00:51:39.966448Z",
     "shell.execute_reply": "2023-05-10T00:51:39.966176Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [186], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e1ef4",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bac17c",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3091af77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.967995Z",
     "iopub.status.busy": "2023-05-10T00:51:39.967909Z",
     "iopub.status.idle": "2023-05-10T00:51:39.970634Z",
     "shell.execute_reply": "2023-05-10T00:51:39.970398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "90d4596e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.972021Z",
     "iopub.status.busy": "2023-05-10T00:51:39.971917Z",
     "iopub.status.idle": "2023-05-10T00:51:39.973435Z",
     "shell.execute_reply": "2023-05-10T00:51:39.973204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "857591b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.974793Z",
     "iopub.status.busy": "2023-05-10T00:51:39.974716Z",
     "iopub.status.idle": "2023-05-10T00:51:39.976313Z",
     "shell.execute_reply": "2023-05-10T00:51:39.976081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7ddc13d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:39.977595Z",
     "iopub.status.busy": "2023-05-10T00:51:39.977517Z",
     "iopub.status.idle": "2023-05-10T00:51:40.004314Z",
     "shell.execute_reply": "2023-05-10T00:51:40.004031Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [190], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6aeb89e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.005779Z",
     "iopub.status.busy": "2023-05-10T00:51:40.005696Z",
     "iopub.status.idle": "2023-05-10T00:51:40.012269Z",
     "shell.execute_reply": "2023-05-10T00:51:40.011964Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [191], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "93046f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.013785Z",
     "iopub.status.busy": "2023-05-10T00:51:40.013659Z",
     "iopub.status.idle": "2023-05-10T00:51:40.020051Z",
     "shell.execute_reply": "2023-05-10T00:51:40.019791Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [192], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a910f",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20260da",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "54710ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.021429Z",
     "iopub.status.busy": "2023-05-10T00:51:40.021351Z",
     "iopub.status.idle": "2023-05-10T00:51:40.027758Z",
     "shell.execute_reply": "2023-05-10T00:51:40.027513Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [193], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7de323",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3ab9fdb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.029137Z",
     "iopub.status.busy": "2023-05-10T00:51:40.029060Z",
     "iopub.status.idle": "2023-05-10T00:51:40.035608Z",
     "shell.execute_reply": "2023-05-10T00:51:40.035369Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [194], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd01b6",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e6ab",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "76a921b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.036952Z",
     "iopub.status.busy": "2023-05-10T00:51:40.036877Z",
     "iopub.status.idle": "2023-05-10T00:51:40.039516Z",
     "shell.execute_reply": "2023-05-10T00:51:40.039287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3d9a9b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.040854Z",
     "iopub.status.busy": "2023-05-10T00:51:40.040774Z",
     "iopub.status.idle": "2023-05-10T00:51:40.042411Z",
     "shell.execute_reply": "2023-05-10T00:51:40.042180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9baa680a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.043697Z",
     "iopub.status.busy": "2023-05-10T00:51:40.043616Z",
     "iopub.status.idle": "2023-05-10T00:51:40.045216Z",
     "shell.execute_reply": "2023-05-10T00:51:40.044989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7d75830f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.046434Z",
     "iopub.status.busy": "2023-05-10T00:51:40.046362Z",
     "iopub.status.idle": "2023-05-10T00:51:40.072750Z",
     "shell.execute_reply": "2023-05-10T00:51:40.072411Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [198], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "55d6493f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.074567Z",
     "iopub.status.busy": "2023-05-10T00:51:40.074442Z",
     "iopub.status.idle": "2023-05-10T00:51:40.081197Z",
     "shell.execute_reply": "2023-05-10T00:51:40.080847Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [199], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "aeec629e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.082889Z",
     "iopub.status.busy": "2023-05-10T00:51:40.082751Z",
     "iopub.status.idle": "2023-05-10T00:51:40.089178Z",
     "shell.execute_reply": "2023-05-10T00:51:40.088914Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [200], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6553db",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41598ce6",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e057dc02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.090533Z",
     "iopub.status.busy": "2023-05-10T00:51:40.090451Z",
     "iopub.status.idle": "2023-05-10T00:51:40.097045Z",
     "shell.execute_reply": "2023-05-10T00:51:40.096800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [201], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m rslt\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "plt.to_file('Trump.png')\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760bd93",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ffc39464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.098474Z",
     "iopub.status.busy": "2023-05-10T00:51:40.098392Z",
     "iopub.status.idle": "2023-05-10T00:51:40.104668Z",
     "shell.execute_reply": "2023-05-10T00:51:40.104429Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [202], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680213c",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da600ec",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "751200f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.105975Z",
     "iopub.status.busy": "2023-05-10T00:51:40.105890Z",
     "iopub.status.idle": "2023-05-10T00:51:40.108514Z",
     "shell.execute_reply": "2023-05-10T00:51:40.108293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d529d72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.109792Z",
     "iopub.status.busy": "2023-05-10T00:51:40.109713Z",
     "iopub.status.idle": "2023-05-10T00:51:40.111279Z",
     "shell.execute_reply": "2023-05-10T00:51:40.111049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3dc7b5cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.112533Z",
     "iopub.status.busy": "2023-05-10T00:51:40.112457Z",
     "iopub.status.idle": "2023-05-10T00:51:40.114075Z",
     "shell.execute_reply": "2023-05-10T00:51:40.113842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dbc3959f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.115422Z",
     "iopub.status.busy": "2023-05-10T00:51:40.115342Z",
     "iopub.status.idle": "2023-05-10T00:51:40.141915Z",
     "shell.execute_reply": "2023-05-10T00:51:40.141613Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [206], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "995a6fb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.143391Z",
     "iopub.status.busy": "2023-05-10T00:51:40.143310Z",
     "iopub.status.idle": "2023-05-10T00:51:40.149579Z",
     "shell.execute_reply": "2023-05-10T00:51:40.149355Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [207], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6e01295a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.150923Z",
     "iopub.status.busy": "2023-05-10T00:51:40.150847Z",
     "iopub.status.idle": "2023-05-10T00:51:40.156861Z",
     "shell.execute_reply": "2023-05-10T00:51:40.156604Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [208], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cb3b6",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c70866",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "92851a18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.158279Z",
     "iopub.status.busy": "2023-05-10T00:51:40.158193Z",
     "iopub.status.idle": "2023-05-10T00:51:40.164746Z",
     "shell.execute_reply": "2023-05-10T00:51:40.164497Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [209], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.DataFrame.from(words_list,columns=['words'])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m rslt\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "# print(df)\n",
    "plt.savefig('Trump.png')\n",
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb223960",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9561546b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.166195Z",
     "iopub.status.busy": "2023-05-10T00:51:40.166112Z",
     "iopub.status.idle": "2023-05-10T00:51:40.172508Z",
     "shell.execute_reply": "2023-05-10T00:51:40.172276Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [210], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fe852",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd7557",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cc4fedc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.173838Z",
     "iopub.status.busy": "2023-05-10T00:51:40.173751Z",
     "iopub.status.idle": "2023-05-10T00:51:40.176434Z",
     "shell.execute_reply": "2023-05-10T00:51:40.176208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1de065a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.177802Z",
     "iopub.status.busy": "2023-05-10T00:51:40.177725Z",
     "iopub.status.idle": "2023-05-10T00:51:40.179438Z",
     "shell.execute_reply": "2023-05-10T00:51:40.179209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1187c404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.180738Z",
     "iopub.status.busy": "2023-05-10T00:51:40.180661Z",
     "iopub.status.idle": "2023-05-10T00:51:40.182284Z",
     "shell.execute_reply": "2023-05-10T00:51:40.182005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a94fd169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.184592Z",
     "iopub.status.busy": "2023-05-10T00:51:40.184469Z",
     "iopub.status.idle": "2023-05-10T00:51:40.211942Z",
     "shell.execute_reply": "2023-05-10T00:51:40.211626Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [214], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9a78cfc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.213513Z",
     "iopub.status.busy": "2023-05-10T00:51:40.213411Z",
     "iopub.status.idle": "2023-05-10T00:51:40.219846Z",
     "shell.execute_reply": "2023-05-10T00:51:40.219610Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [215], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "be6e6eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.221129Z",
     "iopub.status.busy": "2023-05-10T00:51:40.221047Z",
     "iopub.status.idle": "2023-05-10T00:51:40.227250Z",
     "shell.execute_reply": "2023-05-10T00:51:40.226992Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [216], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb306b03",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74d67d",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02044b04",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "61d2a919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.228617Z",
     "iopub.status.busy": "2023-05-10T00:51:40.228536Z",
     "iopub.status.idle": "2023-05-10T00:51:40.235174Z",
     "shell.execute_reply": "2023-05-10T00:51:40.234940Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.show()\n",
    "plt.savefig('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031da6e",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "16ee992f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.236514Z",
     "iopub.status.busy": "2023-05-10T00:51:40.236431Z",
     "iopub.status.idle": "2023-05-10T00:51:40.242679Z",
     "shell.execute_reply": "2023-05-10T00:51:40.242439Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [218], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca0f80",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9899d3e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8decf012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.244004Z",
     "iopub.status.busy": "2023-05-10T00:51:40.243918Z",
     "iopub.status.idle": "2023-05-10T00:51:40.246674Z",
     "shell.execute_reply": "2023-05-10T00:51:40.246440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2321c046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.247980Z",
     "iopub.status.busy": "2023-05-10T00:51:40.247902Z",
     "iopub.status.idle": "2023-05-10T00:51:40.249630Z",
     "shell.execute_reply": "2023-05-10T00:51:40.249386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f9111f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.250935Z",
     "iopub.status.busy": "2023-05-10T00:51:40.250858Z",
     "iopub.status.idle": "2023-05-10T00:51:40.252486Z",
     "shell.execute_reply": "2023-05-10T00:51:40.252249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9bde6e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.253808Z",
     "iopub.status.busy": "2023-05-10T00:51:40.253727Z",
     "iopub.status.idle": "2023-05-10T00:51:40.280281Z",
     "shell.execute_reply": "2023-05-10T00:51:40.279997Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [222], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d4fc9c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.281795Z",
     "iopub.status.busy": "2023-05-10T00:51:40.281711Z",
     "iopub.status.idle": "2023-05-10T00:51:40.287978Z",
     "shell.execute_reply": "2023-05-10T00:51:40.287751Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [223], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c70b4d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.289305Z",
     "iopub.status.busy": "2023-05-10T00:51:40.289230Z",
     "iopub.status.idle": "2023-05-10T00:51:40.295245Z",
     "shell.execute_reply": "2023-05-10T00:51:40.295003Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [224], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51bb09",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314c838",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f3ccc",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "20728c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.296562Z",
     "iopub.status.busy": "2023-05-10T00:51:40.296484Z",
     "iopub.status.idle": "2023-05-10T00:51:40.303019Z",
     "shell.execute_reply": "2023-05-10T00:51:40.302791Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [225], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40bb79",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "36b7840b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.304355Z",
     "iopub.status.busy": "2023-05-10T00:51:40.304279Z",
     "iopub.status.idle": "2023-05-10T00:51:40.310513Z",
     "shell.execute_reply": "2023-05-10T00:51:40.310294Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [226], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04502c84",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c21778",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "acfa74a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.311830Z",
     "iopub.status.busy": "2023-05-10T00:51:40.311744Z",
     "iopub.status.idle": "2023-05-10T00:51:40.314431Z",
     "shell.execute_reply": "2023-05-10T00:51:40.314197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2b3e121d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.315857Z",
     "iopub.status.busy": "2023-05-10T00:51:40.315776Z",
     "iopub.status.idle": "2023-05-10T00:51:40.317363Z",
     "shell.execute_reply": "2023-05-10T00:51:40.317142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "aa2c9b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.319124Z",
     "iopub.status.busy": "2023-05-10T00:51:40.319036Z",
     "iopub.status.idle": "2023-05-10T00:51:40.320665Z",
     "shell.execute_reply": "2023-05-10T00:51:40.320438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2a2ee4df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.321990Z",
     "iopub.status.busy": "2023-05-10T00:51:40.321911Z",
     "iopub.status.idle": "2023-05-10T00:51:40.348378Z",
     "shell.execute_reply": "2023-05-10T00:51:40.348071Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [230], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3f168fb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.349905Z",
     "iopub.status.busy": "2023-05-10T00:51:40.349824Z",
     "iopub.status.idle": "2023-05-10T00:51:40.356116Z",
     "shell.execute_reply": "2023-05-10T00:51:40.355881Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [231], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "86781892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.357429Z",
     "iopub.status.busy": "2023-05-10T00:51:40.357347Z",
     "iopub.status.idle": "2023-05-10T00:51:40.363465Z",
     "shell.execute_reply": "2023-05-10T00:51:40.363217Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [232], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f14eff",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb9818",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb6faf",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d31cf390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.364909Z",
     "iopub.status.busy": "2023-05-10T00:51:40.364832Z",
     "iopub.status.idle": "2023-05-10T00:51:40.371906Z",
     "shell.execute_reply": "2023-05-10T00:51:40.371651Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [233], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m rslt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "rslt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a993d",
   "metadata": {},
   "source": [
    "This script reads a text file, clean it in part and generates a word cloud using the words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d03d6bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.373290Z",
     "iopub.status.busy": "2023-05-10T00:51:40.373215Z",
     "iopub.status.idle": "2023-05-10T00:51:40.379737Z",
     "shell.execute_reply": "2023-05-10T00:51:40.379506Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [234], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# nltk.download()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3964f76",
   "metadata": {},
   "source": [
    "Read the whole text\n",
    "file_read = open('ai_trends.txt' 'rU')\n",
    "text_raw = file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364629e",
   "metadata": {},
   "source": [
    "Replace end of line character with space\n",
    "text_raw.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a69e368e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.381149Z",
     "iopub.status.busy": "2023-05-10T00:51:40.381068Z",
     "iopub.status.idle": "2023-05-10T00:51:40.383848Z",
     "shell.execute_reply": "2023-05-10T00:51:40.383641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save a lower-case version of each word to a list\n",
    "words_list = ['Inspire', 'artists', 'In', 'years', 'sort', 'algorithm', 'involved', 'decisions', 'big', 'small', 'Artists', 'like', 'involved', 'artificial', 'intelligence', 'While', 'mission', 'sound', 'daunting', 'really', 'starting', 'use', 'exploring', 'capabilities', 'algorithms', 'express', 'AI', 'systems', 'I', 'encourage', 'artists', 'try', 'use', 'AI', 'create', 'beautiful', 'expressive', 'way', 'medium', 'However', 'I', 'want', 'technology', 'artist', 'We', 'think', 'AI', 'tool', 'augmentation', 'human', 'thought', 'creation', 'make', 'effort', 'turn', 'reigns', 'creativity', 'ethics', 'machines', 'When', 'deploying', 'AI', 'individuals', 'businesses', 'governments', 'consider', 'maintain', 'sense', 'civility', 'creativity', 'equity', 'AI', 'released', 'world', 'For', 'industries', 'driving', 'question', 'AI', 'used', 'increase', 'productivity', 'respecting', 'human', 'diversity', 'dignity', 'cultural', 'specificities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "79323b75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.385237Z",
     "iopub.status.busy": "2023-05-10T00:51:40.385160Z",
     "iopub.status.idle": "2023-05-10T00:51:40.386851Z",
     "shell.execute_reply": "2023-05-10T00:51:40.386609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminate non alpha elements\n",
    "text_list = [word.lower() for word in words_list if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7eadfb1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.388140Z",
     "iopub.status.busy": "2023-05-10T00:51:40.388063Z",
     "iopub.status.idle": "2023-05-10T00:51:40.389679Z",
     "shell.execute_reply": "2023-05-10T00:51:40.389463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming the list into a string for displaying\n",
    "text_str = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e6613f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.390983Z",
     "iopub.status.busy": "2023-05-10T00:51:40.390907Z",
     "iopub.status.idle": "2023-05-10T00:51:40.417200Z",
     "shell.execute_reply": "2023-05-10T00:51:40.416906Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [238], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Crating and updating the stopword list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stpwords = set(STOPWORDS)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stpwords.add('will')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# stpwords.add('said')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m top_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 6\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m word_dist \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist(words)\n\u001b[1;32m      8\u001b[0m rslt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_dist\u001b[38;5;241m.\u001b[39mmost_common(top_N),\n\u001b[1;32m      9\u001b[0m                     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/weihaochen/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Crating and updating the stopword list\n",
    "# stpwords = set(STOPWORDS)\n",
    "# stpwords.add('will')\n",
    "# stpwords.add('said')\n",
    "top_N=7\n",
    "words = nltk.tokenize.word_tokenize(text_str)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b5258931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.418675Z",
     "iopub.status.busy": "2023-05-10T00:51:40.418591Z",
     "iopub.status.idle": "2023-05-10T00:51:40.424850Z",
     "shell.execute_reply": "2023-05-10T00:51:40.424619Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [239], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining the wordcloud parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wc \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining the wordcloud parameters\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "aa6d1554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.426202Z",
     "iopub.status.busy": "2023-05-10T00:51:40.426127Z",
     "iopub.status.idle": "2023-05-10T00:51:40.432275Z",
     "shell.execute_reply": "2023-05-10T00:51:40.432034Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [240], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(text_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wc' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word cloud\n",
    "wc.generate(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a92b7a",
   "metadata": {},
   "source": [
    "Store to file\n",
    "wc.to_file('Trump.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c52af",
   "metadata": {},
   "source": [
    "Show the cloud\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a262982",
   "metadata": {},
   "source": [
    "df = pd.DataFrame.from(words_list,columns=['words'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2980a86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-10T00:51:40.433647Z",
     "iopub.status.busy": "2023-05-10T00:51:40.433570Z",
     "iopub.status.idle": "2023-05-10T00:51:40.440029Z",
     "shell.execute_reply": "2023-05-10T00:51:40.439793Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rslt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [241], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrslt\u001b[49m\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrump.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rslt' is not defined"
     ]
    }
   ],
   "source": [
    "rslt.plot.bar(x=\"Word\",y=\"Frequency\")\n",
    "plt.savefig('Trump.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
