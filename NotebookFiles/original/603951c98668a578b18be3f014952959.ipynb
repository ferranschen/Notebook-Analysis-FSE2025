{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdfcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"%logstop\\n%logstart -t -r -q ipython_command_log.py global\\n\\n#- IRONHACKS RESEARCH TRACKING CODE\\n#----------------------------------\\n# The following code is used to help our research team understand how you \\n# our notebook environment. We do not collect any personal information with\\n# the following code, it is used to measure when and how often you work on\\n# your submission files.\\n\\nimport os\\nfrom datetime import datetime\\nimport IPython.core.history as history\\n\\nha = history.HistoryAccessor()\\nha_tail = ha.get_tail(1)\\nha_cmd = next(ha_tail)\\nsession_id = str(ha_cmd[0])\\ncommand_id = str(ha_cmd[1])\\ntimestamp = datetime.utcnow().isoformat()\\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\\\n'\\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\\nlogfile.write(history_line)\\nlogfile.close()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"\\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\\n#------------------------------------------\\n# This is normally not required. The hub environment comes preinstaled with \\n# many packages that you can already use without setup. In case there is some\\n# other library you would like to use that isn't on the list you run this command\\n# once to install them.  If it is already installed this command has no effect.\\n\\n!pip install db-dtypes pmdarima 'google-cloud-bigquery[pandas]' tqdm\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3612a4",
   "metadata": {},
   "source": [
    "- IMPORT THE LIBRARIES YOU WILL USE\n",
    "------------------------------------------\n",
    "You only need to import packages one time per notebook session. To keep your\n",
    "notebook clean and organized you can handle all imports at the top of your file.\n",
    "The following are included for example purposed, feel free to modify or delete \n",
    "anything in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGQUERY_PROJECT = 'ironhacks-data'\n",
    "bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tables = bigquery_client.query(f\"\"\"\n",
    "    SELECT table_catalog, table_schema, table_name\n",
    "    FROM `ironhacks_competition.INFORMATION_SCHEMA.TABLES`\n",
    "\"\"\").to_dataframe()\n",
    "print(data_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8134e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data tables in ironhacks-data.ironhacks_competition\n",
    "data_dict = {\n",
    "    table_name: bigquery_client.query(f\"\"\"\n",
    "        SELECT * FROM `ironhacks-data.ironhacks_competition.{table_name}`\n",
    "    \"\"\").to_dataframe()\n",
    "    for table_name in data_tables['table_name'].tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = list(set(data_dict['unemployment_data'].columns) & set(data_dict['wage_data'].columns))\n",
    "print(f'Common columns: {common_cols}')\n",
    "data = data_dict['unemployment_data'].merge(\n",
    "    data_dict['wage_data'],\n",
    "    on=common_cols,\n",
    "    how='left',\n",
    ").sort_values(['countyfips', 'week_number']).drop_duplicates().reset_index(drop=True)\n",
    "data['timeperiod'] = pd.to_datetime(data['timeperiod'], format='%Y%m%d')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our own convenience, create a correspondance DataFrame for `week_number` and `timeperiod`.\n",
    "wt = pd.Series(\n",
    "    range(1, 53),\n",
    "    name='week_number',\n",
    "    index=pd.date_range('2022-01-01', periods=52, freq='W-SAT').rename('timeperiod'),\n",
    ").reset_index()\n",
    "wt['timeperiod'] = wt['timeperiod'].astype(str)\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following shows that there are a lot of missing data in exogenous features...\n",
    "for cname, cvalues in data.items():\n",
    "    print('Column {} has {} ({}%) missing value(s)'.format(\n",
    "        cname,\n",
    "        cvalues.isna().sum(),\n",
    "        round(100.0 * cvalues.isna().sum() / len(cvalues), 2),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weeks = 37\n",
    "train_timeperiods = pd.to_datetime(wt.set_index('week_number')['timeperiod'].loc[:train_weeks])\n",
    "target_week = 39\n",
    "pred_results = []\n",
    "for i in tqdm(data['uu_id'].unique()):\n",
    "    y = data[\n",
    "        data['uu_id'] == i\n",
    "    ].set_index('timeperiod')['total_claims'].asfreq('W-SAT').reindex(train_timeperiods).fillna(0)\n",
    "    n_periods = target_week - train_weeks\n",
    "    pred = AutoARIMA(\n",
    "        seasonal=False,\n",
    "    ).fit_predict(y, n_periods=n_periods).iloc[-1]\n",
    "    pred_results.append([i, max(0.0, pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1f0a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Getting the output CSV for submission ready\n",
    "data_dict['prediction_list'].merge(\n",
    "    pd.DataFrame(pred_results, columns=['uu_id', 'total_claims']),\n",
    "    on=['uu_id'],\n",
    "    how='left',\n",
    ").to_csv('submission_prediction_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
