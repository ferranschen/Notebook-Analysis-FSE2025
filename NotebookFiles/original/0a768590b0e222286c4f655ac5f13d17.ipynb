{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708942eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"\\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\\n#------------------------------------------\\n# This is normally not required. The hub environment comes preinstaled with \\n# many packages that you can already use without setup. In case there is some\\n# other library you would like to use that isn't on the list you run this command\\n# once to install them.  If it is already installed this command has no effect.\\n\\n!python3 -m pip install pandas\\n!pip install db-dtypes\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc69c7",
   "metadata": {},
   "source": [
    "- IMPORT THE LIBRARIES YOU WILL USE\n",
    "------------------------------------------\n",
    "You only need to import packages one time per notebook session. To keep your\n",
    "notebook clean and organized you can handle all imports at the top of your file.\n",
    "The following are included for example purposed, feel free to modify or delete \n",
    "anything in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80ddfa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import magics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime,itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98602f",
   "metadata": {},
   "source": [
    "- DEFINE YOUR CLASSES AND FUNCTIONS \n",
    "-----------------------------------\n",
    "This is not required, but is helpful in keeping your notebook organized. \n",
    "You can use the following cell or several cells to define your functions\n",
    "and classes to keep them separate from your analysis or results code.\n",
    "In general it useful to define your methods in a separate cell from where\n",
    "it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2ae97",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def example_function():\n",
    "    print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529aee8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_column_info(df):\n",
    "    print(f'No. of columns: {len(df.columns)}')\n",
    "    for col in df.columns:\n",
    "        print(len(df[col].unique()),col,df[col].dtypes)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1331c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_na_info(df):\n",
    "    for col in df.columns:\n",
    "        print(df[col].isnull().sum(),col,df[col].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3a974",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_datetime(week_no):\n",
    "    date = datetime.datetime.strptime(\"2022-\"+str(week_no)+\"-1\",\"%Y-%W-%w\")\n",
    "    #print(date)\n",
    "    return pd.to_datetime(date,format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722f580",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sarimax_gridsearch(ts, pdq, pdqs, maxiter=100, freq='D',disp=False):\n",
    "    # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value\n",
    "    ans = []\n",
    "    for comb in pdq:\n",
    "        for combs in pdqs:\n",
    "            #try:\n",
    "            mod = SARIMAX(ts,order=comb,\n",
    "                          seasonal_order=combs,\n",
    "                          enforce_stationarity=False,\n",
    "                          enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit(maxiter=maxiter,disp=False) \n",
    "            ans.append([comb, combs, output.bic])\n",
    "            #print('SARIMAX {} x {}12 : BIC Calculated ={}'.format(comb, combs, output.bic))\n",
    "            #except:\n",
    "            #    continue\n",
    "    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic'])\n",
    "    ans_df = ans_df.sort_values(by=['bic'],ascending=True)[0:5]\n",
    "    \n",
    "    return ans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658502b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_stationary(data,Print=0):\n",
    "    adft = adfuller(data,autolag=\"BIC\")\n",
    "    output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                            \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "\n",
    "    critical_value = adft[4]['5%']\n",
    "    if Print==1:\n",
    "        print(output_df)\n",
    "    if adft[1] < 0.05 and adft[0] < critical_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e708dfd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def diff_inv(series, last_observation):\n",
    "\n",
    "    series_undifferenced = series.copy()\n",
    "\n",
    "    series_undifferenced.iat[0] = series_undifferenced.iat[0] + last_observation\n",
    "\n",
    "    series_undifferenced = series_undifferenced.cumsum()\n",
    "\n",
    "    return series_undifferenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801c1b6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def loss(pred,actual):\n",
    "    pred = np.round(pred)\n",
    "    errors = abs(actual-pred)\n",
    "    print(f'Mean Absolute Error: {round(np.mean(errors), 2)}')\n",
    "    print(f'Mean squared Error: {round(np.mean(errors**2), 2)}')\n",
    "    mape = 100 * (errors/actual)\n",
    "    # Calcualte and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print(f'Accuracy: {round(accuracy, 2)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ca0c1",
   "metadata": {},
   "source": [
    "BIGQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c06361",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGQUERY_PROJECT = 'ironhacks-data'\n",
    "bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ea609",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#query = \"\"\"\n",
    "#SELECT \n",
    "#week_number,\n",
    "#cases \n",
    "#FROM `ironhacks-data.ironhacks_training.covid19_cases`\n",
    "#Where week_number between 1 and 3\n",
    "#order by week_number\n",
    "#\"\"\"\n",
    "print(\"Datasets available:\")\n",
    "for dataset in list(bigquery_client.list_datasets()):\n",
    "    print(dataset.dataset_id)\n",
    "    if dataset.dataset_id == \"ironhacks_competition\":\n",
    "        mydataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTables available:\")\n",
    "for table in bigquery_client.list_tables(\"ironhacks_competition\"):\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8aa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.prediction_list`\n",
    "\"\"\"\n",
    "query2 = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.unemployment_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.wage_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job1 = bigquery_client.query(query1)\n",
    "query_job2 = bigquery_client.query(query2)\n",
    "query_job3 = bigquery_client.query(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35709c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = query_job1.to_dataframe()\n",
    "unemployment_data = query_job2.to_dataframe()\n",
    "wage_data = query_job3.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dumping the df to csv\n",
    "week = \"week2\"\n",
    "prediction_data.to_csv(\"prediction_data_\"+week+\".csv\",index=False)\n",
    "unemployment_data.to_csv(\"unemployment_data_\"+week+\".csv\",index=False)\n",
    "wage_data.to_csv(\"wage_data_\"+week+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(unemployment_data,wage_data[[\"uu_id\",\"average_wage\"]],on=\"uu_id\")\n",
    "merged_data = merged_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44af20",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "query4 = \"\"\"\n",
    "SELECT table_id,\n",
    "DATE(TIMESTAMP_MILLIS(creation_time)) AS creation_date,\n",
    "DATE(TIMESTAMP_MILLIS(last_modified_time)) AS last_modified_date,\n",
    "row_count,\n",
    "size_bytes,\n",
    "CASE\n",
    "    WHEN type = 1 THEN 'table'\n",
    "    WHEN type = 2 THEN 'view'\n",
    "    WHEN type = 3 THEN 'external'\n",
    "    ELSE '?'\n",
    "END AS type,\n",
    "TIMESTAMP_MILLIS(creation_time) AS creation_time,\n",
    "TIMESTAMP_MILLIS(last_modified_time) AS last_modified_time,\n",
    "dataset_id,\n",
    "project_id\n",
    "FROM `ironhacks-data.ironhacks_competition.__TABLES__`\"\"\"\n",
    "query_job4 = bigquery_client.query(query4)\n",
    "timestamp_data = query_job4.to_dataframe()\n",
    "for cnt,row in timestamp_data.iterrows():\n",
    "    print(\"\\n\")\n",
    "    print(row[\"table_id\"])\n",
    "    print(row[\"creation_time\"])\n",
    "    print(row[\"last_modified_time\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
