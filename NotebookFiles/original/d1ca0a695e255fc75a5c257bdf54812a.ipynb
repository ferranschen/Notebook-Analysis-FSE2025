{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de389d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\\n!pip install keras\\n!pip install tensorflow\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"%logstop\\n%logstart -t -r -q ipython_command_log.py global\\n\\n#- IRONHACKS RESEARCH TRACKING CODE\\n#----------------------------------\\n# The following code is used to help our research team understand how you \\n# our notebook environment. We do not collect any personal information with\\n# the following code, it is used to measure when and how often you work on\\n# your submission files.\\n\\nimport os\\nfrom datetime import datetime\\nimport IPython.core.history as history\\n\\nha = history.HistoryAccessor()\\nha_tail = ha.get_tail(1)\\nha_cmd = next(ha_tail)\\nsession_id = str(ha_cmd[0])\\ncommand_id = str(ha_cmd[1])\\ntimestamp = datetime.utcnow().isoformat()\\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\\\n'\\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\\nlogfile.write(history_line)\\nlogfile.close()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', 'import pandas as pd\\nimport numpy as np\\nimport os\\nfrom google.cloud import bigquery\\nfrom google.oauth2 import service_account\\nfrom google.cloud.bigquery import magics\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.svm import SVR\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.model_selection import RepeatedKFold\\nfrom sklearn.linear_model import BayesianRidge\\nfrom sklearn.linear_model import ElasticNetCV\\nfrom keras.models import Sequential\\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGQUERY_PROJECT = 'ironhacks-data'\n",
    "bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddfbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_main = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.unemployment_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97210c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bigquery_client.query(query_main)\n",
    "unemployment_data = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.wage_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2602868",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bigquery_client.query(query)\n",
    "wage_data = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pred = \"\"\"\n",
    "SELECT * \n",
    "FROM `ironhacks-data.ironhacks_competition.prediction_list`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bigquery_client.query(query_pred)\n",
    "prediction_list = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unemployment_data.info())\n",
    "print(wage_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of both frames to see if they are joinable\n",
    "print('Unemployment df size:', unemployment_data.shape)\n",
    "print('Wage df size:', wage_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_data.isnull().sum() / len(unemployment_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c82ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_data.isnull().sum() / len(wage_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4221553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values with 0\n",
    "clean_unemploymentDf = unemployment_data.copy()\n",
    "clean_unemploymentDf.fillna(0, inplace=True)\n",
    "clean_unemploymentDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_unemploymentDf.isnull().sum() #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c98f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"unemp_dupl = clean_unemploymentDf[clean_unemploymentDf.duplicated()]\\nprint('Duplicate rows: ', unemp_dupl)\\n# ignore duplicates - different time periods\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "correlation = clean_unemploymentDf.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wage information\n",
    "wage_data[wage_data['average_wage'].isnull()] # there's 3 nulls here - might as well drop them and use this tract to attempt to join the datasets together; or impute with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f98785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wage_data.dropna(axis=0, inplace=True)\n",
    "wage_data['average_wage'].fillna(wage_data['average_wage'].mean(), inplace=True)\n",
    "wage_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_dupl = wage_data[wage_data.duplicated()]\n",
    "print('Duplicate rows: ', wage_dupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df on tract\n",
    "main_df = pd.merge(clean_unemploymentDf, wage_data, on=['tract', 'uu_id'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns created by merge and rename existing columns to original\n",
    "main_df.drop(['countyfips_y','tract_name_y'], axis=1, inplace=True)\n",
    "main_df.rename({'countyfips_x':'countyfips', 'tract_name_x':'tract_name'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72100e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.isnull().sum() # check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.dropna(axis=0, inplace=True) #most of the rows with NA values have nothing to add and cannot be imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation again for new, merged frame\n",
    "correlation = main_df.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95109bbe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "main_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedc851",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_regressor(prediction_dataframe):\n",
    "    # Takes in a prediction dataframe of 2 columns, Actual values and Predicted values generated by a regressor\n",
    "    # Outputs MSE, MAR, RMSE and MAPE metrics. Must have columns named Actual and Predicted.\n",
    "    print('MSE:', mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))\n",
    "    print('MAE:', mean_absolute_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted']))\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(prediction_dataframe['Actual'], prediction_dataframe['Predicted'])))\n",
    "    print('MAPE:', np.mean(np.abs((prediction_dataframe['Actual'] - prediction_dataframe['Predicted']) / prediction_dataframe['Actual'])) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4812f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_predictions(regressor, model_type, name):\n",
    "    # generates predictions for any model and writes out a dataframe in csv containing them\n",
    "    # takes a regressor and learning method type as input: DL and ML\n",
    "    # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape\n",
    "    result_list = []\n",
    "    uu_id_transform = LE.fit_transform(prediction_list['uu_id'])\n",
    "    if model_type == 'DL':\n",
    "        predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))\n",
    "        for val in uu_id_transform:\n",
    "            to_predict = np.insert(predict_arr, 0, val, axis=1)\n",
    "            to_predict = np.insert(to_predict, 1, 43, axis=1)\n",
    "            to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))\n",
    "            r = regressor.predict(to_predict)\n",
    "            result_list.append(r)\n",
    "        result_list = np.array(result_list)\n",
    "        result_list = np.reshape(result_list, (619,))\n",
    "    elif model_type == 'ML':\n",
    "        predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))\n",
    "        for val in uu_id_transform:\n",
    "            to_predict = np.insert(predict_arr, 0, val, axis=1)\n",
    "            to_predict = np.insert(to_predict, 1, 43, axis=1)\n",
    "            r = regressor.predict(to_predict)\n",
    "            result_list.append(r)\n",
    "    result_df = pd.DataFrame(result_list, columns = ['Predictions'])\n",
    "    prediction_sub = prediction_list.copy()\n",
    "    prediction_sub['total_claims'] = result_df.values\n",
    "    prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]\n",
    "    os.makedirs('lost+found/submission_files', exist_ok=True)\n",
    "    prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)\n",
    "    return prediction_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607b5cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_pred_frame(test_frame, prediction_array):\n",
    "    prediction_frame = pd.DataFrame({'Actual': test_frame, 'Predicted': prediction_array.flatten()})\n",
    "    return prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_ingest = pd.concat([merged_ingest, combined_ingest])\n",
    "ingest = pd.read_csv('lost+found/submission_files/updated_ingest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ff5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = main_df.copy()\n",
    "complete_ingest = pd.concat([ingest, new_data])\n",
    "complete_ingest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_ingest.shape)\n",
    "print(ingest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed324deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ingest.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ingest.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ingest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if new data is duplicate\n",
    "complete_ingest.equals(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write out combined data for use later\n",
    "os.makedirs('lost+found/submission_files', exist_ok=True)\n",
    "complete_ingest.to_csv('lost+found/submission_files/complete_ingest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1367a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list.to_csv('prediction_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342fa1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', '!pip install db-dtypes\\n!pip install keras\\n!pip install tensorflow\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54281f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"%logstop\\n%logstart -t -r -q ipython_command_log.py global\\n\\n#- IRONHACKS RESEARCH TRACKING CODE\\n#----------------------------------\\n# The following code is used to help our research team understand how you \\n# our notebook environment. We do not collect any personal information with\\n# the following code, it is used to measure when and how often you work on\\n# your submission files.\\n\\nimport os\\nfrom datetime import datetime\\nimport IPython.core.history as history\\n\\nha = history.HistoryAccessor()\\nha_tail = ha.get_tail(1)\\nha_cmd = next(ha_tail)\\nsession_id = str(ha_cmd[0])\\ncommand_id = str(ha_cmd[1])\\ntimestamp = datetime.utcnow().isoformat()\\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\\\n'\\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\\nlogfile.write(history_line)\\nlogfile.close()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeb9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', 'import pandas as pd\\nimport numpy as np\\nimport os\\nfrom google.cloud import bigquery\\nfrom google.oauth2 import service_account\\nfrom google.cloud.bigquery import magics\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.svm import SVR\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.model_selection import RepeatedKFold\\nfrom sklearn.linear_model import BayesianRidge\\nfrom sklearn.linear_model import ElasticNetCV\\nfrom keras.models import Sequential\\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGQUERY_PROJECT = 'ironhacks-data'\n",
    "bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_main = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.unemployment_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8728fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bigquery_client.query(query_main)\n",
    "unemployment_data = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcdbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.wage_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bigquery_client.query(query)\n",
    "wage_data = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb70d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pred = \"\"\"\n",
    "SELECT * \n",
    "FROM `ironhacks-data.ironhacks_competition.prediction_list`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bigquery_client.query(query_pred)\n",
    "prediction_list = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values with 0\n",
    "clean_unemploymentDf = unemployment_data.copy()\n",
    "clean_unemploymentDf.fillna(0, inplace=True)\n",
    "clean_unemploymentDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_unemploymentDf.isnull().sum() #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "correlation = clean_unemploymentDf.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0179e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wage information\n",
    "wage_data[wage_data['average_wage'].isnull()] # there's 3 nulls here - might as well drop them and use this tract to attempt to join the datasets together; or impute with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wage_data.dropna(axis=0, inplace=True)\n",
    "wage_data['average_wage'].fillna(wage_data['average_wage'].mean(), inplace=True)\n",
    "wage_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_dupl = wage_data[wage_data.duplicated()]\n",
    "print('Duplicate rows: ', wage_dupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join df on tract\n",
    "main_df = pd.merge(clean_unemploymentDf, wage_data, on=['tract', 'uu_id'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7305cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns created by merge and rename existing columns to original\n",
    "main_df.drop(['countyfips_y','tract_name_y'], axis=1, inplace=True)\n",
    "main_df.rename({'countyfips_x':'countyfips', 'tract_name_x':'tract_name'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.isnull().sum() # check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.dropna(axis=0, inplace=True) #most of the rows with NA values have nothing to add and cannot be imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation again for new, merged frame\n",
    "correlation = main_df.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1efabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', 'import pandas as pd\\nimport numpy as np\\nimport os\\nfrom google.cloud import bigquery\\nfrom google.oauth2 import service_account\\nfrom google.cloud.bigquery import magics\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.svm import SVR\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.model_selection import RepeatedKFold\\nfrom sklearn.linear_model import BayesianRidge\\nfrom sklearn.linear_model import ElasticNetCV\\nfrom keras.models import Sequential\\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\\nfrom keras.models import load_model\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "StackLSTM_Regressor = load_model('BiDLSTM_v1-04.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary for viewers\n",
    "StackLSTM_Regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec920ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick preprocess to keep uu_id and scale values\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "LE = LabelEncoder()\n",
    "RB_other = RobustScaler()\n",
    "SC_other = StandardScaler()\n",
    "# RB_claims = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35556085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs different feature engineering, so I'm starting from scratch\n",
    "DL_data = updated_ingest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768068b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs different feature engineering, so I'm starting from scratch\n",
    "DL_data = complete_ingest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c399c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these = ['timeperiod','tract','top_category_employer1','top_category_employer2','top_category_employer3','tract_name','countyfips', 'edu_unknown', 'gender_na', 'race_noanswer']\n",
    "DL_data['uu_id'] = LE.fit_transform(DL_data['uu_id'])\n",
    "scale_these = ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male', \n",
    "            'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "DL_data.drop(drop_these, axis = 1, inplace=True)\n",
    "DL_data[scale_these] = SC_other.fit_transform(DL_data[scale_these])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d16ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7420885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set\n",
    "DL_Y = DL_data['total_claims']\n",
    "DL_X = DL_data[['uu_id', 'week_number', 'edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male',\n",
    "               'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']]\n",
    "DL_XTrain, DL_XTest, DL_YTrain, DL_YTest = train_test_split(DL_X, DL_Y, test_size=0.20, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to np vectors\n",
    "DL_XTrain = DL_XTrain.to_numpy()\n",
    "DL_XTest = DL_XTest.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ea214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape because F*** tensors\n",
    "DL_XTrain = np.reshape(DL_XTrain, (DL_XTrain.shape[0], DL_XTrain.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X and Y train to float because input dtype accepts floats only\n",
    "DL_YTrain = DL_YTrain.astype(float)\n",
    "DL_XTrain = DL_XTrain.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb71335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float cast\n",
    "DL_XTest = DL_XTest.astype(float)\n",
    "# make predictions\n",
    "DL_XTest = np.reshape(DL_XTest, (DL_XTest.shape[0], DL_XTest.shape[1],1))\n",
    "predictions = StackLSTM_Regressor.predict(DL_XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a02e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred_frame(DL_YTest, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_regressor(get_pred_frame(DL_YTest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb151e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(DL_YTest.values, color = 'red', label = 'Actual values')\n",
    "plt.plot(predictions, color='blue', label='Predicted values')\n",
    "plt.title('Model Prediction Visual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ef307",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "get_predictions(StackLSTM_Regressor, 'DL', 'submission_prediction_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7fe69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_predictions(regressor, model_type, name):\n",
    "    # generates predictions for any model and writes out a dataframe in csv containing them\n",
    "    # takes a regressor and learning method type as input: DL and ML\n",
    "    # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape\n",
    "    result_list = []\n",
    "    uu_id_transform = LE.fit_transform(prediction_list['uu_id'])\n",
    "    if model_type == 'DL':\n",
    "        predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))\n",
    "        for val in uu_id_transform:\n",
    "            to_predict = np.insert(predict_arr, 0, val, axis=1)\n",
    "            to_predict = np.insert(to_predict, 1, 43, axis=1)\n",
    "            to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))\n",
    "            r = regressor.predict(to_predict)\n",
    "            result_list.append(r)\n",
    "        result_list = np.array(result_list)\n",
    "        result_list = np.reshape(result_list, (525,))\n",
    "    elif model_type == 'ML':\n",
    "        predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))\n",
    "        for val in uu_id_transform:\n",
    "            to_predict = np.insert(predict_arr, 0, val, axis=1)\n",
    "            to_predict = np.insert(to_predict, 1, 43, axis=1)\n",
    "            r = regressor.predict(to_predict)\n",
    "            result_list.append(r)\n",
    "    result_df = pd.DataFrame(result_list, columns = ['Predictions'])\n",
    "    prediction_sub = prediction_list.copy()\n",
    "    prediction_sub['total_claims'] = result_df.values\n",
    "    prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]\n",
    "    os.makedirs('lost+found/submission_files', exist_ok=True)\n",
    "    prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)\n",
    "    return prediction_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6eef3f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "get_predictions(StackLSTM_Regressor, 'DL', 'submission_prediction_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8734c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_predictions(regressor, model_type, name, week):\n",
    "    # generates predictions for any model and writes out a dataframe in csv containing them\n",
    "    # takes a regressor and learning method type as input: DL and ML\n",
    "    # DL/ML variable basically changes the shape for an input from a 2D array to 3D arry, as required tensor shape\n",
    "    result_list = []\n",
    "    uu_id_transform = LE.fit_transform(prediction_list['uu_id'])\n",
    "    if model_type == 'DL':\n",
    "        predict_arr = np.array(SC_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))\n",
    "        for val in uu_id_transform:\n",
    "            to_predict = np.insert(predict_arr, 0, val, axis=1)\n",
    "            to_predict = np.insert(to_predict, 1, week, axis=1)\n",
    "            to_predict = np.reshape(to_predict, (to_predict.shape[0], to_predict.shape[1],1))\n",
    "            r = regressor.predict(to_predict)\n",
    "            result_list.append(r)\n",
    "        result_list = np.array(result_list)\n",
    "        result_list = np.reshape(result_list, (525,))\n",
    "    elif model_type == 'ML':\n",
    "        predict_arr = np.array(RB_other.transform([[-0.04, -0.140, 0.328, -0.671, -0.420, -0.432, -0.0013, -0.0023, -0.347, -0.0004, 3.211, -0.532, -0.329]]))\n",
    "        for val in uu_id_transform:\n",
    "            to_predict = np.insert(predict_arr, 0, val, axis=1)\n",
    "            to_predict = np.insert(to_predict, 1, week, axis=1)\n",
    "            r = regressor.predict(to_predict)\n",
    "            result_list.append(r)\n",
    "    result_df = pd.DataFrame(result_list, columns = ['Predictions'])\n",
    "    prediction_sub = prediction_list.copy()\n",
    "    prediction_sub['total_claims'] = result_df.values\n",
    "    prediction_sub = prediction_sub[['uu_id','total_claims','week_number']]\n",
    "    os.makedirs('lost+found/submission_files', exist_ok=True)\n",
    "    prediction_sub.to_csv('lost+found/submission_files/'+name+'.csv', index=False)\n",
    "    return prediction_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(StackLSTM_Regressor, 'DL', 'submission_prediction_output', 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eab289",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions(StackLSTM_Regressor, 'DL', 'submission_prediction_output', 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3dbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', 'import pandas as pd\\nimport numpy as np\\nimport os\\nfrom google.cloud import bigquery\\nfrom google.oauth2 import service_account\\nfrom google.cloud.bigquery import magics\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.svm import SVR\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.linear_model import LassoCV\\nfrom sklearn.model_selection import RepeatedKFold\\nfrom sklearn.linear_model import BayesianRidge\\nfrom sklearn.linear_model import ElasticNetCV\\nfrom keras.models import Sequential\\nfrom keras.layers import Bidirectional, LSTM, Dropout, Dense\\nfrom keras.models import load_model\\nimport joblib\\nfrom joblib import Parallel, delayed\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_ingest = pd.concat([merged_ingest, combined_ingest])\n",
    "ingest = pd.read_csv('lost+found/submission_files/completed_ingest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_ingest = pd.concat([merged_ingest, combined_ingest])\n",
    "ingest = pd.read_csv('lost+found/submission_files/complete_ingest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bf1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick preprocess to keep uu_id and scale values\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "LE = LabelEncoder()\n",
    "RB_other = RobustScaler()\n",
    "SC_other = StandardScaler()\n",
    "# RB_claims = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target and preprocess again\n",
    "to_drop = ['timeperiod','tract','total_claims','top_category_employer1','top_category_employer2','top_category_employer3','tract_name','countyfips', 'edu_unknown', 'gender_na', 'race_noanswer']\n",
    "Y = ingest['total_claims']\n",
    "X = ingest.drop(to_drop, axis=1)\n",
    "X['uu_id'] = LE.fit_transform(X['uu_id'])\n",
    "to_scale = ['edu_8th_or_less', 'edu_grades_9_11', 'edu_hs_grad_equiv', 'edu_post_hs', 'gender_female', 'gender_male', \n",
    "            'race_amerindian', 'race_asian', 'race_black', 'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "X[to_scale] = RB_other.fit_transform(X[to_scale])\n",
    "#X[['total_claims']] = RB_claims.fit_transform(main_df[['total_claims']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "RFR_Regressor = joblib.load('RF_v1-5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_RFR = RFR_Regressor.predict(X_test.values).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dba4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_regressor(get_pred_frame(Y_test,Y_pred_RFR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0079cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call func\n",
    "get_predictions(RFR_Regressor, 'ML', 'submission_prediction_output_RFR', 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(Y_Test.values, color = 'red', label = 'Actual values')\n",
    "plt.plot(Y_pred_RFR, color='blue', label='Predicted values')\n",
    "plt.title('Model Prediction Visual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01374e7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(Y_test.values, color = 'red', label = 'Actual values')\n",
    "plt.plot(Y_pred_RFR, color='blue', label='Predicted values')\n",
    "plt.title('Model Prediction Visual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
