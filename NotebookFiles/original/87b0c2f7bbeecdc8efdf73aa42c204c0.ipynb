{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26958039",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"%logstop\\n%logstart -t -r -q ipython_command_log.py global\\n\\n#- IRONHACKS RESEARCH TRACKING CODE\\n#----------------------------------\\n# The following code is used to help our research team understand how you \\n# our notebook environment. We do not collect any personal information with\\n# the following code, it is used to measure when and how often you work on\\n# your submission files.\\n\\nimport os\\nfrom datetime import datetime\\nimport IPython.core.history as history\\n\\nha = history.HistoryAccessor()\\nha_tail = ha.get_tail(1)\\nha_cmd = next(ha_tail)\\nsession_id = str(ha_cmd[0])\\ncommand_id = str(ha_cmd[1])\\ntimestamp = datetime.utcnow().isoformat()\\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\\\n'\\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\\nlogfile.write(history_line)\\nlogfile.close()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"\\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\\n#------------------------------------------\\n# This is normally not required. The hub environment comes preinstaled with \\n# many packages that you can already use without setup. In case there is some\\n# other library you would like to use that isn't on the list you run this command\\n# once to install them.  If it is already installed this command has no effect.\\n\\n!python3 -m pip install pandas\\n!pip install db-dtypes\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d66d3",
   "metadata": {},
   "source": [
    "- IMPORT THE LIBRARIES YOU WILL USE\n",
    "------------------------------------------\n",
    "You only need to import packages one time per notebook session. To keep your\n",
    "notebook clean and organized you can handle all imports at the top of your file.\n",
    "The following are included for example purposed, feel free to modify or delete \n",
    "anything in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import magics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime,itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"%logstop\\n%logstart -t -r -q ipython_command_log.py global\\n\\n#- IRONHACKS RESEARCH TRACKING CODE\\n#----------------------------------\\n# The following code is used to help our research team understand how you \\n# our notebook environment. We do not collect any personal information with\\n# the following code, it is used to measure when and how often you work on\\n# your submission files.\\n\\nimport os\\nfrom datetime import datetime\\nimport IPython.core.history as history\\n\\nha = history.HistoryAccessor()\\nha_tail = ha.get_tail(1)\\nha_cmd = next(ha_tail)\\nsession_id = str(ha_cmd[0])\\ncommand_id = str(ha_cmd[1])\\ntimestamp = datetime.utcnow().isoformat()\\nhistory_line = ','.join([session_id, command_id, timestamp]) + '\\\\n'\\nlogfile = open(os.environ['HOME']+'/ipython_session_log.csv', 'a')\\nlogfile.write(history_line)\\nlogfile.close()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('capture', '', \"\\n#- INSTALL ADDITIONAL LIBRARIES IF REQUIRED\\n#------------------------------------------\\n# This is normally not required. The hub environment comes preinstaled with \\n# many packages that you can already use without setup. In case there is some\\n# other library you would like to use that isn't on the list you run this command\\n# once to install them.  If it is already installed this command has no effect.\\n\\n!python3 -m pip install pandas\\n!pip install db-dtypes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fb84d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "get_ipython().system('pip install pmdarima --quiet')\n",
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f57c63",
   "metadata": {},
   "source": [
    "- DEFINE YOUR CLASSES AND FUNCTIONS \n",
    "-----------------------------------\n",
    "This is not required, but is helpful in keeping your notebook organized. \n",
    "You can use the following cell or several cells to define your functions\n",
    "and classes to keep them separate from your analysis or results code.\n",
    "In general it useful to define your methods in a separate cell from where\n",
    "it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e724670",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def example_function():\n",
    "    print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7030a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_column_info(df):\n",
    "    print(f'No. of columns: {len(df.columns)}')\n",
    "    for col in df.columns:\n",
    "        print(len(df[col].unique()),col,df[col].dtypes)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892411f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_na_info(df):\n",
    "    for col in df.columns:\n",
    "        print(df[col].isnull().sum(),col,df[col].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba0444",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_datetime(week_no):\n",
    "    date = datetime.datetime.strptime(\"2022-\"+str(week_no)+\"-1\",\"%Y-%W-%w\")\n",
    "    #print(date)\n",
    "    return pd.to_datetime(date,format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ada4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sarimax_gridsearch(ts, pdq, pdqs, maxiter=100, freq='D',disp=False):\n",
    "    # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value\n",
    "    ans = []\n",
    "    for comb in pdq:\n",
    "        for combs in pdqs:\n",
    "            #try:\n",
    "            mod = SARIMAX(ts,order=comb,\n",
    "                          seasonal_order=combs,\n",
    "                          enforce_stationarity=False,\n",
    "                          enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit(maxiter=maxiter,disp=False) \n",
    "            ans.append([comb, combs, output.bic])\n",
    "            #print('SARIMAX {} x {}12 : BIC Calculated ={}'.format(comb, combs, output.bic))\n",
    "            #except:\n",
    "            #    continue\n",
    "    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic'])\n",
    "    ans_df = ans_df.sort_values(by=['bic'],ascending=True)[0:5]\n",
    "    \n",
    "    return ans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c44e1b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_stationary(data,Print=0):\n",
    "    adft = adfuller(data,autolag=\"AIC\")\n",
    "    output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                            \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "\n",
    "    critical_value = adft[4]['5%']\n",
    "    if Print==1:\n",
    "        print(output_df)\n",
    "    if adft[1] < 0.05 and adft[0] < critical_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bae154",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def diff_inv(series, last_observation):\n",
    "\n",
    "    series_undifferenced = series.copy()\n",
    "\n",
    "    series_undifferenced.iat[0] = series_undifferenced.iat[0] + last_observation\n",
    "\n",
    "    series_undifferenced = series_undifferenced.cumsum()\n",
    "\n",
    "    return series_undifferenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87c604",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def loss(pred,actual):\n",
    "    pred = np.round(pred)\n",
    "    errors = abs(actual-pred)\n",
    "    print(f'Mean Absolute Error: {round(np.mean(errors), 2)}')\n",
    "    print(f'Mean squared Error: {round(np.mean(errors**2), 2)}')\n",
    "    mape = 100 * (errors/actual)\n",
    "    # Calcualte and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print(f'Accuracy: {round(accuracy, 2)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd45cda",
   "metadata": {},
   "source": [
    "BIGQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab699851",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGQUERY_PROJECT = 'ironhacks-data'\n",
    "bigquery_client = bigquery.Client(project=BIGQUERY_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8112f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#query = \"\"\"\n",
    "#SELECT \n",
    "#week_number,\n",
    "#cases \n",
    "#FROM `ironhacks-data.ironhacks_training.covid19_cases`\n",
    "#Where week_number between 1 and 3\n",
    "#order by week_number\n",
    "#\"\"\"\n",
    "print(\"Datasets available:\")\n",
    "for dataset in list(bigquery_client.list_datasets()):\n",
    "    print(dataset.dataset_id)\n",
    "    if dataset.dataset_id == \"ironhacks_competition\":\n",
    "        mydataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTables available:\")\n",
    "for table in bigquery_client.list_tables(\"ironhacks_competition\"):\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.prediction_list`\n",
    "\"\"\"\n",
    "query2 = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.unemployment_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"\"\"\n",
    "SELECT *\n",
    "FROM `ironhacks-data.ironhacks_competition.wage_data`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13429a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job1 = bigquery_client.query(query1)\n",
    "query_job2 = bigquery_client.query(query2)\n",
    "query_job3 = bigquery_client.query(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = query_job1.to_dataframe()\n",
    "unemployment_data = query_job2.to_dataframe()\n",
    "wage_data = query_job3.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dumping the df to csv\n",
    "week = \"week2\"\n",
    "prediction_data.to_csv(\"prediction_data_\"+week+\".csv\",index=False)\n",
    "unemployment_data.to_csv(\"unemployment_data_\"+week+\".csv\",index=False)\n",
    "wage_data.to_csv(\"wage_data_\"+week+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(unemployment_data,wage_data[[\"uu_id\",\"average_wage\"]],on=\"uu_id\",how=\"left\")\n",
    "merged_data = merged_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fe1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"\"\"\n",
    "SELECT table_id,\n",
    "DATE(TIMESTAMP_MILLIS(creation_time)) AS creation_date,\n",
    "DATE(TIMESTAMP_MILLIS(last_modified_time)) AS last_modified_date,\n",
    "row_count,\n",
    "size_bytes,\n",
    "CASE\n",
    "    WHEN type = 1 THEN 'table'\n",
    "    WHEN type = 2 THEN 'view'\n",
    "    WHEN type = 3 THEN 'external'\n",
    "    ELSE '?'\n",
    "END AS type,\n",
    "TIMESTAMP_MILLIS(creation_time) AS creation_time,\n",
    "TIMESTAMP_MILLIS(last_modified_time) AS last_modified_time,\n",
    "dataset_id,\n",
    "project_id\n",
    "FROM `ironhacks-data.ironhacks_competition.__TABLES__`\"\"\"\n",
    "query_job4 = bigquery_client.query(query4)\n",
    "timestamp_data = query_job4.to_dataframe()\n",
    "for cnt,row in timestamp_data.iterrows():\n",
    "    print(\"\\n\")\n",
    "    print(row[\"table_id\"])\n",
    "    print(row[\"creation_time\"])\n",
    "    print(row[\"last_modified_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Printing (unique_vals, col name, datatype)\n",
    "print(f'\\nunemployment_data : No. of columns = {len(unemployment_data.columns)},Max week number: {max(unemployment_data[\"week_number\"])},Min week number: {min(unemployment_data[\"week_number\"])}')\n",
    "print_column_info(unemployment_data)\n",
    "print(f'\\nwage_data : No. of columns = {len(wage_data.columns)}')\n",
    "print_column_info(wage_data)\n",
    "print(f'\\nprediction_data : No. of columns = {len(prediction_data.columns)},Max week number: {max(prediction_data[\"week_number\"])},Min week number: {min(prediction_data[\"week_number\"])}')\n",
    "print_column_info(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for uuid in unemployment_data[\"uu_id\"].unique():\n",
    "    if len(wage_data.loc[wage_data[\"uu_id\"]==uuid])==0:\n",
    "        count+=1\n",
    "        #print(f'UUID {uuid} found in unemployment_data but not in wage_data')\n",
    "print(f'{count} UUIDs found in unemployment_data but not in wage_data')\n",
    "count = 0\n",
    "for uuid in wage_data[\"uu_id\"].unique():\n",
    "    if len(unemployment_data.loc[unemployment_data[\"uu_id\"]==uuid])==0:\n",
    "        count+=1\n",
    "        #print(f'UUID {uuid} found in unemployment_data but not in wage_data')\n",
    "print(f'{count} UUIDs found in wage_data but not in unemployment_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba671ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for uuid in unemployment_data[\"uu_id\"].unique():\n",
    "    if len(merged_data.loc[merged_data[\"uu_id\"]==uuid])==0:\n",
    "        count+=1\n",
    "        #print(f'UUID {uuid} found in unemployment_data but not in wage_data')\n",
    "print(f'{count} UUIDs found in unemployment_data but not in merged_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for uuid in merged_data[\"uu_id\"].unique():\n",
    "    if len(unemployment_data.loc[unemployment_data[\"uu_id\"]==uuid])==0:\n",
    "        count+=1\n",
    "        #print(f'UUID {uuid} found in unemployment_data but not in wage_data')\n",
    "print(f'{count} UUIDs found in merged_data but not in unemployment_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd74ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Merging average wage with unemployment data\n",
    "print(f'unemployment_data: Unique uu_id = {len(unemployment_data[\"uu_id\"].unique())}, Row count = {len(unemployment_data.index)}, Col count = {len(unemployment_data.columns)}')\n",
    "print(f'wage_data: Unique uu_id = {len(wage_data[\"uu_id\"].unique())}, Row count = {len(wage_data.index)}, Col count = {len(wage_data.columns)}')\n",
    "#print(f'Unique uu_id in wage_data = {len(wage_data[\"uu_id\"].unique())}')\n",
    "print(f'\\nunemployment_data: NaN info, Total rows={len(unemployment_data.index)}')\n",
    "print_na_info(unemployment_data)\n",
    "print(f'merged_data: Unique uu_id = {len(merged_data[\"uu_id\"].unique())}, Row count = {len(merged_data.index)}, Col count = {len(merged_data.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nmerged_data : No. of columns = {len(merged_data.columns)},Max week number: {max(merged_data[\"week_number\"])},Min week number: {min(merged_data[\"week_number\"])}')\n",
    "print_column_info(merged_data)\n",
    "print(f'\\nmerged_data: NaN info, Total rows={len(merged_data.index)}')\n",
    "print_na_info(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(merged_data[\"week_number\"])\n",
    "merged_data[\"datetime\"] = [get_datetime(val) for val in merged_data[\"week_number\"]]\n",
    "merged_data = merged_data.set_index(['datetime'])\n",
    "merged_data = merged_data.sort_index()\n",
    "merged_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75975fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import magics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime,itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c53fbb",
   "metadata": {},
   "source": [
    "- DEFINE YOUR CLASSES AND FUNCTIONS \n",
    "-----------------------------------\n",
    "This is not required, but is helpful in keeping your notebook organized. \n",
    "You can use the following cell or several cells to define your functions\n",
    "and classes to keep them separate from your analysis or results code.\n",
    "In general it useful to define your methods in a separate cell from where\n",
    "it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60203701",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def example_function():\n",
    "    print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da1f2b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_column_info(df):\n",
    "    print(f'No. of columns: {len(df.columns)}')\n",
    "    for col in df.columns:\n",
    "        print(len(df[col].unique()),col,df[col].dtypes)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8580bad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def print_na_info(df):\n",
    "    for col in df.columns:\n",
    "        print(df[col].isnull().sum(),col,df[col].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a04d72",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_datetime(week_no):\n",
    "    date = datetime.datetime.strptime(\"2022-\"+str(week_no)+\"-1\",\"%Y-%W-%w\")\n",
    "    #print(date)\n",
    "    return pd.to_datetime(date,format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99309014",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sarimax_gridsearch(ts, pdq, pdqs, maxiter=100, freq='D',disp=False):\n",
    "    # Run a grid search with pdq and seasonal pdq parameters and get the best BIC value\n",
    "    ans = []\n",
    "    for comb in pdq:\n",
    "        for combs in pdqs:\n",
    "            #try:\n",
    "            mod = SARIMAX(ts,order=comb,\n",
    "                          seasonal_order=combs,\n",
    "                          enforce_stationarity=False,\n",
    "                          enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit(maxiter=maxiter,disp=False) \n",
    "            ans.append([comb, combs, output.bic])\n",
    "            #print('SARIMAX {} x {}12 : BIC Calculated ={}'.format(comb, combs, output.bic))\n",
    "            #except:\n",
    "            #    continue\n",
    "    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'bic'])\n",
    "    ans_df = ans_df.sort_values(by=['bic'],ascending=True)[0:5]\n",
    "    \n",
    "    return ans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63810040",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_stationary(data,Print=0):\n",
    "    adft = adfuller(data,autolag=\"AIC\")\n",
    "    output_df = pd.DataFrame({\"Values\":[adft[0],adft[1],adft[2],adft[3], adft[4]['1%'], adft[4]['5%'], adft[4]['10%']]  , \"Metric\":[\"Test Statistics\",\"p-value\",\"No. of lags used\",\"Number of observations used\", \n",
    "                                                            \"critical value (1%)\", \"critical value (5%)\", \"critical value (10%)\"]})\n",
    "\n",
    "    critical_value = adft[4]['5%']\n",
    "    if Print==1:\n",
    "        print(output_df)\n",
    "    if adft[1] < 0.05 and adft[0] < critical_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57462c26",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def diff_inv(series, last_observation):\n",
    "\n",
    "    series_undifferenced = series.copy()\n",
    "\n",
    "    series_undifferenced.iat[0] = series_undifferenced.iat[0] + last_observation\n",
    "\n",
    "    series_undifferenced = series_undifferenced.cumsum()\n",
    "\n",
    "    return series_undifferenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a565afd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def loss(pred,actual):\n",
    "    pred = np.round(pred)\n",
    "    errors = abs(actual-pred)\n",
    "    print(f'Mean Absolute Error: {round(np.mean(errors), 2)}')\n",
    "    print(f'Mean squared Error: {round(np.mean(errors**2), 2)}')\n",
    "    mape = 100 * (errors/actual)\n",
    "    # Calcualte and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print(f'Accuracy: {round(accuracy, 2)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(merged_data[\"week_number\"])\n",
    "merged_data[\"datetime\"] = [get_datetime(val) for val in merged_data[\"week_number\"]]\n",
    "merged_data = merged_data.set_index(['datetime'])\n",
    "merged_data = merged_data.sort_index()\n",
    "merged_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'merged_data: Unique uu_id = {len(merged_data[\"uu_id\"].unique())}, Row count = {len(merged_data.index)}, Col count = {len(merged_data.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nmerged_data : No. of columns = {len(merged_data.columns)},Max week number: {max(merged_data[\"week_number\"])},Min week number: {min(merged_data[\"week_number\"])}')\n",
    "print_column_info(merged_data)\n",
    "print(f'\\nmerged_data: NaN info, Total rows={len(merged_data.index)}')\n",
    "print_na_info(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_count = []\n",
    "for uuid in merged_data[\"uu_id\"].unique():\n",
    "    entry_count.append([uuid,len(merged_data.loc[merged_data[\"uu_id\"]==uuid])])\n",
    "entry_count.sort(key=lambda x: x[1])\n",
    "for entry in entry_count:\n",
    "    print(f'uuid: {entry[0]}, weeks of data available: {entry[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9d6fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    #print(data_train)\n",
    "    #SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(method=\"ffill\"),\n",
    "    #                       start_p=0, start_q=0,test=\"adf\",\n",
    "    #                       max_p=5, max_q=5,m=4,d=None,\n",
    "    #                       trace=False,\n",
    "    #                       suppress_warnings=True, \n",
    "    #                       maxiter=200,\n",
    "    #                       stepwise=True,seasonal=True,D=None)\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,d=None,max_order=0)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd428e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sarimax_forecast(SARIMAX_model, periods=10,plot=0):\n",
    "    # Forecast\n",
    "    n_periods = periods\n",
    "\n",
    "    forecast_df = data[data.index>=data_train.index[-1]]\n",
    "    fitted, confint = SARIMAX_model.predict(n_periods=n_periods, \n",
    "                                            return_conf_int=True)\n",
    "    #print(confint)\n",
    "    index_of_fc = pd.date_range(data_train.index[-1], periods = n_periods, freq='W-MON')\n",
    "\n",
    "    # make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "    \n",
    "    # Plot\n",
    "    if plot:\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(data[\"total_claims\"], color='#1f76b4')\n",
    "        plt.plot(fitted_series, color='darkgreen')\n",
    "        plt.fill_between(lower_series.index, \n",
    "                        lower_series, \n",
    "                        upper_series, \n",
    "                        color='k', alpha=.15)\n",
    "\n",
    "        plt.title(\"SARIMAX\")\n",
    "        plt.show()\n",
    "    return fitted_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,d=None,max_order=0)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "uuid_list = []\n",
    "week_number_list = []\n",
    "total_claims_list = []\n",
    "#stat=[]\n",
    "for l in output:\n",
    "    uuid_list.append(l[0])\n",
    "    week_number_list.append(l[1])\n",
    "    if l[2]<0:\n",
    "        l[2] = 0\n",
    "    total_claims_list.append(int(np.floor(l[2])))\n",
    "    #stat.append(l[1])\n",
    "pred_df[\"uu_id\"] = uuid_list\n",
    "pred_df[\"week_number\"] = week_number_list\n",
    "pred_df[\"total_claims\"] = total_claims_list\n",
    "#pred_df[\"stationarity\"] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "pred_stat = []\n",
    "actual_stat = []\n",
    "pred_nonstat = []\n",
    "actual_nonstat = []\n",
    "ts_data = merged_data\n",
    "for id,row in pred_df.iterrows():\n",
    "    uuid = row[\"uu_id\"]\n",
    "    week_number = row[\"week_number\"]\n",
    "    if len(ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"])==0:\n",
    "        continue\n",
    "    actual_val = ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"].item()\n",
    "    pred.append(row[\"total_claims\"])\n",
    "    actual.append(actual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(pred=pred,actual=actual)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552807f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,max_order=0)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "uuid_list = []\n",
    "week_number_list = []\n",
    "total_claims_list = []\n",
    "#stat=[]\n",
    "for l in output:\n",
    "    uuid_list.append(l[0])\n",
    "    week_number_list.append(l[1])\n",
    "    if l[2]<0:\n",
    "        l[2] = 0\n",
    "    total_claims_list.append(int(np.floor(l[2])))\n",
    "    #stat.append(l[1])\n",
    "pred_df[\"uu_id\"] = uuid_list\n",
    "pred_df[\"week_number\"] = week_number_list\n",
    "pred_df[\"total_claims\"] = total_claims_list\n",
    "#pred_df[\"stationarity\"] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddfe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "pred_stat = []\n",
    "actual_stat = []\n",
    "pred_nonstat = []\n",
    "actual_nonstat = []\n",
    "ts_data = merged_data\n",
    "for id,row in pred_df.iterrows():\n",
    "    uuid = row[\"uu_id\"]\n",
    "    week_number = row[\"week_number\"]\n",
    "    if len(ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"])==0:\n",
    "        continue\n",
    "    actual_val = ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"].item()\n",
    "    pred.append(row[\"total_claims\"])\n",
    "    actual.append(actual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(pred=pred,actual=actual)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabd93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,d=None)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "uuid_list = []\n",
    "week_number_list = []\n",
    "total_claims_list = []\n",
    "#stat=[]\n",
    "for l in output:\n",
    "    uuid_list.append(l[0])\n",
    "    week_number_list.append(l[1])\n",
    "    if l[2]<0:\n",
    "        l[2] = 0\n",
    "    total_claims_list.append(int(np.floor(l[2])))\n",
    "    #stat.append(l[1])\n",
    "pred_df[\"uu_id\"] = uuid_list\n",
    "pred_df[\"week_number\"] = week_number_list\n",
    "pred_df[\"total_claims\"] = total_claims_list\n",
    "#pred_df[\"stationarity\"] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "pred_stat = []\n",
    "actual_stat = []\n",
    "pred_nonstat = []\n",
    "actual_nonstat = []\n",
    "ts_data = merged_data\n",
    "for id,row in pred_df.iterrows():\n",
    "    uuid = row[\"uu_id\"]\n",
    "    week_number = row[\"week_number\"]\n",
    "    if len(ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"])==0:\n",
    "        continue\n",
    "    actual_val = ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"].item()\n",
    "    pred.append(row[\"total_claims\"])\n",
    "    actual.append(actual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(pred=pred,actual=actual)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f663790",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,d=None,maxiter=200)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "data = merged_data.loc[merged_data[\"uu_id\"]==ids[500]].copy()\n",
    "print(data[\"week_number\"].max())\n",
    "data_train = data[data[\"week_number\"]<32]\n",
    "data_train = data_train.asfreq('W-MON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"])\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0))\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3abbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e799bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=1)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12179fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "plt.plot(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=1)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=3)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c43c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0))\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=20)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99728037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=10)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30607d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=10)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=15)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767be1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d55456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=12)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcafce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=15)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff70ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61dfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(data_train[\"total_claims\"].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f372d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ax = seasonal.plot(label='Seasonality', color='blue')\n",
    "min_ = seasonal.idxmin()\n",
    "max_ = seasonal.idxmax()\n",
    "min_2 = seasonal[max_:].idxmin()\n",
    "max_2 = seasonal[min_2:].idxmax()\n",
    "ax.axvline(min_,label='min 1',c='red')\n",
    "ax.axvline(min_2,label='min 2',c='red', ls=':')\n",
    "ax.axvline(max_,label='max 1',c='green')\n",
    "ax.axvline(max_2,label='max 2',c='green', ls=':')\n",
    "plt.legend(loc='upper right', fontsize='x-small')\n",
    "print(f'The time difference between the two minimums is {min_2-min_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9070dc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sarimax_forecast(SARIMAX_model, periods=10,plot=0):\n",
    "    # Forecast\n",
    "    n_periods = periods\n",
    "\n",
    "    forecast_df = data[data.index>=data_train.index[-1]]\n",
    "    fitted, confint = SARIMAX_model.predict(n_periods=n_periods, \n",
    "                                            return_conf_int=True)\n",
    "    #print(confint)\n",
    "    index_of_fc = pd.date_range(data_train.index[-1], periods = n_periods, freq='W-MON')\n",
    "\n",
    "    # make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "    \n",
    "    # Plot\n",
    "    if plot:\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.plot(data[\"total_claims\"], color='#1f76b4')\n",
    "        plt.plot(fitted_series, color='darkgreen')\n",
    "        plt.fill_between(lower_series.index, \n",
    "                        lower_series, \n",
    "                        upper_series, \n",
    "                        color='k', alpha=.15)\n",
    "\n",
    "        plt.title(\"SARIMAX\")\n",
    "        plt.show()\n",
    "    return fitted_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,m=14,maxiter=200)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),alpha=0.05,m=14)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),start_p=0, start_q=0, max_p=5,\n",
    "                           max_q=5, start_P=0, start_Q=0, max_P=5,\n",
    "                           max_Q=5, m=12, max_order=None,\n",
    "                           trace=True)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2329a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),start_p=0, start_q=0, max_p=5,\n",
    "                           max_q=5, start_P=0, start_Q=0, max_P=5,\n",
    "                           max_Q=5, m=12, max_order=None)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),start_p=0, start_q=0, max_p=5,\n",
    "                           max_q=5, start_P=0, start_Q=0, max_P=5,\n",
    "                           max_Q=5, m=None, max_order=None)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ddfbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),start_p=0, start_q=0, max_p=5,\n",
    "                           max_q=5, start_P=0, start_Q=0, max_P=5,\n",
    "                           max_Q=5,  max_order=None)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b82b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "uuid_list = []\n",
    "week_number_list = []\n",
    "total_claims_list = []\n",
    "#stat=[]\n",
    "for l in output:\n",
    "    uuid_list.append(l[0])\n",
    "    week_number_list.append(l[1])\n",
    "    if l[2]<0:\n",
    "        l[2] = 0\n",
    "    total_claims_list.append(int(np.floor(l[2])))\n",
    "    #stat.append(l[1])\n",
    "pred_df[\"uu_id\"] = uuid_list\n",
    "pred_df[\"week_number\"] = week_number_list\n",
    "pred_df[\"total_claims\"] = total_claims_list\n",
    "#pred_df[\"stationarity\"] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea375814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "pred_stat = []\n",
    "actual_stat = []\n",
    "pred_nonstat = []\n",
    "actual_nonstat = []\n",
    "ts_data = merged_data\n",
    "for id,row in pred_df.iterrows():\n",
    "    uuid = row[\"uu_id\"]\n",
    "    week_number = row[\"week_number\"]\n",
    "    if len(ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"])==0:\n",
    "        continue\n",
    "    actual_val = ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"].item()\n",
    "    pred.append(row[\"total_claims\"])\n",
    "    actual.append(actual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(pred=pred,actual=actual)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9caac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),start_p=0, start_q=0, max_p=5,\n",
    "                           max_q=5, start_P=0, start_Q=0, max_P=5,\n",
    "                           max_Q=5, m=7, max_order=None)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = ['edu_8th_or_less', 'edu_grades_9_11',\n",
    "       'edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown',\n",
    "       'top_category_employer1', 'top_category_employer2',\n",
    "       'top_category_employer3', 'gender_female', 'gender_male', 'gender_na',\n",
    "       'race_amerindian', 'race_asian', 'race_black', 'race_noanswer',\n",
    "       'race_hawaiiannative', 'race_other', 'race_white', 'average_wage']\n",
    "exog = ['edu_8th_or_less', 'edu_grades_9_11','edu_hs_grad_equiv', 'edu_post_hs', 'edu_unknown']\n",
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "output = []\n",
    "for cnt,idd in enumerate(ids):\n",
    "    if cnt > 100: break\n",
    "    data = merged_data.loc[merged_data[\"uu_id\"]==idd].copy()\n",
    "    data = data.asfreq('W-MON')\n",
    "    is_na = []\n",
    "    for idx,row in data.iterrows():\n",
    "        if pd.isna(row[\"total_claims\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data[\"is_na\"] = is_na\n",
    "    #data[\"total_claims\"] = data[\"total_claims\"].fillna(0)\n",
    "    start_week = 0\n",
    "    #print(data[\"week_number\"].max())\n",
    "    boundary_week = data[\"week_number\"].max()-2\n",
    "    data_train = data.loc[(data[\"week_number\"]<boundary_week) & (data[\"week_number\"]>start_week)].drop(\"is_na\",axis=1)\n",
    "    is_na = []\n",
    "    for idx,row in data_train.iterrows():\n",
    "        if pd.isna(row[\"week_number\"]):\n",
    "            #print(f'NA found in week {idx}')\n",
    "            is_na.append(1)\n",
    "        else:\n",
    "            is_na.append(0)\n",
    "    data_train[\"is_na\"] = is_na\n",
    "    data_train = data_train.asfreq('W-MON')\n",
    "    SARIMAX_model = pm.auto_arima(data_train[\"total_claims\"].fillna(0),start_p=0, start_q=0, max_p=5,\n",
    "                           max_q=5, start_P=0, start_Q=0, max_P=5,\n",
    "                           max_Q=5, max_order=None,seasonal=False)\n",
    "    max_week_no = data[\"week_number\"].max()\n",
    "    pred_periods =  max_week_no - data_train[\"week_number\"].max()\n",
    "    #SARIMAX_model_fit = SARIMAX_model.fit(data_train[\"total_claims\"].fillna(0))\n",
    "    #check_st = check_stationary(data_train[\"total_claims\"].fillna(0),1)\n",
    "    prediction = sarimax_forecast(SARIMAX_model, periods=int(pred_periods)+1,plot=0)\n",
    "    out = prediction.loc[prediction.index[-1]]\n",
    "    #print(prediction)\n",
    "    #print(prediction.loc[prediction.index[-1]])\n",
    "    output.append([idd,max_week_no,out])\n",
    "    #print(check_stationary(data_train[\"total_claims\"].fillna(method=\"bfill\"),1))\n",
    "    if len(data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"])>0:\n",
    "        print(idd,max_week_no,out,data.loc[data[\"week_number\"]==max_week_no,\"total_claims\"].item(),\"--\",len(data_train.index))\n",
    "    else:\n",
    "        print(idd,max_week_no,out,\"--\",len(data_train.index))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "uuid_list = []\n",
    "week_number_list = []\n",
    "total_claims_list = []\n",
    "#stat=[]\n",
    "for l in output:\n",
    "    uuid_list.append(l[0])\n",
    "    week_number_list.append(l[1])\n",
    "    if l[2]<0:\n",
    "        l[2] = 0\n",
    "    total_claims_list.append(int(np.floor(l[2])))\n",
    "    #stat.append(l[1])\n",
    "pred_df[\"uu_id\"] = uuid_list\n",
    "pred_df[\"week_number\"] = week_number_list\n",
    "pred_df[\"total_claims\"] = total_claims_list\n",
    "#pred_df[\"stationarity\"] = stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "pred_stat = []\n",
    "actual_stat = []\n",
    "pred_nonstat = []\n",
    "actual_nonstat = []\n",
    "ts_data = merged_data\n",
    "for id,row in pred_df.iterrows():\n",
    "    uuid = row[\"uu_id\"]\n",
    "    week_number = row[\"week_number\"]\n",
    "    if len(ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"])==0:\n",
    "        continue\n",
    "    actual_val = ts_data.loc[(ts_data[\"uu_id\"]==uuid) & (ts_data[\"week_number\"] == week_number),\"total_claims\"].item()\n",
    "    pred.append(row[\"total_claims\"])\n",
    "    actual.append(actual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661df7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(pred=pred,actual=actual)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0).diff(),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].diff(),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].diff().fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].diff().diff().fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e473c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].diff().diff().diff().fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8979cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "data = merged_data.loc[merged_data[\"uu_id\"]==ids[500]].copy()\n",
    "print(data[\"week_number\"].max())\n",
    "data_train = data[data[\"week_number\"]<32]\n",
    "#data_train = data_train.asfreq('W-MON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"],period=14)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcf54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"])\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "data = merged_data.loc[merged_data[\"uu_id\"]==ids[500]].copy()\n",
    "print(data[\"week_number\"].max())\n",
    "data_train = data[data[\"week_number\"]<32]\n",
    "#data_train = data_train.asfreq('W-MON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a989bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"])\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"],period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9849392",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = merged_data[\"uu_id\"].unique()[:]\n",
    "data = merged_data.loc[merged_data[\"uu_id\"]==ids[500]].copy()\n",
    "print(data[\"week_number\"].max())\n",
    "data_train = data[data[\"week_number\"]<32]\n",
    "data_train = data_train.asfreq('W-MON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Taking the decomposition\n",
    "decomposition = seasonal_decompose(data_train[\"total_claims\"].fillna(0),period=7)\n",
    "# Gathering and plotting the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "decomposition.plot()\n",
    "print(len(data_train[\"total_claims\"].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e24afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = [i for i in range(len(data_train[\"total_claims\"].fillna(0)))]\n",
    "y = data_train[\"total_claims\"].fillna(0)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e73f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = model.predict(X)\n",
    "plt.plot(y)\n",
    "plt.plot(trend)\n",
    "plt.plot(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef8b99",
   "metadata": {},
   "source": [
    "detrended = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = [i for i in range(0,len(data_train[\"total_claims\"].fillna(0)))]\n",
    "y = data_train[\"total_claims\"].fillna(0)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = model.predict(X)\n",
    "plt.plot(y)\n",
    "plt.plot(trend)\n",
    "plt.plot(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78947b78",
   "metadata": {},
   "source": [
    "detrended = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = [i for i in range(0,len(data_train[\"total_claims\"].fillna(0)))].reshape(-1,1)\n",
    "y = data_train[\"total_claims\"].fillna(0)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = model.predict(X)\n",
    "plt.plot(y)\n",
    "plt.plot(trend)\n",
    "plt.plot(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715602d",
   "metadata": {},
   "source": [
    "detrended = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102da2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828decf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = [i for i in range(0,len(data_train[\"total_claims\"].fillna(0)))]\n",
    "X = np.reshape(X,(len(X),1))\n",
    "y = data_train[\"total_claims\"].fillna(0)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = model.predict(X)\n",
    "plt.plot(y)\n",
    "plt.plot(trend)\n",
    "plt.plot(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae667e",
   "metadata": {},
   "source": [
    "detrended = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9453ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = [i for i in range(0,len(data_train[\"total_claims\"].fillna(0)))]\n",
    "X = np.reshape(X,(len(X),1))\n",
    "y = data_train[\"total_claims\"].fillna(0)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = model.predict(X)\n",
    "plt.plot(y)\n",
    "plt.plot(trend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c7076",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "detrended = "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
